{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EAS21_beginner_workshop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMQU7g9b-gds"
      },
      "source": [
        "# EAS 2021 workshop: Introduction to Machine Learning\n",
        "\n",
        "By Eliot Ayache & Tanmoy Laskar (University of Bath)\n",
        "\n",
        "Delivered on 29 June 2021 at the 2021 EAS Annual Meeting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPKGkt405Amx"
      },
      "source": [
        "**Special Session 32c (SS32c)**\n",
        "\n",
        "[EAS Abstract](https://eas.kuoni-congress.info/2021/programme/grid/29.06.2021):\n",
        "The objective of this workshop is to provide beginners in machine learning with hands-on experience in implementing and using statistical-learning methods as well as background knowledge necessary to identify the additional tools they would require to solve specific astrophysical problems. In this tutorial, participants will implement a full machine-learning method from scratch and apply it to an astrophysical problem, while being given the opportunity to get a grasp of the potential and limitations of these statistical methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTYOOTTiCZbU"
      },
      "source": [
        "## Resources\n",
        "For more information on convolutional neural networks, see: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
        "\n",
        "For a related tutorial on Keras, see the BathML Keras workshop: https://github.com/owenjonesuob/keras-workshop \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsNIAu3nzf0q"
      },
      "source": [
        "# Initial import statements\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFUnv4TNBcYc"
      },
      "source": [
        "Our task is going to be to perform classification using supervised machine learning using an astronomy pseudo-example. We will classify images of galaxies into ellipticals and spirals using a convolutional neural network (CNN) with tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8bc_Q3CMCKj"
      },
      "source": [
        "## Generate synthetic data\n",
        "To keep everything self-contained, we are going to generate our (synthetic) data directly in this tutorial. Let's start with the functions to make ellipticals and spirals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k668jP9L0Iig"
      },
      "source": [
        "N_image = 28 # Number of pixels per side in image\n",
        "\n",
        "# The getprimes function is just a coordinate transformation\n",
        "# that makes it easy to calculate brightness as a function of pixel position\n",
        "# for the ellipsoid and spiral functions below\n",
        "def getprimes(theta):\n",
        "  x = np.tile(np.linspace(-1,1,N_image).reshape(1,N_image), (N_image,1))\n",
        "  y = np.tile(-np.linspace(-1,1,N_image).reshape(N_image,1), (1,N_image))\n",
        "  sintheta = np.sin(theta)\n",
        "  costheta = np.cos(theta)\n",
        "  if np.ndim(theta) > 0:\n",
        "    x_prime = np.einsum('ij,k->ijk',x,costheta) - \\\n",
        "              np.einsum('ij,k->ijk',y,sintheta)\n",
        "    y_prime = np.einsum('ij,k->ijk',x,sintheta) + \\\n",
        "              np.einsum('ij,k->ijk',y,costheta)\n",
        "  else:\n",
        "    x_prime = x * costheta - y * sintheta\n",
        "    y_prime = x * sintheta + y * costheta\n",
        "  return x_prime, y_prime\n",
        "\n",
        "def ellipsoid(A,e,theta):\n",
        "    B = A*(1.-e)    \n",
        "    x_prime, y_prime = getprimes(theta)  \n",
        "    r_prime = np.sqrt((x_prime/A)**2 + (y_prime/B)**2)\n",
        "    img = np.exp(-r_prime)\n",
        "    return(img)\n",
        "\n",
        "def spiral(A,e,theta,orient=1):    \n",
        "    A *= 2\n",
        "    B = A*(1.-e)\n",
        "    x_prime, y_prime = getprimes(theta)    \n",
        "    r_prime = np.sqrt((x_prime/A)**2 + (y_prime/B)**2)\n",
        "    theta_prime = np.arctan(x_prime/y_prime)\n",
        "    img = np.maximum(np.exp(-r_prime*r_prime * 4)*(4.+np.cos(3* np.pi * r_prime + orient * 2 * theta_prime + np.pi / 2.)) / 5.,\n",
        "                    np.exp(-r_prime*3))\n",
        "    return(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTaLW7iqCn2J"
      },
      "source": [
        "What do these functions do? Let's have a look at a couple of examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u4QRjrj6IHF"
      },
      "source": [
        "el = ellipsoid(A=0.5,e=0.5,theta=0.3); plt.imshow(el, cmap='Greys'); plt.show()\n",
        "sp = spiral(A=0.5,e=0.5,theta=0.3); plt.imshow(sp, cmap='Greys'); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrygvGhWCqsg"
      },
      "source": [
        "Real data will have some noise, so we'll write a short function that adds noise to our images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucHKb6MVwq9F"
      },
      "source": [
        "def addnoise(image, sigma=0.05):\n",
        "  noise_img = np.random.normal(0, sigma, image.shape)\n",
        "  return image+noise_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FALoejrRC95Y"
      },
      "source": [
        "Let's try this noise-adding function on the examples we generated earlier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHlwunOnC_0A"
      },
      "source": [
        "noisy_el = addnoise(el); plt.imshow(noisy_el, cmap='Greys'); plt.show()\n",
        "noisy_sp = addnoise(sp); plt.imshow(noisy_sp, cmap='Greys'); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb7LAC8rDW-p"
      },
      "source": [
        "That's starting to look more realistic! Now let's make a bunch of these for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG6dhqQ2zj8i"
      },
      "source": [
        "N_els = 5000\n",
        "N_sps = 5000\n",
        "np.random.seed(seed=17)\n",
        "els = ellipsoid(A    = np.random.uniform(low=0.3, high=0.9, size=N_els), \n",
        "                e    = np.random.uniform(low=0.1, high=0.5, size=N_els), \n",
        "                theta= np.random.uniform(low=0.1, high=0.9, size=N_els))\n",
        "sps =    spiral(A    = np.random.uniform(low=0.3, high=0.9, size=N_sps), \n",
        "                e    = np.random.uniform(low=0.1, high=0.5, size=N_sps), \n",
        "                theta= np.random.uniform(low=0.1, high=0.9, size=N_sps),\n",
        "                orient=np.random.randint(2, size=N_sps)*2-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf8GWqC3EmVc"
      },
      "source": [
        "Let's plot a random subset of these images to see what we've generated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgv-eT6KBFvl"
      },
      "source": [
        "# Function to plot a random subset \n",
        "def plotrandomsubset(imagelist, n):  \n",
        "  randx = np.random.randint(0,len(imagelist)-1,n*n)\n",
        "  from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "  \n",
        "  fig = plt.figure(figsize=(2*n, 2*n))\n",
        "  grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
        "                  nrows_ncols=(n, n),  # creates nxn grid of axes\n",
        "                  axes_pad=0.0,  # pad between axes in inch.\n",
        "                  )\n",
        "  for ax, im in zip(grid, [imagelist[:,:,i] for i in randx]):\n",
        "      # Iterating over the grid returns the Axes.\n",
        "      ax.imshow(im, cmap='Greys')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-3uFQ1zE5at"
      },
      "source": [
        "We'll plot a 4x4 grid of images from each class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv41DG3GE0Ut"
      },
      "source": [
        "print(\"Subset of elliptical class:\")\n",
        "plotrandomsubset(els, 4)\n",
        "\n",
        "print(\"Subset of spiral class:\")\n",
        "plotrandomsubset(sps, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa9c-AMpE-gb"
      },
      "source": [
        "Right, those don't have any noise added, so let's go ahead and \"observe\" them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnFom2rFx_MG"
      },
      "source": [
        "# Now with noise\n",
        "noisy_els = addnoise(els,sigma=0.1)\n",
        "noisy_sps = addnoise(sps,sigma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71QenJs_FJsU"
      },
      "source": [
        "Again, let's check out a random set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6ChKencFO3p"
      },
      "source": [
        "print(\"Subset of elliptical class:\")\n",
        "plotrandomsubset(noisy_els, 4)\n",
        "\n",
        "print(\"Subset of spiral class:\")\n",
        "plotrandomsubset(noisy_sps, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chhFTIgDGvUg"
      },
      "source": [
        "Now things start getting interesting. Let's set up the labels for our input data. We will start with zeros for ellipticals and 1 for spirals. This will later be coded into numpy arrays using one-hot encoding (depending on the kind of encoding you use for the class labels, you will need to pick a compatible kind of cross-entropy during in the model training process, but we'll get to that later). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SRYjTKo5iNr"
      },
      "source": [
        "# 0 = elliptical, 1 = spiral\n",
        "input_labels = np.array([[0]*N_els+[1]*N_sps])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txPqZ00_HiYG"
      },
      "source": [
        "This is a binary classification problem (0 or 1), and it is possible to proceed with these labels. But for flexibility, we are now going to transform the class labels from binary to categorical (one-hot-encoded) labels. This will be useful if we later on want to expand our model to try and classify data from more than two categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUFPQ2vbHeQ_"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "synthetic_labels_grouped = to_categorical(input_labels)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zW3ANO_JAzz"
      },
      "source": [
        "What did that do? Let's take a look at the shape of the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LigwSgZEIudr"
      },
      "source": [
        "print(synthetic_labels_grouped.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNJKWtorJPvK"
      },
      "source": [
        "The \"grouped\" reminds us that these labels are still grouped by N_els ellipticals followed by N_sps spirals:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3dq9CHDIuem"
      },
      "source": [
        "print(synthetic_labels_grouped[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPCPj_UqI4ay"
      },
      "source": [
        "print(synthetic_labels_grouped[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9G0hMx_JW3J"
      },
      "source": [
        "We'll shuffle it up in a second.\n",
        "\n",
        "But first, we also need to concatenate the actual images of N_els ellipticals and N_sps spirals together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ7M1duFIiTc"
      },
      "source": [
        "synthetic_data_grouped  = np.append(noisy_els, noisy_sps, axis=2)\n",
        "synthetic_data_grouped.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoPX3-IQKDnK"
      },
      "source": [
        "OK, that has stacked all of the images together into a giant N_image x N_image x N_els + N_sps cube. It turns out that most ML algorithms require the sample number to be the *first* axis. So we need to swap the first and third axes of this object. \n",
        "\n",
        "**Warning**\n",
        "\n",
        "If you miss this step, it *will* cause trouble later!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfqN_zl1JkRy"
      },
      "source": [
        "synthetic_data_grouped  = np.moveaxis(np.append(noisy_els, noisy_sps, axis=2), 2, 0)\n",
        "print(synthetic_data_grouped.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V4pW8gFK2rn"
      },
      "source": [
        "Now we are going to shuffle the data and label rows so that they are not in any particular order. There are automatic tools for doing this, but we'll do it manually here because we want to make sure that the labels are shuffled in exactly the same way as the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVEZuLFyCAu1"
      },
      "source": [
        "m = synthetic_data_grouped.shape[0]\n",
        "shuffle_ix = np.arange(m)\n",
        "np.random.shuffle(shuffle_ix)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip11INZ8IYrd"
      },
      "source": [
        "print(shuffle_ix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SveSx7yxIVVz"
      },
      "source": [
        "synthetic_data   = synthetic_data_grouped[shuffle_ix]\n",
        "synthetic_labels = synthetic_labels_grouped[shuffle_ix]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8wxjP9ILnZR"
      },
      "source": [
        "To check what that did, let's plot a random subset from this data. We need to swap the first and third axes for the plotting function we wrote earlier to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQKJOWGxLjoe"
      },
      "source": [
        "print(\"Subset of data\")\n",
        "plotrandomsubset(np.moveaxis(synthetic_data,0,2), 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC07ds0XL6il"
      },
      "source": [
        "## Generate training and testing data\n",
        "There are multiple ways of doing this - we could take the simple first step\n",
        "of splitting the data into an 90% training sample and a 10% testing sample. \n",
        "We will not perform cross-validation in this tutorial (where the test/train split is randomly shuffled and the model is refit several times, in order to reduce bias at the cost of introducing additional model variance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ3mMfji_Hd2"
      },
      "source": [
        "# If you want to do a simple split, run the following commands.\n",
        "\n",
        "# Ntrain = int(np.floor(m*0.9))\n",
        "# train_indices = indices[:Ntrain]\n",
        "# test_indices  = indices[Ntrain:]\n",
        "# X_train = synthetic_data[train_indices, :, :]\n",
        "# X_test  = synthetic_data[test_indices, :, :]\n",
        "# Y_train = synthetic_labels[train_indices, :]\n",
        "# Y_test  = synthetic_labels[test_indices, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsiNK5cNMrKo"
      },
      "source": [
        "Another way of doing this is to load the data into a tf.data.Dataset object (i.e. turn the numpy arrays into tensorflow Tensors) and then use keras preprocessing tools to do this split in one line. This is especially helpful if you want to use tf operations later, see https://www.tensorflow.org/tutorials/load_data/numpy\n",
        "\n",
        "We will not attempt this here, but here's the code to do that if you want to try it later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RstuAdUMhRR"
      },
      "source": [
        "# import tensorflow as tf\n",
        "# test_dataset  = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "\n",
        "# But then we also need to batch the datasets, otherwise training will not work.\n",
        "# We can optionally shuffle the data now, though in our case\n",
        "# we have already done that manually earlier\n",
        "\n",
        "# BATCH_SIZE = 64\n",
        "# SHUFFLE_BUFFER_SIZE = 100\n",
        "# train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "# test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89wmZKULMXnz"
      },
      "source": [
        "We will do a 60:20:20 :: train:test:validation split. The validation set here is just to see how the model performs on unseen data *during* the training process. In the future, this could be used for k-fold cross-validation if you are performing (i) hyperparameter tuning or (ii) model averaging\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOTJWlauEdCF"
      },
      "source": [
        "indices = np.arange(m)\n",
        "\n",
        "train_indices = indices[:int(m*0.6)]\n",
        "val_indices = indices[int(m*0.6):int(m*0.8)]\n",
        "test_indices = indices[int(m*0.8):]\n",
        "\n",
        "# this will automatically slice on the first index\n",
        "X_train = synthetic_data[train_indices]\n",
        "X_val   = synthetic_data[val_indices]\n",
        "X_test  = synthetic_data[test_indices]\n",
        "\n",
        "y_train = synthetic_labels[train_indices]\n",
        "y_val   = synthetic_labels[val_indices]\n",
        "y_test  = synthetic_labels[test_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYCsXkHsRfp5"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw8wuK6_V7C1"
      },
      "source": [
        "## Build and train CNN model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEm_AdAAV_kG"
      },
      "source": [
        "*New vocabulary: Neural network, dense network, neuron, weights, bias, activation*\n",
        "\n",
        "Before we dive into *convolutional* neural nets, it might be a good idea to remind ourselves of the basic principles of neural networks. Conceptually, neural networks consist of nodes (neurons) that spit out a weighted sum of a section of the input. Fundamentally, these are simply a series of matrix operations (hence \"tensor\"flow).\n",
        "\n",
        "Here is the simplest form of a neural network, a *a fully connected* (aka *densely connected*, or simply, *dense*) neural network:\n",
        "\n",
        "<img src=\"https://otexts.com/fpp2/nnet2.png\" alt=\"Fully connected Neural Network\" width=\"400\"/>\n",
        "\n",
        "All inputs are connected to each neuron in the hidden layer. The output* of each blue neuron can be written as \n",
        "\n",
        "$z_j = b_j + \\sum_{i=1}^{N_{\\rm input}} w_{ij} x_i$, \n",
        "\n",
        "where $x_i$ are the inputs, $w_{ij}$ are the weights of the $j^{\\rm th}$ neuron, and $b_j$ are the *biases*. \n",
        "\n",
        "\\*Technically, this is the output before the activation function is applied.\n",
        "\n",
        "Each neuron acts on *all* input elements: \n",
        "\n",
        "<img src=\"https://miro.medium.com/max/500/1*Kg5cA0WNLjDnS3F6gbwFYQ.gif\" alt=\"Fully connected Neural Network\" width=\"400\"/>\n",
        "\n",
        "The final step is to introduce an *activation*. Above, we have assumed that the activation of each neuron is linear, i.e., the neuron's output is a simple linear combination of the inputs. Mathematically, we could write, the neuron's final output as,\n",
        "\n",
        "$\\phi_j(z_j) = z_j$\n",
        "\n",
        "A neural network with only linear activation functions essentially reduces to a linear regression problem... and there are well-known ways of solving those problems (involving matrix inversion) that do not require the full machinery of neural networks. Introducing non-linearity into the neuron's activation is what gives neural networks their rich, complex behaviour. **We do this by using non-linear activation functions.**\n",
        "\n",
        "Some common examples of non-linear activations are \n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1800/1*EmTYifwsrA6YNPI2vYRf7g.gif\" alt=\"Fully connected Neural Network\" width=\"600\"/> \n",
        "\n",
        "Note ELU = Exponential Linear Unit, ReLU = Rectified Linear Unit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO7KhA1j03oz"
      },
      "source": [
        "### Convolutional neural networks (CNNs)\n",
        "\n",
        "*New vocabulary: CNN, filter, stride, channel, slice, padding*\n",
        "\n",
        "A *convolutional* neural network additionally performs a convolution on the inputs using a set of *filters*. Here is an example of a 3x3 filter operating on a 5x5 input grid (in our case, this will be a 28x28 image).\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2000/1*YvlCSNzDEBGEWkZWNffPvw.gif\" alt=\"Fully connected Neural Network\" width=\"400\"/>\n",
        "\n",
        "Q1: How many weights does each 3x3 filter have? \n",
        "(a) 1 \n",
        "(b) 3 \n",
        "(c) 9 \n",
        "(d) 28x28x9=7056\n",
        "\n",
        "Q2: How many biases does each 3x3 filter have? \n",
        "(a) 1 \n",
        "(b) 3 \n",
        "(c) 9 \n",
        "(d) 7056 \n",
        "\n",
        "Q3: How many free parameters are associated with each 3x3 filter?\n",
        "\n",
        "#### Padding\n",
        "\n",
        "If we don't want the input dimensions to change, we can *pad* the convolution. Here's what happens when we pad by 1.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2000/1*gXAcHnbTxmPb8KjSryki-g.gif\" alt=\"Fully connected Neural Network\" width=\"400\"/>\n",
        "\n",
        "We will use the *padding=\"same\"* option in our convolutional layer to calculate the amount of padding for us automatically. \n",
        "\n",
        "Note that in this way of convolving the same input pixel contributes to multiple output pixels. We can make the outputs more independent using a *stride* greater than 1. \n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2000/1*34_365CJB5seboQDUrbI5A.gif\" alt=\"Fully connected Neural Network\" width=\"400\"/>\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2000/1*WpOcRWlofm0Z0EDUTKefzg.gif\" alt=\"Fully connected Neural Network\" width=\"400\"/>\n",
        "\n",
        "(Gifs source: [Aqeel Anwar, What is a CNN?](https://towardsdatascience.com/a-visualization-of-the-basic-elements-of-a-convolutional-neural-network-75fea30cd78d))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyILv8VKxN72"
      },
      "source": [
        "With that in mind, it is now time to build our own neural network, huzzah! We'll start with some imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMWTwMQh8LON"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DDRZIuLWTWv"
      },
      "source": [
        "What is *Keras*? from keras.io : \n",
        "> Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow.\n",
        "\n",
        "*Keras* is a programming interface that abstracts away and packages up the complexity of networks into objects that represent data and results and functions that represent actions, which makes coding up ML applications extremely easy.\n",
        "\n",
        "We are now going to define our network architecture using Keras *layers* objects. Here's where some of the art of machine learning comes in (though, as we'll see later, even this step can be optimized!). \n",
        "\n",
        "Let's build a very simple model with one hidden layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE9Ehh6i1QLd"
      },
      "source": [
        "fs = 3 # filter_size\n",
        "\n",
        "# Define layers (named, so we can nab them later)\n",
        "inputs = layers.Input(shape=(N_image, N_image, 1))\n",
        "conv1A = layers.Conv2D(16, (fs, fs), activation='relu', padding=\"same\",strides=(1,1))(inputs)\n",
        "model = models.Model(inputs, conv1A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZRI8j431r5G"
      },
      "source": [
        "Q4: How many free parameters have we introduced to the model at this point (remember to count the bias!)\n",
        "\n",
        "Let's see what our model looks like so far:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF71Qfbq1mhy"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VN4Y7RF77Hb"
      },
      "source": [
        "Cool, let's write out the rest of the model. This will include a series of convolution layers, pooling layers, and flatten step, and finally, a series of densely connected layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjndCCLXWBxh"
      },
      "source": [
        "fs = 3 # filter_size\n",
        "\n",
        "inputs = layers.Input(shape=(N_image, N_image, 1))\n",
        "conv1A = layers.Conv2D(16, (fs, fs), activation='relu', padding=\"same\",strides=(1,1))(inputs)\n",
        "conv1B = layers.Conv2D(16, (fs, fs), activation='relu', padding=\"same\",strides=(1,1))(conv1A)\n",
        "pool1 = layers.MaxPooling2D((2, 2))(conv1B)\n",
        "conv2A = layers.Conv2D(32, (fs, fs), activation='relu', padding=\"same\")(pool1)\n",
        "conv2B = layers.Conv2D(32, (fs, fs), activation='relu', padding=\"same\")(conv2A)\n",
        "pool2 = layers.MaxPooling2D((2, 2))(conv2B)\n",
        "conv3A = layers.Conv2D(64, (fs, fs), activation='relu', padding=\"same\")(pool2)\n",
        "conv3B = layers.Conv2D(64, (fs, fs), activation='relu', padding=\"same\")(conv3A)\n",
        "flatten1 = layers.Flatten()(conv3B) \n",
        "dense1 = layers.Dense(64,activation='relu')(flatten1)\n",
        "dense2 = layers.Dense(32,activation='relu')(dense1)\n",
        "dense3 = layers.Dense(2,activation='softmax')(dense2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dPyXifB439i"
      },
      "source": [
        "We have used the *softmax* activation to turn our outputs into probabilities that will sum to unity. The softmax function is defined as \n",
        "$\\sigma (\\vec{z})_i = \\frac{e^{z_i}}{\\Sigma_1^{K}{e^{z_i}}}$, for $i$ = 1, ..., $K$.\n",
        "\n",
        "There's more about softmax functions here:\n",
        "https://machinelearningmastery.com/softmax-activation-function-with-python/\n",
        "and https://en.wikipedia.org/wiki/Softmax_function\n",
        "\n",
        "Right, let's connect up our model and see what it looks like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv7qUZU56AHv"
      },
      "source": [
        "model = models.Model(inputs, dense3)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPHUT83z82-B"
      },
      "source": [
        "Q5: Can you make sense of the number of parameters for each of the dense layers? Hint: all outputs from the previous layer are now connected as inputs to each dense layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnt5bn8l80Dh"
      },
      "source": [
        "print(3136*64+64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O7mfRMWR2fv"
      },
      "source": [
        "#### Training\n",
        "\n",
        "Before we can use our model, we must compile it. This is also where we define our loss function (what exactly we are trying to optimize)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pG4uaYKIITC"
      },
      "source": [
        "model.compile(optimizer='SGD',\n",
        "              metrics=['accuracy'],\n",
        "              loss='categorical_crossentropy')\n",
        "            # learning_rate=...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80yF0AL3Pyzw"
      },
      "source": [
        "---\n",
        "Alright, let's take take a quick break from the coding here and explain what's happening. Compiling the model means specifying the various aspects of the training process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmcOdS2o9ePW"
      },
      "source": [
        "#####The training process - Backpropagation\n",
        "\n",
        "Training is done using the **backpropagation of error** algorithm. Intuitively, we measure the difference between the output and a target output, the error, and we update the network parameters (the weights) to match the output to the target. This is done with **gradient descent** in which we follow the direction of steepest decrease of the error as a function of the weights. For this we need to compute the partial derivatives with respect to the weights. Each partial derivative depends on the absolute error and the derivatives in the layers placed after it. \n",
        "\n",
        "For those of us that need a bit more to be convinced, here is the mathematical expression for the partial derivatives:\n",
        "\n",
        "(The derivation is available on https://en.wikipedia.org/wiki/Backpropagation)\n",
        "\n",
        "$ \\frac{\\partial E}{\\partial w_{ij}} = o_i \\delta_j $\n",
        "with\n",
        "$ \\delta_j = \\begin{cases} \\frac{\\partial E }{\\partial o_j} \\frac{\\mathrm{d}\\phi (z_j)}{\\mathrm{d}z_j} & \\text{if $j$ is an output neuron} \\\\\n",
        "( \\sum_{l \\in L} w_{jl} \\delta_l ) \\frac{\\mathrm{d}\\phi (z_j)}{\\mathrm{d}z_j} & \\text{if $j$ is an inner neuron} \\end{cases}, $\n",
        "\n",
        "where $E$ is the error, $w_{ij}$ the weights, $z_j$ the weighted sum computed by each neuron before activation, $\\phi$ the activation function applied on this sum, and $L$ all the neurons directly conneted to the ouput $o_j$ of neuron $j$. The recursive character of the gradient calculation is clearly visible in the sum on the elements of $L$ in which we need $\\delta_l$.\n",
        "\n",
        "\n",
        "And so we can update the weights using a pre-specified learning rate $\\eta$:\n",
        "\n",
        "$ \\Delta w_{ij} = -\\eta \\frac{\\partial E}{\\partial w_{ij}} $.\n",
        "\n",
        "Hence, for each training step, we need to: \n",
        "- first compute the error **(forward pass)**,\n",
        "- propagate the error from the last layer to the first **(backwards pass - or backpropagation)**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4Tc2UrWOYsu"
      },
      "source": [
        "##### Loss function\n",
        "Of course, we are still missing an error (or loss) function for the output layer. \n",
        "\n",
        "There is a whole zoo of loss functions that can be used. Traditionally, the *mean squared error* is used on *regression* problems. Here we are doing **binary classification**. However we can apply the same methods as in the more general **multi-class** problem which relies on one-hot encoding (which is the way we prepared our dataset).\n",
        "\n",
        "We can use the **categorical-cross entropy** which measures the difference between the output discrete probability distribution (obtained with our softmax activation) and our target distribution (here a one-hot encoding).\n",
        "\n",
        "(For more info there is a great introduction to cross-entropy from Jason Brownlee https://machinelearningmastery.com/cross-entropy-for-machine-learning/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF67Eur6SLmj"
      },
      "source": [
        "#####Optimizer\n",
        "\n",
        "Finally, the simple gradient descent we have described can be significantly improved and several optimizers are implemented in Keras, which offer better results depending on the problem: Stochastic gradient Descent (SGD; which we use in this tutorial, as it is the most basic optimizer), ADAM, AdaDelta, RMSProp... \n",
        "\n",
        "<img src=\"https://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif\" alt=\"Comparison of optimizer performance\" width=\"300\"/>\n",
        "(Image credit Sebastian Ruder)\n",
        "\n",
        "(For more info see this excellent article by Sebastian Ruder: https://ruder.io/optimizing-gradient-descent/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpHraDNqTt3i"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Okay, let's fit the model! That's easy to do with a model.fit() command. \n",
        "\n",
        "\n",
        "\n",
        "The first argument to model.fit() is the \"x\" or \"independent variable\", which in our case is a set of images. The second argument is the \"y\" or \"dependent variable\", or \"target classification\", which in our case is an array of one-hot encoded labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGEmokb2MQh8"
      },
      "source": [
        "#model.fit(synthetic_data_train, synthetic_labels_train, epochs=20)\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=100, validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgOjVS20T3sJ"
      },
      "source": [
        "We can also directly fit using the tf.data.Dataset objects we created earlier.\n",
        "If you want to try this, be sure to re-run the model definition and compilation steps first! Otherwise you will be using the pre-trained model from the previous step. Then, comment out the first command in this cell, and uncomment this next one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxRih56zT36N"
      },
      "source": [
        "# model.fit(train_dataset, epochs=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zj2FLOmT_gR"
      },
      "source": [
        "Note that when the data have already been packaged into tf.data.Dataset objects,we no longer need to provide data & labels to model.fit() separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwh7cX3LUcZg"
      },
      "source": [
        "## Validation\n",
        "\n",
        "Great, we can do very well on the training set! But how well does the model work on data it has never seen before? For that, let's generate predictions on the test data set. Remember, the model has not seen these data yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo1CYc0RHu6g"
      },
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQgFiLh7VHaS"
      },
      "source": [
        "This tells us a \"probability\" that the object is in class 0 or class 1. We can now convert each of these to a class label, running from 0 to N, where N is the number of classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvfopa6lVCyb"
      },
      "source": [
        "predictions_labels = tf.argmax(predictions, axis = 1)\n",
        "test_labels        = tf.argmax(y_test, axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrlPe3AXVc89"
      },
      "source": [
        "We can use these class labels to calculate a confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8pTtFptVCzj"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(test_labels,predictions_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B26E47arViZ_"
      },
      "source": [
        "There's also a tensorflow version of this function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDPDrHzCVilj"
      },
      "source": [
        "# tf.math.confusion_matrix(test_labels,predictions_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSG7JXkGVnKL"
      },
      "source": [
        "This tells us how many objects of class 0 are classified as 0 or 1 and how many objects of class 1 are classified as class 0 or 1. You can get fancy and turn this into fractions and then plot it to make the shiny confusion matrix plots people have in their papers. As far as we know, you need to write your own code to make such a plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QllLcmUyBbKX"
      },
      "source": [
        "# Let us see how fast the model has converged\n",
        "# For this we can plot the training and validation accuracy (or the loss)\n",
        "# as a function of epoch number\n",
        "\n",
        "def convergence(history):\n",
        "\n",
        "    history = history.history\n",
        "\n",
        "    loss = history[\"loss\"]\n",
        "    val_loss = history[\"val_loss\"]\n",
        "    nepochs = len(loss)\n",
        "\n",
        "    accuracy = history[\"accuracy\"]\n",
        "    val_accuracy = history[\"val_accuracy\"]\n",
        "\n",
        "    plt.plot(np.arange(nepochs), loss, \"k-\", label=\"training loss\")\n",
        "    plt.plot(np.arange(nepochs), val_loss, \"k--\", label=\"validation loss\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend(frameon=False)\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(nepochs), accuracy, \"k-\", label=\"training accuracy\")\n",
        "    plt.plot(np.arange(nepochs), val_accuracy, \"k--\", label=\"validation accuracy\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend(frameon=False)\n",
        "    plt.show()\n",
        "\n",
        "convergence(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFxxOoRe3vlt"
      },
      "source": [
        "# sklearn also has a shiny classification report function, which will \n",
        "# calculate the precision, recall, and f1-scores for us\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_labels,predictions_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAMkpoTv-7-w"
      },
      "source": [
        "# Test the model on unknown data\n",
        "# Generate data with \"unknown\" labels\n",
        "T = np.array([ellipsoid(0.5,.2,0.5),spiral(0.5,.2,0.5)])\n",
        "print(model.predict(T))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK89MvbIKgxc"
      },
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "The model can further be improved by optimizing various parameters.\n",
        "- Optimisation of network architecture (number/size of layers...)\n",
        "- Optimisation of training parameters (learning rate, number of epochs...)\n",
        "\n",
        "Several modules exist to train Keras models: Hyperas, Talos, Keras Tuner...\n",
        "\\\n",
        "Hyperas does not support Tensorflow 2.0 and Talos does not have Bayesian Optimisation. \n",
        "\n",
        "**Keras Tuner** is the most complete package currently available\n",
        "and can be used on Sequential and Functional models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLFhGWswKfEq"
      },
      "source": [
        "#installing and importing Keras Tuner\n",
        "!pip install -q -U keras-tuner\n",
        "import kerastuner as kt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEMWo6GgNKhq"
      },
      "source": [
        "The model architecture, including the parameters to optimise, are contained in a model builder function that takes as argument a hyperparameter attribute of the HyperModel class.\n",
        "- Parameter tuning: Instead of passing constants to layer keyword arguments, we pass a set of hyperparameters\n",
        "- Model Architecture: can be changed functionally based on a set of hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iZ_HGiaLKMW"
      },
      "source": [
        "# We need to create a model builder function that returns our compiled model:\n",
        "# This can be used to optimise hyperparameters but also model architecture.\n",
        "# Here we optimise for:\n",
        "#   - Number of convolutional blocks\n",
        "#   - Number of units in the first dense layer\n",
        "#   - Learning rate\n",
        "\n",
        "# https://kegui.medium.com/a-few-pitfalls-for-kerastuner-beginner-users-13116759435b\n",
        "\n",
        "# Keras Tuner does not allow you to use 'layers.*'.\n",
        "# So we need to import all the layers we are going to use\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten                                    \n",
        "\n",
        "# Example inspired from https://github.com/keras-team/keras-tuner/blob/master/examples/cifar10.py\n",
        "def model_builder(hp):\n",
        "\n",
        "  filter_size = 3\n",
        "  fs = filter_size\n",
        "\n",
        "  # Beginning or our model\n",
        "  # ----------------------\n",
        "  inputs = Input(shape=(N_image, N_image, 1)) # input layer\n",
        "  x = inputs  # giving a generic name to the tensor going through the model\n",
        "  # We can re-use the same name for multiple layers, making it easier to\n",
        "  # amend the functional model.\n",
        "\n",
        "  # Choosing the number of convolutional blocks\n",
        "  for i in range(hp.Int(\"conv_blocks\", 2, 4, default=3)):\n",
        "    # choosing the number of filters\n",
        "    filters = hp.Int(\"filters_\" + str(i), 8, 32, step=8) \n",
        "    for _ in range(2): # 2 layers per conv block\n",
        "      x = tf.keras.layers.Convolution2D(\n",
        "          filters, kernel_size=(fs, fs), padding=\"same\", activation='relu'\n",
        "          )(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "\n",
        "  # optimising the nuber of units in the first dense layer\n",
        "  hp_units_01 = hp.Int('units_01', min_value=32, max_value=128, step=32)\n",
        "  x = Dense(units=hp_units_01, activation='relu')(x)\n",
        "\n",
        "  # optimising the nuber of units in the second dense layer\n",
        "  # (notice that both hyperparameter objects are name differently)\n",
        "  hp_units_02 = hp.Int('units_02', min_value=16, max_value=64, step=16)\n",
        "  x = Dense(units=hp_units_02,activation='relu')(x)\n",
        "\n",
        "  outputs = Dense(2,activation='softmax')(x)\n",
        "  # ---------------\n",
        "  # End of our model\n",
        "\n",
        "  # Set up the model using the functional API\n",
        "  model = models.Model(inputs, outputs)\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  # copile the model\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz4ZNskedjm2"
      },
      "source": [
        "Now that we have declared our model builder, we need to initialise the tuner which will operate the search in the hyperparameters.\n",
        "Keras Tuner has four tuners available - *RandomSearch*, *Hyperband*, *BayesianOptimization*, and *Sklearn*\n",
        "\n",
        "Since it is currently the EURO2021, We will use **Hyperband** which runs a sports bracket competition between models. However the process remains the same for all other optimisers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW5nnZZJLLEd"
      },
      "source": [
        "tuner = kt.Hyperband(model_builder,   # hypermodel\n",
        "                     objective='val_accuracy',  # score\n",
        "                     max_epochs=20,\n",
        "                     factor=3  # Number of competitors in each bracket\n",
        "                     )\n",
        "\n",
        "# create a callback for early stopping\n",
        "# stops if validation loss does not increase for more than 'patience' epochs\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE1EA3mOLQIH"
      },
      "source": [
        "# Searching for the best parameters\n",
        "tuner.search(X_train, y_train,\n",
        "             epochs=20, validation_split=0.2, callbacks=[stop_early])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX7mfIr9ZT0t"
      },
      "source": [
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "  # returns a list of num_trials hyperparameters ranked from best to worst\n",
        "\n",
        "# printing information about best model:\n",
        "print(f\"\"\"\n",
        "Best Model:\n",
        "conv_blocks: {best_hps.get('conv_blocks')}\n",
        "filters_0 = {best_hps.get('filters_0')}\n",
        "filters_1 = {best_hps.get('filters_1')}\n",
        "filters_2 = {best_hps.get('filters_2')}\n",
        "filters_3 = {best_hps.get('filters_3')}\n",
        "units_01 = {best_hps.get('units_01')}\n",
        "units_02 = {best_hps.get('units_02')}\n",
        "learning_rate = {best_hps.get('learning_rate')}\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dky0uMggaIQ6"
      },
      "source": [
        "Keras Tuner can be used to optimise more parameters such as the **activation function** of a given layer, the **gradient descent** algorithm to use etc...\n",
        "\n",
        "We can now train our best model before evaluating its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ib0t2JPUjoE"
      },
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_split=0.2)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHX99EVsXEGb"
      },
      "source": [
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the model\n",
        "hyperhistory = hypermodel.fit(X_train, y_train, epochs=best_epoch, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDYL5vaPb0aH"
      },
      "source": [
        "Let's evaluate our new model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_18qVhdCXlak"
      },
      "source": [
        "convergence(hyperhistory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJaXmiRqbXwN"
      },
      "source": [
        "hyperpredictions = hypermodel.predict(X_test)\n",
        "hyperpredictions_labels = tf.argmax(hyperpredictions, axis = 1)\n",
        "test_labels        = tf.argmax(y_test, axis = 1)\n",
        "print(confusion_matrix(test_labels,hyperpredictions_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAHjjgYib425"
      },
      "source": [
        "This is doing better than the first model we trained, that needed 30 epochs to converge. We have improved training efficiency and accuracy!"
      ]
    }
  ]
}