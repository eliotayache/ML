@ARTICLE{2019APh...111...12G,
       author = {{Guill{\'e}n}, A. and {Bueno}, A. and {Carceller}, J.~M. and
         {Mart{\'\i}nez-Vel{\'a}zquez}, J.~C. and {Rubio}, G. and
         {Todero Peixoto}, C.~J. and {Sanchez-Lucas}, P.},
        title = "{Deep learning techniques applied to the physics of extensive air showers}",
      journal = {Astroparticle Physics},
     keywords = {Machine learning, Deep neural networks, Ultra-high-energy, Cosmic rays, Pierre auger observatory, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2019",
        month = "Sep",
       volume = {111},
        pages = {12-22},
     abstract = "{Deep neural networks are a powerful technique that have found ample
        applications in several branches of physics. In this work, we
        apply deep neural networks to a specific problem of cosmic ray
        physics: the estimation of the muon content of extensive air
        showers when measured at the ground. As a working case, we
        explore the performance of a deep neural network applied to
        large sets of simulated signals recorded for the water-Cherenkov
        detectors of the Surface Detector of the Pierre Auger
        Observatory. The inner structure of the neural network is
        optimized through the use of genetic algorithms. To obtain a
        prediction of the recorded muon signal in each individual
        detector, we train neural networks with a mixed sample of
        simulated events that contain light, intermediate and heavy
        nuclei. When true and predicted signals are compared at detector
        level, the primary values of the Pearson correlation
        coefficients are above 95\%. The relative errors of the
        predicted muon signals are below 10\% and do not depend on the
        event energy, zenith angle, total signal size, distance range or
        the hadronic model used to generate the events.}",
          doi = {10.1016/j.astropartphys.2019.03.001},
archivePrefix = {arXiv},
       eprint = {1807.09024},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019APh...111...12G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PEPI..29306267Z,
       author = {{Zenonos}, Aristides and {De Siena}, Luca and {Widiyantoro}, Sri and
         {Rawlinson}, Nicholas},
        title = "{P and S wave travel time tomography of the SE Asia-Australia collision zone}",
      journal = {Physics of the Earth and Planetary Interiors},
     keywords = {Tomography, Travel-time, Body-wave, SE Asia, Earthquakes \%p},
         year = "2019",
        month = "Aug",
       volume = {293},
          eid = {106267},
        pages = {106267},
     abstract = "{The southeast (SE) Asia - Australia collision zone is one of the most
        tectonically active and seismogenic regions in the world. Here,
        we present new 3-D P- and S-wave velocity models of the crust
        and upper mantle by applying regional earthquake travel-time
        tomography to global catalogue data. We first re-locate
        earthquakes provided by the standard ISC-Reviewed and ISC-EHB
        catalogues using a non-linear oct-tree scheme. A machine
        learning algorithm that clusters earthquakes depending on their
        spatiotemporal density was then applied to significantly improve
        the consistency of travel-time picks. We used the Fast Marching
        Tomography software package to retrieve 3-D velocity and
        interface structures from starting 1-D velocity and Moho models.
        Synthetic resolution and sensitivity tests demonstrate that the
        final models are robust, with P-wave speed variations ( 130 km
        horizontal resolution) generally recovered more robustly than
        S-wave speed variations ( 220 km horizontal resolution). The
        retrieved crust and mantle anomalies offer a new perspective on
        the broad-scale tectonic setting and underlying mantle
        architecture of SE Asia. While we observe clear evidence of
        subducted slabs as high velocity anomalies penetrating into the
        mantle along the Sunda arc, Banda arc and Halmahera arc, we also
        see evidence for slab gaps or holes in the vicinity of east
        Java. In the Banda arc, we image the slab as a single curved
        subduction zone. Furthermore, a high-velocity region in the
        mantle lithosphere connects northern Australia with Timor and
        West Papua. The S-wave model shows broad-scale features similar
        to those of the P-wave model, with mantle earthquakes generally
        distributed within high-velocity slabs. The high velocity mantle
        connection between northern Australia and the eastern margin of
        the Sunda arc is also present in the S-wave model. While the
        S-wave model has a lower resolution than the P-wave model due to
        the availability of fewer paths, it nonetheless provides new and
        complementary insights into the structure of the upper mantle
        beneath southeast Asia.}",
          doi = {10.1016/j.pepi.2019.05.010},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PEPI..29306267Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.487.2522M,
       author = {{Marton}, G. and {{\'A}brah{\'a}m}, P. and {Szegedi-Elek}, E. and
         {Varga}, J. and {Kun}, M. and {K{\'o}sp{\'a}l}, {\'A}. and
         {Varga-Vereb{\'e}lyi}, E. and {Hodgkin}, S. and {Szabados}, L. and
         {Beck}, R. and {Kiss}, Cs},
        title = "{Identification of Young Stellar Object candidates in the Gaia DR2 x AllWISE catalogue with machine learning methods}",
      journal = {\mnras},
     keywords = {accretion, accretion discs, methods: data analysis, methods: statistical, astronomical data bases: miscellaneous, stars: evolution, stars: pre-main-sequence, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2019",
        month = "Aug",
       volume = {487},
       number = {2},
        pages = {2522-2537},
     abstract = "{The second Gaia Data Release (DR2) contains astrometric and photometric
        data for more than 1.6 billion objects with mean Gaia G
        magnitude \&lt;20.7, including many Young Stellar Objects (YSOs)
        in different evolutionary stages. In order to explore the YSO
        population of the Milky Way, we combined the Gaia DR2 data base
        with Wide-field Infrared Survey Explorer (WISE) and Planck
        measurements and made an all-sky probabilistic catalogue of YSOs
        using machine learning techniques, such as Support Vector
        Machines, Random Forests, or Neural Networks. Our input
        catalogue contains 103 million objects from the DR2xAllWISE
        cross-match table. We classified each object into four main
        classes: YSOs, extragalactic objects, main-sequence stars, and
        evolved stars. At a 90 per cent probability threshold, we
        identified 1 129 295 YSO candidates. To demonstrate the quality
        and potential of our YSO catalogue, here we present two
        applications of it. (1) We explore the 3D structure of the Orion
        A star-forming complex and show that the spatial distribution of
        the YSOs classified by our procedure is in agreement with recent
        results from the literature. (2) We use our catalogue to
        classify published Gaia Science Alerts. As Gaia measures the
        sources at multiple epochs, it can efficiently discover
        transient events, including sudden brightness changes of YSOs
        caused by dynamic processes of their circumstellar disc.
        However, in many cases the physical nature of the published
        alert sources are not known. A cross-check with our new
        catalogue shows that about 30 per cent more of the published
        Gaia alerts can most likely be attributed to YSO activity. The
        catalogue can be also useful to identify YSOs among future Gaia
        alerts.}",
          doi = {10.1093/mnras/stz1301},
archivePrefix = {arXiv},
       eprint = {1905.03063},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.487.2522M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.487.1729L,
       author = {{Lukic}, V. and {Br{\"u}ggen}, M. and {Mingo}, B. and {Croston}, J.~H. and
         {Kasieczka}, G. and {Best}, P.~N.},
        title = "{Morphological classification of radio galaxies: capsule networks versus convolutional neural networks}",
      journal = {\mnras},
     keywords = {instrumentation: miscellaneous, methods: miscellaneous, methods: data analysis, surveys, radio continuum: galaxies, radio continuum: general, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2019",
        month = "Aug",
       volume = {487},
       number = {2},
        pages = {1729-1744},
     abstract = "{Next-generation radio surveys will yield an unprecedented amount of
        data, warranting analysis by use of machine learning techniques.
        Convolutional neural networks are the deep learning technique
        that has proven to be the most successful in classifying image
        data. Capsule networks are a more recently developed technique
        that use capsules comprised of groups of neurons that describe
        properties of an image including the relative spatial locations
        of features. This work explores the performance of different
        capsule network architectures against simpler convolutional
        neural network architectures, in reproducing the classifications
        into the classes of unresolved, FRI, and FRII morphologies. We
        utilize images from a LOFAR survey which is the deepest, wide-
        area radio survey to date, revealing more complex radio-source
        structures compared to previous surveys, presenting further
        challenges for machine learning algorithms. The four- and eight-
        layer convolutional networks attain an average precision of 93.3
        per cent and 94.3 per cent, respectively, compared to 89.7 per
        cent obtained with the capsule network, when training on
        original and augmented images. Implementing transfer learning
        achieves a precision of 94.4 per cent, which is within the
        confidence interval of the eight-layer convolutional network.
        The convolutional networks always outperform any variation of
        the capsule network, as they prove to be more robust to the
        presence of noise in images. The use of pooling appears to allow
        more freedom for the intra-class variability of radio galaxy
        morphologies, as well as reducing the impact of noise.}",
          doi = {10.1093/mnras/stz1289},
archivePrefix = {arXiv},
       eprint = {1905.03274},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.487.1729L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019E&PSL.519...40B,
       author = {{Bradbury}, Harold J. and {Turchyn}, Alexandra V.},
        title = "{Reevaluating the carbon sink due to sedimentary carbonate formation in modern marine sediments}",
      journal = {Earth and Planetary Science Letters},
     keywords = {authigenic carbonate, machine learning},
         year = "2019",
        month = "Aug",
       volume = {519},
        pages = {40-49},
     abstract = "{Previous attempts to quantify the amount of sedimentary carbonate
        precipitation in modern marine sediments have been derived from
        the flux of calcium into the sediments due to diffusion under
        assumed steady state, the application of Fick's first law, and
        then extrapolation of these site-specific rates to the global
        ocean sediment column. This approach is limited, however, as
        much of the ocean floor has not been sampled. We take a machine
        learning approach to update and refine the estimate of the
        amount of sedimentary carbonate precipitation, as well as define
        whether sedimentary carbonate precipitation is driven by
        organoclastic microbial sulfate reduction or anaerobic methane
        oxidation. We identify areas where there is sedimentary
        carbonate formation using machine learning, based upon oceanic
        physical and chemical properties including bathymetry,
        temperature, water depth, distance from shore, and tracers of
        primary production, and data from the global ODP/IODP database.
        Our results suggest that the total amount of sedimentary
        carbonate formation is much lower than previous estimates, at
        1.35 {\ensuremath{\pm}} 0.5 {\texttimes}{}10$^{11}$ mol C/yr. We
        suggest that this rate is a lower estimate and discuss why
        machine-learning approaches may always produce lower-bound
        estimates of global processes. Our calculations suggest that the
        formation of sedimentary carbonate today is mainly driven by
        anaerobic methane oxidation (77\%), with the remainder
        attributed to organoclastic sulfate reduction. We use our
        machine-learning results to speculate the impact that
        sedimentary carbonate precipitation may have had on the carbon
        isotope composition of the surface dissolved inorganic carbon
        reservoir over Earth history.}",
          doi = {10.1016/j.epsl.2019.04.044},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019E&PSL.519...40B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PASP..131g8002Y,
       author = {{Ye}, Quanzhi and {Masci}, Frank J. and {Lin}, Hsing Wen and
         {Bolin}, Bryce and {Chang}, Chan-Kao and {Duev}, Dmitry A. and
         {Helou}, George and {Ip}, Wing-Huen and {Kaplan}, David L. and
         {Kramer}, Emily and {Mahabal}, Ashish and {Ngeow}, Chow-Choong and
         {Nielsen}, Avery J. and {Prince}, Thomas A. and {Tan}, Hanjie and
         {Yeh}, Ting-Shuo and {Bellm}, Eric C. and {Dekany}, Richard and
         {Giomi}, Matteo and {Graham}, Matthew J. and {Kulkarni}, Shrinivas R. and
         {Kupfer}, Thomas and {Laher}, Russ R. and {Rusholme}, Ben and
         {Shupe}, David L. and {Ward}, Charlotte},
        title = "{Toward Efficient Detection of Small Near-Earth Asteroids Using the Zwicky Transient Facility (ZTF)}",
      journal = {\pasp},
     keywords = {Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2019",
        month = "Jul",
       volume = {131},
       number = {1001},
        pages = {078002},
     abstract = "{We describe ZStreak, a semi-real-time pipeline specialized in detecting
        small, fast-moving, near-Earth asteroids (NEAs), which is
        currently operating on the data from the newly commissioned
        Zwicky Transient Facility (ZTF) survey. Based on a prototype
        originally developed by Waszczak et al. (2017) for the Palomar
        Transient Factory (PTF), the predecessor of ZTF, ZStreak
        features an improved machine-learning model that can cope with
        the 10{\texttimes} data rate increment between PTF and ZTF.
        Since its first discovery on 2018 February 5 (2018 CL),
        ZTF/ZStreak has discovered 45 confirmed new NEAs over a total of
        232 observable nights until 2018 December 31. Most of the
        discoveries are small NEAs, with diameters less than
        {\ensuremath{\sim}}100 m. By analyzing the discovery
        circumstances, we find that objects having the first to last
        detection time interval under 2 hr are at risk of being lost. We
        will further improve real-time follow-up capabilities, and work
        on suppressing false positives using deep learning.}",
          doi = {10.1088/1538-3873/ab1b18},
archivePrefix = {arXiv},
       eprint = {1904.09645},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PASP..131g8002Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.487..801Z,
       author = {{Zhao}, Yinan and {Ge}, Jian and {Yuan}, Xiaoyong and {Zhao}, Tiffany and
         {Wang}, Cindy and {Li}, Xiaolin},
        title = "{Identifying Mg II narrow absorption lines with deep learning}",
      journal = {\mnras},
     keywords = {methods: data analysis, quasars: absorption lines, techniques: spectroscopic, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2019",
        month = "Jul",
       volume = {487},
       number = {1},
        pages = {801-811},
     abstract = "{Metal absorption line systems in distant quasar spectra are probes of
        the history of the gas content in the universe. The Mg II
        {\ensuremath{\lambda}}{\ensuremath{\lambda}} 2796, 2803 doublet
        is one of the most important absorption lines since it is a
        proxy of the star formation rate and a tracer of the cold gas
        associated with high-redshift galaxies. Machine learning
        algorithms have been used to detect absorption lines systems in
        large sky surveys, such as principal component analysis,
        Gaussian process, and decision trees. A very powerful algorithm
        in the field of machine learning called deep neural networks, or
        `deep learning', is a new structure of neural network that
        automatically extracts semantic features from raw data and
        represents them at a high level. In this paper, we apply a deep
        convolutional neural network for absorption line detection. We
        use the previously published DR7 Mg II catalogue as the training
        and validation sample and the DR12 Mg II catalogue as the test
        set. Our deep learning algorithm is capable of detecting Mg II
        absorption lines with an accuracy of ̃94 per cent. It takes only
        ̃9 s to analyse ̃50 000 quasar spectra with our deep neural
        network, which is ten thousand times faster than traditional
        methods, while preserving high accuracy with little human
        interference. Our study shows that Mg II absorption line
        detection accuracy of a deep neutral network model strongly
        depends on the filter size in the filter layer of the neural
        network, and the best results are obtained when the filter size
        closely matches the absorption feature size.}",
          doi = {10.1093/mnras/stz1197},
archivePrefix = {arXiv},
       eprint = {1904.12192},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.487..801Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.487..104M,
       author = {{Merten}, Julian and {Giocoli}, Carlo and {Baldi}, Marco and
         {Meneghetti}, Massimo and {Peel}, Austin and {Lalande}, Florian and
         {Starck}, Jean-Luc and {Pettorino}, Valeria},
        title = "{On the dissection of degenerate cosmologies with machine learning}",
      journal = {\mnras},
     keywords = {gravitation, neutrinos, methods: numerical, large-scale structure of Universe, Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Computer Vision and Pattern Recognition, General Relativity and Quantum Cosmology},
         year = "2019",
        month = "Jul",
       volume = {487},
       number = {1},
        pages = {104-122},
     abstract = "{Based on the DUSTGRAIN-pathfinder suite of simulations, we investigate
        observational degeneracies between nine models of modified
        gravity and massive neutrinos. Three types of machine learning
        techniques are tested for their ability to discriminate lensing
        convergence maps by extracting dimensional reduced
        representations of the data. Classical map descriptors such as
        the power spectrum, peak counts, and Minkowski functionals are
        combined into a joint feature vector and compared to the
        descriptors and statistics that are common to the field of
        digital image processing. To learn new features directly from
        the data, we use a convolutional neural network (CNN). For the
        mapping between feature vectors and the predictions of their
        underlying model, we implement two different classifiers; one
        based on a nearest-neighbour search and one that is based on a
        fully connected neural network. We find that the neural network
        provides a much more robust classification than the nearest-
        neighbour approach and that the CNN provides the most
        discriminating representation of the data. It achieves the
        cleanest separation between the different models and the highest
        classification success rate of 59 per cent for a single source
        redshift. Once we perform a tomographic CNN analysis, the total
        classification accuracy increases significantly to 76 per cent
        with no observational degeneracies remaining. Visualizing the
        filter responses of the CNN at different network depths provides
        us with the unique opportunity to learn from very complex models
        and to understand better why they perform so well.}",
          doi = {10.1093/mnras/stz972},
archivePrefix = {arXiv},
       eprint = {1810.11027},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.487..104M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.486.5646Y,
       author = {{Yatawatta}, Sarod},
        title = "{Statistical performance of radio interferometric calibration}",
      journal = {\mnras},
     keywords = {Instrumentation: interferometers, methods: statistical, techniques: interferometric, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning},
         year = "2019",
        month = "Jul",
       volume = {486},
       number = {4},
        pages = {5646-5655},
     abstract = "{Calibration is an essential step in radio interferometric data
        processing that corrects the data for systematic errors and, in
        addition, subtracts bright foreground interference to reveal
        weak signals hidden in the residual. These weak and unknown
        signals are much sought after to reach many science goals but
        the effect of calibration on such signals is an ever present
        concern. The main reason for this is the incompleteness of the
        model used in calibration. Distributed calibration based on
        consensus optimization has been shown to mitigate the effect due
        to model incompleteness by calibrating data covering a wide
        bandwidth in a computationally efficient manner. In this paper,
        we study the statistical performance of direction-dependent
        distributed calibration, i.e. the distortion caused by
        calibration on the residual statistics. In order to study this,
        we consider the mapping between the input uncalibrated data and
        the output residual data. We derive an analytical relationship
        for the influence of the input on the residual and use this to
        find the relationship between the input and output probability
        density functions. The eigenspectrum of the Jacobian of this
        mapping is a direct indicator of the statistical performance of
        calibration. The analysis developed in this paper can also be
        applied to other data processing steps in radio interferometry
        such as imaging and foreground subtraction as well as to many
        other machine learning problems.}",
          doi = {10.1093/mnras/stz1222},
archivePrefix = {arXiv},
       eprint = {1902.10448},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.486.5646Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.486.5405G,
       author = {{Gao}, Xin-hua},
        title = "{Membership and fundamental parameters of the Praesepe cluster based on Gaia-DR2}",
      journal = {\mnras},
     keywords = {methods: statistical, astrometry, parallaxes, proper motions, stars: kinematics and dynamics, open clusters and associations:individual: Praesepe},
         year = "2019",
        month = "Jul",
       volume = {486},
       number = {4},
        pages = {5405-5413},
     abstract = "{In this paper, we investigate membership and fundamental astrophysical
        parameters of the nearby, intermediate-age Praesepe star cluster
        (M44) based on the Gaia data release 2 (Gaia-DR2). Based on 54
        425 stars within a sky area of 5.5{\textdegree} radius, we
        identify 1111 likely cluster members ({\ensuremath{\geq}}0.6) in
        an 11D parameter space based on a combined machine-learning
        method. Of these cluster members, 1052 stars are identified as
        high-probability (\&gt;0.8) members. We identify 13 white dwarf
        (WD) candidates and one M dwarf candidate in the cluster, one of
        the 13 WD candidates has never been reported before. The updated
        member list with high-precision astrometric and photometric data
        allows us to more precisely determine the fundamental
        astrophysical parameters of the cluster. We find significant
        correlations between the parallaxes and proper motions of the
        cluster members, which may be caused by the depth effect along
        the line-of-sight direction. We also find clear evidence for the
        existence of mass segregation in the cluster. Using a Monte
        Carlo simulation technique, the most likely distance, proper
        motion, and radial velocity of the cluster are determined to be
        \&lt;D\&gt; = 187.0 {\ensuremath{\pm}} 0.2 pc, (\&lt;{\ensuremat
        h{\mu}}$_{{\ensuremath{\alpha}}}$cos{\ensuremath{\delta}}\&gt;,
        \&lt;{\ensuremath{\mu}}$_{{\ensuremath{\delta}}}$\&gt;) =
        (-36.136 {\ensuremath{\pm}} 0.020, -12.950 {\ensuremath{\pm}}
        0.014) mas yr$^{-1}$, and \&lt;RV\&gt; = + 35.0
        {\ensuremath{\pm}} 0.1 km s$^{-1}$, respectively. The core and
        limiting radius of the cluster are determined to be 29.9
        {\ensuremath{\pm}} 1.5 (1.6 pc) and 208.1 {\ensuremath{\pm}}
        12.6 arcmin (11.3 pc), respectively. We find that the low-mass
        MD candidate may be escaping from the cluster according to its
        present spatial positions and velocity. In addition, the total
        mass of the cluster within the limiting radius is determined to
        be 568 {\ensuremath{\pm}} 105 M$_{☉}$ using the Monte Carlo
        simulation technique.}",
          doi = {10.1093/mnras/stz1213},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.486.5405G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.486.3702S,
       author = {{Snyder}, Gregory F. and {Rodriguez-Gomez}, Vicente and
         {Lotz}, Jennifer M. and {Torrey}, Paul and {Quirk}, Amanda C.~N. and
         {Hernquist}, Lars and {Vogelsberger}, Mark and {Freeman}, Peter E.},
        title = "{Automated distant galaxy merger classifications from Space Telescope images using the Illustris simulation}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: numerical, galaxies: formation, galaxies: statistics, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Jul",
       volume = {486},
       number = {3},
        pages = {3702-3720},
     abstract = "{We present image-based evolution of galaxy mergers from the Illustris
        cosmological simulation at 12 time-steps over 0.5 \&lt; z \&lt;
        5. To do so, we created approximately one million synthetic deep
        Hubble Space Telescope and James Webb Space Telescope images and
        measured common morphological indicators. Using the merger tree,
        we assess methods to observationally select mergers with stellar
        mass ratios as low as 10:1 completing within
        {\ensuremath{\pm}}250 Myr of the mock observation. We confirm
        that common one- or two-dimensional statistics select mergers so
        defined with low purity and completeness, leading to high
        statistical errors. As an alternative, we train redshift-
        dependent random forests (RFs) based on 5-10 inputs. Cross-
        validation shows the RFs yield superior, yet still imperfect,
        measurements of the late-stage merger fraction, and they select
        more mergers in bulge-dominated galaxies. When applied to
        CANDELS morphology catalogues, the RFs estimate a merger rate
        increasing to at least z = 3, albeit two times higher than
        expected by theory. This suggests possible mismatches in the
        feedback-determined morphologies, but affirms the basic
        understanding of galaxy merger evolution. The RFs achieve
        completeness of roughly 70\{\{ per cent\}\} at 0.5 \&lt; z \&lt;
        3, and purity increasing from 10\{\{ per cent\}\} at z = 0.5-60
        per cent at z = 3. At earlier times, the training sets are
        insufficient, motivating larger simulations and smaller time
        sampling. By blending large surveys and large simulations, such
        machine learning techniques offer a promising opportunity to
        teach us the strengths and weaknesses of inferences about galaxy
        evolution.}",
          doi = {10.1093/mnras/stz1059},
archivePrefix = {arXiv},
       eprint = {1809.02136},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.486.3702S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.486.3415L,
       author = {{Liodakis}, I. and {Blinov}, D.},
        title = "{Probing the unidentified Fermi blazar-like population using optical polarization and machine learning}",
      journal = {\mnras},
     keywords = {methods: statistical, galaxies: active, galaxies: jets, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2019",
        month = "Jul",
       volume = {486},
       number = {3},
        pages = {3415-3422},
     abstract = "{The Fermi {\ensuremath{\gamma}}-ray space telescope has revolutionized
        our view of the {\ensuremath{\gamma}}-ray sky and the high-
        energy processes in the Universe. While the number of known
        {\ensuremath{\gamma}}-ray emitters has increased by orders of
        magnitude since the launch of Fermi, there is an ever increasing
        number of, now more than a thousand, detected point sources
        whose low-energy counterpart is to this day unknown. To address
        this problem, we combined optical polarization measurements from
        the RoboPol survey as well as other discriminants of blazars
        from publicly available all-sky surveys in machine learning (ML,
        random forest and logistic regression) frameworks that could be
        used to identify blazars in the Fermi unidentified fields with
        an accuracy of \&gt;95 per cent. Out of the potential
        observational biases considered, blazar variability seems to
        have the most significant effect reducing the predictive power
        of the frameworks to ̃ 80-85 per cent. We apply our ML framework
        to six unidentified Fermi fields observed using the RoboPol
        polarimeter. We identified the same candidate source proposed by
        Mandarakas et al. for 3FGL J0221.2 + 2518.}",
          doi = {10.1093/mnras/stz1008},
archivePrefix = {arXiv},
       eprint = {1904.04278},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.486.3415L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019A&C....2800284S,
       author = {{S{\'a}nchez}, B. and {Dom{\'\i}nguez R.}, M.~J. and {Lares}, M. and
         {Beroiz}, M. and {Cabral}, J.~B. and {Gurovich}, S. and
         {Qui{\~n}ones}, C. and {Artola}, R. and {Colazo}, C. and
         {Schneiter}, M. and {Girardini}, C. and {Tornatore}, M. and
         {Nilo Castell{\'o}n}, J.~L. and {Garc{\'\i}a Lambas}, D. and
         {D{\'\i}az}, M.~C.},
        title = "{Machine learning on difference image analysis: A comparison of methods for transient detection}",
      journal = {Astronomy and Computing},
     keywords = {Methods, Data analysis, Techniques, Image processing, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2019",
        month = "Jul",
       volume = {28},
          eid = {100284},
        pages = {100284},
     abstract = "{We present a comparison of several Difference Image Analysis (DIA)
        techniques, in combination with Machine Learning (ML)
        algorithms, applied to the identification of optical transients
        associated to gravitational wave events. Each technique is
        assessed based on the scoring metrics of Precision, Recall, and
        their harmonic mean F 1 , measured on the DIA results as
        standalone techniques, and also in the results after the
        application of ML algorithms, on transient source injections
        over simulated and real data. These simulations cover a wide
        range of instrumental configurations, as well as a variety of
        scenarios of observation conditions, by exploring a multi
        dimensional set of relevant parameters, allowing us to extract
        general conclusions related to the identification of transient
        astrophysical events. The newest subtraction techniques, and
        particularly the methodology published in Zackay et al., (2016)
        are implemented on an Open Source Python package, named
        properimage, suitable for many other astronomical image
        analyses. This together, with the ML libraries we describe,
        provides an effective transient detection software pipeline.
        Here we study the effects of the different ML techniques, and
        the relative feature importances for classification of transient
        candidates, and propose an optimal combined strategy. This
        constitutes the basic elements of pipelines that could be
        applied in searches of electromagnetic counterparts to GW
        sources.}",
          doi = {10.1016/j.ascom.2019.05.002},
archivePrefix = {arXiv},
       eprint = {1812.10518},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019A&C....2800284S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019RAA....19...88K,
       author = {{Kong}, Xiao and {Luo}, A. -Li and {Li}, Xiang-Ru},
        title = "{A catalog of DB white dwarfs from the LAMOST DR5 and construction of templates}",
      journal = {Research in Astronomy and Astrophysics},
         year = "2019",
        month = "Jun",
       volume = {19},
       number = {6},
          eid = {088},
        pages = {088},
     abstract = "{In this study, we employ machine learning to build a catalog of DB white
        dwarfs (DBWDs) from the LAMOST Data Release (DR) 5. Using known
        DBs from SDSS DR14, we selected samples of high-quality DB
        spectra from the LAMOST database and applied them to train the
        machine learning process. Following the recognition procedure,
        we chose 351 DB spectra of 287 objects, 53 of which were new
        identifications. We then utilized all the DBWD spectra from both
        SDSS DR14 and LAMOST DR5 to construct DB templates for LAMOST 1D
        pipeline reductions. Finally, by applying DB parameter models
        provided by D. Koester and the distance from Gaia DR2, we
        calculated the effective temperatures, surface gravities and
        distributions of the 3D locations and velocities of all DBWDs.}",
          doi = {10.1088/1674-4527/19/6/88},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019RAA....19...88K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PhRvC..99f4307N,
       author = {{Niu}, Z.~M. and {Liang}, H.~Z. and {Sun}, B.~H. and {Long}, W.~H. and
         {Niu}, Y.~F.},
        title = "{Predictions of nuclear {\ensuremath{\beta}} -decay half-lives with machine learning and their impact on r -process nucleosynthesis}",
      journal = {\prc},
     keywords = {Nuclear Theory, Astrophysics - Solar and Stellar Astrophysics, Nuclear Experiment},
         year = "2019",
        month = "Jun",
       volume = {99},
       number = {6},
          eid = {064307},
        pages = {064307},
     abstract = "{Nuclear {\ensuremath{\beta}} decay is a key process to understand the
        origin of heavy elements in the universe, while the accuracy is
        far from satisfactory for the predictions of
        {\ensuremath{\beta}} -decay half-lives by nuclear models to
        date. In this work, we pave a novel way to accurately predict
        {\ensuremath{\beta}} -decay half-lives with the machine learning
        based on the Bayesian neural network, in which the known physics
        has been explicitly embedded, including the ones described by
        the Fermi theory of {\ensuremath{\beta}} decay, and the
        dependence of half-lives on pairing correlations and decay
        energies. The other potential physics, which is not clear or
        even missing in nuclear models nowadays, will be learned by the
        Bayesian neural network. The results well reproduce the
        experimental data with a very high accuracy and further provide
        reasonable uncertainty evaluations in half-life predictions.
        These accurate predictions for half-lives with uncertainties are
        essential for the r -process simulations.}",
          doi = {10.1103/PhysRevC.99.064307},
archivePrefix = {arXiv},
       eprint = {1810.03156},
 primaryClass = {nucl-th},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PhRvC..99f4307N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PASP..131f4502C,
       author = {{Chintarungruangchai}, Pattana and {Jiang}, Ing-Guey},
        title = "{Detecting Exoplanet Transits through Machine-learning Techniques with Convolutional Neural Networks}",
      journal = {\pasp},
     keywords = {Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Physics - Computational Physics},
         year = "2019",
        month = "Jun",
       volume = {131},
       number = {1000},
        pages = {064502},
     abstract = "{A machine-learning technique with two-dimension convolutional neural
        network is proposed for detecting exoplanet transits. To test
        this new method, five different types of deep-learning models
        with or without folding are constructed and studied. The light
        curves of the Kepler Data Release 25 are employed as the input
        of these models. The accuracy, reliability, and completeness are
        determined and their performances are compared. These results
        indicate that a combination of two-dimension convolutional
        neural network with folding would be an excellent choice for the
        future transit analysis.}",
          doi = {10.1088/1538-3873/ab13d3},
archivePrefix = {arXiv},
       eprint = {1904.12419},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PASP..131f4502C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PASJ...71...62G,
       author = {{Gao}, Xin-Hua},
        title = "{Membership and fundamental parameters of the intermediate-age open clusters NGC 2281 and NGC 2539 based on Gaia-DR2}",
      journal = {\pasj},
     keywords = {methods: data analysis, methods: statistical, open clusters and associations: individual (NGC 2281, NGC 2539), parallaxes, proper motions},
         year = "2019",
        month = "Jun",
       volume = {71},
       number = {3},
          eid = {62},
        pages = {62},
     abstract = "{This paper presents an investigation on the two intermediate-age open
        clusters NGC 2281 and NGC 2539 based on Gaia Data Release 2
        (Gaia-DR2). A combined machine-learning method is used to
        identify likely cluster members of the two clusters, taking into
        account the astrometric and photometric data of Gaia-DR2. It is
        found that NGC 2281 and NGC 2539 each have more than 600 likely
        cluster members, which are highly suitable for further
        investigation of the fundamental parameters of the two clusters.
        Robust estimates of the distances, proper motions, and radial
        velocities for the two clusters are obtained via a Monte Carlo
        simulation technique. NGC 2281 is found to have a distance of
        522 {\ensuremath{\pm}} 12 pc, a proper motion of
        (\&lt;{\ensuremath{\mu}}$_{{\ensuremath{\alpha}}}$cos
        {\ensuremath{\delta}}\&gt;,
        \&lt;{\ensuremath{\mu}}$_{{\ensuremath{\delta}}}$\&gt;) =
        (-2.947 {\ensuremath{\pm}} 0.014, -8.344 {\ensuremath{\pm}}
        0.014) mas yr$^{-1}$, and a radial velocity of +20.0
        {\ensuremath{\pm}} 0.7 km s$^{-1}$. The distance, proper motion,
        and radial velocity of NGC 2539 are determined to be 1271
        {\ensuremath{\pm}} 70 pc,
        (\&lt;{\ensuremath{\mu}}$_{{\ensuremath{\alpha}}}$cos
        {\ensuremath{\delta}}\&gt;,
        \&lt;{\ensuremath{\mu}}$_{{\ensuremath{\delta}}}$\&gt;) =
        (-2.335 {\ensuremath{\pm}} 0.007, -0.583 {\ensuremath{\pm}}
        0.006) mas yr$^{-1}$, and +29.6 {\ensuremath{\pm}} 0.4 km
        s$^{-1}$, respectively. We find that NGC 2281 has a core radius
        of \{8\{\^'$_{.}$\}89\} {\ensuremath{\pm}} \{0\{\^'$_{.}$\}27\}
        (1.3 pc) and a limiting radius of \{79\{\^'$_{.}$\}26\}
        {\ensuremath{\pm}} \{2\{\^'$_{.}$\}86\} (12.0 pc). The core and
        limiting radius of NGC 2539 are determined to be
        \{5\{\^'$_{.}$\}62\} {\ensuremath{\pm}} \{0\{\^'$_{.}$\}28\}
        (2.1 pc) and \{33\{\^'$_{.}$\}65\} {\ensuremath{\pm}}
        \{1\{\^'$_{.}$\}96\} (12.4 pc), respectively. In addition, we
        find strong evidence for the existence of mass segregation in
        the two clusters.}",
          doi = {10.1093/pasj/psz039},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PASJ...71...62G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019NatAs...3..212S,
       author = {{Salvato}, Mara and {Ilbert}, Olivier and {Hoyle}, Ben},
        title = "{The many flavours of photometric redshifts}",
      journal = {Nature Astronomy},
     keywords = {Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Jun",
       volume = {3},
        pages = {212-222},
     abstract = "{Since more than 70 years ago, the colours of galaxies derived from flux
        measurements at different wavelengths have been used to estimate
        their cosmological distances. Such distance measurements, called
        photometric redshifts, are necessary for many scientific
        projects, ranging from investigations of the formation and
        evolution of galaxies and active galactic nuclei to precision
        cosmology. The primary benefit of photometric redshifts is that
        distance estimates can be obtained relatively cheaply for all
        sources detected in photometric images. The drawback is that
        these cheap estimates have low precision compared with resource-
        expensive spectroscopic ones. The methodology for estimating
        redshifts has been through several revolutions in recent
        decades, triggered by increasingly stringent requirements on the
        photometric redshift accuracy. Here, we review the various
        techniques for obtaining photometric redshifts, from template-
        fitting to machine learning and hybrid schemes. We also describe
        state-of-the-art results on current extragalactic samples and
        explain how survey strategy choices affect redshift accuracy. We
        close with a description of the photometric redshift efforts
        planned for upcoming wide-field surveys, which will collect data
        on billions of galaxies, aiming to investigate, among other
        matters, the stellar mass assembly and the nature of dark
        energy.}",
          doi = {10.1038/s41550-018-0478-0},
archivePrefix = {arXiv},
       eprint = {1805.12574},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019NatAs...3..212S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.487L..24T,
       author = {{Tr{\"o}ster}, Tilman and {Ferguson}, Cameron and
         {Harnois-D{\'e}raps}, Joachim and {McCarthy}, Ian G.},
        title = "{Painting with baryons: augmenting N-body simulations with gas using deep generative models}",
      journal = {\mnras},
     keywords = {methods: numerical, large-scale structure of the Universe, galaxies: clusters: intracluster medium, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Machine Learning},
         year = "2019",
        month = "Jun",
       volume = {487},
       number = {1},
        pages = {L24-L29},
     abstract = "{Running hydrodynamical simulations to produce mock data of large-scale
        structure and baryonic probes, such as the thermal Sunyaev-
        Zeldovich (tSZ) effect, at cosmological scales is
        computationally challenging. We propose to leverage the
        expressive power of deep generative models to find an effective
        description of the large-scale gas distribution and temperature.
        We train two deep generative models, a variational auto-encoder
        and a generative adversarial network, on pairs of matter density
        and pressure slices from the BAHAMAS hydrodynamical simulation.
        The trained models are able to successfully map matter density
        to the corresponding gas pressure. We then apply the trained
        models on 100 lines of sight from SLICS, a suite of N-body
        simulations optimized for weak lensing covariance estimation, to
        generate maps of the tSZ effect. The generated tSZ maps are
        found to be statistically consistent with those from BAHAMAS. We
        conclude by considering a specific observable, the angular
        cross-power spectrum between the weak lensing convergence and
        the tSZ effect and its variance, where we find excellent
        agreement between the predictions from BAHAMAS and SLICS, thus
        enabling the use of SLICS for tSZ covariance estimation.}",
          doi = {10.1093/mnrasl/slz075},
archivePrefix = {arXiv},
       eprint = {1903.12173},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.487L..24T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.486.1944V,
       author = {{Vernardos}, Georgios and {Tsagkatakis}, Grigorios},
        title = "{Quasar microlensing light-curve analysis using deep machine learning}",
      journal = {\mnras},
     keywords = {accretion, accretion discs, gravitational lensing: micro, quasars: general, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2019",
        month = "Jun",
       volume = {486},
       number = {2},
        pages = {1944-1952},
     abstract = "{We introduce a deep machine learning approach to studying quasar
        microlensing light curves for the first time by analysing
        hundreds of thousands of simulated light curves with respect to
        the accretion disc size and temperature profile. Our results
        indicate that it is possible to successfully classify very large
        numbers of diverse light-curve data and measure the accretion
        disc structure. The detailed shape of the accretion disc
        brightness profile is found to play a negligible role. The speed
        and efficiency of our deep machine learning approach is ideal
        for quantifying physical properties in a `big-data' problem set-
        up. This proposed approach looks promising for analysing decade-
        long light curves for thousands of microlensed quasars, expected
        to be provided by the Large Synoptic Survey Telescope.}",
          doi = {10.1093/mnras/stz868},
archivePrefix = {arXiv},
       eprint = {1903.09170},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.486.1944V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.486.1539K,
       author = {{Katebi}, Reza and {Zhou}, Yadi and {Chornock}, Ryan and
         {Bunescu}, Razvan},
        title = "{Galaxy morphology prediction using Capsule Networks}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: image processing, catalogues, galaxy: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
         year = "2019",
        month = "Jun",
       volume = {486},
       number = {2},
        pages = {1539-1547},
     abstract = "{Understanding morphological types of galaxies is a key parameter for
        studying their formation and evolution. Neural networks that
        have been used previously for galaxy morphology classification
        have some disadvantages, such as not being inherently invariant
        under rotation. In this work, we studied the performance of
        Capsule Network (CapsNet), a recently introduced neural network
        architecture that is rotationally invariant and spatially aware,
        on the task of galaxy morphology classification. We designed two
        evaluation scenarios based on the answers from the question tree
        in the Galaxy Zoo project. In the first scenario, we used
        CapsNet for regression and predicted probabilities for all of
        the questions. In the second scenario, we chose the answer to
        the first morphology question that had the highest user
        agreement as the class of the object and trained a CapsNet
        classifier, where we also reconstructed galaxy images. We
        achieved promising results in both of these scenarios. Automated
        approaches such as the one introduced here will play a critical
        role in the upcoming large sky surveys.}",
          doi = {10.1093/mnras/stz915},
archivePrefix = {arXiv},
       eprint = {1809.08377},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.486.1539K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.486.1377D,
       author = {{Delli Veneri}, M. and {Cavuoti}, S. and {Brescia}, M. and {Longo}, G. and
         {Riccio}, G.},
        title = "{Star formation rates for photometric samples of galaxies using machine learning methods}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: photometric, catalogues, galaxies: distances and redshifts, galaxies: photometry, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Jun",
       volume = {486},
       number = {1},
        pages = {1377-1391},
     abstract = "{Star formation rates (SFRs) are crucial to constrain theories of galaxy
        formation and evolution. SFRs are usually estimated via
        spectroscopic observations requiring large amounts of telescope
        time. We explore an alternative approach based on the
        photometric estimation of global SFRs for large samples of
        galaxies, by using methods such as automatic parameter space
        optimisation, and supervised machine learning models. We
        demonstrate that, with such approach, accurate multiband
        photometry allows to estimate reliable SFRs. We also investigate
        how the use of photometric rather than spectroscopic redshifts,
        affects the accuracy of derived global SFRs. Finally, we provide
        a publicly available catalogue of SFRs for more than 27 million
        galaxies extracted from the Sloan Digital Sky Survey Data
        Release 7. The catalogue will be made available through the
        Vizier facility.}",
          doi = {10.1093/mnras/stz856},
archivePrefix = {arXiv},
       eprint = {1902.02522},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.486.1377D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.485.5345A,
       author = {{Askar}, Ammar and {Askar}, Abbas and {Pasquato}, Mario and
         {Giersz}, Mirek},
        title = "{Finding black holes with black boxes - using machine learning to identify globular clusters with black hole subsystems}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: numerical, methods: statistical, stars: black holes, globular clusters: general, Astrophysics - Astrophysics of Galaxies, Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2019",
        month = "Jun",
       volume = {485},
       number = {4},
        pages = {5345-5362},
     abstract = "{Machine learning is a powerful technique, becoming increasingly popular
        in astrophysics. In this paper, we apply machine learning to
        more than a thousand globular cluster (GC) models simulated with
        the MOCCA-Survey Database I project in order to correlate
        present-day observable properties with the presence of a
        subsystem of stellar mass black holes (BHs). The machine
        learning model is then applied to available observed parameters
        for Galactic GCs to identify which of them that are most likely
        to be hosting a sizeable number of BHs and reveal insights into
        what properties lead to the formation of BH subsystems. With our
        machine learning model, we were able to shortlist 18 Galactic
        GCs that are most likely to contain a BH subsystem. We show that
        the clusters shortlisted by the machine learning classifier
        include those in which BH candidates have been observed (M22,
        M10, and NGC 3201) and that our results line up well with
        independent simulations and previous studies that manually
        compared simulated GC models with observed properties of
        Galactic GCs. These results can be useful for observers
        searching for elusive stellar mass BH candidates in GCs and
        further our understanding of the role BHs play in GC evolution.
        In addition, we have released an online tool that allows one to
        get predictions from our model after they input observable
        properties.}",
          doi = {10.1093/mnras/stz628},
archivePrefix = {arXiv},
       eprint = {1811.06473},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.485.5345A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.485.5276F,
       author = {{Fang}, Feng and {Forero-Romero}, Jaime and {Rossi}, Graziano and
         {Li}, Xiao-Dong and {Feng}, Long-Long},
        title = "{{\ensuremath{\beta}}-Skeleton analysis of the cosmic web}",
      journal = {\mnras},
     keywords = {methods: statistical, cosmological parameters, cosmology: observations, large-scale structure of Universe, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Jun",
       volume = {485},
       number = {4},
        pages = {5276-5284},
     abstract = "{The {\ensuremath{\beta}}-skeleton is a mathematical method to construct
        graphs from a set of points that has been widely applied in the
        areas of image analysis, machine learning, visual perception,
        and pattern recognition. In this work, we apply the
        {\ensuremath{\beta}}-skeleton to study the cosmic web. We use
        this tool on simulated data to identify the filamentary
        structures and characterize the statistical properties of the
        skeleton. We find that the {\ensuremath{\beta}}-skeleton is able
        to reveal the underlying structures without any parameter fine-
        tuning. A different degree of sparseness can be obtained by
        adjusting the value of {\ensuremath{\beta}}. In addition, the
        statistical properties of the length and direction of the
        skeleton connections show a clear dependence on the redshift
        space distortions, volume effect, Alcock-Paczynski effect, and
        galaxy bias. Our proof-of-concept study shows that the
        statistical properties of the {\ensuremath{\beta}}-skeleton can
        be used to probe cosmological parameters and galaxy evolution.}",
          doi = {10.1093/mnras/stz773},
archivePrefix = {arXiv},
       eprint = {1809.00438},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.485.5276F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.485.4539J,
       author = {{Jin}, Xin and {Zhang}, Yanxia and {Zhang}, Jingyi and {Zhao}, Yongheng and
         {Wu}, Xue-bing and {Fan}, Dongwei},
        title = "{Efficient selection of quasar candidates based on optical and infrared photometric data using machine learning}",
      journal = {\mnras},
     keywords = {methods: statistical, astronomical data bases: miscellaneous, catalogues, galaxies: distances and redshifts, quasars: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Jun",
       volume = {485},
       number = {4},
        pages = {4539-4549},
     abstract = "{We aim to select quasar candidates based on the two large survey
        databases, Pan-STARRS and AllWISE. Exploring the distribution of
        quasars and stars in the colour spaces, we find that the
        combination of infrared and optical photometry is more conducive
        to select quasar candidates. Two new colour criterions (yW1W2
        and iW1zW2) are constructed to distinguish quasars from stars
        efficiently. With iW1zW2, 98.30 per cent of star contamination
        is eliminated, while 99.50 per cent of quasars are retained, at
        least to the magnitude limit of our training set of stars. Based
        on the optical and infrared colour features, we put forward an
        efficient schema to select quasar candidates and high-redshift
        quasar candidates, in which two machine learning algorithms
        (XGBoost and SVM) are implemented. The XGBoost and SVM
        classifiers have proven to be very effective with accuracy of
        99.46\{\{ per cent\}\} when 8Color as input pattern and default
        model parameters. Applying the two optimal classifiers to the
        unknown Pan-STARRS and AllWISE cross-matched data set, a total
        of 2 006 632 intersected sources are predicted to be quasar
        candidates given quasar probability larger than 0.5 (i.e.
        P$_{QSO}$ \&gt; 0.5). Among them, 1 201 211 have high
        probability (P$_{QSO}$ \&gt; 0.95). For these newly predicted
        quasar candidates, a regressor is constructed to estimate their
        redshifts. Finally 7402 z \&gt; 3.5 quasars are obtained. Given
        the magnitude limitation and site of the LAMOST telescope, part
        of these candidates will be used as the input catalogue of the
        LAMOST telescope for follow-up observation, and the rest may be
        observed by other telescopes.}",
          doi = {10.1093/mnras/stz680},
archivePrefix = {arXiv},
       eprint = {1903.03335},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.485.4539J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019Icar..325...16C,
       author = {{Cambioni}, Saverio and {Delbo}, Marco and {Ryan}, Andrew J. and
         {Furfaro}, Roberto and {Asphaug}, Erik},
        title = "{Constraining the thermal properties of planetary surfaces using machine learning: Application to airless bodies}",
      journal = {\icarus},
     keywords = {Asteroids, Surfaces, Regoliths, Infrared observations, Machine learning, Astrophysics - Earth and Planetary Astrophysics},
         year = "2019",
        month = "Jun",
       volume = {325},
        pages = {16-30},
     abstract = "{We present a new method for the determination of the surface properties
        of airless bodies from measurements of the emitted infrared
        flux. Our approach uses machine learning techniques to train,
        validate, and test a neural network representation of the
        thermophysical behavior of the atmosphereless body given shape
        model, illumination and observational geometry of the remote
        sensors. The networks are trained on a dataset of thermal
        simulations of the emitted infrared flux for different values of
        surface rock abundance, roughness, and values of the thermal
        inertia of the regolith and of the rock components. These
        surrogate models are then employed to retrieve the surface
        thermal properties by Markov Chain Monte Carlo Bayesian
        inversion of observed infrared fluxes. We apply the method to
        the inversion of simulated infrared fluxes of asteroid (101195)
        Bennu - according to a geometry of observations similar to those
        planned for NASA's OSIRIS-REx mission - and infrared
        observations of asteroid (25143) Itokawa. In both cases, the
        surface properties of the asteroid - such as surface roughness,
        thermal inertia of the regolith and rock component, and relative
        rock abundance - are retrieved; the contribution from the
        regolith and rock components are well separated. For the case of
        Itokawa, we retrieve a rock abundance of about 85\% for pebbles
        larger than the diurnal skin depth, which is about 2 cm. The
        thermal inertia of the rock is found to be lower than the
        expected value for LL chondrites, indicating that the rocks on
        Itokawa could be fractured. The average thermal inertia of the
        surface is around 750 J s$^{-1/2}$ K$^{-1}$ m$^{-2}$ and the
        measurement of thermal inertia of the regolith corresponds to an
        average regolith particle diameter of about 10 mm, consistently
        with in situ measurements as well as results from previous
        studies.}",
          doi = {10.1016/j.icarus.2019.01.017},
archivePrefix = {arXiv},
       eprint = {1902.08631},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019Icar..325...16C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019EPJC...79..479P,
       author = {{P{\'a}nis}, Radim and {Kolo{\v{s}}}, Martin and
         {Stuchl{\'\i}k}, Zden{\v{e}}k},
        title = "{Determination of chaotic behaviour in time series generated by charged particle motion around magnetized Schwarzschild black holes}",
      journal = {European Physical Journal C},
     keywords = {General Relativity and Quantum Cosmology, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2019",
        month = "Jun",
       volume = {79},
       number = {6},
          eid = {479},
        pages = {479},
     abstract = "{We study behaviour of ionized region of a Keplerian disk orbiting a
        Schwarzschild black hole immersed in an asymptotically uniform
        magnetic field. In dependence on the magnetic parameter B, and
        inclination angle {\ensuremath{\theta}} of the disk plane with
        respect to the magnetic field direction, the charged particles
        of the ionized disk can enter three regimes: (1) regular
        oscillatory motion, (2) destruction due to capture by the
        magnetized black hole, (3) chaotic regime of the motion. In
        order to study transition between the regular and chaotic type
        of the charged particle motion, we generate time series of the
        solution of equations of motion under various conditions, and
        study them by non-linear (box counting, correlation dimension,
        Lyapunov exponent, recurrence analysis, machine learning)
        methods of chaos determination. We demonstrate that the machine
        learning method appears to be the most efficient in determining
        the chaotic region of the {\ensuremath{\theta}} -r space. We
        show that the chaotic character of the ionized particle motion
        increases with the inclination angle. For the inclination angles
        {\ensuremath{\theta}} ̃ 0 whole the ionized internal part of the
        Keplerian disk is captured by the black hole.}",
          doi = {10.1140/epjc/s10052-019-6961-7},
archivePrefix = {arXiv},
       eprint = {1905.01186},
 primaryClass = {gr-qc},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019EPJC...79..479P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019ApJS..242...15E,
       author = {{Erasmus}, N. and {McNeill}, A. and {Mommert}, M. and {Trilling}, D.~E. and
         {Sickafoose}, A.~A. and {Paterson}, K.},
        title = "{A Taxonomic Study of Asteroid Families from KMTNET-SAAO Multiband Photometry}",
      journal = {\apjs},
     keywords = {minor planets, asteroids: individual: Main-Belt asteroids, surveys, techniques: photometric, Astrophysics - Earth and Planetary Astrophysics},
         year = "2019",
        month = "Jun",
       volume = {242},
       number = {2},
          eid = {15},
        pages = {15},
     abstract = "{We present here multiband photometry for over 2000 Main-Belt asteroids.
        For each target, we report the probabilistic taxonomy using the
        measured V ─ R and V ─ I colors in combination with a machine-
        learning-generated decision surface in color─color space.
        Through this method, we classify \&gt;85\% of our targets as one
        the four main Bus─DeMeo complexes: S-, C-, X-, or D-type.
        Roughly one-third of our targets have a known associated dynamic
        family, with 69 families represented in our data. Within
        uncertainties, our results show no discernible difference in
        taxonomic distribution between family members and non-family
        members. Nine of the sixty-nine families represented in our
        observed sample had 20 or more members present, and therefore,
        we investigate the taxonomy of these families in more detail and
        find excellent agreement with the literature. Out of these nine
        well-sampled families, our data show that the Themis, Koronis,
        Hygiea, Massalia, and Eunomia families display a high degree of
        taxonomic homogeneity and that the Vesta, Flora, Nysa─Polana,
        and Eos families show a significant level of mixture in
        taxonomies. Using the taxonomic purity and the degree of
        dispersion in observed colors for each of the nine well-sampled
        collisional families, we also speculate which of these families
        potentially originated from a differentiated parent body and/or
        is a family with a possible undetermined nested family. In
        addition, we obtained sufficient photometric data for 433 of our
        targets to extract reliable rotation periods and observe no
        obvious correlation between rotation properties and family
        membership.}",
          doi = {10.3847/1538-4365/ab1344},
archivePrefix = {arXiv},
       eprint = {1903.08019},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019ApJS..242...15E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019ApJS..242...13Q,
       author = {{Qin}, Li and {Luo}, A. -Li and {Hou}, Wen and {Li}, Yin-Bi and
         {Zhang}, Shuo and {Wang}, Rui and {Wang}, Li-Li and {Kong}, Xiao and
         {Han}, Jin-Shu},
        title = "{Metallic-line Stars Identified from Low-resolution Spectra of LAMOST DR5}",
      journal = {\apjs},
     keywords = {catalogs, infrared: stars, methods: data analysis, methods: statistical, stars: chemically peculiar, surveys, Astrophysics - Solar and Stellar Astrophysics},
         year = "2019",
        month = "Jun",
       volume = {242},
       number = {2},
          eid = {13},
        pages = {13},
     abstract = "{The Large Sky Area Multi-Object Fibre Spectroscopic Telescope data
        release 5 (DR5) released more than 200,000 low-resolution
        spectra of early-type stars with a signal-to-noise ratio \&gt;
        50. The search for metallic-line (Am) stars in such a large
        database and a study of their statistical properties are
        presented in this paper. Six machine-learning algorithms were
        experimented with using known Am spectra, and both the empirical
        criteria method and the MKCLASS package were also investigated.
        Comparing their performance, the random forest (RF) algorithm
        won, not only because the RF has high successful rate, but
        because it can also derive rank features. Then the RF was
        applied to the early-type stars of DR5, and 15,269 Am candidates
        were picked out. Manual identification was conducted based on
        the spectral features derived from the RF algorithm; 9372 Am
        stars and 1131 Ap candidates were compiled into a catalog.
        Statistical studies were conducted including temperature
        distribution, space distribution, and infrared photometry. The
        spectral types of Am stars are mainly between F0 and A4 with a
        peak around A7, which is similar to previous works. With the
        Gaia distances, we calculated the vertical height Z from the
        Galactic plane for each Am star. The distribution of Z suggests
        that the incidence rate of Am stars shows a descending gradient
        with an increasing | Z| . On the other hand, Am stars do not
        show a noteworthy pattern in the infrared band. As the
        wavelength gets longer, the infrared excess of Am stars
        decreases, until there is little or no excess in W1 and W2
        bands.}",
          doi = {10.3847/1538-4365/ab17d8},
archivePrefix = {arXiv},
       eprint = {1904.03242},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019ApJS..242...13Q},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019ApJ...877..121L,
       author = {{Liu}, Hao and {Liu}, Chang and {Wang}, Jason T.~L. and {Wang}, Haimin},
        title = "{Predicting Solar Flares Using a Long Short-term Memory Network}",
      journal = {\apj},
     keywords = {magnetic fields, methods: data analysis, Sun: activity, Sun: flares, Astrophysics - Solar and Stellar Astrophysics, Computer Science - Machine Learning},
         year = "2019",
        month = "Jun",
       volume = {877},
       number = {2},
          eid = {121},
        pages = {121},
     abstract = "{We present a long short-term memory (LSTM) network for predicting
        whether an active region (AR) would produce a ϒ-class flare
        within the next 24 hr. We consider three ϒ classes, namely
        {\ensuremath{\geq}}M5.0 class, {\ensuremath{\geq}}M class, and
        {\ensuremath{\geq}}C class, and build three LSTM models
        separately, each corresponding to a ϒ class. Each LSTM model is
        used to make predictions of its corresponding ϒ-class flares.
        The essence of our approach is to model data samples in an AR as
        time series and use LSTMs to capture temporal information of the
        data samples. Each data sample has 40 features including 25
        magnetic parameters obtained from the Space-weather HMI Active
        Region Patches and related data products as well as 15 flare
        history parameters. We survey the flare events that occurred
        from 2010 May to 2018 May, using the Geostationary Operational
        Environmental Satellite X-ray flare catalogs provided by the
        National Centers for Environmental Information (NCEI), and
        select flares with identified ARs in the NCEI flare catalogs.
        These flare events are used to build the labels (positive versus
        negative) of the data samples. Experimental results show that
        (i) using only 14─22 most important features including both
        flare history and magnetic parameters can achieve better
        performance than using all 40 features together; (ii) our LSTM
        network outperforms related machine-learning methods in
        predicting the labels of the data samples. To our knowledge,
        this is the first time that LSTMs have been used for solar-flare
        prediction.}",
          doi = {10.3847/1538-4357/ab1b3c},
archivePrefix = {arXiv},
       eprint = {1905.07095},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019ApJ...877..121L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019A&A...626A..21A,
       author = {{Alibert}, Y. and {Venturini}, J.},
        title = "{Using deep neural networks to compute the mass of forming planets}",
      journal = {\aap},
     keywords = {methods: numerical, planets and satellites: formation, Astrophysics - Earth and Planetary Astrophysics},
         year = "2019",
        month = "Jun",
       volume = {626},
          eid = {A21},
        pages = {A21},
     abstract = "{Context. Computing the mass of planetary envelopes and the critical mass
        beyond which planets accrete gas in a runaway fashion is
        important for studying planet formation, in particular, for
        planets up to the Neptune-mass range. This computation in
        principle requires solving a set of differential equations, the
        internal structure equations, for some boundary conditions
        (pressure, temperature in the protoplanetary disc where a planet
        forms, core mass, and the rate of accretion of solids by the
        planet). Solving these equations in turn proves to be time-
        consuming and sometimes numerically unstable. <BR /> Aims: The
        aim is to provide a way to approximate the result of integrating
        the internal structure equations for a variety of boundary
        conditions. <BR /> Methods: We computed a set of internal
        planetary structures for a very large number (millions) of
        boundary conditions, considering two opacities: that of the
        interstellar medium, and a reduced opacity. This database was
        then used to train deep neural networks (DNN) in order to
        predict the critical core mass and the mass of planetary
        envelopes as a function of the boundary conditions. <BR />
        Results: We show that our neural networks provide a very good
        approximation (at the percent level) of the result obtained by
        solving interior structure equations, but the required computer
        time is much shorter. The difference with the real solution is
        much smaller than the difference that is obtained with some
        analytical formulas that are available in the literature, which
        only provide the correct order of magnitude at best. We compare
        the results of the DNN with other popular machine-learning
        methods (random forest, gradient boost, support vector
        regression) and show that the DNN outperforms these methods by a
        factor of at least two. <BR /> Conclusions: We show that some
        analytical formulas that can be found in various papers can
        severely overestimate the mass of planets and therefore predict
        the formation of planets in the Jupiter-mass regime instead of
        the Neptune-mass regime. The python tools that we provide allow
        computing the critical mass and the mass of planetary envelopes
        in a variety of cases, without the requirement of solving the
        internal structure equations. These tools can easily replace
        previous analytical formulas and provide far more accurate
        results. A copy of the Python code is available at the CDS via
        anonymous ftp to <A href=``http://cdsarc.u-strasbg.fr''>http://c
        dsarc.u-strasbg.fr</A> (ftp://130.79.128.5) or via <A
        href=``http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/626/A21''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/626/A21</A>}",
          doi = {10.1051/0004-6361/201834942},
archivePrefix = {arXiv},
       eprint = {1903.00320},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019A&A...626A..21A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PhRvL.122u1101C,
       author = {{Chua}, Alvin J.~K. and {Galley}, Chad R. and {Vallisneri}, Michele},
        title = "{Reduced-Order Modeling with Artificial Neurons for Gravitational-Wave Inference}",
      journal = {\prl},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, General Relativity and Quantum Cosmology, Statistics - Machine Learning},
         year = "2019",
        month = "May",
       volume = {122},
       number = {21},
          eid = {211101},
        pages = {211101},
     abstract = "{Gravitational-wave data analysis is rapidly absorbing techniques from
        deep learning, with a focus on convolutional networks and
        related methods that treat noisy time series as images. We
        pursue an alternative approach, in which waveforms are first
        represented as weighted sums over reduced bases (reduced-order
        modeling); we then train artificial neural networks to map
        gravitational-wave source parameters into basis coefficients.
        Statistical inference proceeds directly in coefficient space,
        where it is theoretically straightforward and computationally
        efficient. The neural networks also provide analytic waveform
        derivatives, which are useful for gradient-based sampling
        schemes. We demonstrate fast and accurate coefficient
        interpolation for the case of a four-dimensional binary-inspiral
        waveform family and discuss promising applications of our
        framework in parameter estimation.}",
          doi = {10.1103/PhysRevLett.122.211101},
archivePrefix = {arXiv},
       eprint = {1811.05491},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PhRvL.122u1101C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.tmp.1417M,
       author = {{MacDonald}, Mariah G.},
        title = "{Examining the Radius Valley: a Machine Learning Approach}",
      journal = {\mnras},
     keywords = {planetary systems, methods: statistical, Astrophysics - Earth and Planetary Astrophysics},
         year = "2019",
        month = "May",
        pages = {1417},
     abstract = "{The ``radius valley'' is a relative dearth separating two potential
        populations of exoplanets, super-Earths and mini-Neptunes. This
        feature appears in examining the distribution of planetary
        radii, but has only ever been studied on small samples. Here, we
        investigate the relationship between planetary radius and
        orbital period through 2-dimensional kernel density estimator,
        hierarchical clustering and k-means clustering, using exoplanets
        from all known multi-planet systems. We confirm the ``radius
        valley'' that is suggested to separate super-Earths from mini-
        Neptunes and characterize it as a line in multiple ways. For all
        methods we use, the resulting line has a negative slope,
        suggesting that the dearth is an artifact of photo-evaporation
        and not of the planets forming in a gas-poor disk.}",
          doi = {10.1093/mnras/stz1480},
archivePrefix = {arXiv},
       eprint = {1905.12048},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.tmp.1417M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.485.3642H,
       author = {{Hoyle}, Ben and {Rau}, Markus Michael},
        title = "{Self-consistent redshift estimation using correlation functions without a spectroscopic reference sample}",
      journal = {\mnras},
     keywords = {catalogues, surveys, galaxies: distances and redshifts, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "May",
       volume = {485},
       number = {3},
        pages = {3642-3660},
     abstract = "{We present a new method to estimate redshift distributions and galaxy-
        dark matter bias parameters using correlation functions in a
        fully data driven and self-consistent manner. Unlike other
        machine learning, template, or correlation redshift methods,
        this approach does not require a reference sample with known
        redshifts. By measuring the projected cross- and auto-
        correlations of different galaxy sub-samples, e.g. as chosen by
        simple cells in colour-magnitude space, we are able to estimate
        the galaxy-dark matter bias model parameters, and the shape of
        the redshift distributions of each sub-sample. This method fully
        marginalizes over a flexible parametrization of the redshift
        distribution and galaxy-dark matter bias parameters of sub-
        samples of galaxies, and thus provides a general Bayesian
        framework to incorporate redshift uncertainty into the
        cosmological analysis in a data-driven, consistent, and
        reproducible manner. This result is improved by an order of
        magnitude by including cross-correlations with the cosmic
        microwave background and with galaxy-galaxy lensing. We showcase
        how this method could be applied to real galaxies. By using
        idealized data vectors, in which all galaxy-dark matter model
        parameters and redshift distributions are known, this method is
        demonstrated to recover unbiased estimates on important
        quantities, such as the offset {\ensuremath{\Delta}}$_{z}$
        between the mean of the true and estimated redshift distribution
        and the 68 per cent, 95 per cent, and 99.5 per cent widths of
        the redshift distribution to an accuracy required by current and
        future surveys.}",
          doi = {10.1093/mnras/stz502},
archivePrefix = {arXiv},
       eprint = {1802.02581},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.485.3642H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.485.3569H,
       author = {{Ho}, I. -Ting},
        title = "{A machine learning artificial neural network calibration of the strong-line oxygen abundance}",
      journal = {\mnras},
     keywords = {methods: data analysis, ISM: abundances, H II regions, galaxies: ISM, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "May",
       volume = {485},
       number = {3},
        pages = {3569-3579},
     abstract = "{The H II region oxygen abundance is a key observable for studying
        chemical properties of galaxies. Deriving oxygen abundances
        using optical spectra often relies on empirical strong-line
        calibrations calibrated to the direct method. Existing
        calibrations usually adopt linear or polynomial functions to
        describe the non-linear relationships between strong-line ratios
        and T$_{e}$ oxygen abundances. Here, I explore the possibility
        of using an artificial neural network model to construct a non-
        linear strong-line calibration. Using about 950 literature H II
        region spectra with auroral line detections, I build multilayer
        perceptron models under the machine learning framework of
        training and testing. I show that complex models, like the
        neural network, are preferred at the current sample size and can
        better predict oxygen abundance than simple linear models. I
        demonstrate that the new calibration can reproduce metallicity
        gradients in nearby galaxies and the mass-metallicity
        relationship. Finally, I discuss the prospects of developing new
        neural network calibrations using forthcoming large samples of H
        II region and also the challenges faced.}",
          doi = {10.1093/mnras/stz649},
archivePrefix = {arXiv},
       eprint = {1903.01506},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.485.3569H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.485.3203F,
       author = {{Fussell}, Levi and {Moews}, Ben},
        title = "{Forging new worlds: high-resolution synthetic galaxies with chained generative adversarial networks}",
      journal = {\mnras},
     keywords = {methods: statistical, techniques: image processing, galaxies: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Computer Science - Machine Learning, Statistics - Machine Learning, 85A04, 62P35, 68T05},
         year = "2019",
        month = "May",
       volume = {485},
       number = {3},
        pages = {3203-3214},
     abstract = "{Astronomy of the 21st century increasingly finds itself with extreme
        quantities of data. This growth in data is ripe for modern
        technologies such as deep image processing, which has the
        potential to allow astronomers to automatically identify,
        classify, segment, and deblend various astronomical objects. In
        this paper, we explore the use of chained generative adversarial
        networks (GANs), a class of generative models that learn
        mappings from latent spaces to data distributions by modelling
        the joint distribution of the data, to produce physically
        realistic galaxy images as one use case of such models. In
        cosmology, such data sets can aid in the calibration of shape
        measurements for weak lensing by augmenting data with synthetic
        images. By measuring the distributions of multiple physical
        properties, we show that images generated with our approach
        closely follow the distributions of real galaxies, further
        establishing state-of-the-art GAN architectures as a valuable
        tool for modern-day astronomy.}",
          doi = {10.1093/mnras/stz602},
archivePrefix = {arXiv},
       eprint = {1811.03081},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.485.3203F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.485.2628L,
       author = {{Li}, Weitian and {Xu}, Haiguang and {Ma}, Zhixian and {Zhu}, Ruimin and
         {Hu}, Dan and {Zhu}, Zhenghao and {Gu}, Junhua and {Shan}, Chenxi and
         {Zhu}, Jie and {Wu}, Xiang-Ping},
        title = "{Separating the EoR signal with a convolutional denoising autoencoder: a deep-learning-based method}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: interferometric, dark ages, reionization, first stars, radio continuum: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Machine Learning},
         year = "2019",
        month = "May",
       volume = {485},
       number = {2},
        pages = {2628-2637},
     abstract = "{When applying the foreground removal methods to uncover the faint
        cosmological signal from the epoch of reionization (EoR), the
        foreground spectra are assumed to be smooth. However, this
        assumption can be seriously violated in practice since the
        unresolved or mis-subtracted foreground sources, which are
        further complicated by the frequency-dependent beam effects of
        interferometers, will generate significant fluctuations along
        the frequency dimension. To address this issue, we propose a
        novel deep-learning-based method that uses a nine-layer
        convolutional denoising autoencoder (CDAE) to separate the EoR
        signal. After being trained on the SKA images simulated with
        realistic beam effects, the CDAE achieves excellent performance
        as the mean correlation coefficient
        (\textbackslashbar\{{\ensuremath{\rho}} \}) between the
        reconstructed and input EoR signals reaches 0.929
        {\ensuremath{\pm}} 0.045. In comparison, the two representative
        traditional methods, namely the polynomial fitting method and
        the continuous wavelet transform method, both have difficulties
        in modelling and removing the foreground emission complicated
        with the beam effects, yielding only
        \textbackslashbar\{{\ensuremath{\rho}} \}\_\{poly\} = \{0.296
        {\ensuremath{\pm}} 0.121\} and
        \textbackslashbar\{{\ensuremath{\rho}} \}\_\{cwt\} = \{0.198
        {\ensuremath{\pm}} 0.160\}, respectively. We conclude that, by
        hierarchically learning sophisticated features through multiple
        convolutional layers, the CDAE is a powerful tool that can be
        used to overcome the complicated beam effects and accurately
        separate the EoR signal. Our results also exhibit the great
        potential of deep-learning-based methods in future EoR
        experiments.}",
          doi = {10.1093/mnras/stz582},
archivePrefix = {arXiv},
       eprint = {1902.09278},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.485.2628L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.485.2617R,
       author = {{Reiman}, David M. and {G{\"o}hre}, Brett E.},
        title = "{Deblending galaxy superpositions with branched generative adversarial networks}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: image processing, galaxies: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2019",
        month = "May",
       volume = {485},
       number = {2},
        pages = {2617-2627},
     abstract = "{Near-future large galaxy surveys will encounter blended galaxy images at
        a fraction of up to 50 per cent in the densest regions of the
        Universe. Current deblending techniques may segment the
        foreground galaxy while leaving missing pixel intensities in the
        background galaxy flux. The problem is compounded by the diffuse
        nature of galaxies in their outer regions, making segmentation
        significantly more difficult than in traditional object
        segmentation applications. We propose a novel branched
        generative adversarial network to deblend overlapping galaxies,
        where the two branches produce images of the two deblended
        galaxies. We show that generative models are a powerful engine
        for deblending given their innate ability to infill missing
        pixel values occluded by the superposition. We maintain high
        peak signal-to-noise ratio and structural similarity scores with
        respect to ground truth images upon deblending. Our model also
        predicts near-instantaneously, making it a natural choice for
        the immense quantities of data soon to be created by large
        surveys such as Large Synoptic Survey Telescope, Euclid, and
        Wide-Field Infrared Survey Telescope.}",
          doi = {10.1093/mnras/stz575},
archivePrefix = {arXiv},
       eprint = {1810.10098},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.485.2617R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019JSWSC...9A..13B,
       author = {{Balasis}, Georgios},
        title = "{A machine learning approach for automated ULF wave recognition}",
      journal = {Journal of Space Weather and Space Climate},
         year = "2019",
        month = "May",
       volume = {9},
        pages = {13},
          doi = {10.1051/swsc/2019010},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019JSWSC...9A..13B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019JHEP...05..065D,
       author = {{Dias}, Mafalda and {Frazer}, Jonathan and {Westphal}, Alexander},
        title = "{Inflation as an information bottleneck: a strategy for identifying universality classes and making robust predictions}",
      journal = {Journal of High Energy Physics},
     keywords = {Cosmology of Theories beyond the SM, Flux compactifications, High Energy Physics - Theory, Astrophysics - Cosmology and Nongalactic Astrophysics, High Energy Physics - Phenomenology},
         year = "2019",
        month = "May",
       volume = {2019},
       number = {5},
          eid = {65},
        pages = {65},
     abstract = "{In this work we propose a statistical approach to handling sources of
        theoretical uncertainty in string theory models of inflation. By
        viewing a model of inflation as a probabilistic graph, we show
        that there is an inevitable information bottleneck between the
        ultraviolet input of the theory and observables, as a simple
        consequence of the data processing theorem. This information
        bottleneck can result in strong hierarchies in the sensitivity
        of observables to the parameters of the underlying model and
        hence universal predictions with respect to at least some
        microphysical considerations. We also find other intriguing
        behaviour, such as sharp transitions in the predictions when
        certain hyperparameters cross a critical value. We develop a
        robust numerical approach to studying these behaviours by
        adapting methods often seen in the context of machine learning.
        We first test our approach by applying it to well known examples
        of universality, sharp transitions, and concentration phenomena
        in random matrix theory. We then apply the method to inflation
        with axion monodromy. We find universality with respect to a
        number of model parameters and that consistency with
        observational constraints implies that with very high
        probability certain perturbative corrections are non-negligible.}",
          doi = {10.1007/JHEP05(2019)065},
archivePrefix = {arXiv},
       eprint = {1810.05199},
 primaryClass = {hep-th},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019JHEP...05..065D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019GeoRL..46.5234Z,
       author = {{Zhao}, Yong and {Zhang}, Yigang and {Geng}, Ming and {Jiang}, Jilian and
         {Zou}, Xinyu},
        title = "{Involvement of Slab-Derived Fluid in the Generation of Cenozoic Basalts in Northeast China Inferred From Machine Learning}",
      journal = {\grl},
         year = "2019",
        month = "May",
       volume = {46},
       number = {10},
        pages = {5234-5242},
     abstract = "{The origin and involvement of fluid in the generation of Cenozoic
        basalts in Northeast China are still under debate. Here we apply
        the machine learning methods of random forest and deep neural
        network to train models using data sets of global island arc and
        ocean island basalts. The trained models predict that most
        Cenozoic basalts in Northeast China are influenced by fluid and
        that the fluid activity decreases from east to west. The
        boundary defined by fluid activity coincides with the
        westernmost edge of the present-day stagnant Pacific slab
        determined by seismic tomography and with the geochemical
        boundary defined by magnesium isotopes. These observations
        support the view that the fluid involved in the generation of
        the basalts is controlled by the stagnant Pacific slab instead
        of driven by the plume induced by the sinking Izanagi Plate.}",
          doi = {10.1029/2019GL082322},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019GeoRL..46.5234Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019GeoRL..46.4694S,
       author = {{Slinski}, Kimberly M. and {Hogue}, Terri S. and {McCray}, John E.},
        title = "{Active-Passive Surface Water Classification: A New Method for High-Resolution Monitoring of Surface Water Dynamics}",
      journal = {\grl},
     keywords = {hydrology, drought, Sentinel 1, Landsat imagery, water body, classification},
         year = "2019",
        month = "May",
       volume = {46},
       number = {9},
        pages = {4694-4704},
     abstract = "{This study develops a new, highly efficient method to produce accurate,
        high-resolution surface water maps. The ``active-passive surface
        water classification'' method leverages cloud-based computing
        resources and machine learning techniques to merge Sentinel 1
        synthetic aperture radar and Landsat observations and generate
        monthly 10-m-resolution water body maps. The skill of the
        active-passive surface water classification method is
        demonstrated by mapping surface water change over the Awash
        River basin in Ethiopia during the 2015 East African regional
        drought and 2016 localized flood events. Errors of omission
        (water incorrectly classified as nonwater) and commission
        (nonwater incorrectly classified as water) in the case study
        area are 7.16\% and 1.91\%, respectively. The case study
        demonstrates the method's ability to generate accurate, high-
        resolution water body maps depicting surface water dynamics in
        data-sparse regions. The developed technique will facilitate
        better monitoring and understanding of the impact of
        environmental change and climate extremes on global freshwater
        ecosystems.}",
          doi = {10.1029/2019GL082562},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019GeoRL..46.4694S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019ComAC...6....1M,
       author = {{Mustafa}, Mustafa and {Bard}, Deborah and {Bhimji}, Wahid and
         {Luki{\'c}}, Zarija and {Al-Rfou}, Rami and {Kratochvil}, Jan M.},
        title = "{CosmoGAN: creating high-fidelity weak lensing convergence maps using Generative Adversarial Networks}",
      journal = {Computational Astrophysics and Cosmology},
     keywords = {Weak lensing convergence maps, Generative models, Generative Adversarial Networks, Deep learning, Machine learning, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning},
         year = "2019",
        month = "May",
       volume = {6},
       number = {1},
          eid = {1},
        pages = {1},
     abstract = "{Inferring model parameters from experimental data is a grand challenge
        in many sciences, including cosmology. This often relies
        critically on high fidelity numerical simulations, which are
        prohibitively computationally expensive. The application of deep
        learning techniques to generative modeling is renewing interest
        in using high dimensional density estimators as computationally
        inexpensive emulators of fully-fledged simulations. These
        generative models have the potential to make a dramatic shift in
        the field of scientific simulations, but for that shift to
        happen we need to study the performance of such generators in
        the precision regime needed for science applications. To this
        end, in this work we apply Generative Adversarial Networks to
        the problem of generating weak lensing convergence maps. We show
        that our generator network produces maps that are described by,
        with high statistical confidence, the same summary statistics as
        the fully simulated maps.}",
          doi = {10.1186/s40668-019-0029-9},
archivePrefix = {arXiv},
       eprint = {1706.02390},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019ComAC...6....1M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019ApJS..242....7G,
       author = {{Galvez}, Richard and {Fouhey}, David F. and {Jin}, Meng and
         {Szenicer}, Alexandre and {Mu{\~n}oz-Jaramillo}, Andr{\'e}s and
         {Cheung}, Mark C.~M. and {Wright}, Paul J. and {Bobra}, Monica G. and
         {Liu}, Yang and {Mason}, James and {Thomas}, Rajat},
        title = "{A Machine-learning Data Set Prepared from the NASA Solar Dynamics Observatory Mission}",
      journal = {\apjs},
     keywords = {astronomical databases: miscellaneous, catalogs, editorials, notices, miscellaneous, surveys, Astrophysics - Solar and Stellar Astrophysics, Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning},
         year = "2019",
        month = "May",
       volume = {242},
       number = {1},
          eid = {7},
        pages = {7},
     abstract = "{In this paper, we present a curated data set from the NASA Solar
        Dynamics Observatory (SDO) mission in a format suitable for
        machine-learning research. Beginning from level 1 scientific
        products we have processed various instrumental corrections,
        down-sampled to manageable spatial and temporal resolutions, and
        synchronized observations spatially and temporally. We
        illustrate the use of this data set with two example
        applications: forecasting future extreme ultraviolet (EUV)
        Variability Experiment (EVE) irradiance from present EVE
        irradiance and translating Helioseismic and Magnetic Imager
        observations into Atmospheric Imaging Assembly observations. For
        each application, we provide metrics and baselines for future
        model comparison. We anticipate this curated data set will
        facilitate machine-learning research in heliophysics and the
        physical sciences generally, increasing the scientific return of
        the SDO mission. This work is a direct result of the 2018 NASA
        Frontier Development Laboratory Program. Please see the Appendix
        for access to the data set, totaling 6.5TBs.}",
          doi = {10.3847/1538-4365/ab1005},
archivePrefix = {arXiv},
       eprint = {1903.04538},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019ApJS..242....7G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019ApJ...876...82N,
       author = {{Ntampaka}, M. and {ZuHone}, J. and {Eisenstein}, D. and {Nagai}, D. and
         {Vikhlinin}, A. and {Hernquist}, L. and {Marinacci}, F. and
         {Nelson}, D. and {Pakmor}, R. and {Pillepich}, A. and {Torrey}, P. and
         {Vogelsberger}, M.},
        title = "{A Deep Learning Approach to Galaxy Cluster X-Ray Masses}",
      journal = {\apj},
     keywords = {galaxies: clusters: general, methods: statistical, X-rays: galaxies: clusters, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2019",
        month = "May",
       volume = {876},
       number = {1},
          eid = {82},
        pages = {82},
     abstract = "{We present a machine-learning (ML) approach for estimating galaxy
        cluster masses from Chandra mock images. We utilize a
        Convolutional Neural Network (CNN), a deep ML tool commonly used
        in image recognition tasks. The CNN is trained and tested on our
        sample of 7896 Chandra X-ray mock observations, which are based
        on 329 massive clusters from the
        \{\textbackslashtext\{\}\}\{IllustrisTNG\} simulation. Our CNN
        learns from a low resolution spatial distribution of photon
        counts and does not use spectral information. Despite our
        simplifying assumption to neglect spectral information, the
        resulting mass values estimated by the CNN exhibit small bias in
        comparison to the true masses of the simulated clusters
        ({\ensuremath{-}}0.02 dex) and reproduce the cluster masses with
        low intrinsic scatter, 8\% in our best fold and 12\% averaging
        over all. In contrast, a more standard core-excised luminosity
        method achieves 15\%─18\% scatter. We interpret the results with
        an approach inspired by Google DeepDream and find that the CNN
        ignores the central regions of clusters, which are known to have
        high scatter with mass.}",
          doi = {10.3847/1538-4357/ab14eb},
archivePrefix = {arXiv},
       eprint = {1810.07703},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019ApJ...876...82N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019AJ....157..169D,
       author = {{Dattilo}, Anne and {Vanderburg}, Andrew and {Shallue}, Christopher J. and
         {Mayo}, Andrew W. and {Berlind}, Perry and {Bieryla}, Allyson and
         {Calkins}, Michael L. and {Esquerdo}, Gilbert A. and
         {Everett}, Mark E. and {Howell}, Steve B. and {Latham}, David W. and
         {Scott}, Nicholas J. and {Yu}, Liang},
        title = "{Identifying Exoplanets with Deep Learning. II. Two New Super-Earths Uncovered by a Neural Network in K2 Data}",
      journal = {\aj},
     keywords = {planetary systems, planets and satellites: detection, Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2019",
        month = "May",
       volume = {157},
       number = {5},
          eid = {169},
        pages = {169},
     abstract = "{For years, scientists have used data from NASA{\textquoteright}s Kepler
        Space Telescope to look for and discover thousands of transiting
        exoplanets. In its extended K2 mission, Kepler observed stars in
        various regions of the sky all across the ecliptic plane, and
        therefore in different galactic environments. Astronomers want
        to learn how the populations of exoplanets are different in
        these different environments. However, this requires an
        automatic and unbiased way to identify exoplanets in these
        regions and rule out false-positive signals that mimic
        transiting planet signals. We present a method for classifying
        these exoplanet signals using deep learning, a class of machine
        learning algorithms that have become popular in fields ranging
        from medical science to linguistics. We modified a neural
        network previously used to identify exoplanets in the Kepler
        field to be able to identify exoplanets in different K2
        campaigns that exist in a range of galactic environments. We
        train a convolutional neural network, called AstroNet- K2, to
        predict whether a given possible exoplanet signal is really
        caused by an exoplanet or a false positive. AstroNet- K2 is
        highly successful at classifying exoplanets and false positives,
        with accuracy of 98\% on our test set. It is especially
        efficient at identifying and culling false positives, but for
        now, it still needs human supervision to create a complete and
        reliable planet candidate sample. We use AstroNet- K2 to
        identify and validate two previously unknown exoplanets. Our
        method is a step toward automatically identifying new exoplanets
        in K2 data and learning how exoplanet populations depend on
        their galactic birthplace.}",
          doi = {10.3847/1538-3881/ab0e12},
archivePrefix = {arXiv},
       eprint = {1903.10507},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019AJ....157..169D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019A&A...625A.146D,
       author = {{D{\'\i}az-Garc{\'\i}a}, S. and {D{\'\i}az-Su{\'a}rez}, S. and
         {Knapen}, J.~H. and {Salo}, H.},
        title = "{Inner and outer rings are not strongly coupled with stellar bars}",
      journal = {\aap},
     keywords = {galaxies: evolution, galaxies: structure, galaxies: statistics, galaxies: spiral, galaxies: fundamental parameters, galaxies: photometry, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "May",
       volume = {625},
          eid = {A146},
        pages = {A146},
     abstract = "{Rings are distinctive features of many disc galaxies and their location
        and properties are closely related to the disc dynamics. In
        particular, rings are often associated to stellar bars, but the
        details of this connection are far from clear. We have studied
        the frequency and dimensions of inner and outer rings in the
        local Universe as a function of disc parameters and the
        amplitude of non-axisymmetries. We used the 1320 not highly
        inclined disc galaxies (i \&lt; 65{\textdegree}) from the
        S$^{4}$G survey. The ring fraction increases with bar Fourier
        density amplitude: this can be interpreted as evidence for the
        role of bars in ring formation. The sizes of inner rings are
        positively correlated with bar strength: this can be linked to
        the radial displacement of the 1/4 ultraharmonic resonance while
        the bar grows and the pattern speed decreases. The ring's
        intrinsic ellipticity is weakly controlled by the non-
        axisymmetric perturbation strength: this relation is not as
        strong as expected from simulations, especially when we include
        the dark matter halo in the force calculation. The ratio of
        outer-to-inner ring semi-major axes is uncorrelated with bar
        strength: this questions the manifold origin of rings. In
        addition, we confirm that (i) ̃1/3 (̃1/4) of the galaxies
        hosting inner (outer) rings are not barred; (ii) on average, the
        sizes and shapes of rings are roughly the same for barred and
        non-barred galaxies; and (iii) the fraction of inner (outer)
        rings is a factor of 1.2-1.4 (1.65-1.9) larger in barred
        galaxies than in their non-barred counterparts. Finally, we
        apply unsupervised machine learning (self-organising maps, SOMs)
        to show that, among early-type galaxies, ringed or barred
        galaxies cannot be univocally distinguished based on 20 internal
        and external fundamental parameters. We confirm, with the aid of
        SOMs, that rings are mainly hosted by red, massive, gas-
        deficient, dark-matter poor, and centrally concentrated
        galaxies. We conclude that the present-day coupling between
        rings and bars is not as robust as predicted by numerical
        models, and diverse physical mechanisms and timescales determine
        ring formation and evolution.}",
          doi = {10.1051/0004-6361/201935455},
archivePrefix = {arXiv},
       eprint = {1904.04222},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019A&A...625A.146D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PhRvD..99h2002C,
       author = {{Coughlin}, S. and {Bahaadini}, S. and {Rohani}, N. and {Zevin}, M. and
         {Patane}, O. and {Harandi}, M. and {Jackson}, C. and {Noroozi}, V. and
         {Allen}, S. and {Areeda}, J. and {Coughlin}, M. and {Ruiz}, P. and
         {Berry}, C.~P.~L. and {Crowston}, K. and {Katsaggelos}, A.~K. and
         {Lundgren}, A. and {{\O}sterlund}, C. and {Smith}, J.~R. and
         {Trouille}, L. and {Kalogera}, V.},
        title = "{Classifying the unknown: Discovering novel gravitational-wave detector glitches using similarity learning}",
      journal = {\prd},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, General Relativity and Quantum Cosmology},
         year = "2019",
        month = "Apr",
       volume = {99},
       number = {8},
          eid = {082002},
        pages = {082002},
     abstract = "{The observation of gravitational waves from compact binary coalescences
        by LIGO and Virgo has begun a new era in astronomy. A critical
        challenge in making detections is determining whether loud
        transient features in the data are caused by gravitational waves
        or by instrumental or environmental sources. The citizen-science
        project Gravity Spy has been demonstrated as an efficient
        infrastructure for classifying known types of noise transients
        (glitches) through a combination of data analysis performed by
        both citizen volunteers and machine learning. We present the
        next iteration of this project, using similarity indices to
        empower citizen scientists to create large data sets of unknown
        transients, which can then be used to facilitate supervised
        machine-learning characterization. This new evolution aims to
        alleviate a persistent challenge that plagues both citizen-
        science and instrumental detector work: the ability to build
        large samples of relatively rare events. Using two families of
        transient noise that appeared unexpectedly during LIGO's second
        observing run, we demonstrate the impact that the similarity
        indices could have had on finding these new glitch types in the
        Gravity Spy program.}",
          doi = {10.1103/PhysRevD.99.082002},
archivePrefix = {arXiv},
       eprint = {1903.04058},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PhRvD..99h2002C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PASP..131d4101G,
       author = {{Gao}, Xin-hua},
        title = "{An Investigation of the Pleiades Cluster Using Machine Learning}",
      journal = {\pasp},
         year = "2019",
        month = "Apr",
       volume = {131},
       number = {998},
        pages = {044101},
     abstract = "{This paper presents an investigation on fundamental astrophysical
        properties of the Pleiades cluster (M 45) using high-precision
        astrometric and photometric data from the Gaia Data Release 2
        (Gaia-DR2). To obtain reliable cluster members, a machine-
        learning (ML) method is used to compute membership probabilities
        for 31462 sample stars within a radius of 6.5{\textdegree} from
        the cluster center, both the astrometric and photometric data
        are taken into account. We obtain a total number of 1454 likely
        cluster members with membership probabilities larger than 0.6,
        including a well-known white dwarf (LB 1497) with a high
        membership probability of {\ensuremath{\sim}}0.96. We find a
        well-defined relationship between the parallaxes and proper
        motions of the cluster members, the most likely explanation for
        the relationship is that the depth effect of the cluster along
        the line of sight must be taken into consideration. Using Monte
        Carlo simulations, the most likely distance, proper motion, and
        radial velocity of the cluster are determined to be D = 136.0
        {\ensuremath{\pm}} 0.1 pc, (\&lt; \{{\ensuremath{\mu}}
        \}$_{{\ensuremath{\alpha}} }$\textbackslashcos
        {\ensuremath{\delta}} \&gt; ,\&lt; \{{\ensuremath{\mu}}
        \}$_{{\ensuremath{\delta}} }$\&gt; ) = (+20.141
        {\ensuremath{\pm}} 0.093, {\ensuremath{-}}45.536
        {\ensuremath{\pm}} 0.081) mas yr$^{{\ensuremath{-}}1}$, and
        \&lt; \{V\}$_{r}$\&gt; =+5.8+/- 0.1 \{km\} \{\{\{s\}\}\}$^{-1}$,
        respectively. It is found that the likely cluster members extend
        outward to a limiting radius of R $_{ lim }$ = 310′
        {\ensuremath{\pm}} 12′ (12.3 {\ensuremath{\pm}} 0.5 pc) from the
        cluster center, and the total mass of the cluster within this
        radius is M $_{ tot }$ = 721 {\ensuremath{\pm}} 93 M $_{☉}$. We
        find clear evidence for the presence of spatial mass segregation
        in this young cluster by analyzing the photometry and spatial
        positions of the likely cluster members. Interestingly, we also
        find that four high-mass cluster members with high membership
        probabilities (\&gt;0.99) are being ejected from the inner
        region of the cluster, they may have formed via close encounters
        between single and binary stars.}",
          doi = {10.1088/1538-3873/ab010e},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PASP..131d4101G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PASJ...71...41L,
       author = {{Lei}, Zhenxin and {Bu}, Yude and {Zhao}, Jingkun and
         {N{\'e}meth}, P{\'e}ter and {Zhao}, Gang},
        title = "{Searching for hot subdwarf stars from the LAMOST Spectra. II. Pure spectroscopic identification method for hot subdwarfs}",
      journal = {\pasj},
     keywords = {methods: miscellaneous, subdwarfs, surveys},
         year = "2019",
        month = "Apr",
       volume = {71},
       number = {2},
          eid = {41},
        pages = {41},
     abstract = "{Employing a new machine-learning method, named the hierarchical extreme
        learning machine (HELM) algorithm, we identified 56 hot subdwarf
        stars in the first data release (DR1) of the Large Sky Area
        Multi-Object Fibre Spectroscopic Telescope (LAMOST) survey. The
        atmospheric parameters of the stars are obtained by fitting the
        profiles of hydrogen (H) Balmer lines and helium (He) lines with
        synthetic spectra calculated from non-local thermodynamic
        equilibrium (NLTE) model atmospheres. Five He-rich hot subdwarf
        stars were found in our sample with their log (nHe/nH) \&gt; -1,
        while 51 stars are He-poor sdB, sdO and sdOB stars. We also
        confirmed the two He sequences of hot subdwarf stars found by
        Edelmann et al. (2003, A\&amp;A, 400, 939) in a
        T$_{eff}$-log(nHe/nH) diagram. The HELM algorithm works directly
        on the observed spectroscopy and is able to filter out spectral
        properties without supplementary photometric data. The results
        presented in this study demonstrate that the HELM algorithm is a
        reliable method to search for hot subdwarf stars after suitable
        training is performed, and it can also be used to search for
        other objects which have obvious features in their spectra or
        images.}",
          doi = {10.1093/pasj/psz006},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PASJ...71...41L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019OEJV..197...50C,
       author = {{Caga{\v{s}}}, P.},
        title = "{Using neural networks in searching for variable stars}",
      journal = {Open European Journal on Variable Stars},
         year = "2019",
        month = "Apr",
       volume = {197},
        pages = {50},
     abstract = "{Neural networks recently prove to be a very powerful tool in the field
        of machine learning and artificial intelligence. Neural networks
        operation principle is non-algorithmic and thus can be used for
        some tasks better suited for a human's mind rather than
        traditional computer algorithms. One such task is a recognition
        of stars within astronomical images. However, an algorithmic
        recognition already reached a very good quality and requires
        significantly less computational resources and, therefore, the
        motivation to replace algorithms with neural networks is low in
        this case. Possibly more interesting application is a
        recognition of variable stars in a set of time-based light
        curves of all stars within a field of view, for which neural
        networks provide better results compared to algorithm-based
        methods. At the same time, computational time of the neural
        network detection is comparable to the algorithm-based ones.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019OEJV..197...50C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.484.5315W,
       author = {{Wu}, Yaqian and {Xiang}, Maosheng and {Zhao}, Gang and {Bi}, Shaolan and
         {Liu}, Xiaowei and {Shi}, Jianrong and {Huang}, Yang and {Yuan}, Haibo and
         {Wang}, Chun and {Chen}, Bingqiu and {Huo}, Zhiying and
         {Ren}, Juanjuan and {Tian}, Zhijia and {Liu}, Kang and
         {Zhang}, Xianfei and {Li}, Yaguang and {Zhang}, Jinghua},
        title = "{Ages and masses of 0.64 million red giant branch stars from the LAMOST Galactic Spectroscopic Survey}",
      journal = {\mnras},
     keywords = {Galaxy: abundances, Galaxy: fundamental parameters, stars, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Apr",
       volume = {484},
       number = {4},
        pages = {5315-5329},
     abstract = "{We present a catalogue of stellar age and mass estimates for a sample of
        640 986 red giant branch (RGB) stars of the Galactic disk from
        the LAMOST Galactic Spectroscopic Survey (DR4). The RGB stars
        are distinguished from the red clump (RC) stars utilizing period
        spacing derived from the spectra with a machine-learning method
        based on kernel principal component analysis (KPCA). Cross-
        validation suggests our method is capable of distinguishing RC
        from RGB stars with an only 2 per cent contamination rate for
        stars with signal-to-noise ratio (SNR) higher than 50. The ages
        and masses of these RGB stars are determined from their LAMOST
        spectra with the KPCA method by taking the LAMOST-Kepler giant
        stars having asteroseismic parameters and the LAMOST-TGAS sub-
        giant stars based on isochrones as training sets. Examination
        suggests that the age and mass estimates of our RGB sample stars
        with SNR \&gt; 30 have a median error of 30 per cent and 10 per
        cent, respectively. Stellar ages are found to exhibit positive
        vertical and negative radial gradients across the disk, and the
        age structure of the disk is strongly flared across the whole
        disk of 6 \&lt; R \&lt; 13 kpc. The data set demonstrates good
        correlations among stellar age, [Fe/H], and
        [{\ensuremath{\alpha}}/Fe]. There are two separate sequences in
        the [Fe/H]-[{\ensuremath{\alpha}}/Fe] plane: a
        high-{\ensuremath{\alpha}} sequence with stars older than ̃ 8
        Gyr and a low-{\ensuremath{\alpha}} sequence composed of stars
        with ages covering the whole range of possible ages of stars. We
        also examine relations between age and kinematic parameters
        derived from the Gaia DR2 parallax and proper motions. Both the
        median value and dispersion of the orbital eccentricity are
        found to increase with age. The vertical angular momentum is
        found to fairly smoothly decrease with age from 2 to 12 Gyr,
        with a rate of about -50 kpc km s$^{-1}$ Gyr$^{-1}$. A full
        table of the catalogue is publicly available online
        http://dr4.lamost.org/doc/vac.}",
          doi = {10.1093/mnras/stz256},
archivePrefix = {arXiv},
       eprint = {1901.07233},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.484.5315W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.484.3573C,
       author = {{Conlon}, Joseph P. and {Rummel}, Markus},
        title = "{Improving statistical sensitivity of X-ray searches for axion-like particles}",
      journal = {\mnras},
     keywords = {astroparticle physics, elementary particles, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - High Energy Astrophysical Phenomena, High Energy Physics - Phenomenology},
         year = "2019",
        month = "Apr",
       volume = {484},
       number = {3},
        pages = {3573-3581},
     abstract = "{X-ray observations of bright AGNs in or behind galaxy clusters offer
        unique capabilities to constrain axion-like particles. Existing
        analysis technique relies on measurements of the global goodness
        of fit. We develop a new analysis methodology that improves the
        statistical sensitivity to ALP-photon oscillations by isolating
        the characteristic quasi-sinusoidal modulations induced by ALPs.
        This involves analysing residuals in wavelength space allowing
        the Fourier structure to be made manifest as well as a machine
        learning approach. For telescopes with microcalorimeter
        resolution, simulations suggest these methods give an additional
        factor of 2 in sensitivity to ALPs compared to previous
        approaches.}",
          doi = {10.1093/mnras/stz211},
archivePrefix = {arXiv},
       eprint = {1808.05916},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.484.3573C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.484.3071P,
       author = {{Pieringer}, Christian and {Pichara}, Karim and
         {Catel{\'a}n}, M{\'a}rcio and {Protopapas}, Pavlos},
        title = "{An Algorithm for the Visualization of Relevant Patterns in Astronomical Light Curves}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, techniques: miscellaneous, stars: general, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, 85-08},
         year = "2019",
        month = "Apr",
       volume = {484},
       number = {3},
        pages = {3071-3077},
     abstract = "{Within the last years, the classification of variable stars with machine
        learning has become a mainstream area of research. Recently,
        visualization of time series is attracting more attention in
        data science as a tool to visually help scientists to recognize
        significant patterns in complex dynamics. Within the machine
        learning literature, dictionary-based methods have been widely
        used to encode relevant parts of image data. These methods
        intrinsically assign a degree of importance to patches in
        pictures, according to their contribution in the image
        reconstruction. Inspired by dictionary-based techniques, we
        present an approach that naturally provides the visualization of
        salient parts in astronomical light curves, making the analogy
        between image patches and relevant pieces in time series. Our
        approach encodes the most meaningful patterns such that we can
        approximately reconstruct light curves by just using the encoded
        information. We test our method in light curves from the OGLE-
        III and STARLIGHT data bases. Our results show that the proposed
        model delivers an automatic and intuitive visualization of
        relevant light curve parts, such as local peaks and drops in
        magnitude.}",
          doi = {10.1093/mnras/stz106},
archivePrefix = {arXiv},
       eprint = {1903.03254},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.484.3071P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.484.2793V,
       author = {{Vafaei Sadr}, A. and {Vos}, Etienne E. and {Bassett}, Bruce A. and
         {Hosenie}, Zafiirah and {Oozeer}, N. and {Lochner}, Michelle},
        title = "{DEEPSOURCE: point source detection using deep learning}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: observational, techniques: image processing, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, High Energy Physics - Phenomenology, Statistics - Machine Learning},
         year = "2019",
        month = "Apr",
       volume = {484},
       number = {2},
        pages = {2793-2806},
     abstract = "{Point source detection at low signal-to-noise ratio (SNR) is challenging
        for astronomical surveys, particularly in radio interferometry
        images where the noise is correlated. Machine learning is a
        promising solution, allowing the development of algorithms
        tailored to specific telescope arrays and science cases. We
        present DEEPSOURCE - a deep learning solution - that uses
        convolutional neural networks to achieve these goals. DEEPSOURCE
        enhances the SNR of the sources in the original map and then
        uses dynamic blob detection to detect sources. Trained and
        tested on two sets of 500 simulated 1{\textdegree} {\texttimes}
        1{\textdegree} MeerKAT images with a total of 300 000 sources,
        DEEPSOURCE is essentially perfect in both purity and
        completeness down to SNR = 4 and outperforms PYBDSF in all
        metrics. For uniformly weighted images, it achieves a Purity
        {\texttimes} Completeness (PC) score at SNR = 3 of 0.73,
        compared to 0.31 for the best PYBDSF model. For natural
        weighting, we find a smaller improvement of \{̃ \} 40\{\{ per
        cent\}\} in the PC score at SNR = 3. If instead we ask where
        either of the purity or completeness first drops to 90\{\{ per
        cent\}\}, we find that DEEPSOURCE reaches this value at SNR =
        3.6 compared to the 4.3 of PYBDSF (natural weighting). A key
        advantage of DEEPSOURCE is that it can learn to optimally trade
        off purity and completeness for any science case under
        consideration. Our results show that deep learning is a
        promising approach to point source detection in astronomical
        images.}",
          doi = {10.1093/mnras/stz131},
archivePrefix = {arXiv},
       eprint = {1807.02701},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.484.2793V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.484.1526A,
       author = {{Armitage}, Thomas J. and {Kay}, Scott T. and {Barnes}, David J.},
        title = "{An application of machine learning techniques to galaxy cluster mass estimation using the MACSIS simulations}",
      journal = {\mnras},
     keywords = {galaxies: clusters: general, galaxies: kinematics and dynamics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Apr",
       volume = {484},
       number = {2},
        pages = {1526-1537},
     abstract = "{Machine learning (ML) techniques, in particular supervised regression
        algorithms, are a promising new way to use multiple observables
        to predict a cluster's mass or other key features. To
        investigate this approach, we use the MACSIS sample of simulated
        hydrodynamical galaxy clusters to train a variety of ML models,
        mimicking different data sets. We find that compared to
        predicting the cluster mass from the {\ensuremath{\sigma}}-M
        relation, the scatter in the predicted-to-true mass ratio can be
        reduced by a factor of 4, from 0.130 {\ensuremath{\pm}} 0.004
        dex (≃35 per cent) to 0.031 {\ensuremath{\pm}} 0.001 dex (≃7 per
        cent) when using the same, interloper contaminated (out to
        5r$_{200c}$), spectroscopic galaxy sample. Interestingly,
        omitting line-of-sight galaxy velocities from the training set
        has no effect on the scatter when the galaxies are taken from
        within r$_{200c}$. We also train ML models to reproduce
        estimated masses derived from mock X-ray and weak-lensing
        analyses. While the weak-lensing masses can be recovered with a
        similar scatter to that when training on the true mass, the
        hydrostatic mass suffers from significantly higher scatter of
        ≃0.13 dex (≃35 per cent). Training models using dark matter only
        simulations does not significantly increase the scatter in
        predicted cluster mass compared to training on simulated
        clusters with hydrodynamics. In summary, we find ML techniques
        to offer a powerful method to predict masses for large samples
        of clusters, a vital requirement for cosmological analysis with
        future surveys.}",
          doi = {10.1093/mnras/stz039},
archivePrefix = {arXiv},
       eprint = {1810.08430},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.484.1526A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019GeoRL..46.3643L,
       author = {{Linville}, Lisa and {Pankow}, Kristine and {Draelos}, Timothy},
        title = "{Deep Learning Models Augment Analyst Decisions for Event Discrimination}",
      journal = {\grl},
     keywords = {Utah, event classification, event discrimination, deep learning, convolutional neural network, recurrent neural network},
         year = "2019",
        month = "Apr",
       volume = {46},
       number = {7},
        pages = {3643-3651},
     abstract = "{Long-term seismic monitoring networks are well positioned to leverage
        advances in machine learning because of the abundance of labeled
        training data that curated event catalogs provide. We explore
        the use of convolutional and recurrent neural networks to
        accomplish discrimination of explosive and tectonic sources for
        local distances. Using a 5-year event catalog generated by the
        University of Utah Seismograph Stations, we train models to
        produce automated event labels using 90-s event spectrograms
        from three-component and single-channel sensors. Both network
        architectures are able to replicate analyst labels above 98\%.
        Most commonly, model error is the result of label error (70\% of
        cases). Accounting for mislabeled events ( 1\% of the catalog)
        model accuracy for both models increases to above 99\%.
        Classification accuracy remains above 98\% for shallow tectonic
        events, indicating that spectral characteristics controlled by
        event depth do not play a dominant role in event discrimination.}",
          doi = {10.1029/2018GL081119},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019GeoRL..46.3643L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019ExA....47..129C,
       author = {{Cao}, Haitao and {Bastieri}, Denis and {Rando}, Riccardo and
         {Urso}, Giorgio and {Luo}, Gaoyong and {Paccagnella}, Alessandro},
        title = "{Machine learning on compton event identification for a nano-satellite mission}",
      journal = {Experimental Astronomy},
     keywords = {Machine learning, Neural network, Ensemble methods, Imbalance problem, MeV telescope, Loss functions},
         year = "2019",
        month = "Apr",
       volume = {47},
       number = {1-2},
        pages = {129-144},
     abstract = "{Nano-satellite MeV telescope is becoming attractive nowadays. The
        dominant interaction mechanism of the electromagnetic spectrum
        around 1MeV is Compton scattering. However, the gamma-rays
        generated by primary particles hitting the atmosphere and the
        pair production events are the two significant background events
        when the satellite is operating in Low Earth Orbit. In this
        paper, we applied Machine Learning models to identify and reject
        the two troublesome background event types. Ensemble technique
        and imbalance solution are explored in order to obtain a better
        performance. Experiments demonstrated that the proposed methods
        can discriminate the pair events with a high accuracy, and the
        satellite's sensitivity has also been improved dramatically.}",
          doi = {10.1007/s10686-019-09620-4},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019ExA....47..129C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019CQGra..36h5005M,
       author = {{Mukund}, Nikhil and {Coughlin}, Michael and {Harms}, Jan and
         {Biscans}, Sebastien and {Warner}, Jim and {Pele}, Arnaud and
         {Thorne}, Keith and {Barker}, David and {Arnaud}, Nicolas and
         {Donovan}, Fred and {Fiori}, Irene and {Gabbard}, Hunter and
         {Lantz}, Brian and {Mittleman}, Richard and {Radkins}, Hugh and
         {Swinkels}, Bas},
        title = "{Ground motion prediction at gravitational wave observatories using archival seismic data}",
      journal = {Classical and Quantum Gravity},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, General Relativity and Quantum Cosmology},
         year = "2019",
        month = "Apr",
       volume = {36},
       number = {8},
          eid = {085005},
        pages = {085005},
     abstract = "{Gravitational wave observatories have always been affected by tele-
        seismic earthquakes leading to a decrease in duty cycle and
        coincident observation time. In this analysis, we leverage the
        power of machine learning algorithms and archival seismic data
        to predict the ground motion and the state of the gravitational
        wave interferometer during the event of an earthquake. We
        demonstrate improvement from a factor of 5 to a factor of 2.5 in
        scatter of the error in the predicted ground velocity over a
        previous model fitting based approach. The level of accuracy
        achieved with this scheme makes it possible to switch control
        configuration during periods of excessive ground motion thus
        preventing the interferometer from losing lock. To further
        assess the accuracy and utility of our approach, we use IRIS
        seismic network data and obtain similar levels of agreement
        between the estimates and the measured amplitudes. The
        performance indicates that such an archival or prediction scheme
        can be extended beyond the realm of gravitational wave detector
        sites for hazard-based early warning alerts.}",
          doi = {10.1088/1361-6382/ab0d2c},
archivePrefix = {arXiv},
       eprint = {1812.05185},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019CQGra..36h5005M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019CQGra..36g5005L,
       author = {{Llorens-Monteagudo}, Miquel and {Torres-Forn{\'e}}, Alejandro and
         {Font}, Jos{\'e} A. and {Marquina}, Antonio},
        title = "{Classification of gravitational-wave glitches via dictionary learning}",
      journal = {Classical and Quantum Gravity},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, General Relativity and Quantum Cosmology},
         year = "2019",
        month = "Apr",
       volume = {36},
       number = {7},
          eid = {075005},
        pages = {075005},
     abstract = "{We present a new method for the classification of transient noise
        signals (or glitches) in advanced gravitational-wave
        interferometers. The method uses learned dictionaries (a
        supervised machine learning algorithm) for signal denoising, and
        untrained dictionaries for the final sparse reconstruction and
        classification. We use a data set of 3000 simulated glitches of
        three different waveform morphologies, comprising 1000 glitches
        per morphology. These data are embedded in non-white Gaussian
        noise to simulate the background noise of advanced LIGO in its
        broadband configuration. Our classification method yields a 96\%
        accuracy for a large range of initial parameters, showing that
        learned dictionaries are an interesting approach for glitch
        classification. This work constitutes a preliminary step before
        assessing the performance of dictionary-learning methods with
        actual detector glitches.}",
          doi = {10.1088/1361-6382/ab0657},
archivePrefix = {arXiv},
       eprint = {1811.03867},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019CQGra..36g5005L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019ApJ...875...40C,
       author = {{Cambioni}, Saverio and {Asphaug}, Erik and {Emsenhuber}, Alexandre and
         {Gabriel}, Travis S.~J. and {Furfaro}, Roberto and
         {Schwartz}, Stephen R.},
        title = "{Realistic On-the-fly Outcomes of Planetary Collisions: Machine Learning Applied to Simulations of Giant Impacts}",
      journal = {\apj},
     keywords = {methods: numerical, planetary systems, planets and satellites: terrestrial planets, Astrophysics - Earth and Planetary Astrophysics},
         year = "2019",
        month = "Apr",
       volume = {875},
       number = {1},
          eid = {40},
        pages = {40},
     abstract = "{Planet formation simulations are capable of directly integrating the
        evolution of hundreds to thousands of planetary embryos and
        planetesimals as they accrete pairwise to become planets. In
        principle, these investigations allow us to better understand
        the final configuration and geochemistry of the terrestrial
        planets, and also to place our solar system in the context of
        other exosolar systems. While these simulations classically
        prescribe collisions to result in perfect mergers, recent
        computational advances have begun to allow for more complex
        outcomes to be implemented. Here we apply machine learning to a
        large but sparse database of giant impact studies, which allows
        us to streamline the simulations into a classifier of collision
        outcomes and a regressor of accretion efficiency. The classifier
        maps a four-dimensional (4D) parameter space (target mass,
        projectile-to-target mass ratio, impact velocity, impact angle)
        into the four major collision types: merger, graze-and-merge,
        hit-and-run, and disruption. The definition of the four regimes
        and their boundary is fully data-driven. The results do not
        suffer from any model assumption in the fitting. The classifier
        maps the structure of the parameter space and it provides
        insights into the outcome regimes. The regressor is a neural
        network that is trained to closely mimic the functional
        relationship between the 4D space of collision parameters, and a
        real-variable outcome, the mass of the largest remnant. This
        work is a prototype of a more complete surrogate model, that
        will be based on extended sets of simulations (big data), that
        will quickly and reliably predict specific collision outcomes
        for use in realistic N-body dynamical studies of planetary
        formation.}",
          doi = {10.3847/1538-4357/ab0e8a},
archivePrefix = {arXiv},
       eprint = {1903.04507},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019ApJ...875...40C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019A&C....27..130P,
       author = {{Perraudin}, N. and {Defferrard}, M. and {Kacprzak}, T. and {Sgier}, R.},
        title = "{DeepSphere: Efficient spherical convolutional neural network with HEALPix sampling for cosmological applications}",
      journal = {Astronomy and Computing},
     keywords = {Spherical convolutional neural network, DeepSphere, Graph CNN, Cosmological data analysis, Mass mapping, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
         year = "2019",
        month = "Apr",
       volume = {27},
          eid = {130},
        pages = {130},
     abstract = "{Convolutional Neural Networks (CNNs) are a cornerstone of the Deep
        Learning toolbox and have led to many breakthroughs in
        Artificial Intelligence. So far, these neural networks (NNs)
        have mostly been developed for regular Euclidean domains such as
        those supporting images, audio, or video. Because of their
        success, CNN-based methods are becoming increasingly popular in
        Cosmology. Cosmological data often comes as spherical maps,
        which make the use of the traditional CNNs more complicated. The
        commonly used pixelization scheme for spherical maps is the
        Hierarchical Equal Area isoLatitude Pixelisation (HEALPix). We
        present a spherical CNN for analysis of full and partial HEALPix
        maps, which we call DeepSphere. The spherical CNN is constructed
        by representing the sphere as a graph. Graphs are versatile data
        structures that can represent pairwise relationships between
        objects or act as a discrete representation of a continuous
        manifold. Using the graph-based representation, we define many
        of the standard CNN operations, such as convolution and pooling.
        With filters restricted to being radial, our convolutions are
        equivariant to rotation on the sphere, and DeepSphere can be
        made invariant or equivariant to rotation. This way, DeepSphere
        is a special case of a graph CNN, tailored to the HEALPix
        sampling of the sphere. This approach is computationally more
        efficient than using spherical harmonics to perform
        convolutions. We demonstrate the method on a classification
        problem of weak lensing mass maps from two cosmological models
        and compare its performance with that of three baseline
        classifiers, two based on the power spectrum and pixel density
        histogram, and a classical 2D CNN. Our experimental results show
        that the performance of DeepSphere is always superior or equal
        to the baselines. For high noise levels and for data covering
        only a smaller fraction of the sphere, DeepSphere achieves
        typically 10\% better classification accuracy than the
        baselines.Finally, we show how learned filters can be visualized
        to introspect the NN. Code and examples are available at
        https://github.com/SwissDataScienceCenter/DeepSphere.}",
          doi = {10.1016/j.ascom.2019.03.004},
archivePrefix = {arXiv},
       eprint = {1810.12186},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019A&C....27..130P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019A&C....27..111R,
       author = {{Rowlinson}, A. and {Stewart}, A.~J. and {Broderick}, J.~W. and
         {Swinbank}, J.~D. and {Wijers}, R.~A.~M.~J. and {Carbone}, D. and
         {Cendes}, Y. and {Fender}, R. and {van der Horst}, A. and
         {Molenaar}, G. and {Scheers}, B. and {Staley}, T. and {Farrell}, S. and
         {Grie{\ss}meier}, J. -M. and {Bell}, M. and {Eisl{\"o}ffel}, J. and
         {Law}, C.~J. and {van Leeuwen}, J. and {Zarka}, P.},
        title = "{Identifying transient and variable sources in radio images}",
      journal = {Astronomy and Computing},
     keywords = {Methods, Data analysis, Statistical, Radio continuum, General, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2019",
        month = "Apr",
       volume = {27},
          eid = {111},
        pages = {111},
     abstract = "{With the arrival of a number of wide-field snapshot image-plane radio
        transient surveys, there will be a huge influx of images in the
        coming years making it impossible to manually analyse the
        datasets. Automated pipelines to process the information stored
        in the images are being developed, such as the LOFAR Transients
        Pipeline, outputting light curves and various transient
        parameters. These pipelines have a number of tuneable parameters
        that require training to meet the survey requirements. This
        paper utilises both observed and simulated datasets to
        demonstrate different machine learning strategies that can be
        used to train these parameters. We use a simple anomaly
        detection algorithm and a penalised logistic regression
        algorithm. The datasets used are from LOFAR observations and we
        process the data using the LOFAR Transients Pipeline; however
        the strategies developed are applicable to any light curve
        datasets at different frequencies and can be adapted to
        different automated pipelines. These machine learning strategies
        are publicly available as PYTHON tools that can be downloaded
        and adapted to different datasets
        (https://github.com/AntoniaR/TraP\_ML\_tools).}",
          doi = {10.1016/j.ascom.2019.03.003},
archivePrefix = {arXiv},
       eprint = {1808.07781},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019A&C....27..111R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019A&A...624A.102D,
       author = {{Dobbels}, Wouter and {Krier}, Serge and {Pirson}, Stephan and
         {Viaene}, S{\'e}bastien and {De Geyter}, Gert and {Salim}, Samir and
         {Baes}, Maarten},
        title = "{Morphology-assisted galaxy mass-to-light predictions using deep learning}",
      journal = {\aap},
     keywords = {galaxies: fundamental parameters, galaxies: stellar content, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Apr",
       volume = {624},
          eid = {A102},
        pages = {A102},
     abstract = "{Context. One of the most important properties of a galaxy is the total
        stellar mass, or equivalently the stellar mass-to-light ratio
        (M/L). It is not directly observable, but can be estimated from
        stellar population synthesis. Currently, a galaxy's M/L is
        typically estimated from global fluxes. For example, a single
        global g - i colour correlates well with the stellar M/L.
        Spectral energy distribution (SED) fitting can make use of all
        available fluxes and their errors to make a Bayesian estimate of
        the M/L. <BR /> Aims: We want to investigate the possibility of
        using morphology information to assist predictions of M/L. Our
        first goal is to develop and train a method that only requires a
        g-band image and redshift as input. This will allows us to study
        the correlation between M/L and morphology. Next, we can also
        include the i-band flux, and determine if morphology provides
        additional constraints compared to a method that only uses g-
        and i-band fluxes. <BR /> Methods: We used a machine learning
        pipeline that can be split in two steps. First, we detected
        morphology features with a convolutional neural network. These
        are then combined with redshift, pixel size and g-band
        luminosity features in a gradient boosting machine. Our training
        target was the M/L acquired from the GALEX-SDSS-WISE Legacy
        Catalog, which uses global SED fitting and contains galaxies
        with z ̃ 0.1. <BR /> Results: Morphology is a useful attribute
        when no colour information is available, but can not outperform
        colour methods on its own. When we combine the morphology
        features with global g- and i-band luminosities, we find an
        improved estimate compared to a model which does not make use of
        morphology. <BR /> Conclusions: While our method was trained to
        reproduce global SED fitted M/L, galaxy morphology gives us an
        important additional constraint when using one or two bands. Our
        framework can be extended to other problems to make use of
        morphological information.}",
          doi = {10.1051/0004-6361/201834575},
archivePrefix = {arXiv},
       eprint = {1903.05091},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019A&A...624A.102D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019A&A...624A..45A,
       author = {{Alibert}, Y.},
        title = "{New metric to quantify the similarity between planetary systems: application to dimensionality reduction using T-SNE}",
      journal = {\aap},
     keywords = {planets and satellites: formation, methods: data analysis, methods: numerical, methods: statistical, Astrophysics - Earth and Planetary Astrophysics},
         year = "2019",
        month = "Apr",
       volume = {624},
          eid = {A45},
        pages = {A45},
     abstract = "{Context. Planet formation models now often consider the formation of
        planetary systems with more than one planet per system. This
        raises the question of how to represent planetary systems in a
        convenient way (e.g. for visualisation purpose) and how to
        define the similarity between two planetary systems, for example
        to compare models and observations. <BR /> Aims: We define a new
        metric to infer the similarity between two planetary systems,
        based on the properties of planets that belong to these systems.
        We then compare the similarity of planetary systems with the
        similarity of protoplanetary discs in which they form. <BR />
        Methods: We first define a new metric based on mixture of
        Gaussians, and then use this metric to apply a dimensionality
        reduction technique in order to represent planetary systems
        (which should be represented in a high-dimensional space) in a
        two-dimensional space. This allows us study the structure of a
        population of planetary systems and its relation with the
        characteristics of protoplanetary discs in which planetary
        systems form. <BR /> Results: We show that the new metric can
        help to find the underlying structure of populations of
        planetary systems. In addition, the similarity between planetary
        systems, as defined in this paper, is correlated with the
        similarity between the protoplanetary discs in which these
        systems form. We finally compare the distribution of inter-
        system distances for a set of observed exoplanets with the
        distributions obtained from two models: a population synthesis
        model and a model where planetary systems are constructed by
        randomly picking synthetic planets. The observed distribution is
        shown to be closer to the one derived from the population
        synthesis model than from the random systems. <BR />
        Conclusions: The new metric can be used in a variety of
        unsupervised machine learning techniques, such as dimensionality
        reduction and clustering, to understand the results of
        simulations and compare them with the properties of observed
        planetary systems. The movie associated to Fig. 11 is available
        at <A href=``https://www.aanda.org/10.1051/0004-6361/201834592/o
        lm''>http://https://www.aanda.org</A>}",
          doi = {10.1051/0004-6361/201834592},
archivePrefix = {arXiv},
       eprint = {1901.09719},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019A&A...624A..45A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019A&A...624A..13N,
       author = {{Nakoneczny}, S. and {Bilicki}, M. and {Solarz}, A. and {Pollo}, A. and
         {Maddox}, N. and {Spiniello}, C. and {Brescia}, M. and
         {Napolitano}, N.~R.},
        title = "{Catalog of quasars from the Kilo-Degree Survey Data Release 3}",
      journal = {\aap},
     keywords = {catalogs, surveys, quasars: general, large-scale structure of Universe, methods: data analysis, methods: observational, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Machine Learning},
         year = "2019",
        month = "Apr",
       volume = {624},
          eid = {A13},
        pages = {A13},
     abstract = "{We present a catalog of quasars selected from broad-band photometric
        ugri data of the Kilo-Degree Survey Data Release 3 (KiDS DR3).
        The QSOs are identified by the random forest (RF) supervised
        machine learning model, trained on Sloan Digital Sky Survey
        (SDSS) DR14 spectroscopic data. We first cleaned the input KiDS
        data of entries with excessively noisy, missing or otherwise
        problematic measurements. Applying a feature importance
        analysis, we then tune the algorithm and identify in the KiDS
        multiband catalog the 17 most useful features for the
        classification, namely magnitudes, colors, magnitude ratios, and
        the stellarity index. We used the t-SNE algorithm to map the
        multidimensional photometric data onto 2D planes and compare the
        coverage of the training and inference sets. We limited the
        inference set to r \&lt; 22 to avoid extrapolation beyond the
        feature space covered by training, as the SDSS spectroscopic
        sample is considerably shallower than KiDS. This gives 3.4
        million objects in the final inference sample, from which the
        random forest identified 190 000 quasar candidates. Accuracy of
        97\% (percentage of correctly classified objects), purity of
        91\% (percentage of true quasars within the objects classified
        as such), and completeness of 87\% (detection ratio of all true
        quasars), as derived from a test set extracted from SDSS and not
        used in the training, are confirmed by comparison with external
        spectroscopic and photometric QSO catalogs overlapping with the
        KiDS footprint. The robustness of our results is strengthened by
        number counts of the quasar candidates in the r band, as well as
        by their mid-infrared colors available from the Wide-field
        Infrared Survey Explorer (WISE). An analysis of parallaxes and
        proper motions of our QSO candidates found also in Gaia DR2
        suggests that a probability cut of p$_{QSO}$ \&gt; 0.8 is
        optimal for purity, whereas p$_{QSO}$ \&gt; 0.7 is preferable
        for better completeness. Our study presents the first
        comprehensive quasar selection from deep high-quality KiDS data
        and will serve as the basis for versatile studies of the QSO
        population detected by this survey. A copy of the catalog is
        available at the CDS via anonymous ftp to <A href=``http://cdsar
        c.u-strasbg.fr/''>http://cdsarc.u-strasbg.fr</A>
        (ftp://130.79.128.5) or via <A href=``http://cdsarc.u-strasbg.fr
        /viz-bin/qcat?J/A+A/624/A13''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/624/A13</A>We publicly release the resulting
        catalog at <A href=``http://kids.strw.leidenuniv.nl/DR3/quasarca
        talog.php''>http://kids.strw.leidenuniv.nl/DR3/quasarcatalog.php
        </A>, and the code at <A href=``https://github.com/snakoneczny
        /kids-quasars''>http://https://github.com/snakoneczny/kids-
        quasars</A>.}",
          doi = {10.1051/0004-6361/201834794},
archivePrefix = {arXiv},
       eprint = {1812.03084},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019A&A...624A..13N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019SoPh..294...24C,
       author = {{Covas}, Eurico and {Peixinho}, Nuno and {Fernandes}, Jo{\~a}o},
        title = "{Neural Network Forecast of the Sunspot Butterfly Diagram}",
      journal = {\solphys},
     keywords = {Sunspots, Statistics, Solar cycle, Observations, Astrophysics - Solar and Stellar Astrophysics},
         year = "2019",
        month = "Mar",
       volume = {294},
       number = {3},
          eid = {24},
        pages = {24},
     abstract = "{Using neural networks as a prediction method, we attempt to demonstrate
        that forecasting of the Sun's sunspot time series can be
        extended to the spatio-temporal case. We employ this machine-
        learning method to forecast not only in time but also in space
        (in this case, latitude) on a spatio-temporal dataset
        representing the solar sunspot diagram extending to a total of
        142 years. The analysis shows that this approach seems to be
        able to reconstruct the overall qualitative aspects of the
        spatial-temporal series, namely the overall shape and amplitude
        of the latitude and time pattern of sunspots. This is, as far as
        we are aware, the first time that neural networks have been used
        to forecast the Sun's sunspot butterfly diagram, and although
        the results are limited in the quantitative prediction aspects,
        it points to the way to use the full spatio-temporal series as
        opposed to just the time series for machine-learning approaches
        to forecasting. Additionally, we use the method to predict that
        the upcoming Cycle 25 maximum sunspot number will be around
        R\_\{25\}=57 {\ensuremath{\pm}}17. This implies a very weak
        cycle and, in fact, the weakest cycle on record.}",
          doi = {10.1007/s11207-019-1412-z},
archivePrefix = {arXiv},
       eprint = {1801.04435},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019SoPh..294...24C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PDU....24..293B,
       author = {{Bertone}, Gianfranco and {Deisenroth}, Marc Peter and {Kim}, Jong Soo and
         {Liem}, Sebastian and {Austri}, Roberto Ruiz de and {Welling}, Max},
        title = "{Accelerating the BSM interpretation of LHC data with machine learning}",
      journal = {Physics of the Dark Universe},
         year = "2019",
        month = "Mar",
       volume = {24},
          eid = {100293},
        pages = {100293},
     abstract = "{The interpretation of Large Hadron Collider (LHC) data in the framework
        of Beyond the Standard Model (BSM) theories is hampered by the
        need to run computationally expensive event generators and
        detector simulators. Performing statistically convergent scans
        of high-dimensional BSM theories is consequently challenging,
        and in practice unfeasible for very high-dimensional BSM
        theories. We present here a new machine learning method that
        accelerates the interpretation of LHC data, by learning the
        relationship between BSM theory parameters and data. As a proof-
        of-concept, we demonstrate that this technique accurately
        predicts natural SUSY signal events in two signal regions at the
        High Luminosity LHC, up to four orders of magnitude faster than
        standard techniques. The new approach makes it possible to
        rapidly and accurately reconstruct the theory parameters of
        complex BSM theories, should an excess in the data be discovered
        at the LHC.}",
          doi = {10.1016/j.dark.2019.100293},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PDU....24..293B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PASP..131c8002M,
       author = {{Mahabal}, Ashish and {Rebbapragada}, Umaa and {Walters}, Richard and
         {Masci}, Frank J. and {Blagorodnova}, Nadejda and {van Roestel}, Jan and
         {Ye}, Quan-Zhi and {Biswas}, Rahul and {Burdge}, Kevin and
         {Chang}, Chan-Kao and {Duev}, Dmitry A. and {Golkhou}, V. Zach and
         {Miller}, Adam A. and {Nordin}, Jakob and {Ward}, Charlotte and
         {Adams}, Scott and {Bellm}, Eric C. and {Branton}, Doug and
         {Bue}, Brian and {Cannella}, Chris and {Connolly}, Andrew and
         {Dekany}, Richard and {Feindt}, Ulrich and {Hung}, Tiara and
         {Fortson}, Lucy and {Frederick}, Sara and {Fremling}, C. and
         {Gezari}, Suvi and {Graham}, Matthew and {Groom}, Steven and
         {Kasliwal}, Mansi M. and {Kulkarni}, Shrinivas and {Kupfer}, Thomas and
         {Lin}, Hsing Wen and {Lintott}, Chris and {Lunnan}, Ragnhild and
         {Parejko}, John and {Prince}, Thomas A. and {Riddle}, Reed and
         {Rusholme}, Ben and {Saunders}, Nicholas and {Sedaghat}, Nima and
         {Shupe}, David L. and {Singer}, Leo P. and {Soumagnac}, Maayane T. and
         {Szkody}, Paula and {Tachibana}, Yutaro and {Tirumala}, Kushal and
         {van Velzen}, Sjoert and {Wright}, Darryl},
        title = "{Machine Learning for the Zwicky Transient Facility}",
      journal = {\pasp},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2019",
        month = "Mar",
       volume = {131},
       number = {997},
        pages = {038002},
     abstract = "{The Zwicky Transient Facility is a large optical survey in multiple
        filters producing hundreds of thousands of transient alerts per
        night. We describe here various machine learning (ML)
        implementations and plans to make the maximal use of the large
        data set by taking advantage of the temporal nature of the data,
        and further combining it with other data sets. We start with the
        initial steps of separating bogus candidates from real ones,
        separating stars and galaxies, and go on to the classification
        of real objects into various classes. Besides the usual methods
        (e.g., based on features extracted from light curves) we also
        describe early plans for alternate methods including the use of
        domain adaptation, and deep learning. In a similar fashion we
        describe efforts to detect fast moving asteroids. We also
        describe the use of the Zooniverse platform for helping with
        classifications through the creation of training samples, and
        active learning. Finally we mention the synergistic aspects of
        ZTF and LSST from the ML perspective.}",
          doi = {10.1088/1538-3873/aaf3fa},
archivePrefix = {arXiv},
       eprint = {1902.01936},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PASP..131c8002M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.484..834G,
       author = {{Giles}, Daniel and {Walkowicz}, Lucianne},
        title = "{Systematic serendipity: a test of unsupervised machine learning as a method for anomaly detection}",
      journal = {\mnras},
     keywords = {methods: data analysis, surveys, stars: individual: KIC 846285, stars: individual: KIC 8462852, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2019",
        month = "Mar",
       volume = {484},
       number = {1},
        pages = {834-849},
     abstract = "{Advances in astronomy are often driven by serendipitous discoveries. As
        survey astronomy continues to grow, the size and complexity of
        astronomical data bases will increase, and the ability of
        astronomers to manually scour data and make such discoveries
        decreases. In this work, we introduce a machine learning-based
        method to identify anomalies in large data sets to facilitate
        such discoveries, and apply this method to long cadence light
        curves from NASA's Kepler Mission. Our method clusters data
        based on density, identifying anomalies as data that lie outside
        of dense regions. This work serves as a proof-of-concept case
        study and we test our method on four quarters of the Kepler long
        cadence light curves. We use Kepler's most notorious anomaly,
        Boyajian's star (KIC 8462852), as a rare `ground truth' for
        testing outlier identification to verify that objects of genuine
        scientific interest are included among the identified anomalies.
        We evaluate the method's ability to identify known anomalies by
        identifying unusual behaviour in Boyajian's star; we report the
        full list of identified anomalies for these quarters, and
        present a sample subset of identified outliers that includes
        unusual phenomena, objects that are rare in the Kepler field,
        and data artefacts. By identifying \&lt;4 per cent of each
        quarter as outlying data, we demonstrate that this anomaly
        detection method can create a more targeted approach in
        searching for rare and novel phenomena.}",
          doi = {10.1093/mnras/sty3461},
archivePrefix = {arXiv},
       eprint = {1812.07156},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.484..834G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.484..409R,
       author = {{Rau}, Markus Michael and {Koposov}, Sergey E. and {Trac}, Hy and {Mand
        elbaum}, Rachel},
        title = "{Calibrating long-period variables as standard candles with machine learning}",
      journal = {\mnras},
     keywords = {Magellanic Clouds, distance scale, cosmology: observations, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2019",
        month = "Mar",
       volume = {484},
       number = {1},
        pages = {409-421},
     abstract = "{Variable stars with well-calibrated period-luminosity (PL) relationships
        provide accurate distance measurements to nearby galaxies and
        are therefore a vital tool for cosmology and astrophysics. While
        these measurements typically rely on samples of Cepheid and RR-
        Lyrae stars, abundant populations of luminous variable stars
        with longer periods of 10-1000 d remain largely unused. We apply
        machine learning to derive a mapping between light-curve
        features of these variable stars and their magnitude to extend
        the traditional PL relation commonly used for Cepheid samples.
        Using photometric data for long-period variable stars in the
        Large Magellanic Cloud (LMC), we demonstrate that our
        predictions produce residual errors comparable to those obtained
        on the corresponding Cepheid population. We show that our model
        generalizes well to other samples by performing a blind test on
        photometric data from the Small Magellanic Cloud (SMC). Our
        predictions on the SMC again show small residual errors and
        biases, comparable to results that employ PL relations fitted on
        Cepheid samples. The residual biases are complementary between
        the long-period variable and Cepheid fits, which provides
        exciting prospects to better control sources of systematic error
        in cosmological distance measurements. We finally show that the
        proposed methodology can be used to optimize samples of variable
        stars as standard candles independent of any prior variable star
        classification.}",
          doi = {10.1093/mnras/sty3495},
archivePrefix = {arXiv},
       eprint = {1806.02841},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.484..409R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.484..294D,
       author = {{Das}, Payel and {Sanders}, Jason L.},
        title = "{MADE: a spectroscopic mass, age, and distance estimator for red giant stars with Bayesian machine learning}",
      journal = {\mnras},
     keywords = {methods: data analysis, surveys, Galaxy: evolution, Galaxy: kinematics and dynamics, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Mar",
       volume = {484},
       number = {1},
        pages = {294-304},
     abstract = "{We present a new approach (MADE) that generates mass, age, and distance
        estimates of red giant stars from a combination of astrometric,
        photometric, and spectroscopic data. The core of the approach is
        a Bayesian artificial neural network (ANN) that learns from and
        completely replaces stellar isochrones. The ANN is trained using
        a sample of red giant stars with mass estimates from
        asteroseismology. A Bayesian isochrone pipeline uses the
        astrometric, photometric, spectroscopic, and asteroseismology
        data to determine posterior distributions for the training
        outputs: mass, age, and distance. Given new inputs, posterior
        predictive distributions for the outputs are computed, taking
        into account both input uncertainties, and uncertainties in the
        ANN parameters. We apply MADE to \{̃ \}10 000 red giants in the
        overlap between the 14th data release from the APO Galactic
        Evolution Experiment (APOGEE) and the Tycho-Gaia astrometric
        solution (TGAS). The ANN is able to reduce the uncertainty on
        mass, age, and distance estimates for training-set stars with
        high output uncertainties allocated through the Bayesian
        isochrone pipeline. The fractional uncertainties on mass are
        \&lt; 10 per cent and on age are between 10 to 25 per cent.
        Moreover, the time taken for our ANN to predict masses, ages,
        and distances for the entire catalogue of APOGEE-TGAS stars is
        of a similar order of the time taken by the Bayesian isochrone
        pipeline to run on a handful of stars. Our resulting catalogue
        clearly demonstrates the expected thick- and thin-disc
        components in the [M/H]-[{\ensuremath{\alpha}}/M] plane, when
        examined by age.}",
          doi = {10.1093/mnras/sty2776},
archivePrefix = {arXiv},
       eprint = {1804.09596},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.484..294D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.483.5534S,
       author = {{Schanche}, N. and {Collier Cameron}, A. and {H{\'e}brard}, G. and
         {Nielsen}, L. and {Triaud}, A.~H.~M.~J. and {Almenara}, J.~M. and
         {Alsubai}, K.~A. and {Anderson}, D.~R. and {Armstrong}, D.~J. and
         {Barros}, S.~C.~C. and {Bouchy}, F. and {Boumis}, P. and
         {Brown}, D.~J.~A. and {Faedi}, F. and {Hay}, K. and {Hebb}, L. and
         {Kiefer}, F. and {Mancini}, L. and {Maxted}, P.~F.~L. and {Palle}, E. and
         {Pollacco}, D.~L. and {Queloz}, D. and {Smalley}, B. and {Udry}, S. and
         {West}, R. and {Wheatley}, P.~J.},
        title = "{Machine-learning approaches to exoplanet transit detection and candidate validation in wide-field ground-based surveys}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, planets and satellites: detection, Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2019",
        month = "Mar",
       volume = {483},
       number = {4},
        pages = {5534-5547},
     abstract = "{Since the start of the Wide-angle Search for Planets (WASP) program,
        more than 160 transiting exoplanets have been discovered in the
        WASP data. In the past, possible transit-like events identified
        by the WASP pipeline have been vetted by human inspection to
        eliminate false alarms and obvious false positives. The goal of
        this paper is to assess the effectiveness of machine learning as
        a fast, automated, and reliable means of performing the same
        functions on ground-based wide-field transit-survey data without
        human intervention. To this end, we have created training and
        test data sets made up of stellar light curves showing a variety
        of signal types including planetary transits, eclipsing
        binaries, variable stars, and non-periodic signals. We use a
        combination of machine-learning methods including Random Forest
        Classifiers (RFCs) and convolutional neural networks (CNNs) to
        distinguish between the different types of signals. The final
        algorithms correctly identify planets in the test data ̃90 per
        cent of the time, although each method on its own has a
        significant fraction of false positives. We find that in
        practice, a combination of different methods offers the best
        approach to identifying the most promising exoplanet transit
        candidates in data from WASP, and by extension similar transit
        surveys.}",
          doi = {10.1093/mnras/sty3146},
archivePrefix = {arXiv},
       eprint = {1811.07754},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.483.5534S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.483.5077A,
       author = {{Akras}, Stavros and {Leal-Ferreira}, Marcelo L. and
         {Guzman-Ramirez}, Lizette and {Ramos-Larios}, Gerardo},
        title = "{A machine learning approach for identification and classification of symbiotic stars using 2MASS and WISE}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, general: catalogues, stars: binaries: symbiotic, stars: fundamental parameters, Astrophysics - Solar and Stellar Astrophysics},
         year = "2019",
        month = "Mar",
       volume = {483},
       number = {4},
        pages = {5077-5104},
     abstract = "{In this second paper in a series of papers based on the most-up-to-date
        catalogue of symbiotic stars (SySts), we present a new approach
        for identifying and distinguishing SySts from other H
        {\ensuremath{\alpha}} emitters in photometric surveys using
        machine learning algorithms such as classification tree, linear
        discriminant analysis, and K-nearest neighbour. The motivation
        behind this work is to seek for possible colour indices in the
        regime of near- and mid-infrared covered by the 2MASS and WISE
        surveys. A number of diagnostic colour-colour diagrams are
        generated for all the known Galactic SySts and several classes
        of stellar objects that mimic SySts such as planetary nebulae,
        post-AGB, Mira, single K and M giants, cataclysmic variables,
        Be, AeBe, YSO, weak and classical T Tauri stars, and Wolf-Rayet.
        The classification tree algorithm unveils that primarily J-H,
        W1-W4, and K$_{s}$-W3, and secondarily, H-W2, W1-W2, and W3-W4
        are ideal colour indices to identify SySts. Linear discriminant
        analysis method is also applied to determine the linear
        combination of 2MASS and AllWISE magnitudes that better
        distinguish SySts. The probability of a source being an SySt is
        determined using the K-nearest neighbour method on the LDA
        components. By applying our classification tree model to the
        list of candidate SySts (Paper I), the IPHAS list of candidate
        SySts, and the DR2 VPHAS + catalogue, we find 125 (72 new
        candidates) sources that pass our criteria while we also recover
        90 per cent of the known Galactic SySts.}",
          doi = {10.1093/mnras/sty3359},
archivePrefix = {arXiv},
       eprint = {1901.03016},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.483.5077A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.483.5045P,
       author = {{Papadogiannakis}, S. and {Goobar}, A. and {Amanullah}, R. and
         {Bulla}, M. and {Dhawan}, S. and {Doran}, G. and {Feindt}, U. and
         {Ferretti}, R. and {Hangard}, L. and {Howell}, D.~A. and
         {Johansson}, J. and {Kasliwal}, M.~M. and {Laher}, R. and {Masci}, F. and
         {Nyholm}, A. and {Ofek}, E. and {Sollerman}, J. and {Yan}, L.},
        title = "{R-band light-curve properties of Type Ia supernovae from the (intermediate) Palomar Transient Factory}",
      journal = {\mnras},
     keywords = {supernovae: general, cosmology: observations, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2019",
        month = "Mar",
       volume = {483},
       number = {4},
        pages = {5045-5076},
     abstract = "{We present the best 265 sampled R-band light curves of spectroscopically
        identified Type Ia supernovae (SNe) from the Palomar Transient
        Factory (PTF; 2009-2012) survey and the intermediate Palomar
        Transient Factory (iPTF; 2013-2017). A model-independent light-
        curve template is built from our data-set with the purpose to
        investigate average properties and diversity in our sample. We
        searched for multiple populations in the light-curve properties
        using machine learning tools. We also utilized the long history
        of our light curves, up to 4000 days, to exclude any significant
        pre- or post- supernova flares. From the shapes of light curves
        we found the average rise time in the R band to be
        16.8\^\{+0.5\}\_\{-0.6\} days. Although PTF/iPTF were single-
        band surveys, by modelling the residuals of the SNe in the
        Hubble-Lema{\^\i}tre diagram, we estimate the average colour
        excess of our sample to be \&lt;E(B - V)\&gt;
        {\ensuremath{\approx}} 0.05(2) mag and thus the mean corrected
        peak brightness to be M$_{R}$ = -19.02 {\ensuremath{\pm}} 0.02
        +5 log (H\_0 [ km s\^\{-1\} Mpc\^\{-1\}]/70) mag with only weak
        dependennce on light-curve shape. The intrinsic scatter is found
        to be {\ensuremath{\sigma}}$_{R}$ = 0.186 {\ensuremath{\pm}}
        0.033 mag for the redshift range 0.05 \&lt; z \&lt; 0.1, without
        colour corrections of individual SNe. Our analysis shows that
        Malmquist bias becomes very significant at z = 0.13. A similar
        limitation is expected for the ongoing Zwicky Transient Facility
        (ZTF) survey using the same telescope, but new camera expressly
        designed for ZTF.}",
          doi = {10.1093/mnras/sty3301},
archivePrefix = {arXiv},
       eprint = {1812.01439},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.483.5045P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.483.4866A,
       author = {{Abbott}, T.~M.~C. and {Abdalla}, F.~B. and {Alarcon}, A. and
         {Allam}, S. and {Andrade-Oliveira}, F. and {Annis}, J. and {Avila}, S. and
         {Banerji}, M. and {Banik}, N. and {Bechtol}, K. and {Bernstein}, R.~A. and
         {Bernstein}, G.~M. and {Bertin}, E. and {Brooks}, D. and
         {Buckley-Geer}, E. and {Burke}, D.~L. and {Camacho}, H. and
         {Carnero Rosell}, A. and {Carrasco Kind}, M. and {Carretero}, J. and
         {Castander}, F.~J. and {Cawthon}, R. and {Chan}, K.~C. and
         {Crocce}, M. and {Cunha}, C.~E. and {D'Andrea}, C.~B. and
         {da Costa}, L.~N. and {Davis}, C. and {De Vicente}, J. and
         {DePoy}, D.~L. and {Desai}, S. and {Diehl}, H.~T. and {Doel}, P. and
         {Drlica-Wagner}, A. and {Eifler}, T.~F. and {Elvin-Poole}, J. and
         {Estrada}, J. and {Evrard}, A.~E. and {Flaugher}, B. and {Fosalba}, P. and
         {Frieman}, J. and {Garc{\'\i}a-Bellido}, J. and {Gaztanaga}, E. and
         {Gerdes}, D.~W. and {Giannantonio}, T. and {Gruen}, D. and
         {Gruendl}, R.~A. and {Gschwend}, J. and {Gutierrez}, G. and
         {Hartley}, W.~G. and {Hollowood}, D. and {Honscheid}, K. and
         {Hoyle}, B. and {Jain}, B. and {James}, D.~J. and {Jeltema}, T. and
         {Johnson}, M.~D. and {Kent}, S. and {Kokron}, N. and {Krause}, E. and
         {Kuehn}, K. and {Kuhlmann}, S. and {Kuropatkin}, N. and {Lacasa}, F. and
         {Lahav}, O. and {Lima}, M. and {Lin}, H. and {Maia}, M.~A.~G. and
         {Manera}, M. and {Marriner}, J. and {Marshall}, J.~L. and
         {Martini}, P. and {Melchior}, P. and {Menanteau}, F. and
         {Miller}, C.~J. and {Miquel}, R. and {Mohr}, J.~J. and {Neilsen}, E. and
         {Percival}, W.~J. and {Plazas}, A.~A. and {Porredon}, A. and
         {Romer}, A.~K. and {Roodman}, A. and {Rosenfeld}, R. and {Ross}, A.~J. and
         {Rozo}, E. and {Rykoff}, E.~S. and {Sako}, M. and {Sanchez}, E. and
         {Santiago}, B. and {Scarpine}, V. and {Schindler}, R. and
         {Schubnell}, M. and {Serrano}, S. and {Sevilla-Noarbe}, I. and
         {Sheldon}, E. and {Smith}, R.~C. and {Smith}, M. and {Sobreira}, F. and
         {Suchyta}, E. and {Swanson}, M.~E.~C. and {Tarle}, G. and {Thomas}, D. and
         {Troxel}, M.~A. and {Tucker}, D.~L. and {Vikram}, V. and
         {Walker}, A.~R. and {Wechsler}, R.~H. and {Weller}, J. and {Yanny}, B. and
         {Zhang}, Y.},
        title = "{Dark Energy Survey Year 1 Results: Measurement of the Baryon Acoustic Oscillation scale in the distribution of galaxies to redshift 1}",
      journal = {\mnras},
     keywords = {cosmology: observations, (cosmology:) large-scale structure of Universe},
         year = "2019",
        month = "Mar",
       volume = {483},
       number = {4},
        pages = {4866-4883},
     abstract = "{We present angular diameter distance measurements obtained by locating
        the BAO scale in the distribution of galaxies selected from the
        first year of Dark Energy Survey data. We consider a sample of
        over 1.3 million galaxies distributed over a footprint of 1336
        deg$^{2}$ with 0.6 \&lt; z$_{photo}$ \&lt; 1 and a typical
        redshift uncertainty of 0.03(1 + z). This sample was selected,
        as fully described in a companion paper, using a color/magnitude
        selection that optimizes trade-offs between number density and
        redshift uncertainty. We investigate the BAO signal in the
        projected clustering using three conventions, the angular
        separation, the co-moving transverse separation, and spherical
        harmonics. Further, we compare results obtained from template
        based and machine learning photometric redshift determinations.
        We use 1800 simulations that approximate our sample in order to
        produce covariance matrices and allow us to validate our
        distance scale measurement methodology. We measure the angular
        diameter distance, D$_{A}$, at the effective redshift of our
        sample divided by the true physical scale of the BAO feature,
        r$_{d}$. We obtain close to a 4 per cent distance measurement of
        D$_{A}$(z$_{eff}$ = 0.81)/r$_{d}$ = 10.75 {\ensuremath{\pm}}
        0.43. These results are consistent with the flat
        {\ensuremath{\Lambda}}CDM concordance cosmological model
        supported by numerous other recent experimental results.}",
          doi = {10.1093/mnras/sty3351},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.483.4866A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.483.4828H,
       author = {{Higson}, Edward and {Handley}, Will and {Hobson}, Michael and
         {Lasenby}, Anthony},
        title = "{Bayesian sparse reconstruction: a brute-force approach to astronomical imaging and machine learning}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: numerical, methods: statistical, techniques: image processing, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Methodology, Statistics - Machine Learning},
         year = "2019",
        month = "Mar",
       volume = {483},
       number = {4},
        pages = {4828-4846},
     abstract = "{We present a principled Bayesian framework for signal reconstruction, in
        which the signal is modelled by basis functions whose number
        (and form, if required) is determined by the data themselves.
        This approach is based on a Bayesian interpretation of
        conventional sparse reconstruction and regularization
        techniques, in which sparsity is imposed through priors via
        Bayesian model selection. We demonstrate our method for noisy
        one- and two-dimensional signals, including astronomical images.
        Furthermore, by using a product-space approach, the number and
        type of basis functions can be treated as integer parameters and
        their posterior distributions sampled directly. We show that
        order-of-magnitude increases in computational efficiency are
        possible from this technique compared to calculating the
        Bayesian evidences separately, and that further computational
        gains are possible using it in combination with dynamic nested
        sampling. Our approach can also be readily applied to neural
        networks, where it allows the network architecture to be
        determined by the data in a principled Bayesian manner by
        treating the number of nodes and hidden layers as parameters.}",
          doi = {10.1093/mnras/sty3307},
archivePrefix = {arXiv},
       eprint = {1809.04598},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.483.4828H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.483.4277C,
       author = {{Chen}, B. -Q. and {Huang}, Y. and {Yuan}, H. -B. and {Wang}, C. and
         {Fan}, D. -W. and {Xiang}, M. -S. and {Zhang}, H. -W. and
         {Tian}, Z. -J. and {Liu}, X. -W.},
        title = "{Three-dimensional interstellar dust reddening maps of the Galactic plane}",
      journal = {\mnras},
     keywords = {dust, extinction, ISM: structure, Galaxy: structure, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Mar",
       volume = {483},
       number = {4},
        pages = {4277-4289},
     abstract = "{We present new three-dimensional (3D) interstellar dust reddening maps
        of the Galactic plane in three colours, E(G - K$_{S}$),
        E(G$_{BP}$ - G$_{RP}$), and E(H - K$_{S}$). The maps have a
        spatial angular resolution of 6 arcmin and covers over 7000
        deg$^{2}$ of the Galactic plane for Galactic longitude
        0{\textdegree} \&lt; l \&lt; 360{\textdegree} and latitude |b|
        \&lt; 10{\textdegree}. The maps are constructed from robust
        parallax estimates from the Gaia Data Release 2 (Gaia DR2)
        combined with the high-quality optical photometry from the Gaia
        DR2 and the infrared photometry from the Two Micron All Sky
        Survey and Wide-Field Infrared Survey Explorer survey. We
        estimate the colour excesses, E(G - K$_{S}$), E(G$_{BP}$ -
        G$_{RP}$), and E(H - K$_{S}$), of over 56 million stars with the
        machine-learning algorithm Random Forest regression, using a
        training data set constructed from the large-scale spectroscopic
        surveys, Large Sky Area Multi-Object Fibre Spectroscopic
        Telescope, Sloan Extension for Galactic Understanding and
        Exploration, and Apache Point Observatory Galactic Evolution
        Experiment. The results reveal the large-scale dust distribution
        in the Galactic disc, showing a number of features consistent
        with the earlier studies. The Galactic dust disc is clearly
        warped and show complex structures possibly spatially associated
        with the Sagittarius, Local, and Perseus arms. We also provide
        the empirical extinction coefficients for the Gaia photometry
        that can be used to convert the colour excesses presented here
        to the line-of-sight extinction values in the Gaia photometric
        bands.}",
          doi = {10.1093/mnras/sty3341},
archivePrefix = {arXiv},
       eprint = {1807.02241},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.483.4277C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.483.3673M,
       author = {{Morello}, V. and {Barr}, E.~D. and {Cooper}, S. and {Bailes}, M. and
         {Bates}, S. and {Bhat}, N.~D.~R. and {Burgay}, M. and
         {Burke-Spolaor}, S. and {Cameron}, A.~D. and {Champion}, D.~J. and
         {Eatough}, R.~P. and {Flynn}, C.~M.~L. and {Jameson}, A. and
         {Johnston}, S. and {Keith}, M.~J. and {Keane}, E.~F. and {Kramer}, M. and
         {Levin}, L. and {Ng}, C. and {Petroff}, E. and {Possenti}, A. and
         {Stappers}, B.~W. and {van Straten}, W. and {Tiburzi}, C.},
        title = "{The High Time Resolution Universe survey - XIV. Discovery of 23 pulsars through GPU-accelerated reprocessing}",
      journal = {\mnras},
     keywords = {methods: data analysis, pulsars: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2019",
        month = "Mar",
       volume = {483},
       number = {3},
        pages = {3673-3685},
     abstract = "{We have performed a new search for radio pulsars in archival data of the
        intermediate and high Galactic latitude parts of the Southern
        High Time Resolution Universe pulsar survey. This is the first
        time the entire dataset has been searched for binary pulsars, an
        achievement enabled by GPU-accelerated dedispersion and
        periodicity search codes nearly 50 times faster than the
        previously used pipeline. Candidate selection was handled
        entirely by a Machine Learning algorithm, allowing for the
        assessment of 17.6 million candidates in a few person-days. We
        have also introduced an outlier detection algorithm for
        efficient radio-frequency interference (RFI) mitigation on
        folded data, a new approach that enabled the discovery of
        pulsars previously masked by RFI. We discuss implications for
        future searches, particularly the importance of expanding work
        on RFI mitigation to improve survey completeness. In total, we
        discovered 23 previously unknown sources, including 6
        millisecond pulsars and at least 4 pulsars in binary systems. We
        also found an elusive but credible redback candidate that we
        have yet to confirm.}",
          doi = {10.1093/mnras/sty3328},
archivePrefix = {arXiv},
       eprint = {1811.04929},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.483.3673M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.483.2907J,
       author = {{Jennings}, W.~D. and {Watkinson}, C.~A. and {Abdalla}, F.~B. and
         {McEwen}, J.~D.},
        title = "{Evaluating machine learning techniques for predicting power spectra from reionization simulations}",
      journal = {\mnras},
     keywords = {methods: statistical, dark ages, reionization, first stars, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2019",
        month = "Mar",
       volume = {483},
       number = {3},
        pages = {2907-2922},
     abstract = "{Upcoming experiments such as the Square Kilometre Array will provide
        huge quantities of data. Fast modelling of the high-redshift 21
        cm signal will be crucial for efficiently comparing these data
        sets with theory. The most detailed theoretical predictions
        currently come from numerical simulations and from faster but
        less accurate seminumerical simulations. Recently, machine
        learning techniques have been proposed to emulate the behaviour
        of these seminumerical simulations with drastically reduced time
        and computing cost. We compare the viability of five such
        machine learning techniques for emulating the 21 cm power
        spectrum of the publicly available code SIMFAST21. Our best
        emulator is a multilayer perceptron with three hidden layers,
        reproducing SIMFAST21 power spectra {}10$^{8}$ times faster than
        the simulation with 4 per cent mean squared error averaged
        across all redshifts and input parameters. The other techniques
        (interpolation, Gaussian processes regression, and support
        vector machine) have slower prediction times and worse
        prediction accuracy than the multilayer perceptron. All our
        emulators can make predictions at any redshift and scale, which
        gives more flexible predictions but results in significantly
        worse prediction accuracy at lower redshifts. We then present a
        proof-of-concept technique for mapping between two different
        simulations, exploiting our best emulator's fast prediction
        speed. We demonstrate this technique to find a mapping between
        SIMFAST21 and another publicly available code 21CMFAST. We
        observe a noticeable offset between the simulations for some
        regions of the input space. Such techniques could potentially be
        used as a bridge between fast seminumerical simulations and
        accurate numerical radiative transfer simulations.}",
          doi = {10.1093/mnras/sty3168},
archivePrefix = {arXiv},
       eprint = {1811.09141},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.483.2907J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019JInst..14P3004S,
       author = {{Simola}, U. and {Pelssers}, B. and {Barge}, D. and {Conrad}, J. and
         {Corander}, J.},
        title = "{Machine learning accelerated likelihood-free event reconstruction in dark matter direct detection}",
      journal = {Journal of Instrumentation},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, High Energy Physics - Experiment, High Energy Physics - Phenomenology, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
         year = "2019",
        month = "Mar",
       volume = {14},
       number = {3},
        pages = {P03004},
     abstract = "{Reconstructing the position of an interaction for any dual-phase time
        projection chamber (TPC) with the best precision is key to
        directly detecting Dark Matter. Using the likelihood-free
        framework, a new algorithm to reconstruct the 2-D (x,y) position
        and the size of the charge signal (e) of an interaction is
        presented. The algorithm uses the secondary scintillation light
        distribution (S2) obtained by simulating events using a waveform
        generator. To deal with the computational effort required by the
        likelihood-free approach, we employ the Bayesian Optimization
        for Likelihood-Free Inference (BOLFI) algorithm. Together with
        BOLFI, prior distributions for the parameters of interest
        (x,y,e) and highly informative discrepancy measures to perform
        the analyses are introduced. We evaluate the quality of the
        proposed algorithm by a comparison against the currently
        existing alternative methods using a large-scale simulation
        study. BOLFI provides a natural probabilistic uncertainty
        measure for the reconstruction and it improved the accuracy of
        the reconstruction over the next best algorithm by up to 15\%
        when focusing on events at large radii (R \&gt; 30 cm, the outer
        37\% of the detector). In addition, BOLFI provides the smallest
        uncertainties among all the tested methods.}",
          doi = {10.1088/1748-0221/14/03/P03004},
archivePrefix = {arXiv},
       eprint = {1810.09930},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019JInst..14P3004S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019GeoRL..46.3446S,
       author = {{Schuddeboom}, Alex and {Varma}, Vidya and {McDonald}, Adrian J. and
         {Morgenstern}, Olaf and {Harvey}, Mike and {Parsons}, Simon and
         {Field}, Paul and {Furtado}, Kalli},
        title = "{Cluster-Based Evaluation of Model Compensating Errors: A Case Study of Cloud Radiative Effect in the Southern Ocean}",
      journal = {\grl},
     keywords = {model evaluation, cloud simulation, Southern Ocean, compensating errors},
         year = "2019",
        month = "Mar",
       volume = {46},
       number = {6},
        pages = {3446-3453},
     abstract = "{Model evaluation is difficult and generally relies on analysis that can
        mask compensating errors. This paper defines new metrics, using
        clusters generated from a machine learning algorithm, to
        estimate mean and compensating errors in different model runs.
        As a test case, we investigate the Southern Ocean shortwave
        radiative bias using clusters derived by applying self-
        organizing maps to satellite data. In particular, the effects of
        changing cloud phase parameterizations in the MetOffice Unified
        Model are examined. Differences in cluster properties show that
        the regional radiative biases are substantially different than
        the global bias, with two distinct regions identified within the
        Southern Ocean, each with a different signed bias. Changing
        cloud phase parameterizations can reduce errors at higher
        latitudes but increase errors at lower latitudes of the Southern
        Ocean. Ranking the parameterizations often shows a contrast in
        mean and compensating errors, notably in all cases large
        compensating errors remain.}",
          doi = {10.1029/2018GL081686},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019GeoRL..46.3446S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019E&SS....6..527G,
       author = {{Geyman}, Emily C. and {Maloof}, Adam C.},
        title = "{A Simple Method for Extracting Water Depth From Multispectral Satellite Imagery in Regions of Variable Bottom Type}",
      journal = {Earth and Space Science},
     keywords = {remote sensing, bathymetry, classification, machine learning, Bahamas},
         year = "2019",
        month = "Mar",
       volume = {6},
       number = {3},
        pages = {527-537},
     abstract = "{Satellite imagery offers an efficient and cost-effective means of
        estimating water depth in shallow environments. However,
        traditional empirical algorithms for calculating water depth
        often are unable to account for varying bottom reflectance, and
        therefore yield biased estimates for certain benthic
        environments. We present a simple method that is grounded in the
        physics of radiative transfer in seawater, but made more robust
        through the calibration of individual color-to-depth
        relationships for separate spectral classes. Our cluster-based
        regression (CBR) algorithm, applied to a portion of the Great
        Bahama Bank, drastically reduces the geographic structure in the
        residual and has a mean absolute error of 0.19 m with quantified
        uncertainties. Our CBR bathymetry is 3-5 times more accurate
        than existing models and outperforms machine learning protocols
        at extrapolating beyond the calibration data. Finally, we
        demonstrate how comparison of CBR with traditional models
        sensitive to bottom type reveals the characteristic length
        scales of biosedimentary facies belts.}",
          doi = {10.1029/2018EA000539},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019E&SS....6..527G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019E&SS....6..351P,
       author = {{Pouquet}, A. and {Rosenberg}, D. and {Stawarz}, J.~E. and {Marino}, R.},
        title = "{Helicity Dynamics, Inverse, and Bidirectional Cascades in Fluid and Magnetohydrodynamic Turbulence: A Brief Review}",
      journal = {Earth and Space Science},
     keywords = {Physics - Fluid Dynamics},
         year = "2019",
        month = "Mar",
       volume = {6},
       number = {3},
        pages = {351-369},
     abstract = "{We briefly review helicity dynamics, inverse and bidirectional cascades
        in fluid and magnetohydrodynamic (MHD) turbulence, with an
        emphasis on the latter. The energy of a turbulent system, an
        invariant in the nondissipative case, is transferred to small
        scales through nonlinear mode coupling. Fifty years ago, it was
        realized that, for a two-dimensional fluid, energy cascades
        instead to larger scales and so does magnetic excitation in MHD.
        However, evidence obtained recently indicates that, in fact, for
        a range of governing parameters, there are systems for which
        their ideal invariants can be transferred, with constant fluxes,
        to both the large scales and the small scales, as for MHD or
        rotating stratified flows, in the latter case including quasi-
        geostrophic forcing. Such bidirectional, split, cascades
        directly affect the rate at which mixing and dissipation occur
        in these flows in which nonlinear eddies interact with fast
        waves with anisotropic dispersion laws, due, for example, to
        imposed rotation, stratification, or uniform magnetic fields.
        The directions of cascades can be obtained in some cases through
        the use of phenomenological arguments, one of which we derive
        here following classical lines in the case of the inverse
        magnetic helicity cascade in electron MHD. With more highly
        resolved data sets stemming from large laboratory experiments,
        high-performance computing, and in situ satellite observations,
        machine learning tools are bringing novel perspectives to
        turbulence research. Such algorithms help devise new explicit
        subgrid-scale parameterizations, which in turn may lead to
        enhanced physical insight, including in the future in the case
        of these new bidirectional cascades.}",
          doi = {10.1029/2018EA000432},
archivePrefix = {arXiv},
       eprint = {1807.03239},
 primaryClass = {physics.flu-dyn},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019E&SS....6..351P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019AdSpR..63.1523F,
       author = {{Furnell}, Will and {Shenoy}, Abhishek and {Fox}, Elliot and
         {Hatfield}, Peter},
        title = "{First results from the LUCID-Timepix spacecraft payload onboard the TechDemoSat-1 satellite in Low Earth Orbit}",
      journal = {Advances in Space Research},
     keywords = {Low Earth Orbit, Space radiation, Trapped radiation, Cosmic rays, Timepix, South Atlantic Anomaly, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2019",
        month = "Mar",
       volume = {63},
       number = {5},
        pages = {1523-1540},
     abstract = "{The Langton Ultimate Cosmic ray Intensity Detector (LUCID) is a payload
        onboard the satellite TechDemoSat-1, used to study the radiation
        environment in Low Earth Orbit ({\ensuremath{\sim}} 635 km).
        LUCID operated from 2014 to 2017, collecting over 2.1 million
        frames of radiation data from its five Timepix detectors on
        board. LUCID is one of the first uses of the Timepix detector
        technology in open space, with the data providing useful insight
        into the performance of this technology in new environments. It
        provides high-sensitivity imaging measurements of the mixed
        radiation field, with a wide dynamic range in terms of spectral
        response, particle type and direction. The data has been
        analysed using computing resources provided by GridPP, with a
        new machine learning algorithm that uses the Tensorflow
        framework. This algorithm provides a new approach to processing
        Medipix data, using a training set of human labelled tracks,
        providing greater particle classification accuracy than other
        algorithms. For managing the LUCID data, we have developed an
        online platform called Timepix Analysis Platform at School
        (TAPAS). This provides a swift and simple way for users to
        analyse data that they collect using Timepix detectors from both
        LUCID and other experiments. We also present some possible
        future uses of the LUCID data and Medipix detectors in space.}",
          doi = {10.1016/j.asr.2018.10.045},
archivePrefix = {arXiv},
       eprint = {1810.12876},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019AdSpR..63.1523F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019AcASn..60...16L,
       author = {{Li}, C. and {Zhang}, W.~H. and {Lin}, J.~M.},
        title = "{Research on Star/Galaxy Classification Based on XGBoost Algorithm}",
      journal = {Acta Astronomica Sinica},
     keywords = {stars: fundamental parameters, galaxies: fundamental parameters, techniques: photometric, methods: data analysis},
         year = "2019",
        month = "Mar",
       volume = {60},
       number = {2},
          eid = {16},
        pages = {16},
     abstract = "{Machine learning, especially the life algorithm, has achieved great
        success in many areas today. The lifting algorithm has a strong
        ability to adapt to various scenarios with high accuracy, and
        has played a great role in many fields. But in astronomy, the
        application of lifting algorithms is rare. In response to the
        low classification accuracy of dark source sets in star/galaxy
        in the Sloan Digital Sky Survey (SDSS), a new research result in
        machine learning, eXtreme Gradient Boosting (XGBoost), was
        introduced. The complete photometric data set is obtained from
        the SDSS-DR7, and divided into a bright source set and a dark
        source set according to the magnitude. Firstly, the ten-fold
        cross-validation method is used for the bright source set and
        the dark source set respectively, and the XGBoost algorithm is
        used to establish the star/galaxy classification model. Then,
        the grid search and other methods are used to tune the XGBoost
        parameters. Finally, based on galaxies' classification accuracy
        and other indicators, the classification results are analyzed,
        comparing with the models of function tree (FT), Adaptive
        boosting (Adaboost), Random Forest (RF), Gradient Boosting
        Decision Tree (GBDT), Stacked Denoising AutoEncoders (SDAE), and
        Deep Belief Nets (DBN). The experimental results show that, the
        XGBoost improves the classification accuracy of galaxies in dark
        source classification by nearly 10\% compared to the function
        tree algorithm, and improves the classification accuracy of
        galaxies in the darkest magnitude of dark source set by nearly
        5\% compared to the function tree algorithm. Compared with other
        traditional machine learning algorithms and deep neural
        networks, the XGBoost also has different degrees of improvement.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019AcASn..60...16L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PhRvD..99d3521V,
       author = {{von Marttens}, Rodrigo and {Marra}, Valerio and {Casarini}, Luciano and
         {Gonzalez}, J.~E. and {Alcaniz}, Jailson},
        title = "{Null test for interactions in the dark sector}",
      journal = {\prd},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, General Relativity and Quantum Cosmology},
         year = "2019",
        month = "Feb",
       volume = {99},
       number = {4},
          eid = {043521},
        pages = {043521},
     abstract = "{Since there is no known symmetry in nature that prevents a nonminimal
        coupling between the dark energy (DE) and cold dark matter (CDM)
        components, such a possibility constitutes an alternative to
        standard cosmology, with its theoretical and observational
        consequences being of great interest. In this paper, we propose
        a new null test on the standard evolution of the dark sector
        based on the time dependence of the ratio between the CDM and DE
        energy densities which, in the standard {\ensuremath{\Lambda}}
        CDM scenario, scales necessarily as a$^{-3}$. We use the latest
        measurements of type Ia supernovae, cosmic chronometers and
        angular baryonic acoustic oscillations to reconstruct the
        expansion history using model-independent machine learning
        techniques, namely, the linear model formalism and Gaussian
        processes. We find that while the standard evolution is
        consistent with the data at 3 {\ensuremath{\sigma}} level, some
        deviations from the {\ensuremath{\Lambda}} CDM model are found
        at low redshifts, which may be associated with the current
        tension between local and global determinations of H$_{0}$.}",
          doi = {10.1103/PhysRevD.99.043521},
archivePrefix = {arXiv},
       eprint = {1812.02333},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PhRvD..99d3521V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PEPS....6...19G,
       author = {{Gichu}, Ryusei and {Ogohara}, Kazunori},
        title = "{Segmentation of dust storm areas on Mars images using principal component analysis and neural network}",
      journal = {Progress in Earth and Planetary Science},
     keywords = {Mars, Dust storm, Segmentation, Machine learning, Principal component analysis},
         year = "2019",
        month = "Feb",
       volume = {6},
       number = {1},
          eid = {19},
        pages = {19},
     abstract = "{We present a method for automated segmentation of dust storm areas on
        Mars images observed by an orbiter. We divide them into small
        patches. Normal basis vectors are obtained from the many small
        patches by principal component analysis. We train a classifier
        using coefficients of these basis vectors as feature vectors.
        All patches in test images are categorized into one of the dust
        storm, cloud, and surface classes by the classifier. Each pixel
        may be included in several dust storm patches. The pixel is
        classified as a dust storm or the other classes based on the
        number of dust storm patches that include the pixel. We evaluate
        the segmentation method by the receiver operator characteristic
        curve and the area under the curve (AUC). AUC for dust storm is
        0.947-0.978 if dust storm areas determined by our visual
        inspection are assumed to be ground truth. Precision, recall,
        and F-measure for dust storm are 0.88, 0.84, and 0.86,
        respectively, if we remove false negative pixels efficiently and
        maintain the size of true positive dust storms using two
        different threshold values. The tuning parameters of the
        classifier used in this study are determined so that the
        accuracy for dust storm is maximized. We can also tune the
        classifier for cloud segmentation by changing the parameters.}",
          doi = {10.1186/s40645-019-0266-1},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PEPS....6...19G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.483.1295U,
       author = {{Ucci}, G. and {Ferrara}, A. and {Gallerani}, S. and {Pallottini}, A. and
         {Cresci}, G. and {Kehrig}, C. and {Hunt}, L.~K. and {Vilchez}, J.~M. and
         {Vanzi}, L.},
        title = "{The interstellar medium of dwarf galaxies: new insights from Machine Learning analysis of emission-line spectra}",
      journal = {\mnras},
     keywords = {galaxies: abundances, galaxies: evolution, galaxies: individual: He 2-10, galaxies: individual: IZw18, galaxies: ISM, galaxies: star formation, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Feb",
       volume = {483},
       number = {1},
        pages = {1295-1313},
     abstract = "{Dwarf galaxies are ideal laboratories to study the physics of the
        interstellar medium (ISM). Emission lines have been widely used
        to this aim. Retrieving the full information encoded in the
        spectra is therefore essential. This can be efficiently and
        reliably done using Machine Learning (ML) algorithms. Here, we
        apply the ML code GAME to MUSE (Multi Unit Spectroscopic
        Explorer) and PMAS (Potsdam Multi Aperture Spectrophotometer)
        integral field unit observations of two nearby blue compact
        galaxies: Henize 2-10 and IZw18. We derive spatially resolved
        maps of several key ISM physical properties. We find that both
        galaxies show a remarkably uniform metallicity distribution.
        Henize 2-10 is a star-forming-dominated galaxy, with a star
        formation rate (SFR) of about 1.2 M$_{☉}$ yr$^{-1}$. Henize 2-10
        features dense and dusty (A$_{V}$ up to 5-7 mag) star-forming
        central sites. We find IZw18 to be very metal-poor (Z = 1/20
        Z$_{☉}$). IZw18 has a strong interstellar radiation field, with
        a large ionization parameter. We also use models of PopIII stars
        spectral energy distribution as a possible ionizing source for
        the He II {\ensuremath{\lambda}}4686 emission detected in the
        IZw18 NW component. We find that PopIII stars could provide a
        significant contribution to the line intensity. The upper limit
        to the PopIII star formation is 52 per cent of the total IZw18
        SFR.}",
          doi = {10.1093/mnras/sty2894},
archivePrefix = {arXiv},
       eprint = {1810.10548},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.483.1295U},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.482.5078A,
       author = {{Aguirre}, C. and {Pichara}, K. and {Becker}, I.},
        title = "{Deep multi-survey classification of variable stars}",
      journal = {\mnras},
     keywords = {light curves, variable stars, supervised classification, neural net, deep learning, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2019",
        month = "Feb",
       volume = {482},
       number = {4},
        pages = {5078-5092},
     abstract = "{During the last decade, a considerable amount of effort has been made to
        classify variable stars using different machine-learning
        techniques. Typically, light curves are represented as vectors
        of statistical descriptors or features that are used to train
        various algorithms. These features demand high computational
        power and can last from hours to days, making it impossible to
        create scalable and efficient ways of classifying variable stars
        automatically. Also, light curves from different surveys cannot
        be integrated and analysed together when using features, because
        of observational differences. For example, having variations in
        cadence and filters, feature distributions become biased and
        require expensive data-calibration models. The vast amount of
        data that will be generated soon makes it necessary to develop
        scalable machine-learning architectures without expensive
        integration techniques. Convolutional neural networks have shown
        impressive results in raw image classification and
        representation within the machine-learning literature. In this
        work, we present a novel deep-learning model for light-curve
        classification, based mainly on convolutional units. Our
        architecture receives as input the differences between the time
        and magnitude of light curves. It captures the essential
        classification patterns, regardless of cadence and filter. In
        addition, we introduce a novel data augmentation schema for
        unevenly sampled time series. We tested our method using three
        different surveys: the Optical Gravitational Lensing Experiment
        (OGLE-III), the Visible and Infrared Survey Telescope (VISTA)
        and Convection, Rotation and planetary Transit (CoRot) which
        differ in filters, cadence and area of the sky. We show that,
        besides the benefit of scalability, our model obtains state-of-
        the-art level accuracy in light-curve classification benchmarks.}",
          doi = {10.1093/mnras/sty2836},
archivePrefix = {arXiv},
       eprint = {1810.09440},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.482.5078A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.482.4503G,
       author = {{Goyal}, Jayesh M. and {Wakeford}, Hannah R. and {Mayne}, Nathan J. and
         {Lewis}, Nikole K. and {Drummond}, Benjamin and {Sing}, David K.},
        title = "{Fully scalable forward model grid of exoplanet transmission spectra}",
      journal = {\mnras},
     keywords = {planets and satellites: atmospheres, planets and satellites: composition, techniques: spectroscopic, Astrophysics - Earth and Planetary Astrophysics},
         year = "2019",
        month = "Feb",
       volume = {482},
       number = {4},
        pages = {4503-4513},
     abstract = "{Simulated exoplanet transmission spectra are critical for planning and
        interpretation of observations and to explore the sensitivity of
        spectral features to atmospheric thermochemical processes. We
        present a publicly available generic model grid of planetary
        transmission spectra, scalable to a wide range of H$_{2}$/He
        dominated atmospheres. The grid is computed using the 1D/2D
        atmosphere model ATMO for two different chemical scenarios,
        first considering local condensation only, secondly considering
        global condensation and removal of species from the atmospheric
        column (rainout). The entire grid consists of 56 320 model
        simulations across 22 equilibrium temperatures (400-2600 K),
        four planetary gravities (5-50 ms$^{-2}$), five atmospheric
        metallicities (1x-200x), four C/O ratios (0.35-1.0), four
        scattering haze parameters, four uniform cloud parameters, and
        two chemical scenarios. We derive scaling equations which can be
        used with this grid, for a wide range of planet-star
        combinations. We validate this grid by comparing it with other
        model transmission spectra available in the literature. We
        highlight some of the important findings, such as the rise of
        SO$_{2}$ features at 100x solar metallicity, differences in
        spectral features at high C/O ratios between two condensation
        approaches, the importance of VO features without TiO to
        constrain the limb temperature and features of TiO/VO both, to
        constrain the condensation processes. Finally, this generic grid
        can be used to plan future observations using the HST, VLT,
        JWST, and various other telescopes. The fine variation of
        parameters in the grid also allows it to be incorporated in a
        retrieval framework, with various machine learning techniques.}",
          doi = {10.1093/mnras/sty3001},
archivePrefix = {arXiv},
       eprint = {1810.12971},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.482.4503G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019JSWSC...9A..12B,
       author = {{Bhaskar}, Ankush and {Vichare}, Geeta},
        title = "{Forecasting of SYMH and ASYH indices for geomagnetic storms of solar cycle 24 including St. Patrick's day, 2015 storm using NARX neural network}",
      journal = {Journal of Space Weather and Space Climate},
     keywords = {space weather forecasting, artificial neural network, machine learning, geomagnetic storm, ring current},
         year = "2019",
        month = "Feb",
       volume = {9},
          eid = {A12},
        pages = {A12},
     abstract = "{Artificial Neural Network (ANN) has proven to be very successful in
        forecasting a variety of irregular magnetospheric/ionospheric
        processes like geomagnetic storms and substorms. SYMH and ASYH
        indices represent longitudinal symmetric and the asymmetric
        component of the ring current. Here, an attempt is made to
        develop a prediction model for these indices using ANN. The ring
        current state depends on its past conditions therefore, it is
        necessary to consider its history for prediction. To account for
        this effect Nonlinear Autoregressive Network with exogenous
        inputs (NARX) is implemented. This network considers input
        history of 30 min and output feedback of 120 min. Solar wind
        parameters mainly velocity, density, and interplanetary magnetic
        field are used as inputs. SYMH and ASYH indices during
        geomagnetic storms of 1998-2013, having minimum SYMH \&lt; -85
        nT are used as the target for training two independent networks.
        We present the prediction of SYMH and ASYH indices during nine
        geomagnetic storms of solar cycle 24 including the recent
        largest storm occurred on St. Patrick's day, 2015. The present
        prediction model reproduces the entire time profile of SYMH and
        ASYH indices along with small variations of ̃10-30 min to the
        good extent within noise level, indicating a significant
        contribution of interplanetary sources and past state of the
        magnetosphere. Therefore, the developed networks can predict
        SYMH and ASYH indices about an hour before, provided, real-time
        upstream solar wind data are available. However, during the main
        phase of major storms, residuals (observed-modeled) are found to
        be large, suggesting the influence of internal factors such as
        magnetospheric processes.}",
          doi = {10.1051/swsc/2019007},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019JSWSC...9A..12B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019JCAP...02..044R,
       author = {{Rudelius}, Tom},
        title = "{Learning to inflate. A gradient ascent approach to random inflation}",
      journal = {Journal of Cosmology and Astro-Particle Physics},
     keywords = {High Energy Physics - Theory},
         year = "2019",
        month = "Feb",
       volume = {2019},
       number = {2},
          eid = {044},
        pages = {044},
     abstract = "{Motivated by machine learning, we introduce a novel method for randomly
        generating inflationary potentials. Namely, we treat the Taylor
        coefficients of the potential as weights in a single-layer
        neural network and use gradient ascent to maximize the number of
        e-folds of inflation. Inflationary potentials ``learned'' in
        this way develop a critical point, which is typically a local
        maximum but may also be an inflection point. We study the
        phenomenology of the models along the gradient ascent
        trajectory, finding substantial agreement with experiment for
        large-field local maximum models and small-field inflection
        point models. For two-field models of inflation, the potential
        eventually learns a genuine multi-field model in which the
        inflaton curves significantly during the course of its descent.}",
          doi = {10.1088/1475-7516/2019/02/044},
archivePrefix = {arXiv},
       eprint = {1810.05159},
 primaryClass = {hep-th},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019JCAP...02..044R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019JASTP.183..110B,
       author = {{Bensafi}, Noureddine and {Lazri}, Mourad and {Ameur}, Soltane},
        title = "{Novel WkNN-based technique to improve instantaneous rainfall estimation over the north of Algeria using the multispectral MSG SEVIRI imagery}",
      journal = {Journal of Atmospheric and Solar-Terrestrial Physics},
     keywords = {Rainfall estimation, Machine learning, WkNN, SEVIRI, MSG},
         year = "2019",
        month = "Feb",
       volume = {183},
        pages = {110-119},
     abstract = "{For the estimation of rainfall in northern Algeria, a new method is
        proposed in this study. It based on the k nearest weighted
        neighbours (WkNN) classification algorithm using the dataset
        from the radiometer-imager SEVIRI (Spinning Enhanced Visible and
        Infra-Red Imager) boarded on the satellite MSG (Meteosat Second
        Generation) to determine the pixel rainfall rate among the 16
        predefined intensity levels observed in the S{\'e}tif
        meteorological radar. The implementation of the classification
        consists in using the spectral characteristics of a new sample
        (pixel) as input variables of the WkNN classifier to predict his
        class of membership based on the weighted distances separating
        him from the samples of the set of learning. The results thus
        obtained are validated with respect to the rainfall intensity
        classes observed and co-located by ground radar. The results
        showed a significant improvement for instantaneous estimate of
        precipitation during the day, with a correlation coefficient r =
        0.87 and statistical parameters RMSE = 2.29 mm / h , Bias = 0.45
        mm / h , MAE = 0.35 mm / h , and we have obtained the results
        similar overnight.}",
          doi = {10.1016/j.jastp.2018.12.004},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019JASTP.183..110B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019GeoRL..46.2111C,
       author = {{Cooley}, Sarah W. and {Smith}, Laurence C. and {Ryan}, Jonathan C. and
         {Pitcher}, Lincoln H. and {Pavelsky}, Tamlin M.},
        title = "{Arctic-Boreal Lake Dynamics Revealed Using CubeSat Imagery}",
      journal = {\grl},
     keywords = {CubeSats, remote sensing, Arctic-Boreal lakes, arctic hydrology, machine learning},
         year = "2019",
        month = "Feb",
       volume = {46},
       number = {4},
        pages = {2111-2120},
     abstract = "{Fine-scale, subseasonal fluctuations in Arctic-Boreal surface water
        reflect regional water balance and modulate trace gas emissions
        to the atmosphere but have eluded detection using traditional
        satellite remote sensing. We use high-resolution ( 3-5 m), high-
        frequency CubeSat sensors to measure near-daily changes in lake
        surface area through an object-based tracking method that
        incorporates machine learning to overcome notable limitations of
        CubeSat imagery. From 76,000 images we obtain \&gt;2.2 million
        individual observations of changing surface areas for 85,358
        lakes in Northern Canada and Alaska between 1 May and 1 October
        2017. We find broad-scale lake area declines across diverse
        climatic, hydrologic, and physiographic terrains. Localized
        exceptions reveal lowland flooding and aquatic vegetation
        phenology cycles. Cumulative small shoreline changes of abundant
        lakes on the Canadian Shield exceed total inundation variations
        of better-studied lowland environments, revealing a surprisingly
        dynamic landscape with respect to subseasonal variations in
        surface water extent and trace gas emissions.}",
          doi = {10.1029/2018GL081584},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019GeoRL..46.2111C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019GeoRL..46.1449Y,
       author = {{Yu}, L. and {Wen}, J. and {Chang}, C.~Y. and {Frankenberg}, C. and
         {Sun}, Y.},
        title = "{High-Resolution Global Contiguous SIF of OCO-2}",
      journal = {\grl},
     keywords = {SIF, OCO-2, GPP, Machine Learning, Carbon Cycle},
         year = "2019",
        month = "Feb",
       volume = {46},
       number = {3},
        pages = {1449-1458},
     abstract = "{The Orbiting Carbon Observatory-2 (OCO-2) collects solar-induced
        chlorophyll fluorescence (SIF) at high spatial resolution along
        orbits (SIF\&gt;̄$_{oco2\_orbit}$), but its discontinuous
        spatial coverage precludes its full potential for understanding
        the mechanistic SIF-photosynthesis relationship. This study
        developed a spatially contiguous global OCO-2 SIF product at
        0.05{\textdegree} and 16-day resolutions
        (SIF\&gt;̄$_{oco2\_005}$) using machine learning constrained by
        physiological understandings. This was achieved by stratifying
        biomes and times for training and predictions, which accounts
        for varying plant physiological properties in space and time.
        SIF\&gt;̄$_{oco2\_005}$ accurately preserved the spatiotemporal
        variations of SIF\&gt;̄$_{oco2\_orbit}$ across the globe.
        Validation of SIF\&gt;̄$_{oco2\_005}$ with Chlorophyll
        Fluorescence Imaging Spectrometer airborne measurements revealed
        striking consistency (R$^{2}$ = 0.72; regression slope = 0.96).
        Further, without time and biome stratification, (1)
        SIF\&gt;̄$_{oco2\_005}$ of croplands, deciduous temperate, and
        needleleaf forests would be underestimated during the peak
        season, (2) SIF\&gt;̄$_{oco2\_005}$ of needleleaf forests would
        be overestimated during autumn, and (3) the capability of
        SIF\&gt;̄$_{oco2\_005}$ to detect drought would be diminished.}",
          doi = {10.1029/2018GL081109},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019GeoRL..46.1449Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019GeoRL..46.1303C,
       author = {{Corbi}, F. and {Sandri}, L. and {Bedford}, J. and {Funiciello}, F. and
         {Brizzi}, S. and {Rosenau}, M. and {Lallemand}, S.},
        title = "{Machine Learning Can Predict the Timing and Size of Analog Earthquakes}",
      journal = {\grl},
     keywords = {megathrust earthquakes, machine learning, analog modeling, slip deficit},
         year = "2019",
        month = "Feb",
       volume = {46},
       number = {3},
        pages = {1303-1311},
     abstract = "{Despite the growing spatiotemporal density of geophysical observations
        at subduction zones, predicting the timing and size of future
        earthquakes remains a challenge. Here we simulate multiple
        seismic cycles in a laboratory-scale subduction zone. The model
        creates both partial and full margin ruptures, simulating
        magnitude M$_{w}$ 6.2-8.3 earthquakes with a coefficient of
        variation in recurrence intervals of 0.5, similar to real
        subduction zones. We show that the common procedure of
        estimating the next earthquake size from slip-deficit is
        unreliable. On the contrary, machine learning predicts well the
        timing and size of laboratory earthquakes by reconstructing and
        properly interpreting the spatiotemporally complex loading
        history of the system. These results promise substantial
        progress in real earthquake forecasting, as they suggest that
        the complex motion recorded by geodesists at subduction zones
        might be diagnostic of earthquake imminence.}",
          doi = {10.1029/2018GL081251},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019GeoRL..46.1303C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019ApJS..240...34M,
       author = {{Ma}, Zhixian and {Xu}, Haiguang and {Zhu}, Jie and {Hu}, Dan and
         {Li}, Weitian and {Shan}, Chenxi and {Zhu}, Zhenghao and {Gu}, Liyi and
         {Li}, Jinjin and {Liu}, Chengze and {Wu}, Xiangping},
        title = "{A Machine Learning Based Morphological Classification of 14,245 Radio AGNs Selected from the Best-Heckman Sample}",
      journal = {\apjs},
     keywords = {catalogs, galaxies: statistics, methods: data analysis, radio continuum: galaxies, techniques: miscellaneous, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Feb",
       volume = {240},
       number = {2},
          eid = {34},
        pages = {34},
     abstract = "{We present a morphological classification of 14,245 radio active
        galactic nuclei (AGNs) into six types, i.e., typical Fanaroff-
        Riley Class I/II (FRI/II), FRI/II-like bent-tailed, X-shaped
        radio galaxy, and ringlike radio galaxy, by designing a
        convolutional neural network based autoencoder, namely MCRGNet,
        and applying it to a labeled radio galaxy (LRG) sample
        containing 1442 AGNs and an unlabeled radio galaxy (unLRG)
        sample containing 14,245 unlabeled AGNs selected from the Best-
        Heckman sample. We train MCRGNet and implement the
        classification task by a three-step strategy, i.e., pre-
        training, fine-tuning, and classification, which combines both
        unsupervised and supervised learnings. A four-layer dichotomous
        tree is designed to classify the radio AGNs, which leads to a
        significantly better performance than the direct six-type
        classification. On the LRG sample, our MCRGNet achieves a total
        precision of ̃93\% and an averaged sensitivity of ̃87\%, which
        are better than those obtained in previous works. On the unLRG
        sample, whose labels have been human-inspected, the neural
        network achieves a total precision of ̃80\%. Also, using Sloan
        Digital Sky Survey Data Release 7 to calculate the r-band
        absolute magnitude (M $_{opt}$) and using the flux densities to
        calculate the radio luminosity (L $_{radio}$), we find that the
        distributions of the unLRG sources on the L $_{radio}$-M
        $_{opt}$ plane do not show an apparent redshift evolution and
        could confirm with a sufficiently large sample that there could
        not exist an abrupt separation between FRIs and FRIIs as
        reported in some previous works.}",
          doi = {10.3847/1538-4365/aaf9a2},
archivePrefix = {arXiv},
       eprint = {1812.07190},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019ApJS..240...34M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019ApJ...872..189K,
       author = {{Kang}, Shi-Ju and {Fan}, Jun-Hui and {Mao}, Weiming and {Wu}, Qingwen and
         {Feng}, Jianchao and {Yin}, Yue},
        title = "{Evaluating the Optical Classification of Fermi BCUs Using Machine Learning}",
      journal = {\apj},
     keywords = {BL Lacertae objects: general, gamma rays: galaxies, methods: statistical, quasars: general, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2019",
        month = "Feb",
       volume = {872},
       number = {2},
          eid = {189},
        pages = {189},
     abstract = "{In the third catalog of active galactic nuclei detected by the Fermi-LAT
        (3LAC) Clean Sample, there are 402 blazar candidates of
        uncertain type (BCUs). Due to the limitations of astronomical
        observation or intrinsic properties, it is difficult to classify
        blazars using optical spectroscopy. The potential classification
        of BCUs using machine-learning algorithms is essential. Based on
        the 3LAC Clean Sample, we collect 1420 Fermi blazars with eight
        parameters of {\ensuremath{\gamma}}-ray photon spectral index;
        radio flux; flux density; curve significance; the integral
        photon flux in 100-300 MeV, 0.3-1 GeV, and 10-100 GeV; and
        variability index. Here we apply four different supervised
        machine-learning (SML) algorithms (decision trees, random
        forests, support vector machines, and Mclust Gaussian finite
        mixture models) to evaluate the classification of BCUs based on
        the direct observational properties. All four methods can
        perform exceedingly well with more accuracy and can effectively
        forecast the classification of Fermi BCUs. The evaluating
        results show that the results of these methods (SML) are valid
        and robust, where about one-fourth of sources are flat-spectrum
        radio quasars (FSRQs) and three-fourths are BL Lacertae (BL
        Lacs) in 400 BCUs, which are consistent with some other recent
        results. Although a number of factors influence the accuracy of
        SML, the results are stable at a fixed ratio 1:3 between FSRQs
        and BL Lacs, which suggests that the SML can provide an
        effective method to evaluate the potential classification of
        BCUs. Among the four methods, Mclust Gaussian Mixture Modeling
        has the highest accuracy for our training sample (4/5, seed =
        123).}",
          doi = {10.3847/1538-4357/ab0383},
archivePrefix = {arXiv},
       eprint = {1902.07717},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019ApJ...872..189K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019A&A...622A.137B,
       author = {{Bonjean}, V. and {Aghanim}, N. and {Salom{\'e}}, P. and {Beelen}, A. and
         {Douspis}, M. and {Soubri{\'e}}, E.},
        title = "{Star formation rates and stellar masses from machine learning}",
      journal = {\aap},
     keywords = {methods: data analysis, galaxies: star formation, galaxies: evolution, large-scale structure of Universe, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2019",
        month = "Feb",
       volume = {622},
          eid = {A137},
        pages = {A137},
     abstract = "{Star-formation activity is a key property to probe the structure
        formation and hence characterise the large-scale structures of
        the universe. This information can be deduced from the star
        formation rate (SFR) and the stellar mass
        (M$_{{\ensuremath{\star}}}$), both of which, but especially the
        SFR, are very complex to estimate. Determining these quantities
        from UV, optical, or IR luminosities relies on complex modeling
        and on priors on galaxy types. We propose a method based on the
        machine-learning algorithm Random Forest to estimate the SFR and
        the M$_{{\ensuremath{\star}}}$ of galaxies at redshifts in the
        range 0.01 \&lt; z \&lt; 0.3, independent of their type. The
        machine-learning algorithm takes as inputs the redshift, WISE
        luminosities, and WISE colours in near-IR, and is trained on
        spectra-extracted SFR and M$_{{\ensuremath{\star}}}$ from the
        SDSS MPA-JHU DR8 catalogue as outputs. We show that our
        algorithm can accurately estimate SFR and
        M$_{{\ensuremath{\star}}}$ with scatters of
        {\ensuremath{\sigma}}$_{SFR}$ = 0.38 dex and
        {\ensuremath{\sigma}}$_{M<SUB>{\ensuremath{\star}}}$</SUB> =
        0.16 dex for SFR and stellar mass, respectively, and that it is
        unbiased with respect to redshift or galaxy type. The full-sky
        coverage of the WISE satellite allows us to characterise the
        star-formation activity of all galaxies outside the Galactic
        mask with spectroscopic redshifts in the range 0.01 \&lt; z
        \&lt; 0.3. The method can also be applied to photometric-
        redshift catalogues, with best scatters of
        {\ensuremath{\sigma}}$_{SFR}$ = 0.42 dex and
        {\ensuremath{\sigma}}$_{M<SUB>{\ensuremath{\star}}}$</SUB> =
        0.24 dex obtained in the redshift range 0.1 \&lt; z \&lt; 0.3.}",
          doi = {10.1051/0004-6361/201833972},
archivePrefix = {arXiv},
       eprint = {1901.01932},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019A&A...622A.137B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019A&A...622A.124A,
       author = {{Alshehhi}, Rasha and {Hanson}, Chris S. and {Gizon}, Laurent and
         {Hanasoge}, Shravan},
        title = "{Supervised neural networks for helioseismic ring-diagram inversions}",
      journal = {\aap},
     keywords = {Sun: helioseismology, Sun: oscillations, Sun: interior, methods: numerical, Astrophysics - Solar and Stellar Astrophysics, Computer Science - Machine Learning},
         year = "2019",
        month = "Feb",
       volume = {622},
          eid = {A124},
        pages = {A124},
     abstract = "{Context. The inversion of ring fit parameters to obtain subsurface flow
        maps in ring-diagram analysis for eight years of SDO
        observations is computationally expensive, requiring ̃3200 CPU
        hours. <BR /> Aims: In this paper we apply machine-learning
        techniques to the inversion step of the ring diagram pipeline in
        order to speed up the calculations. Specifically, we train a
        predictor for subsurface flows using the mode fit parameters and
        the previous inversion results to replace future inversion
        requirements. <BR /> Methods: We utilize artificial neural
        networks (ANNs) as a supervised learning method for predicting
        the flows in 15{\textdegree} ring tiles. We discuss each step of
        the proposed method to determine the optimal approach. In order
        to demonstrate that the machine-learning results still contain
        the subtle signatures key to local helioseismic studies, we use
        the machine-learning results to study the recently discovered
        solar equatorial Rossby waves. <BR /> Results: The ANN is
        computationally efficient, able to make future flow predictions
        of an entire Carrington rotation in a matter of seconds, which
        is much faster than the current ̃31 CPU hours. Initial training
        of the networks requires ̃3 CPU hours. The trained ANN can
        achieve a rms error equal to approximately half that reported
        for the velocity inversions, demonstrating the accuracy of the
        machine learning (and perhaps the overestimation of the original
        errors from the ring-diagram pipeline). We find the signature of
        equatorial Rossby waves in the machine-learning flows covering
        six years of data, demonstrating that small-amplitude signals
        are maintained. The recovery of Rossby waves in the machine-
        learning flow maps can be achieved with only one Carrington
        rotation (27.275 days) of training data. <BR /> Conclusions: We
        show that machine learning can be applied to and perform more
        efficiently than the current ring-diagram inversion. The
        computation burden of the machine learning includes 3 CPU hours
        for initial training, then around 10$^{-4}$ CPU hours for future
        predictions.}",
          doi = {10.1051/0004-6361/201834237},
archivePrefix = {arXiv},
       eprint = {1901.01505},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019A&A...622A.124A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PhRvD..99b4024M,
       author = {{Mytidis}, Antonis and {Panagopoulos}, Athanasios Aris and
         {Panagopoulos}, Orestis P. and {Miller}, Andrew and {Whiting}, Bernard},
        title = "{Sensitivity study using machine learning algorithms on simulated r -mode gravitational wave signals from newborn neutron stars}",
      journal = {\prd},
         year = "2019",
        month = "Jan",
       volume = {99},
       number = {2},
          eid = {024024},
        pages = {024024},
     abstract = "{This is a follow-up sensitivity study on r -mode gravitational wave
        signals from newborn neutron stars illustrating the
        applicability of machine learning algorithms for the detection
        of long-lived gravitational wave transients. In this sensitivity
        study, we examine three machine learning algorithms (MLAs):
        artificial neural networks, support vector machines, and
        constrained subspace classifiers. The objective of this study is
        to compare the detection efficiencies that MLAs can achieve to
        the efficiency of the conventional (seedless clustering)
        detection algorithm discussed in an earlier paper. Comparisons
        are made using two distinct r -mode waveforms. For the training
        of the MLAs, we assumed that some information about the distance
        to the source is given so that the training was performed over
        distance ranges not wider than half an order of magnitude. The
        results of this study suggest that we can use the machine
        learning algorithms as part of an investigative stage in the
        pipeline that would be able to provide very fast and solid
        triggers for further, and more intense, investigation.}",
          doi = {10.1103/PhysRevD.99.024024},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PhRvD..99b4024M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2019PhDT.........1H,
       author = {{Hare}, Jeremy},
        title = "{Search, Identification, and Study of Galactic Compact Objects: Methods, Environments, and Populations}",
     keywords = {Astrophysics;Physics},
       school = {The George Washington University},
         year = "2019",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PhDT.........1H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019PASP..131a8003M,
       author = {{Masci}, Frank J. and {Laher}, Russ R. and {Rusholme}, Ben and
         {Shupe}, David L. and {Groom}, Steven and {Surace}, Jason and
         {Jackson}, Edward and {Monkewitz}, Serge and {Beck}, Ron and
         {Flynn}, David and {Terek}, Scott and {Landry}, Walter and
         {Hacopians}, Eugean and {Desai}, Vandana and {Howell}, Justin and
         {Brooke}, Tim and {Imel}, David and {Wachter}, Stefanie and
         {Ye}, Quan-Zhi and {Lin}, Hsing-Wen and {Cenko}, S. Bradley and
         {Cunningham}, Virginia and {Rebbapragada}, Umaa and {Bue}, Brian and
         {Miller}, Adam A. and {Mahabal}, Ashish and {Bellm}, Eric C. and
         {Patterson}, Maria T. and {Juri{\'c}}, Mario and {Golkhou}, V. Zach and
         {Ofek}, Eran O. and {Walters}, Richard and {Graham}, Matthew and
         {Kasliwal}, Mansi M. and {Dekany}, Richard G. and {Kupfer}, Thomas and
         {Burdge}, Kevin and {Cannella}, Christopher B. and {Barlow}, Tom and
         {Van Sistine}, Angela and {Giomi}, Matteo and {Fremling}, Christoffer and
         {Blagorodnova}, Nadejda and {Levitan}, David and {Riddle}, Reed and
         {Smith}, Roger M. and {Helou}, George and {Prince}, Thomas A. and
         {Kulkarni}, Shrinivas R.},
        title = "{The Zwicky Transient Facility: Data Processing, Products, and Archive}",
      journal = {\pasp},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2019",
        month = "Jan",
       volume = {131},
       number = {995},
        pages = {018003},
     abstract = "{The Zwicky Transient Facility (ZTF) is a new robotic time-domain survey
        currently in progress using the Palomar 48-inch Schmidt
        Telescope. ZTF uses a 47 square degree field with a 600
        megapixel camera to scan the entire northern visible sky at
        rates of {\ensuremath{\sim}}3760 square degrees/hour to median
        depths of g {\ensuremath{\sim}} 20.8 and r {\ensuremath{\sim}}
        20.6 mag (AB, 5{\ensuremath{\sigma}} in 30 sec). We describe the
        Science Data System that is housed at IPAC, Caltech. This
        comprises the data-processing pipelines, alert production
        system, data archive, and user interfaces for accessing and
        analyzing the products. The real-time pipeline employs a novel
        image-differencing algorithm, optimized for the detection of
        point-source transient events. These events are vetted for
        reliability using a machine-learned classifier and combined with
        contextual information to generate data-rich alert packets. The
        packets become available for distribution typically within 13
        minutes (95th percentile) of observation. Detected events are
        also linked to generate candidate moving-object tracks using a
        novel algorithm. Objects that move fast enough to streak in the
        individual exposures are also extracted and vetted. We present
        some preliminary results of the calibration performance
        delivered by the real-time pipeline. The reconstructed
        astrometric accuracy per science image with respect to Gaia DR1
        is typically 45 to 85 milliarcsec. This is the RMS per-axis on
        the sky for sources extracted with photometric S/N
        {\ensuremath{\geq}} 10 and hence corresponds to the typical
        astrometric uncertainty down to this limit. The derived
        photometric precision (repeatability) at bright unsaturated
        fluxes varies between 8 and 25 millimag. The high end of these
        ranges corresponds to an airmass approaching
        {\ensuremath{\sim}}2{\textemdash}the limit of the public survey.
        Photometric calibration accuracy with respect to Pan-STARRS1 is
        generally better than 2\%. The products support a broad range of
        scientific applications: fast and young supernovae; rare flux
        transients; variable stars; eclipsing binaries; variability from
        active galactic nuclei; counterparts to gravitational wave
        sources; a more complete census of Type Ia supernovae; and
        solar-system objects.}",
          doi = {10.1088/1538-3873/aae8ac},
archivePrefix = {arXiv},
       eprint = {1902.01872},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019PASP..131a8003M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019MNRAS.482.3116A,
       author = {{Amaro}, V. and {Cavuoti}, S. and {Brescia}, M. and {Vellucci}, C. and
         {Longo}, G. and {Bilicki}, M. and {de Jong}, J.~T.~A. and
         {Tortora}, C. and {Radovich}, M. and {Napolitano}, N.~R. and
         {Buddelmeijer}, H.},
        title = "{Statistical analysis of probability density functions for photometric redshifts through the KiDS-ESO-DR3 galaxies}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, galaxies: distances and redshifts, galaxies: photometry, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Jan",
       volume = {482},
       number = {3},
        pages = {3116-3134},
     abstract = "{Despite the high accuracy of photometric redshifts (zphot) derived using
        machine learning (ML) methods, the quantification of errors
        through reliable and accurate probability density functions
        (PDFs) is still an open problem. First, because it is difficult
        to accurately assess the contribution from different sources of
        errors, namely internal to the method itself and from the
        photometric features defining the available parameter space.
        Secondly, because the problem of defining a robust statistical
        method, always able to quantify and qualify the PDF estimation
        validity, is still an open issue. We present a comparison among
        PDFs obtained using three different methods on the same data
        set: two ML techniques, METAPHOR (Machine-learning Estimation
        Tool for Accurate PHOtometric Redshifts) and ANNz2 , plus the
        spectral energy distribution template-fitting method, BPZ
        (Bayesian photometric redshift). The photometric data were
        extracted from the Kilo Degree Survey ESO Data Release 3, while
        the spectroscopy was obtained from the Galaxy and Mass Assembly
        Data Release 2. The statistical evaluation of both individual
        and stacked PDFs was done through quantitative and qualitative
        estimators, including a dummy PDF, useful to verify whether
        different statistical estimators can correctly assess PDF
        quality. We conclude that, in order to quantify the reliability
        and accuracy of any zphot PDF method, a combined set of
        statistical estimators is required.}",
          doi = {10.1093/mnras/sty2922},
archivePrefix = {arXiv},
       eprint = {1810.09777},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019MNRAS.482.3116A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019ApJ...871...94K,
       author = {{Kaur}, A. and {Ajello}, M. and {Marchesi}, S. and {Omodei}, N.},
        title = "{Identifying the 3FHL Catalog. I. Archival Swift Observations and Source Classification}",
      journal = {\apj},
     keywords = {catalogs, galaxies: active, X-rays: general, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2019",
        month = "Jan",
       volume = {871},
       number = {1},
          eid = {94},
        pages = {94},
     abstract = "{We present the results of an identification campaign of unassociated
        sources from the Fermi Large Area Telescope 3FHL catalog. Out of
        200 unidentified sources, we selected 110 sources for which
        archival Swift-XRT observations were available, 52 of which were
        found to have exactly one X-ray counterpart within the 3FHL 95\%
        positional uncertainty. In this work, we report the X-ray,
        optical, IR, and radio properties of these 52 sources using
        positional associations with objects in various catalogs. The
        Wide-field Infrared Survey Explorer color-color plot for sources
        suggests that most of these belong to the blazar class family.
        The redshift measurements for these objects range from z = 0.277
        to z = 2.1. Additionally, under the assumption that the majority
        of these sources are blazars, three machine-learning algorithms
        are employed to classify the sample into flat spectrum radio
        quasars or BL Lacertae objects. These suggest that the majority
        of the previously unassociated sources are BL Lac objects, in
        agreement with the fact the BL Lac objects represent by far the
        most numerous population detected above 10 GeV in 3FHL.}",
          doi = {10.3847/1538-4357/aaf649},
archivePrefix = {arXiv},
       eprint = {1901.07596},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019ApJ...871...94K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019AJ....157...16R,
       author = {{Reis}, Itamar and {Baron}, Dalya and {Shahaf}, Sahar},
        title = "{Probabilistic Random Forest: A Machine Learning Algorithm for Noisy Data Sets}",
      journal = {\aj},
     keywords = {methods: data analysis, methods: statistical, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning},
         year = "2019",
        month = "Jan",
       volume = {157},
       number = {1},
          eid = {16},
        pages = {16},
     abstract = "{Machine learning (ML) algorithms have become increasingly important in
        the analysis of astronomical data. However, because most ML
        algorithms are not designed to take data uncertainties into
        account, ML-based studies are mostly restricted to data with
        high signal-to-noise ratios. Astronomical data sets of such high
        quality are uncommon. In this work, we modify the long-
        established Random Forest (RF) algorithm to take into account
        uncertainties in measurements (i.e., features) as well as in
        assigned classes (i.e., labels). To do so, the Probabilistic
        Random Forest (PRF) algorithm treats the features and labels as
        probability distribution functions, rather than deterministic
        quantities. We perform a variety of experiments where we inject
        different types of noise into a data set and compare the
        accuracy of the PRF to that of RF. The PRF outperforms RF in all
        cases, with a moderate increase in running time. We find an
        improvement in classification accuracy of up to 10\% in the case
        of noisy features, and up to 30\% in the case of noisy labels.
        The PRF accuracy decreased by less then 5\% for a data set with
        as many as 45\% misclassified objects, compared to a clean data
        set. Apart from improving the prediction accuracy in noisy data
        sets, the PRF naturally copes with missing values in the data,
        and outperforms RF when applied to a data set with different
        noise characteristics in the training and test sets, suggesting
        that it can be used for transfer learning.}",
          doi = {10.3847/1538-3881/aaf101},
archivePrefix = {arXiv},
       eprint = {1811.05994},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019AJ....157...16R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019AJ....157....9B,
       author = {{Bai}, Yu and {Liu}, JiFeng and {Wang}, Song and {Yang}, Fan},
        title = "{Machine Learning Applied to Star-Galaxy-QSO Classification and Stellar Effective Temperature Regression}",
      journal = {\aj},
     keywords = {methods: data analysis, stars: fundamental parameters, techniques: photometric, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2019",
        month = "Jan",
       volume = {157},
       number = {1},
          eid = {9},
        pages = {9},
     abstract = "{In modern astrophysics, machine learning has increasingly gained
        popularity with its incredibly powerful ability to make
        predictions or calculated suggestions for large amounts of data.
        We describe an application of the supervised machine-learning
        algorithm, random forests (RF), to the star/galaxy/QSO
        classification and the stellar effective temperature regression
        based on the combination of Large Sky Area Multi-Object Fiber
        Spectroscopic Telescope and Sloan Digital Sky Survey
        spectroscopic data. This combination enables us to obtain
        reliable predictions with one of the largest training samples
        ever used. The training samples are built with a nine-color data
        set of about three million objects for the classification and a
        seven-color data set of over one million stars for the
        regression. The performance of the classification and regression
        is examined with validation and blind tests on the objects in
        the RAdial Velocity Extension, 6dFGS, UV-bright Quasar Survey
        and Apache Point Observatory Galactic Evolution Experiment
        surveys. We demonstrate that RF is an effective algorithm, with
        classification accuracies higher than 99\% for stars and
        galaxies, and higher than 94\% for QSOs. These accuracies are
        higher than machine-learning results in former studies. The
        total standard deviations of the regression are smaller than 200
        K, which is similar to those of some spectrum-based methods. The
        machine-learning algorithm with the broad-band photometry
        provides us with a more efficient approach for dealing with
        massive amounts of astrophysical data than do traditional color
        cuts and spectral energy distribution fits.}",
          doi = {10.3847/1538-3881/aaf009},
archivePrefix = {arXiv},
       eprint = {1811.03740},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019AJ....157....9B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019A&A...621A..36T,
       author = {{Tewes}, M. and {Kuntzer}, T. and {Nakajima}, R. and {Courbin}, F. and
         {Hildebrandt}, H. and {Schrabback}, T.},
        title = "{Weak-lensing shear measurement with machine learning. Teaching artificial neural networks about feature noise}",
      journal = {\aap},
     keywords = {methods: data analysis, gravitational lensing: weak, cosmological parameters, Astrophysics - Cosmology and Nongalactic Astrophysics, Statistics - Machine Learning},
         year = "2019",
        month = "Jan",
       volume = {621},
          eid = {A36},
        pages = {A36},
     abstract = "{Cosmic shear, that is weak gravitational lensing by the large-scale
        matter structure of the Universe, is a primary cosmological
        probe for several present and upcoming surveys investigating
        dark matter and dark energy, such as Euclid or WFIRST. The probe
        requires an extremely accurate measurement of the shapes of
        millions of galaxies based on imaging data. Crucially, the shear
        measurement must address and compensate for a range of
        interwoven nuisance effects related to the instrument optics and
        detector, noise in the images, unknown galaxy morphologies,
        colors, blending of sources, and selection effects. This paper
        explores the use of supervised machine learning as a tool to
        solve this inverse problem. We present a simple architecture
        that learns to regress shear point estimates and weights via
        shallow artificial neural networks. The networks are trained on
        simulations of the forward observing process, and take
        combinations of moments of the galaxy images as inputs. A
        challenging peculiarity of the shear measurement task, in terms
        of machine learning applications, is the combination of the
        noisiness of the input features and the requirements on the
        statistical accuracy of the inverse regression. To address this
        issue, the proposed training algorithm minimizes bias over
        multiple realizations of individual source galaxies, reducing
        the sensitivity to properties of the overall sample of source
        galaxies. Importantly, an observational selection function of
        these source galaxies can be straightforwardly taken into
        account via the weights. We first introduce key aspects of our
        approach using toy-model simulations, and then demonstrate its
        potential on images mimicking Euclid data. Finally, we analyze
        images from the GREAT3 challenge, obtaining competitively low
        multiplicative and additive shear biases despite the use of a
        simple training set. We conclude that the further development of
        suited machine learning approaches is of high interest to meet
        the stringent requirements on the shear measurement in current
        and future surveys. We make a demonstration implementation of
        our technique publicly available. A copy of the code is also
        available at the CDS via anonymous ftp to <A href=``http://cdsar
        c.u-strasbg.fr/''>http://cdsarc.u-strasbg.fr</A>
        (ftp://130.79.128.5) or via <A href=``http://cdsarc.u-strasbg.fr
        /viz-bin/qcat?J/A+A/621/A36''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/621/A36</A>}",
          doi = {10.1051/0004-6361/201833775},
archivePrefix = {arXiv},
       eprint = {1807.02120},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019A&A...621A..36T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019A&A...621A..26P,
       author = {{Pasquet}, Johanna and {Bertin}, E. and {Treyer}, M. and {Arnouts}, S. and
         {Fouchez}, D.},
        title = "{Photometric redshifts from SDSS images using a convolutional neural network}",
      journal = {\aap},
     keywords = {galaxies: distances and redshifts, surveys, methods: data analysis, techniques: image processing, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2019",
        month = "Jan",
       volume = {621},
          eid = {A26},
        pages = {A26},
     abstract = "{We developed a deep convolutional neural network (CNN), used as a
        classifier, to estimate photometric redshifts and associated
        probability distribution functions (PDF) for galaxies in the
        Main Galaxy Sample of the Sloan Digital Sky Survey at z \&lt;
        0.4. Our method exploits all the information present in the
        images without any feature extraction. The input data consist of
        64 {\texttimes} 64 pixel ugriz images centered on the
        spectroscopic targets, plus the galactic reddening value on the
        line-of-sight. For training sets of 100k objects or more
        ({\ensuremath{\geq}}20\% of the database), we reach a dispersion
        {\ensuremath{\sigma}}$_{MAD}$ \&lt; 0.01, significantly lower
        than the current best one obtained from another machine learning
        technique on the same sample. The bias is lower than 10$^{-4}$,
        independent of photometric redshift. The PDFs are shown to have
        very good predictive power. We also find that the CNN redshifts
        are unbiased with respect to galaxy inclination, and that
        {\ensuremath{\sigma}}$_{MAD}$ decreases with the signal-to-noise
        ratio (S/N), achieving values below 0.007 for S/N \&gt; 100, as
        in the deep stacked region of Stripe 82. We argue that for most
        galaxies the precision is limited by the S/N of SDSS images
        rather than by the method. The success of this experiment at low
        redshift opens promising perspectives for upcoming surveys.}",
          doi = {10.1051/0004-6361/201833617},
archivePrefix = {arXiv},
       eprint = {1806.06607},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019A&A...621A..26P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2019A&A...621A..13V,
       author = {{Veljanoski}, Jovan and {Helmi}, Amina and {Breddels}, Maarten and
         {Posti}, Lorenzo},
        title = "{Leaves on trees: identifying halo stars with extreme gradient boosted trees}",
      journal = {\aap},
     keywords = {Galaxy: halo, Galaxy: kinematics and dynamics, Astrophysics - Astrophysics of Galaxies},
         year = "2019",
        month = "Jan",
       volume = {621},
          eid = {A13},
        pages = {A13},
     abstract = "{Context. Extended stellar haloes are a natural by-product of the
        hierarchical formation of massive galaxies like the Milky Way.
        If merging is a non-negligible factor in the growth of our
        Galaxy, evidence of such events should be encoded in its stellar
        halo. The reliable identification of genuine halo stars is a
        challenging task, however. <BR /> Aims: With the advent of the
        Gaia space telescope, we are ushered into a new era of Galactic
        astronomy. The first Gaia data release contains the positions,
        parallaxes, and proper motions for over two million stars,
        mostly in the solar neighbourhood. The second Gaia data release
        will enlarge this sample to over 1.5 billion stars, the
        brightest 5 million of which will have full phase-space
        information. Our aim for this paper is to develop a machine
        learning model for reliably identifying halo stars, even when
        their full phase-space information is not available. <BR />
        Methods: We use the Gradient Boosted Trees algorithm to build a
        supervised halo star classifier. The classifier is trained on a
        sample of stars extracted from the Gaia Universe Model Snapshot,
        which is also convolved with the errors of the public TGAS data,
        which is a subset of Gaia DR1, as well as with the expected
        uncertainties for the upcoming Gaia DR2 catalogue. We also
        trained our classifier on a dataset resulting from the cross-
        match between the TGAS and RAVE catalogues, where the halo stars
        are labelled in an entirely model-independent way. We then use
        this model to identify halo stars in TGAS. <BR /> Results: When
        full phase-space information is available and for Gaia DR2-like
        uncertainties, our classifier is able to recover 90\% of the
        halo stars with at most 30\% distance errors, in a completely
        unseen test set and with negligible levels of contamination.
        When line-of-sight velocity is not available, we recover 60\% of
        such halo stars, with less than 10\% contamination. When applied
        to the TGAS catalogue, our classifier detects 337 high
        confidence red giant branch halo stars. At first glance this
        number may seem small, however, it is consistent with the
        expectation from the models, given the uncertainties in the
        data. The large parallax errors are in fact the biggest
        limitation in our ability to identify a large number of halo
        stars in all the cases studied.}",
          doi = {10.1051/0004-6361/201732303},
archivePrefix = {arXiv},
       eprint = {1804.05245},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019A&A...621A..13V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018RRPRA..23..244V,
       author = {{Vavilova}, I.~B. and {Elyiv}, A.~A. and {Vasylenko}, M. Yu.},
        title = "{Behind the Zone of Avoidance of the Milky Way: what can we Restore by Direct and Indirect Methods?}",
      journal = {Russian Radio Physics and Radio Astronomy},
     keywords = {Zone of Avoidance, algorithm of darning the ZoA, Generative adversarial network, machine learning, galaxy clusters, galaxies, Milky Way, large-scale structure of the Universe},
         year = "2018",
        month = "Dec",
       volume = {23},
       number = {4},
        pages = {244-257},
     abstract = "{Purpose: to present a brief overview of methods for restoring the large-
        scale structure of the Universe behind the Zone of Avoidance
        (ZoA) of the Milky Way; to propose a new ``algorithm of darning
        the ZoA'' and new approach based on the Generative adversarial
        network (GAN) to recover galaxy distribution in the ZoA using
        optical surveys as an additional platform for programming the
        artificial neural networks. Design/methodology/approach: Due to
        the extensive monitoring observations in radio (DOGS project, in
        HI line), infrared (IRAS and 2MASS surveys), and X-ray spectral
        ranges, the ZoA has been decreased significantly in size and now
        the obscured part is about 10\% of the sky in the visible
        spectral range. The Cosmic Microwave Background (CMB)
        measurements showed a 180{\textdegree} asymmetry known as the
        dipole: despite the fact that the resulting vector lies within
        20{\textdegree} of the observed CMB dipole, the calculations
        remain highly ambiguous, partly because the galaxies in the ZoA
        are not taken into account and the concept of ``attractors''
        should be reconsidered. Hence, the analysis of the spatial
        distribution of galaxies and their groups in the regions
        surrounding and behind the ZoA of Milky Way remains a complex
        and unresolved problem, and estimation of the ``invisible''
        content of the spatial galaxy distribution, which is obscured by
        this absorption zone, becomes a highly actual one. Restoring the
        ZoA is possible by indirect methods (signal processing applied
        to obscured and incomplete data; Voronoi tessellation, etc.).
        These recovery methods, however, work only for large-scale
        structures in the ZoA; they are practically not sensitive to
        individual galaxies and small galaxy systems. We suggest the
        machine learning technique such as the GAN to apply for modeling
        the ``invisible'' spatial galaxy distribution behind the ZoA.
        Findings: We present ``the algorithm of darning the ZoA'' for
        dividing the real extragalactic surveys (e.g, the SDSS DR 14
        galaxy sample) on the slices by redshifts, stellar magnitudes,
        coordinates and other parameters to form a training sample, and
        the general GAN scheme for the ZoA filling. We discuss principal
        tasks to generate galaxy distributions and their properties in
        the ZoA from latent space of features and describe how the
        discriminative network will compare the obtained artificial
        survey with the real one and evaluate how it is a realistic one.
        Conclusions: The incompleteness of data depending on wavelengths
        indicates that there are steal not resolved problems such as the
        dynamics in the Local Group and the near Universe; the large-
        scale structure of the Universe in the sky region obscured by
        the Milky Way; the velocity flow fields towards the Great
        Attractor; the CMB dipole. Here, we propose a new ``algorithm of
        darning the ZoA'' and the general GAN scheme as an additional
        machine learning platform to recover a spatial distribution
        behind the ZoA of our Galaxy.}",
          doi = {10.15407/rpra23.04.244},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018RRPRA..23..244V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2018PhDT.......125B,
       author = {{Bellinger}, Earl Patrick},
        title = "{Inverse Problems in Asteroseismology}",
     keywords = {asteroseismology, machine learning},
       school = {Max Planck Institute for Solar System Research},
         year = "2018",
        month = "Dec",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhDT.......125B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PASP..130l8001T,
       author = {{Tachibana}, Yutaro and {Miller}, A.~A.},
        title = "{A Morphological Classification Model to Identify Unresolved PanSTARRS1 Sources: Application in the ZTF Real-time Pipeline}",
      journal = {\pasp},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Dec",
       volume = {130},
       number = {994},
        pages = {128001},
     abstract = "{In the era of large photometric surveys, the importance of automated and
        accurate classification is rapidly increasing. Specifically, the
        separation of resolved and unresolved sources in astronomical
        imaging is a critical initial step for a wide array of studies,
        ranging from Galactic science to large scale structure and
        cosmology. Here, we present our method to construct a large,
        deep catalog of point sources utilizing Pan-STARRS1 (PS1)
        3{\ensuremath{\pi}} survey data, which consists of
        {\ensuremath{\sim}}3 {\texttimes} {}10$^{9}$ sources with m
        {\ensuremath{\lesssim}} 23.5 mag. We develop a supervised
        machine-learning methodology, using the random forest (RF)
        algorithm, to construct the PS1 morphology model. We train the
        model using {\ensuremath{\sim}}5 {\texttimes} {}10$^{4}$ PS1
        sources with HST COSMOS morphological classifications and assess
        its performance using {\ensuremath{\sim}}4 {\texttimes}
        {}10$^{6}$ sources with Sloan Digital Sky Survey (SDSS) spectra
        and {\ensuremath{\sim}}2 {\texttimes} {}10$^{8}$ Gaia sources.
        We construct 11 {\textquotedblleft}white
        flux{\textquotedblright} features, which combine PS1 flux and
        shape measurements across five filters, to increase the signal-
        to-noise ratio relative to any individual filter. The RF model
        is compared to three alternative models, including the SDSS and
        PS1 photometric classification models, and we find that the RF
        model performs best. By number the PS1 catalog is dominated by
        faint sources (m {\ensuremath{\gtrsim}} 21 mag), and in this
        regime the RF model significantly outperforms the SDSS and PS1
        models. For time-domain surveys, identifying unresolved sources
        is crucial for inferring the Galactic or extragalactic origin of
        new transients. We have classified {\ensuremath{\sim}}1.5
        {\texttimes} {}10$^{9}$ sources using the RF model, and these
        results are used within the Zwicky Transient Facility real-time
        pipeline to automatically reject stellar sources from the
        extragalactic alert stream.}",
          doi = {10.1088/1538-3873/aae3d9},
archivePrefix = {arXiv},
       eprint = {1902.01935},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PASP..130l8001T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.481.4194M,
       author = {{Morice-Atkinson}, Xan and {Hoyle}, Ben and {Bacon}, David},
        title = "{Learning from the machine: interpreting machine learning algorithms for point- and extended-source classification}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, techniques: photometric, stars: statistics, galaxies: abundances, galaxies: statistics, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies, Physics - Data Analysis, Statistics and Probability},
         year = "2018",
        month = "Dec",
       volume = {481},
       number = {3},
        pages = {4194-4205},
     abstract = "{We investigate star-galaxy classification for astronomical surveys in
        the context of four methods enabling the interpretation of
        black-box machine learning systems. The first explores the
        decision boundaries as given by decision tree based methods,
        enabling the visualization of the classification categories.
        Secondly, we investigate how the Mutual Information based
        Transductive Feature Selection (MINT) algorithm can be used to
        perform feature preselection. If a small number of input
        features is required for the machine learning classification
        algorithm, feature preselection provides a method to determine
        which of the many possible input features should be selected.
        Third is the use of the tree-interpreter package to enable
        popular decision tree based ensemble methods to be opened,
        visualized, and understood. This is done by additional analysis
        of the tree-based model, determining not only which features are
        important to the model, but how important a feature is for a
        particular classification given its value. Lastly, we use
        decision boundaries from the model to revise an already existing
        method of classification, essentially asking the tree-based
        method where decision boundaries are best placed and defining a
        new classification method. We showcase these techniques by
        applying them to the problem of star-galaxy separation using
        data from the Sloan Digital Sky Survey (hereafter SDSS). We use
        the output of MINT and the ensemble methods to demonstrate how
        more complex decision boundaries improve star-galaxy
        classification accuracy over the standard SDSS frames approach
        (reducing misclassifications by up to \{{\ensuremath{\approx}}
        \}33\{\{ per cent\}\}).}",
          doi = {10.1093/mnras/sty2575},
archivePrefix = {arXiv},
       eprint = {1712.03970},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.481.4194M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.481.1624M,
       author = {{Mislis}, D. and {Pyrzas}, S. and {Alsubai}, K.~A.},
        title = "{TSARDI: a Machine Learning data rejection algorithm for transiting exoplanet light curves}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: photometric, Planetary Systems, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Earth and Planetary Astrophysics},
         year = "2018",
        month = "Dec",
       volume = {481},
       number = {2},
        pages = {1624-1630},
     abstract = "{We present TSARDI, an efficient rejection algorithm designed to improve
        the transit detection efficiency in data collected by large-
        scale surveys. TSARDI is based on the Machine Learning
        clustering algorithm DBSCAN, and its purpose is to serve as a
        robust and adaptable filter aiming to identify unwanted noise
        points left over from data detrending processes. TSARDI is an
        unsupervised method, which can treat each light curve
        individually; there is no need of previous knowledge of any
        other field light curves. We conduct a simulated transit search
        by injecting planets on real data obtained by the QES project
        and show that TSARDI leads to an overall transit detection
        efficiency increase of ̃11 per cent, compared to results
        obtained from the same sample, but using a standard sigma-clip
        algorithm. For the brighter end of our sample (host star
        magnitude \&lt;12), TSARDI achieves a detection efficiency of
        ̃80 per cent of injected planets. While our algorithm has been
        developed primarily for the field of exoplanets, it is easily
        adaptable and extendable for use in any time series.}",
          doi = {10.1093/mnras/sty2361},
archivePrefix = {arXiv},
       eprint = {1809.09722},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.481.1624M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018JCAP...12..022B,
       author = {{Bjorkmo}, Theodor and {Marsh}, M.~C. David},
        title = "{Local, algebraic simplifications of Gaussian random fields}",
      journal = {Journal of Cosmology and Astro-Particle Physics},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Machine Learning, Computer Science - Numerical Analysis},
         year = "2018",
        month = "Dec",
       volume = {2018},
       number = {12},
          eid = {022},
        pages = {022},
     abstract = "{Many applications of Gaussian random fields and Gaussian random
        processes are limited by the computational complexity of
        evaluating the probability density function, which involves
        inverting the relevant covariance matrix. In this work, we show
        how that problem can be completely circumvented for the local
        Taylor coefficients of a Gaussian random field with a Gaussian
        (or `square exponential') covariance function. Our results hold
        for any dimension of the field and to any order in the Taylor
        expansion. We present two applications. First, we show that this
        method can be used to explicitly generate non-trivial potential
        energy landscapes with many fields. This application is
        particularly useful when one is concerned with the field locally
        around special points (e.g. maxima or minima), as we exemplify
        by the problem of cosmic `manyfield' inflation in the early
        universe. Second, we show that this method has applications in
        machine learning, and greatly simplifies the regression problem
        of determining the hyperparameters of the covariance function
        given a training data set consisting of local Taylor
        coefficients at single point. An accompanying Mathematica
        notebook is available at \textbackslashhref\{https://doi.org/10.
        17863/CAM.22859\}\{https://doi.org/10.17863/CAM.22859\}.}",
          doi = {10.1088/1475-7516/2018/12/022},
archivePrefix = {arXiv},
       eprint = {1805.03117},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018JCAP...12..022B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018GeoRL..4513269L,
       author = {{Lubbers}, Nicholas and {Bolton}, David C. and {Mohd-Yusof}, Jamaludin and
         {Marone}, Chris and {Barros}, Kipton and {Johnson}, Paul A.},
        title = "{Earthquake Catalog-Based Machine Learning Identification of Laboratory Fault States and the Effects of Magnitude of Completeness}",
      journal = {\grl},
     keywords = {machine learning, laboratory earthquakes, earthquake catalogs, earthquake forecasting, magnitude of completeness, Physics - Geophysics},
         year = "2018",
        month = "Dec",
       volume = {45},
       number = {24},
        pages = {13,269-13,276},
     abstract = "{Machine learning regression can predict macroscopic fault properties
        such as shear stress, friction, and time to failure using
        continuous records of fault zone acoustic emissions. Here we
        show that a similar approach is successful using event catalogs
        derived from the continuous data. Our methods are applicable to
        catalogs of arbitrary scale and magnitude of completeness. We
        investigate how machine learning regression from an event
        catalog of laboratory earthquakes performs as a function of the
        catalog magnitude of completeness. We find that strong model
        performance requires a sufficiently low magnitude of
        completeness, and below this magnitude of completeness, model
        performance saturates.}",
          doi = {10.1029/2018GL079712},
archivePrefix = {arXiv},
       eprint = {1810.11539},
 primaryClass = {physics.geo-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018GeoRL..4513269L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018BGeo...15.7347G,
       author = {{Gazis}, Iason-Zois and {Schoening}, Timm and {Alevizos}, Evangelos and
         {Greinert}, Jens},
        title = "{Quantitative mapping and predictive modeling of Mn nodules' distribution from hydroacoustic and optical AUV data linked by random forests machine learning}",
      journal = {Biogeosciences},
         year = "2018",
        month = "Dec",
       volume = {15},
       number = {23},
        pages = {7347-7377},
     abstract = "{In this study, high-resolution bathymetric multibeam and optical image
        data, both obtained within the Belgian manganese (Mn) nodule
        mining license area by the autonomous underwater vehicle (AUV)
        Abyss, were combined in order to create a predictive random
        forests (RF) machine learning model. AUV bathymetry reveals
        small-scale terrain variations, allowing slope estimations and
        calculation of bathymetric derivatives such as slope, curvature,
        and ruggedness. Optical AUV imagery provides quantitative
        information regarding the distribution (number and median size)
        of Mn nodules. Within the area considered in this study, Mn
        nodules show a heterogeneous and spatially clustered pattern,
        and their number per square meter is negatively correlated with
        their median size. A prediction of the number of Mn nodules was
        achieved by combining information derived from the acoustic and
        optical data using a RF model. This model was tuned by examining
        the influence of the training set size, the number of growing
        trees (ntree), and the number of predictor variables to be
        randomly selected at each node (mtry) on the RF prediction
        accuracy. The use of larger training data sets with higher ntree
        and mtry values increases the accuracy. To estimate the Mn-
        nodule abundance, these predictions were linked to ground-truth
        data acquired by box coring. Linking optical and hydroacoustic
        data revealed a nonlinear relationship between the Mn-nodule
        distribution and topographic characteristics. This highlights
        the importance of a detailed terrain reconstruction for a
        predictive modeling of Mn-nodule abundance. In addition, this
        study underlines the necessity of a sufficient spatial
        distribution of the optical data to provide reliable modeling
        input for the RF.}",
          doi = {10.5194/bg-15-7347-2018},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018BGeo...15.7347G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018AstL...44..735M,
       author = {{Meshcheryakov}, A.~V. and {Glazkova}, V.~V. and {Gerasimov}, S.~V. and
         {Mashechkin}, I.~V.},
        title = "{Measuring the Probabilistic Photometric Redshifts of X-ray Quasars Based on the Quantile Regression of Ensembles of Decision Trees}",
      journal = {Astronomy Letters},
     keywords = {quasars, photometric redshifts, machine learning, quantile regression, decision trees, random forest, gradient boosting, SRG, eRosita, ART-XC, SDSS, WISE, GALEX, UKIDSS, 2MASS, FIRST, ROSAT, XMM-Newton},
         year = "2018",
        month = "Dec",
       volume = {44},
       number = {12},
        pages = {735-753},
     abstract = "{We present empirical machine learning algorithms for measuring the
        probabilistic photometric redshifts (photo-z) of X-ray quasars
        based on the quantile regression of ensembles of decision trees.
        Relying on the data of present-day photometric sky surveys
        (e.g., SDSS, GALEX, WISE, UKIDSS, 2MASS, FIRST), the proposed
        methods allow one to make high-quality photo-z point predictions
        for extragalactic objects, to estimate the confidence intervals,
        and to reconstruct the full probability distribution functions
        for all predictions. The quality of photo-z predictions has been
        tested on samples of X-ray quasars from the 1RASS and 3XMM DR7
        surveys, which have spectroscopic redshift measurements in the
        SDSS DR14Q catalog. The proposed approaches have shown the
        following accuracy (the metrics are the normalized median
        absolute deviation {\ensuremath{\sigma}}NMAD and the percentage
        of outliers n $_{\&gt;0.15}$): {\ensuremath{\sigma}}$_{NMAD}$, n
        $_{\&gt;0.15}$ = 0.043, 12\% (SDSS + WISE), 0.037, 8\% (SDSS +
        WISE + GALEX) and 0.032, 8.6\% (SDSS + WISE + GALEX + UKIDSS) on
        the RASS sample; {\ensuremath{\sigma}}$_{NMAD}$, n
        $_{\&gt;0.15}$ = 0.054, 13\% (SDSS + WISE), 0.045, 7.6\% (SDSS +
        WISE + GALEX), and 0.037, 6.6\% (SDSS + WISE + GALEX + UKIDSS)
        on the 3XMM sample. The presented photo-z algorithms will become
        an important tool for analyzing the multi-wavelength data on
        X-ray quasars in the forthcoming Spectrum-Roentgen-Gamma sky
        survey.}",
          doi = {10.1134/S1063773718120058},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018AstL...44..735M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...869....9G,
       author = {{Gao}, Xinhua},
        title = "{A Machine-learning-based Investigation of the Open Cluster M67}",
      journal = {\apj},
     keywords = {methods: data analysis, methods: statistical, open clusters and associations: individual: M67, parallaxes, proper motions},
         year = "2018",
        month = "Dec",
       volume = {869},
       number = {1},
          eid = {9},
        pages = {9},
     abstract = "{In this paper, we use a machine-learning method, random forest (RF), to
        identify reliable members of the old (4 Gyr) open cluster M67
        based on the high-precision astrometry and photometry taken from
        the second Gaia data release (Gaia-DR2). The RF method is used
        to calculate membership probabilities of 71,117 stars within
        2.{\textdegree}5 of the cluster center in an 11-dimensional
        parameter space, the photometric data are also taken into
        account. Based on the RF membership probabilities, we obtain
        1502 likely cluster members ({\ensuremath{\geq}}0.6), 1361 of
        which are high-probability cluster members
        ({\ensuremath{\geq}}0.8). Based on high-probability memberships
        with high-precision astrometric data, the mean parallax
        (distance) and proper-motion of the cluster are determined to be
        1.1327 {\ensuremath{\pm}} 0.0018 mas (883 {\ensuremath{\pm}} 1
        pc) and (\&lt; \{{\ensuremath{\mu}} \}$_{{\ensuremath{\alpha}}
        }$\textbackslashcos {\ensuremath{\delta}} \&gt; , \&lt;
        \{{\ensuremath{\mu}} \}$_{{\ensuremath{\delta}} }$\&gt; ) =
        (-10.9378 {\ensuremath{\pm}}0.0078, -2.9465 {\ensuremath{\pm}}
        0.0074) mas yr$^{-1}$, respectively. We find the cluster to have
        a mean radial velocity of +34.06 {\ensuremath{\pm}}0.09 km
        s$^{-1}$, using 74 high-probability cluster members with precise
        radial-velocity measures. We investigate the spatial structure
        of the cluster, the core and limiting radius are determined to
        be 4.′80 {\ensuremath{\pm}} 0.′11 (̃1.23 {\ensuremath{\pm}} 0.03
        pc) and 61.′98 {\ensuremath{\pm}} 1.′50 (̃15.92
        {\ensuremath{\pm}} 0.39 pc), respectively. Our results reveal
        that an escaped member with high membership probability (̃0.91)
        is located at a distance of 77′ (̃20 pc) from the cluster
        center. Furthermore, our results reveal that at least 26.4\% of
        the main-sequence stars in M67 are binary stars. We confirm that
        significant mass segregation has taken place within M67.}",
          doi = {10.3847/1538-4357/aae8dd},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...869....9G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...868...99F,
       author = {{French}, K. Decker and {Zabludoff}, Ann I.},
        title = "{Identifying Tidal Disruption Events via Prior Photometric Selection of Their Preferred Hosts}",
      journal = {\apj},
     keywords = {galaxies: active, galaxies: evolution, methods: observational, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Dec",
       volume = {868},
       number = {2},
          eid = {99},
        pages = {99},
     abstract = "{A nuclear transient detected in a post-starburst galaxy or other
        quiescent galaxy with strong Balmer absorption is likely to be a
        tidal disruption event (TDE). Identifying such galaxies within
        the planned survey footprint of the Large Synoptic Survey
        Telescope (LSST) before a transient is detected will make TDE
        classification immediate and follow-up more efficient.
        Unfortunately, spectra for identifying most such galaxies are
        unavailable, and simple photometric selection is ineffective;
        cutting on {\textquotedblleft}green valley{\textquotedblright}
        UV/optical/IR colors produces samples that are highly
        contaminated and incomplete. Here we propose a new strategy
        using only photometric optical/UV/IR data from large surveys.
        Applying a machine-learning random forest classifier to a sample
        of ̃400,000 SDSS galaxies with Galaxy Evolution Explorer (GALEX)
        and Wide-field Infrared Survey Explorer (WISE) photometry,
        including 13,592 quiescent Balmer-strong galaxies, we achieve
        53\%-61\% purity and 8\%-21\% completeness, given the range in
        redshift. For the subset of 1299 post-starburst galaxies, we
        achieve 63\%-73\% purity and 5\%-12\% completeness. Given these
        results, the range of likely TDE and supernova rates, and that
        36\%-75\% of TDEs occur in quiescent Balmer-strong hosts, we
        estimate that 13\%-99\% of transients observed in
        photometrically selected host galaxies will be TDEs and that we
        will discover 119-248 TDEs per year with LSST. Using our
        technique, we present a new catalog of 67,484 candidate galaxies
        expected to have a high TDE rate, drawn from the SDSS, Pan-
        STARRS, DES, and WISE photometric surveys. This sample is
        3.5{\texttimes} larger than the current SDSS sample of similar
        galaxies, thereby providing a new path forward for transient
        science and galaxy evolution studies.}",
          doi = {10.3847/1538-4357/aaea64},
archivePrefix = {arXiv},
       eprint = {1810.09507},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...868...99F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018AJ....156..284C,
       author = {{Cabrera-Vives}, Guillermo and {Miller}, Christopher J. and
         {Schneider}, Jeff},
        title = "{Systematic Labeling Bias in Galaxy Morphologies}",
      journal = {\aj},
     keywords = {galaxies: statistics, methods: data analysis, methods: statistical, Astrophysics - Astrophysics of Galaxies, Statistics - Applications},
         year = "2018",
        month = "Dec",
       volume = {156},
       number = {6},
          eid = {284},
        pages = {284},
     abstract = "{We present a metric to quantify systematic labeling bias in galaxy
        morphology data sets stemming from the quality of the labeled
        data. This labeling bias is independent from labeling errors and
        requires knowledge about the intrinsic properties of the data
        with respect to the observed properties. We conduct a relative
        comparison of label bias for different low-redshift galaxy
        morphology data sets. We show our metric is able to recover
        previous de-biasing procedures based on redshift as biasing
        parameter. By using the image resolution instead, we find biases
        that have not been addressed. We find that the morphologies
        based on supervised machine learning trained over features such
        as colors, shape, and concentration show significantly less bias
        than morphologies based on expert or citizen-science
        classifiers. This result holds even when there is underlying
        bias present in the training sets used in the supervised machine
        learning process. We use catalog simulations to validate our
        bias metric and show how to bin the multi-dimensional intrinsic
        and observed galaxy properties used in the bias quantification.
        Our approach is designed to work on any other labeled multi-
        dimensional data set, and the code is publicly available (<A hre
        f=``http://https://github.com/guille-c/labeling\_bias''>https://
        github.com/guille-c/labeling\_bias</A>).}",
          doi = {10.3847/1538-3881/aae9f4},
archivePrefix = {arXiv},
       eprint = {1811.03577},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018AJ....156..284C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018AdSpR..62.3279M,
       author = {{Maresi}, Luca},
        title = "{Preface: Advances in Technologies, Missions and Applications of Small Satellites}",
      journal = {Advances in Space Research},
         year = "2018",
        month = "Dec",
       volume = {62},
       number = {12},
        pages = {3279-3280},
     abstract = "{It doesn't matter where I turn my attention to, I see Moore's law
        applied everywhere and on any device. The remote control of my
        home digital TV resembles more a device to command a space ship,
        being far more complex than the old piece of hardware I used to
        switch channels just a few years back. Electronics
        miniaturization allows tiny devices to perform complex
        functions; a telephone can now be worn on a wrist and it can
        also measure the ECG. More functions are now possible with the
        electronics we use daily, thanks not only to miniaturization but
        mainly to artificial intelligence and machine learning.}",
          doi = {10.1016/j.asr.2018.10.021},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018AdSpR..62.3279M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018NatAs...2..151N,
       author = {{Naul}, Brett and {Bloom}, Joshua S. and {P{\'e}rez}, Fernando and
         {van der Walt}, St{\'e}fan},
        title = "{A recurrent neural network for classification of unevenly sampled variable stars}",
      journal = {Nature Astronomy},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics, Physics - Data Analysis, Statistics and Probability},
         year = "2018",
        month = "Nov",
       volume = {2},
        pages = {151-155},
     abstract = "{Astronomical surveys of celestial sources produce streams of noisy time
        series measuring flux versus time (`light curves'). Unlike in
        many other physical domains, however, large (and source-
        specific) temporal gaps in data arise naturally due to
        intranight cadence choices as well as diurnal and seasonal
        constraints$^{1-5}$. With nightly observations of millions of
        variable stars and transients from upcoming surveys$^{4,6}$,
        efficient and accurate discovery and classification techniques
        on noisy, irregularly sampled data must be employed with minimal
        human-in-the-loop involvement. Machine learning for inference
        tasks on such data traditionally requires the laborious hand-
        coding of domain-specific numerical summaries of raw data
        (`features')$^{7}$. Here, we present a novel unsupervised
        autoencoding recurrent neural network$^{8}$ that makes explicit
        use of sampling times and known heteroskedastic noise
        properties. When trained on optical variable star catalogues,
        this network produces supervised classification models that
        rival other best-in-class approaches. We find that autoencoded
        features learned in one time-domain survey perform nearly as
        well when applied to another survey. These networks can continue
        to learn from new unlabelled observations and may be used in
        other unsupervised tasks, such as forecasting and anomaly
        detection.}",
          doi = {10.1038/s41550-017-0321-z},
archivePrefix = {arXiv},
       eprint = {1711.10609},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018NatAs...2..151N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.480.3889R,
       author = {{Reis}, Itamar and {Poznanski}, Dovi and {Hall}, Patrick B.},
        title = "{Redshifted broad absorption line quasars found via machine-learned spectral similarity}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: spectroscopic, quasars: absorption lines, quasars: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Nov",
       volume = {480},
       number = {3},
        pages = {3889-3897},
     abstract = "{We report the discovery of 31 new redshifted broad absorption line
        quasars (RSBALs) from the Sloan Digital Sky Survey (SDSS). The
        number of previously known such objects is 19. The
        identification of the new objects was enabled by calculating
        similarities between quasar spectra in the SDSS. Using these
        similarities we look for the objects that are similar to the
        ones in the original sample, visually inspecting only hundreds,
        out of over 160 000 spectra considered. We compare the
        performance of several similarity measures, as well as different
        methods of employing them, in finding the RSBALs. We find that
        decision tree based similarities recover the most objects, and
        that an ensemble of methods performs better than any single one.
        As the similarities are not tailored for the specific problem of
        finding RSBALs, they could be used for searching for other types
        of quasars. The similarities and the code for their calculation
        are available online.}",
          doi = {10.1093/mnras/sty2127},
archivePrefix = {arXiv},
       eprint = {1805.09829},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.480.3889R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.480.3457M,
       author = {{Michilli}, D. and {Hessels}, J.~W.~T. and {Lyon}, R.~J. and
         {Tan}, C.~M. and {Bassa}, C. and {Cooper}, S. and {Kondratiev}, V.~I. and
         {Sanidas}, S. and {Stappers}, B.~W. and {van Leeuwen}, J.},
        title = "{Single-pulse classifier for the LOFAR Tied-Array All-sky Survey}",
      journal = {\mnras},
     keywords = {pulsars: general, surveys, methods: data analysis, methods: statistical, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2018",
        month = "Nov",
       volume = {480},
       number = {3},
        pages = {3457-3467},
     abstract = "{Searches for millisecond-duration, dispersed single pulses have become a
        standard tool used during radio pulsar surveys in the last
        decade. They have enabled the discovery of two new classes of
        sources: rotating radio transients and fast radio bursts.
        However, we are now in a regime where the sensitivity to single
        pulses in radio surveys is often limited more by the strong
        background of radio frequency interference (RFI, which can
        greatly increase the false-positive rate) than by the
        sensitivity of the telescope itself. To mitigate this problem,
        we introduce the Single-pulse Searcher (SPS). This is a new
        machine-learning classifier designed to identify astrophysical
        signals in a strong RFI environment, and optimized to process
        the large data volumes produced by the new generation of
        aperture array telescopes. It has been specifically developed
        for the LOFAR Tied-Array All-Sky Survey (LOTAAS), an ongoing
        survey for pulsars and fast radio transients in the northern
        hemisphere. During its development, SPS discovered seven new
        pulsars and blindly identified ̃80 known sources. The modular
        design of the software offers the possibility to easily adapt it
        to other studies with different instruments and characteristics.
        Indeed, SPS has already been used in other projects, e.g. to
        identify pulses from the fast radio burst source FRB 121102. The
        software development is complete and SPS is now being used to
        re-process all LOTAAS data collected to date.}",
          doi = {10.1093/mnras/sty2072},
archivePrefix = {arXiv},
       eprint = {1808.05424},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.480.3457M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.480.3302P,
       author = {{Pang}, Di and {Goseva-Popstojanova}, Katerina and {Devine}, Thomas and
         {McLaughlin}, Maura},
        title = "{A novel single-pulse search approach to detection of dispersed radio pulses using clustering and supervised machine learning}",
      journal = {\mnras},
     keywords = {methods: data analysis, pulsars: general, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Nov",
       volume = {480},
       number = {3},
        pages = {3302-3323},
     abstract = "{We present a novel two-stage approach that combines unsupervised and
        supervised machine learning to automatically identify and
        classify single pulses in radio pulsar search data. In the first
        stage, we identify astrophysical pulse candidates in the data,
        which were derived from the Pulsar Arecibo L-Band Feed Array
        (PALFA) survey and contain 47 042 independent beams, as trial
        single-pulse event groups (SPEGs) by clustering single-pulse
        events and merging clusters that fall within the expected DM and
        time span of astrophysical pulses. We also present a new peak
        scoring algorithm, to identify astrophysical peaks in signal-to-
        noise versus DM curves. Furthermore, we group SPEGs detected at
        a consistent DM for they were likely emitted by the same source.
        In the second stage, we create a fully labelled benchmark data
        set by selecting a subset of data with SPEGs identified (using
        stage 1 procedures), their features extracted, and individual
        SPEGs manually labelled, and then train classifiers using
        supervised machine learning. Next, using the best trained
        classifier, we automatically classify unlabelled SPEGs
        identified in the full data set. To aid the examination of dim
        SPEGs, we develop an algorithm that searches for an underlying
        periodicity among grouped SPEGs. The results showed that
        RandomForest with SMOTE treatment was the best learner, with a
        recall of 95.6 per cent and a false-positive rate of 2.0 per
        cent. In total, besides all 60 known pulsars from the benchmark
        data set, the model found 32 additional (i.e. not included in
        the benchmark data set) known pulsars, and several potential
        discoveries.}",
          doi = {10.1093/mnras/sty1992},
archivePrefix = {arXiv},
       eprint = {1807.07164},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.480.3302P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018KPCB...34..290D,
       author = {{Dobrycheva}, D.~V. and {Vavilova}, I.~B. and {Melnyk}, O.~V. and
         {Elyiv}, A.~A.},
        title = "{Morphological Type and Color Indices of the SDSS DR9 Galaxies at 0.02 \&lt; z {\ensuremath{\leq}} 0.06}",
      journal = {Kinematics and Physics of Celestial Bodies},
     keywords = {galaxies, SDSS galaxies, morphology, color indices, machine learning},
         year = "2018",
        month = "Nov",
       volume = {34},
       number = {6},
        pages = {290-301},
     abstract = "{The correlations of the color indices of central galaxies (Mr \&lt;
        -20.7) and their faint neighboring galaxies (Mr \&lt; -20.7)
        using the sample based on the SDSS DR9 (N = 60{\textdegree}561)
        have been studied. The galaxy sample was limited by the red
        shift 0.02 \&lt; z {\ensuremath{\leq}} 0.06 and absolute
        magnitude -24m \&lt; Mr \&lt; -19.4m. The ``Random Forest''
        method of machine learning was used to determine the
        morphological type of the galaxies. The statistically
        significant correlation was found only for the ``central
        galaxies-nearest neighbor galaxy'' pairs in which the distance
        between the components is less than 100 kpc and each of the
        components is an early type galaxy. The obtained results testify
        to the hierarchical scenario of the evolution of galaxies.}",
          doi = {10.3103/S0884591318060028},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018KPCB...34..290D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018JSWSC...8A..50A,
       author = {{Aminalragia-Giamini}, S. and {Papadimitriou}, C. and {Sandberg}, I. and
         {Tsigkanos}, A. and {Jiggens}, P. and {Evans}, H. and {Rodgers}, D. and
         {Daglis}, I.~A.},
        title = "{Artificial intelligence unfolding for space radiation monitor data}",
      journal = {Journal of Space Weather and Space Climate},
     keywords = {radiation monitors, unfolding, artificial intelligence, protons, electrons},
         year = "2018",
        month = "Nov",
       volume = {8},
          eid = {A50},
        pages = {A50},
     abstract = "{The reliable and accurate calculation of incident particle radiation
        fluxes from space radiation monitor measurements, i.e. count-
        rates, is of great interest and importance. Radiation monitors
        are relatively simple and easy to implement instruments found on
        board multiple spacecrafts and can thus provide information
        about the radiation environment in various regions of space
        ranging from Low Earth orbit to missions in Lagrangian points
        and even interplanetary missions. However, the unfolding of
        fluxes from monitor count-rates, being an ill-posed inverse
        problem, is not trivial and prone to serious errors due to the
        inherent difficulties present in such problems. In this work we
        present a novel unfolding method which uses tools from the
        fields of Artificial Intelligence and Machine Learning to
        achieve good unfolding of monitor measurements. The unfolding
        method combines a Case Based Reasoning approach with a Genetic
        Algorithm, which are both widely used. We benchmark the method
        on data from European Space Agency's (ESA) Standard Radiation
        Environment Monitor (SREM) on board the INTEGRAL mission by
        calculating proton fluxes during Solar Energetic Particle Events
        and electron fluxes from measurements within the outer Radiation
        Belt. Extensive evaluation studies are made by comparing the
        unfolded proton fluxes with data from the SEPEM Reference
        Dataset v2.0 and the unfolded electron fluxes with data from the
        Van Allen Probes mission instruments Magnetic Electron Ion
        Spectrometer (MagEIS) and Relativistic Electron Proton Telescope
        (REPT).}",
          doi = {10.1051/swsc/2018041},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018JSWSC...8A..50A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018JASTP.179..389L,
       author = {{Lan}, Ting and {Zhang}, Yuannong and {Jiang}, Chunhua and
         {Yang}, Guobin and {Zhao}, Zhengyu},
        title = "{Automatic identification of Spread F using decision trees}",
      journal = {Journal of Atmospheric and Solar-Terrestrial Physics},
     keywords = {Spread F, Decision tree, Ionogram, Automatic identification},
         year = "2018",
        month = "Nov",
       volume = {179},
        pages = {389-395},
     abstract = "{Spread F is a commonly observed phenomenon on ionograms caused by plasma
        irregularities or wave-like structures in the ionosphere. In
        general, Spread F could induce fluctuations of amplitude and
        phase of radio waves which travel through the ionosphere.
        Therefore, investigation of Spread F could be used to not only
        reveal ionospheric electrodynamics process, but also have a
        significant engineering application. Due to a large amount of
        ionograms recorded by ionosondes, it is a challenge work to
        manually identify ionograms with Spread F to study
        characteristics of Spread F. Thus, much work has been devoted to
        automatic identification of Spread F. In the present study, a
        machine learning method related to decision tree was adopted to
        automatically identify Spread F from ionograms. First, ionograms
        were processed by image method and projection techniques to
        provide input parameters for decision tree. The output of the
        proposed decision tree is whether Spread F is present or not on
        ionograms. Then, a set of ionograms was used to construct a
        decision tree. At last, a set of ionograms was adopted to
        validate the performance of this decision tree. In this study,
        ionograms recorded at Puer station (Geographic latitude and
        longitude: 22.7{\textdegree}N, 101.5{\textdegree}E; Geomagnetic
        latitude: 12.8{\textdegree}N) in the Yunnan province were used.
        Results indicate that the decision tree performed well in
        automatic identification of Spread F on ionograms. The accuracy
        of automatic identification in a set of ionograms with Spread F
        was reached up to 89\%. It inspires us to continually improve
        the performance of automatic identification of Spread F in the
        future work.}",
          doi = {10.1016/j.jastp.2018.09.007},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018JASTP.179..389L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018GeoRL..4512616S,
       author = {{Scher}, S.},
        title = "{Toward Data-Driven Weather and Climate Forecasting: Approximating a Simple General Circulation Model With Deep Learning}",
      journal = {\grl},
     keywords = {machine learning, weather prediction, neural networks, deep learning, climate models},
         year = "2018",
        month = "Nov",
       volume = {45},
       number = {22},
        pages = {12,616-12,622},
     abstract = "{It is shown that it is possible to emulate the dynamics of a simple
        general circulation model with a deep neural network. After
        being trained on the model, the network can predict the complete
        model state several time steps ahead{\textemdash}which
        conceptually is making weather forecasts in the model world.
        Additionally, after being initialized with an arbitrary model
        state, the network can through repeatedly feeding back its
        predictions into its inputs create a climate run, which has
        similar climate statistics to the climate of the general
        circulation model. This network climate run shows no long-term
        drift, even though no conservation properties were explicitly
        designed into the network.}",
          doi = {10.1029/2018GL080704},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018GeoRL..4512616S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018GeoRL..4511693C,
       author = {{Chen}, Xiaodong and {Leung}, L. Ruby and {Gao}, Yang and {Liu}, Ying and
         {Wigmosta}, Mark and {Richmond}, Marshall},
        title = "{Predictability of Extreme Precipitation in Western U.S. Watersheds Based on Atmospheric River Occurrence, Intensity, and Duration}",
      journal = {\grl},
     keywords = {precipitation, atmospheric river, extreme events, machine learning, prediction},
         year = "2018",
        month = "Nov",
       volume = {45},
       number = {21},
        pages = {11,693-11,701},
     abstract = "{We quantified the relationship between atmospheric rivers (ARs) and
        occurrence and magnitude of extreme precipitation in western
        U.S. watersheds, using ARs identified by the Atmospheric River
        Tracking Method Intercomparison Project and precipitation from a
        high-resolution regional climate simulation. Our analysis shows
        the potential of ARs in predicting extreme precipitation events
        at a daily scale, with Gilbert Skill Scores of 0.2. Monthly
        extreme precipitation amount in west coast watersheds is closely
        related to AR intensity, with correlation coefficients of up to
        0.6. The relationship between ARs and precipitation is most
        significant in the Pacific Northwest and California. Using a
        K-means clustering algorithm, ARs can be classified into three
        categories: weak ARs, flash ARs, and prolonged ARs. Flash ARs
        and prolonged ARs, though accounting for less than 50\% of total
        AR events, are more important in controlling extreme
        precipitation patterns and should be prioritized for future
        studies of hydrological extreme events.}",
          doi = {10.1029/2018GL079831},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018GeoRL..4511693C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018CosRe..56..434E,
       author = {{Efitorov}, A.~O. and {Myagkova}, I.~N. and {Shirokii}, V.~R. and
         {Dolenko}, S.~A.},
        title = "{The Prediction of the Dst-Index Based on Machine Learning Methods}",
      journal = {Cosmic Research},
         year = "2018",
        month = "Nov",
       volume = {56},
       number = {6},
        pages = {434-441},
     abstract = "{This paper investigates the possibility of predicting the time series of
        the geomagnetic index Dst. The prediction is based on parameters
        of the solar wind and interplanetary magnetic field measured at
        Lagrange point L1 within the Advanced Composition Explorer (
        ACE) spacecraft experiment using machine learning
        methods{\textemdash}artificial neural networks: classical
        perceptrons, recurrent networks of long short-term memory
        (LSTM), and committees of predictive models. Ultimately, the
        best results have been obtained using heterogeneous committees
        based on neural networks of both types.}",
          doi = {10.1134/S0010952518060035},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018CosRe..56..434E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ComAC...5....4R,
       author = {{Rodr{\'\i}guez}, Andres C. and {Kacprzak}, Tomasz and
         {Lucchi}, Aurelien and {Amara}, Adam and {Sgier}, Rapha{\"e}l and
         {Fluri}, Janis and {Hofmann}, Thomas and {R{\'e}fr{\'e}gier}, Alexandre},
        title = "{Fast cosmic web simulations with generative adversarial networks}",
      journal = {Computational Astrophysics and Cosmology},
     keywords = {Methods: numerical, Cosmology, Large-scale structure of Universe, Astrophysics - Cosmology and Nongalactic Astrophysics, Statistics - Machine Learning},
         year = "2018",
        month = "Nov",
       volume = {5},
       number = {1},
          eid = {4},
        pages = {4},
     abstract = "{Dark matter in the universe evolves through gravity to form a complex
        network of halos, filaments, sheets and voids, that is known as
        the cosmic web. Computational models of the underlying physical
        processes, such as classical N-body simulations, are extremely
        resource intensive, as they track the action of gravity in an
        expanding universe using billions of particles as tracers of the
        cosmic matter distribution. Therefore, upcoming cosmology
        experiments will face a computational bottleneck that may limit
        the exploitation of their full scientific potential. To address
        this challenge, we demonstrate the application of a machine
        learning technique called Generative Adversarial Networks (GAN)
        to learn models that can efficiently generate new, physically
        realistic realizations of the cosmic web. Our training set is a
        small, representative sample of 2D image snapshots from N-body
        simulations of size 500 and 100 Mpc. We show that the GAN-
        generated samples are qualitatively and quantitatively very
        similar to the originals. For the larger boxes of size 500 Mpc,
        it is very difficult to distinguish them visually. The agreement
        of the power spectrum P$_{k}$ is 1-2\% for most of the range,
        between k=0.06 and k=0.4. For the remaining values of k, the
        agreement is within 15\%, with the error rate increasing for
        k\&gt;0.8. For smaller boxes of size 100 Mpc, we find that the
        visual agreement to be good, but some differences are noticable.
        The error on the power spectrum is of the order of 20\%. We
        attribute this loss of performance to the fact that the matter
        distribution in 100 Mpc cutouts was very inhomogeneous between
        images, a situation in which the performance of GANs is known to
        deteriorate. We find a good match for the correlation matrix of
        full P$_{k}$ range for 100 Mpc data and of small scales for 500
        Mpc, with ̃20\% disagreement for large scales. An important
        advantage of generating cosmic web realizations with a GAN is
        the considerable gains in terms of computation time. Each new
        sample generated by a GAN takes a fraction of a second, compared
        to the many hours needed by traditional N-body techniques. We
        anticipate that the use of generative models such as GANs will
        therefore play an important role in providing extremely fast and
        precise simulations of cosmic web in the era of large
        cosmological surveys, such as Euclid and Large Synoptic Survey
        Telescope (LSST).}",
          doi = {10.1186/s40668-018-0026-4},
archivePrefix = {arXiv},
       eprint = {1801.09070},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ComAC...5....4R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018Ap&SS.363..232G,
       author = {{Gao}, Xin-Hua},
        title = "{Memberships, distance and proper-motion of the open cluster NGC 188 based on a machine learning method}",
      journal = {\apss},
     keywords = {Open clusters and associations: individual(NGC 188), Astrometry, Proper motions, Methods: data analysis},
         year = "2018",
        month = "Nov",
       volume = {363},
       number = {11},
          eid = {232},
        pages = {232},
     abstract = "{In this paper, we present an investigation of the memberships, distance
        and proper motion for the old open cluster NGC 188 using a
        machine-learning-based method. This method combines two widely
        used algorithms: spectral clustering (SC) and random forest
        (RF). The former one is used to construct a reliable training
        set, the membership probabilities are calculated based on the
        latter one. This method only depends on reliable training set,
        no prior knowledge about the cluster is needed. This method is
        based on the basic assumption that most if not all the
        information about the cluster members and field stars are
        contained in a reliable training set, this makes it highly
        suitable for handling high-dimensional data sets. We use this
        method to investigate the likely memberships of the old open
        cluster NGC 188 based on the high-precision astrometry and
        photometry from the it\{Gaia\} Data Release 2 (it\{Gaia\} DR2).
        Based on seven-dimensional features (positions, parallax, proper
        motions and color-magnitude) of 3780 sample stars in the region
        of NGC 188, 645 likely members with high membership
        probabilities ({\ensuremath{\geq}} 0.75) are obtained. Further
        analysis confirms the effectiveness of our membership
        determination. Based on these high-probability memberships, the
        distance and proper motion of the cluster are determined to be
        1866{\ensuremath{\pm}} 4 pc and
        (\textbackslashoverline\{{\ensuremath{\mu}} \_\{
        {\ensuremath{\alpha}} \}\},
        \textbackslashoverline\{{\ensuremath{\mu}}
        \_\{{\ensuremath{\delta}} \}\})=(-2.33{\ensuremath{\pm}}
        0.01,-0.97 {\ensuremath{\pm}} 0.01) mas/yr, respectively.}",
          doi = {10.1007/s10509-018-3453-4},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018Ap&SS.363..232G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018AMT....11.6203R,
       author = {{Ruske}, Simon and {Topping}, David O. and {Foot}, Virginia E. and
         {Morse}, Andrew P. and {Gallagher}, Martin W.},
        title = "{Machine learning for improved data analysis of biological aerosol using the WIBS}",
      journal = {Atmospheric Measurement Techniques},
         year = "2018",
        month = "Nov",
       volume = {11},
        pages = {6203-6230},
     abstract = "{Primary biological aerosol including bacteria, fungal spores and pollen
        have important implications for public health and the
        environment. Such particles may have different concentrations of
        chemical fluorophores and will respond differently in the
        presence of ultraviolet light, potentially allowing for
        different types of biological aerosol to be discriminated.
        Development of ultraviolet light induced fluorescence (UV-LIF)
        instruments such as the Wideband Integrated Bioaerosol Sensor
        (WIBS) has allowed for size, morphology and fluorescence
        measurements to be collected in real-time. However, it is
        unclear without studying instrument responses in the laboratory,
        the extent to which different types of particles can be
        discriminated. Collection of laboratory data is vital to
        validate any approach used to analyse data and ensure that the
        data available is utilized as effectively as possible. In this
        paper a variety of methodologies are tested on a range of
        particles collected in the laboratory. Hierarchical
        agglomerative clustering (HAC) has been previously applied to
        UV-LIF data in a number of studies and is tested alongside other
        algorithms that could be used to solve the classification
        problem: Density Based Spectral Clustering and Noise (DBSCAN),
        k-means and gradient boosting. Whilst HAC was able to
        effectively discriminate between reference narrow-size
        distribution PSL particles, yielding a classification error of
        only 1.8 \%, similar results were not obtained when testing on
        laboratory generated aerosol where the classification error was
        found to be between 11.5 \% and 24.2 \%. Furthermore, there is a
        large uncertainty in this approach in terms of the data
        preparation and the cluster index used, and we were unable to
        attain consistent results across the different sets of
        laboratory generated aerosol tested. The lowest classification
        errors were obtained using gradient boosting, where the
        misclassification rate was between 4.38 \% and 5.42 \%. The
        largest contribution to the error, in the case of the higher
        misclassification rate, was the pollen samples where 28.5 \% of
        the samples were incorrectly classified as fungal spores. The
        technique was robust to changes in data preparation provided a
        fluorescent threshold was applied to the data. In the event that
        laboratory training data are unavailable, DBSCAN was found to be
        a potential alternative to HAC. In the case of one of the data
        sets where 22.9 \% of the data were left unclassified we were
        able to produce three distinct clusters obtaining a
        classification error of only 1.42 \% on the classified data.
        These results could not be replicated for the other data set
        where 26.8 \% of the data were not classified and a
        classification error of 13.8 \% was obtained. This method, like
        HAC, also appeared to be heavily dependent on data preparation,
        requiring a different selection of parameters depending on the
        preparation used. Further analysis will also be required to
        confirm our selection of the parameters when using this method
        on ambient data. There is a clear need for the collection of
        additional laboratory generated aerosol to improve
        interpretation of current databases and to aid in the analysis
        of data collected from an ambient environment. New instruments
        with a greater resolution are likely to improve on current
        discrimination between pollen, bacteria and fungal spores and
        even between different species, however the need for extensive
        laboratory data sets will grow as a result.}",
          doi = {10.5194/amt-11-6203-2018},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018AMT....11.6203R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018AJ....156..241H,
       author = {{Heinze}, A.~N. and {Tonry}, J.~L. and {Denneau}, L. and
         {Flewelling}, H. and {Stalder}, B. and {Rest}, A. and {Smith}, K.~W. and
         {Smartt}, S.~J. and {Weiland}, H.},
        title = "{A First Catalog of Variable Stars Measured by the Asteroid Terrestrial-impact Last Alert System (ATLAS)}",
      journal = {\aj},
     keywords = {binaries: eclipsing, catalogs, stars: variables: delta Scuti, stars: variables: general, stars: variables: RR Lyrae, surveys, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Nov",
       volume = {156},
       number = {5},
          eid = {241},
        pages = {241},
     abstract = "{The Asteroid Terrestrial-impact Last Alert System (ATLAS) carries out
        its primary planetary defense mission by surveying about 13,000
        deg$^{2}$ at least four times per night. The resulting data set
        is useful for the discovery of variable stars to a magnitude
        limit fainter than r ̃ 18, with amplitudes down to 0.02 mag for
        bright objects. Here, we present a Data Release One catalog of
        variable stars based on analyzing the light curves of 142
        million stars that were measured at least 100 times in the first
        two years of ATLAS operations. Using a Lomb-Scargle periodogram
        and other variability metrics, we identify 4.7 million candidate
        variables. Through the Space Telescope Science Institute, we
        publicly release light curves for all of them, together with a
        vector of 169 classification features for each star. We do this
        at the level of unconfirmed candidate variables in order to
        provide the community with a large set of homogeneously analyzed
        photometry and to avoid pre-judging which types of objects
        others may find most interesting. We use machine learning to
        classify the candidates into 15 different broad categories based
        on light-curve morphology. About 10\% (427,000 stars) pass
        extensive tests designed to screen out spurious variability
        detections: we label these as
        {\textquotedblleft}probable{\textquotedblright} variables. Of
        these, 214,000 receive specific classifications as eclipsing
        binaries, pulsating, Mira-type, or sinusoidal variables: these
        are the {\textquotedblleft}classified{\textquotedblright}
        variables. New discoveries among the probable variables number
        315,000, while 141,000 of the classified variables are new,
        including about 10,400 pulsating variables, 2060 Mira stars, and
        74,700 eclipsing binaries.}",
          doi = {10.3847/1538-3881/aae47f},
archivePrefix = {arXiv},
       eprint = {1804.02132},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018AJ....156..241H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ACP....1816537F,
       author = {{Fuchs}, Julia and {Cermak}, Jan and {Andersen}, Hendrik},
        title = "{Building a cloud in the southeast Atlantic: understanding low-cloud controls based on satellite observations with machine learning}",
      journal = {Atmospheric Chemistry \& Physics},
         year = "2018",
        month = "Nov",
       volume = {18},
       number = {22},
        pages = {16537-16552},
     abstract = "{Understanding the processes that determine low-cloud properties and
        aerosol-cloud interactions (ACIs) is crucial for the estimation
        of their radiative effects. However, the covariation of
        meteorology and aerosols complicates the determination of cloud-
        relevant influences and the quantification of the aerosol-cloud
        relation. This study identifies and analyzes sensitivities of
        cloud fraction and cloud droplet effective radius to their
        meteorological and aerosol environment in the atmospherically
        stable southeast Atlantic during the biomass-burning season
        based on an 8-day-averaged data set. The effect of geophysical
        parameters on clouds is investigated based on a machine learning
        technique, gradient boosting regression trees (GBRTs), using a
        combination of satellite and reanalysis data as well as
        trajectory modeling of air-mass origins. A comprehensive,
        multivariate analysis of important drivers of cloud occurrence
        and properties is performed and evaluated. The statistical model
        reveals marked subregional differences of relevant drivers and
        processes determining low clouds in the southeast Atlantic.
        Cloud fraction is sensitive to changes of lower tropospheric
        stability in the oceanic, southwestern subregion, while in the
        northeastern subregion it is governed mostly by surface winds.
        In the pristine, oceanic subregion large-scale dynamics and
        aerosols seem to be more important for changes of cloud droplet
        effective radius than in the polluted, near-shore subregion,
        where free tropospheric temperature is more relevant. This study
        suggests the necessity to consider distinct ACI regimes in cloud
        studies in the southeast Atlantic.}",
          doi = {10.5194/acp-18-16537-2018},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ACP....1816537F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...620A..64M,
       author = {{M{\"u}ller}, Ancla and {Hackstein}, Moritz and {Greiner}, Maksim and
         {Frank}, Philipp and {Bomans}, Dominik J. and
         {Dettmar}, Ralf-J{\"u}rgen and {En{\ss}lin}, Torsten},
        title = "{Sharpening up Galactic all-sky maps with complementary data. A machine learning approach}",
      journal = {\aap},
     keywords = {ISM: general, Galaxy: general, Galaxy: structure, surveys, methods: data analysis, methods: statistical, Astrophysics - Astrophysics of Galaxies, Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Nov",
       volume = {620},
          eid = {A64},
        pages = {A64},
     abstract = "{Context. Galactic all-sky maps at very disparate frequencies, such as in
        the radio and {\ensuremath{\gamma}}-ray regime, show similar
        morphological structures. This mutual information reflects the
        imprint of the various physical components of the interstellar
        medium. <BR /> Aims: We want to use multifrequency all-sky
        observations to test resolution improvement and restoration of
        unobserved areas for maps in certain frequency ranges. For this
        we aim to reconstruct or predict from sets of other maps all-sky
        maps that, in their original form, lack a high resolution
        compared to other available all-sky surveys or are incomplete in
        their spatial coverage. Additionally, we want to investigate the
        commonalities and differences that the interstellar medium
        components exhibit over the electromagnetic spectrum. <BR />
        Methods: We built an n-dimensional representation of the joint
        pixel-brightness distribution of n maps using a Gaussian mixture
        model and investigate how predictive it is. We study the extend
        to which one map of the training set can be reproduced based on
        subsets of other maps? <BR /> Results: Tests with mock data show
        that reconstructing the map of a certain frequency from other
        frequency regimes works astonishingly well, predicting reliably
        small-scale details well below the spatial resolution of the
        initially learned map. Applied to the observed multifrequency
        data sets of the Milky Way this technique is able to improve the
        resolution of, for example, the low-resolution Fermi-LAT maps as
        well as to recover the sky from artifact-contaminated data such
        as the ROSAT 0.855 keV map. The predicted maps generally show
        less imaging artifacts compared to the original ones. A
        comparison of predicted and original maps highlights surprising
        structures, imaging artifacts (fortunately not reproduced in the
        prediction), and features genuine to the respective frequency
        range that are not present at other frequency bands. We discuss
        limitations of this machine learning approach and ideas how to
        overcome them. In particular, with increasing sophistication of
        the method, such as introducing more internal degrees of
        freedom, it starts to internalize imaging artifacts. <BR />
        Conclusions: The approach is useful to identify particularities
        in astronomical maps and to provide detailed educated guesses of
        the sky morphology at not yet observed resolutions and
        locations.}",
          doi = {10.1051/0004-6361/201833604},
archivePrefix = {arXiv},
       eprint = {1806.04161},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...620A..64M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...620A..38B,
       author = {{Bugnet}, L. and {Garc{\'\i}a}, R.~A. and {Davies}, G.~R. and
         {Mathur}, S. and {Corsaro}, E. and {Hall}, O.~J. and {Rendle}, B.~M.},
        title = "{FliPer: A global measure of power density to estimate surface gravities of main-sequence solar-like stars and red giants}",
      journal = {\aap},
     keywords = {asteroseismology, methods: data analysis, stars: oscillations, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Nov",
       volume = {620},
          eid = {A38},
        pages = {A38},
     abstract = "{Asteroseismology provides global stellar parameters such as masses,
        radii, or surface gravities using mean global seismic parameters
        and effective temperature for thousands of low-mass stars (0.8
        M$_{☉}$ \&lt; M \&lt; 3 M$_{☉}$). This methodology has been
        successfully applied to stars in which acoustic modes excited by
        turbulent convection are measured. Other methods such as the
        Flicker technique can also be used to determine stellar surface
        gravities, but only works for log g above 2.5 dex. In this work,
        we present a new metric called FliPer (Flicker in spectral power
        density, in opposition to the standard Flicker measurement which
        is computed in the time domain); it is able to extend the range
        for which reliable surface gravities can be obtained (0.1 \&lt;
        log g \&lt; 4.6 dex) without performing any seismic analysis for
        stars brighter than Kp \&lt; 14. FliPer takes into account the
        average variability of a star measured in the power density
        spectrum in a given range of frequencies. However, FliPer values
        calculated on several ranges of frequency are required to better
        characterize a star. Using a large set of asteroseismic targets
        it is possible to calibrate the behavior of surface gravity with
        FliPer through machine learning. This calibration made with a
        random forest regressor covers a wide range of surface gravities
        from main-sequence stars to subgiants and red giants, with very
        small uncertainties from 0.04 to 0.1 dex. FliPer values can be
        inserted in automatic global seismic pipelines to either give an
        estimation of the stellar surface gravity or to assess the
        quality of the seismic results by detecting any outliers in the
        obtained {\ensuremath{\nu}}$_{max}$ values. FliPer also
        constrains the surface gravities of main-sequence dwarfs using
        only long-cadence data for which the Nyquist frequency is too
        low to measure the acoustic-mode properties.}",
          doi = {10.1051/0004-6361/201833106},
archivePrefix = {arXiv},
       eprint = {1809.05105},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...620A..38B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...619A..22R,
       author = {{Ram{\'\i}rez V{\'e}lez}, J.~C. and {Y{\'a}{\~n}ez M{\'a}rquez}, C. and
         {C{\'o}rdova Barbosa}, J.~P.},
        title = "{Using machine learning algorithms to measure stellar magnetic fields}",
      journal = {\aap},
     keywords = {magnetic fields, line: profiles, polarization, radiative transfer, methods: data analysis, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Nov",
       volume = {619},
          eid = {A22},
        pages = {A22},
     abstract = "{Context. Regression methods based on machine learning algorithms (MLA)
        have become an important tool for data analysis in many
        different disciplines. <BR /> Aims: In this work, we use MLA in
        an astrophysical context; our goal is to measure the mean
        longitudinal magnetic field in stars (H$_{eff}$) from polarized
        spectra of high resolution, through the inversion of the so-
        called multi-line profiles. <BR /> Methods: Using synthetic
        data, we tested the performance of our technique considering
        different noise levels: In an ideal scenario of noise-free
        multi-line profiles, the inversion results are excellent;
        however, the accuracy of the inversions diminish considerably
        when noise is taken into account. We therefore propose a data
        pre-process in order to reduce the noise impact, which consists
        of a denoising profile process combined with an iterative
        inversion methodology. <BR /> Results: Applying this data pre-
        process, we find a considerable improvement of the inversions
        results, allowing to estimate the errors associated to the
        measurements of stellar magnetic fields at different noise
        levels. <BR /> Conclusions: We have successfully applied our
        data analysis technique to two different stars, attaining for
        the first time the measurement of H$_{eff}$ from multi-line
        profiles beyond the condition of line autosimilarity assumed by
        other techniques. The training data sets used here are available
        at <A href=``http://www.astrosen.unam.mx/
        julio/ML\_mzs''>http://www.astrosen.unam.mx/ julio/ML\_mzs</A>
        and at the CDS via anonymous ftp to <A href=``http://cdsarc.u-st
        rasbg.fr''>http://cdsarc.u-strasbg.fr</A> (ftp://130.79.128.5)
        or via <A href=``http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/619/A22''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/619/A22</A>}",
          doi = {10.1051/0004-6361/201833016},
archivePrefix = {arXiv},
       eprint = {1807.10733},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...619A..22R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018SSPMA..48j7001W,
       author = {{WU}, Wei and {SUN}, Qiang},
        title = "{Applying machine learning to accelerate new materials development}",
      journal = {Scientia Sinica Physica, Mechanica \&amp; Astronomica},
         year = "2018",
        month = "Oct",
       volume = {48},
       number = {10},
        pages = {107001},
          doi = {10.1360/SSPMA2018-00073},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018SSPMA..48j7001W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018RAA....18..118B,
       author = {{Bai}, Yu and {Liu}, Ji-Feng and {Wang}, Song},
        title = "{Machine learning classification of Gaia Data Release 2}",
      journal = {Research in Astronomy and Astrophysics},
     keywords = {Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Oct",
       volume = {18},
       number = {10},
          eid = {118},
        pages = {118},
     abstract = "{Machine learning has increasingly gained more popularity with its
        incredibly powerful ability to make predictions or calculate
        suggestions for large amounts of data. We apply machine learning
        classification to 85 613 922 objects in the Gaia Data Release 2,
        based on a combination of Pan-STARRS 1 and AllWISE data. The
        classification results are cross-matched with the Simbad
        database, and the total accuracy is 91.9\%. Our sample is
        dominated by stars, {\ensuremath{\sim}}98\%, and galaxies make
        up 2\%. For the objects with negative parallaxes, about 2.5\%
        are galaxies and QSOs, while about 99.9\% are stars if the
        relative parallax uncertainties are smaller than 0.2. Our result
        implies that using the threshold of 0 \&lt;
        {\ensuremath{\sigma}}$_{{\ensuremath{\pi}}}$ /{\ensuremath{\pi}}
        \&lt; 0.2 could yield a very clean stellar sample.}",
          doi = {10.1088/1674-4527/18/10/118},
archivePrefix = {arXiv},
       eprint = {1808.05728},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018RAA....18..118B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PhRvD..98h4013T,
       author = {{Torres-Forn{\'e}}, Alejandro and {Cuoco}, Elena and
         {Marquina}, Antonio and {Font}, Jos{\'e} A. and
         {Ib{\'a}{\~n}ez}, Jos{\'e} M.},
        title = "{Total-variation methods for gravitational-wave denoising: Performance tests on Advanced LIGO data}",
      journal = {\prd},
     keywords = {Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, General Relativity and Quantum Cosmology},
         year = "2018",
        month = "Oct",
       volume = {98},
       number = {8},
          eid = {084013},
        pages = {084013},
     abstract = "{We assess total-variation methods to denoise gravitational-wave signals
        in real noise conditions by injecting numerical-relativity
        waveforms from core-collapse supernovae and binary black hole
        mergers in data from the first observing run of Advanced LIGO.
        This work is an extension of our previous investigation in which
        only Gaussian noise was used. Since the quality of the results
        depends on the regularization parameter of the model, we perform
        a heuristic search for the value that produces the best results.
        We discuss various approaches for the selection of this
        parameter, based on the optimal, mean, or multiple values, and
        compare the results of the denoising upon these choices.
        Moreover, we also present a machine-learning-informed approach
        to obtain the Lagrange multiplier of the method through an
        automatic search. Our results provide further evidence that
        total-variation methods can be useful in the field of
        gravitational-wave astronomy as a tool to remove noise.}",
          doi = {10.1103/PhysRevD.98.084013},
archivePrefix = {arXiv},
       eprint = {1806.07329},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhRvD..98h4013T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.480.1218W,
       author = {{Wan}, Zhen and {Kafle}, Prajwal R. and {Lewis}, Geraint F. and
         {Mackey}, Dougal and {Sharma}, Sanjib and {Ibata}, Rodrigo A.},
        title = "{Galactic cartography with SkyMapper - I. Population substructure and the stellar number density of the inner halo}",
      journal = {\mnras},
     keywords = {Survey: SkyMapper, Colour-Magnitude Diagrams, Star: Horizontal Branch, Galaxies: Halo, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Oct",
       volume = {480},
       number = {1},
        pages = {1218-1228},
     abstract = "{The stars within our Galactic halo presents a snapshot of its ongoing
        growth and evolution, probing galaxy formation directly. Here,
        we present our first analysis of the stellar halo from detailed
        maps of Blue Horizontal Branch (BHB) stars drawn from the
        SkyMapper Southern Sky Survey. To isolate candidate BHB stars
        from the overall population, we develop a machine-learning
        approach through the application of an Artificial Neural Network
        (ANN), resulting in a relatively pure sample of target stars.
        From this, we derive the absolute u magnitude for the BHB sample
        to be \{̃ \}2 mag, varying slightly with (v - g)$_{0}$ and (u -
        v)$_{0}$ colours. We examine the BHB number density distribution
        from 5272 candidate stars, deriving a double power law with a
        break radius of r\_s=11.8{\ensuremath{\pm}} 0.3 kpc, and inner
        and outer slopes of {\ensuremath{\alpha}}$_{in}$ = -2.5
        {\ensuremath{\pm}} 0.1 and {\ensuremath{\alpha}}$_{out}$ = -4.5
        {\ensuremath{\pm}} 0.3, respectively. Through isochrone fitting
        of simulated BHB stars, we find a colour-age/metallicity
        correlation, with older/more metal-poor stars being bluer, and
        establish a parameter to indicate this age (or metallicity)
        variation. Using this, we construct the three-dimensional
        population distribution of BHB stars in the halo and identify
        significant substructure. Finally, in agreement with previous
        studies, we also identify a systemic age/metallicity shift
        spanning \{̃ \}3 kpc to ̃ 20 kpc in Galactocentric distance.}",
          doi = {10.1093/mnras/sty1880},
archivePrefix = {arXiv},
       eprint = {1807.03664},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.480.1218W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.480.1057R,
       author = {{Roy}, N. and {Napolitano}, N.~R. and {La Barbera}, F. and
         {Tortora}, C. and {Getman}, F. and {Radovich}, M. and {Capaccioli}, M. and
         {Brescia}, M. and {Cavuoti}, S. and {Longo}, G. and {Raj}, M.~A. and
         {Puddu}, E. and {Covone}, G. and {Amaro}, V. and {Vellucci}, C. and
         {Grado}, A. and {Kuijken}, K. and {Verdoes Kleijn}, G. and
         {Valentijn}, E.},
        title = "{Evolution of galaxy size-stellar mass relation from the Kilo-Degree Survey}",
      journal = {\mnras},
     keywords = {galaxies: evolution, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Oct",
       volume = {480},
       number = {1},
        pages = {1057-1080},
     abstract = "{We have obtained structural parameters of about 340 000 galaxies from
        the Kilo-Degree Survey (KiDS) in 153 deg$^{2}$ of data release
        1, 2, and 3. We have performed a seeing convolved 2D single
        S{\'e}rsic fit to the galaxy images in the four photometric
        bands (u, g, r, i) observed by KiDS, by selecting high signal-
        to-noise ratio (S/N \&gt; 50) systems in every bands. We have
        classified galaxies as spheroids and disc-dominated by combining
        their spectral energy distribution properties and their
        S{\'e}rsic index. Using photometric redshifts derived from a
        machine learning technique, we have determined the evolution of
        the effective radius, R$_{e}$ and stellar mass,
        M$_{{\ensuremath{\star}}}$, versus redshift, for both mass
        complete samples of spheroids and disc-dominated galaxies up to
        z̃0.6. Our results show a significant evolution of the
        structural quantities at intermediate redshift for the massive
        spheroids (log M$_{*}$/M$_{☉}$ \&gt; 11, Chabrier IMF), while
        almost no evolution has found for less massive ones (log
        M$_{*}$/M$_{☉}$ \&lt; 11). On the other hand, disc dominated
        systems show a milder evolution in the less massive systems (log
        M$_{*}$/M$_{☉}$ \&lt; 11) and possibly no evolution of the more
        massive systems. These trends are generally consistent with
        predictions from hydrodynamical simulations and independent
        datasets out to redshift z ̃ 0.6, although in some cases the
        scatter of the data is large to drive final conclusions. These
        results, based on 1/10 of the expected KiDS area, reinforce
        precedent finding based on smaller statistical samples and show
        the route towards more accurate results, expected with the the
        next survey releases.}",
          doi = {10.1093/mnras/sty1917},
archivePrefix = {arXiv},
       eprint = {1807.06085},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.480.1057R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.480..371L,
       author = {{Lomax}, O. and {Bates}, M.~L. and {Whitworth}, A.~P.},
        title = "{Modelling the structure of star clusters with fractional Brownian motion}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, stars: formation, stars: statistics, ISM: clouds, galaxies: star clusters: general, Astrophysics - Astrophysics of Galaxies, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Oct",
       volume = {480},
       number = {1},
        pages = {371-380},
     abstract = "{The degree of fractal substructure in molecular clouds can be quantified
        by comparing them with fractional Brownian motion (FBM) surfaces
        or volumes. These fields are self-similar over all length-scales
        and characterized by a drift exponent H, which describes the
        structural roughness. Given that the structure of molecular
        clouds and the initial structure of star clusters are almost
        certainly linked, it would be advantageous to also apply this
        analysis to clusters. Currently, the structure of star clusters
        is often quantified by applying Q analysis. Q values from
        observed targets are interpreted by comparing them with those
        from artificial clusters. These are typically generated using a
        box-fractal (BF) or radial density profile (RDP) model. We
        present a single cluster model, based on FBM, as an alternative
        to these models. Here, the structure is parametrized by H and
        the standard deviation of the log-surface/volume density
        {\ensuremath{\sigma}}. The FBM model is able to reproduce both
        centrally concentrated and substructured clusters, and is able
        to provide a much better match to observations than the BF
        model. We show that Q analysis is unable to estimate FBM
        parameters. Therefore, we develop and train a machine learning
        algorithm that can estimate values of H and
        {\ensuremath{\sigma}}, with uncertainties. This provides us with
        a powerful method for quantifying the structure of star clusters
        in terms that relate to the structure of molecular clouds. We
        use the algorithm to estimate the H and {\ensuremath{\sigma}}
        for several young star clusters, some of which have no
        measurable BF or RDP analogue.}",
          doi = {10.1093/mnras/sty1788},
archivePrefix = {arXiv},
       eprint = {1804.06844},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.480..371L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.479.5532P,
       author = {{Peng}, Tianrui Rae and {English}, John Edward and {Silva}, Pedro and
         {Davis}, Darren R. and {Hayes}, Wayne B.},
        title = "{SpArcFiRe: morphological selection effects due to reduced visibility of tightly winding arms in distant spiral galaxies}",
      journal = {\mnras},
     keywords = {methods: data analysis, virtual observatory tools, Galaxy: structure, galaxies: distances and redshifts, galaxies: spiral},
         year = "2018",
        month = "Oct",
       volume = {479},
       number = {4},
        pages = {5532-5543},
     abstract = "{The Galaxy Zoo project has provided a plethora of valuable morphological
        data on a large number of galaxies from various surveys, and
        their team members have identified and/or corrected for many
        biases. Here we study a new bias related to spiral arm pitch
        angles, which first requires selecting a sample of spiral
        galaxies that show observable structure. One obvious way is to
        select galaxies using a threshold in spirality, which we define
        as the fraction of Galaxy Zoo humans who have reported seeing
        spiral structure. Using such a threshold, we use the automated
        tool SpArcFiRe (SPiral ARC FInder and REporter) to measure
        spiral arm pitch angles. We observe that the mean pitch angle of
        spiral arms increases linearly with redshift for 0.05 \&lt; z
        \&lt; 0.085. We hypothesize that this is a selection effect due
        to tightly wound arms becoming less visible as image quality
        degrades, leading to fewer such galaxies being above the
        spirality threshold as redshift increases. We corroborate this
        hypothesis by first artificially degrading images of nearby
        galaxies, and then using a machine learning algorithm trained on
        Galaxy Zoo data to provide a spirality for each artificially
        degraded image. We find that SpARcFiRe's ability to accurately
        measure pitch angles decreases as the image degrades, but that
        spirality decreases more quickly in galaxies with tightly wound
        arms, leading to the selection effect. This new bias means one
        must be careful in selecting a sample on which to measure spiral
        structure. Finally, we also include a sensitivity analysis of
        SpArcFiRe's internal parameters.}",
          doi = {10.1093/mnras/sty546},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.479.5532P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.479.4509R,
       author = {{Rafieferantsoa}, Mika and {Andrianomena}, Sambatra and
         {Dav{\'e}}, Romeel},
        title = "{Predicting the neutral hydrogen content of galaxies from optical data using machine learning}",
      journal = {\mnras},
     keywords = {methods: numerical, galaxies: evolution, galaxies: statistics, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Oct",
       volume = {479},
       number = {4},
        pages = {4509-4525},
     abstract = "{We develop a machine learning-based framework to predict the H I content
        of galaxies from optical photometry and environmental
        parameters. We train the algorithm on z = 0-2 outputs from the
        MUFASA cosmological hydrodynamic simulation, which includes star
        formation, feedback, and a heuristic model to quench massive
        galaxies that yields a reasonable match to a range of survey
        data including H I. We employ a variety of machine learning
        methods (regressors), and quantify their performance using the
        slope of the predicted versus true relation, its root mean
        square error (RMSE), and Pearson correlation coefficient
        (\{r\}). Training on only Sloan Digital Sky Survey photometry,
        all regressors give \{r\}\&gt; 0.8 and RMSE ̃ 0.3 at z = 0, led
        by random forests with \{r\}=0.91, and a deep neural network
        (DNN) with comparable accuracy (\{r\}=0.9). Adding near-IR
        photometry improves all regressors. All regressors perform worse
        with redshift, particularly at z {\ensuremath{\gtrsim}} 1. Slope
        values are generally sub-linear, so that we overpredict H I in H
        I-poor galaxies and underpredict H I rich, because the
        regressors do not fully capture the scatter in the data. We test
        our framework on REsolved Spectroscopy Of a Local VolumE
        (RESOLVE) and Arecibo Legacy Fast ALFA (ALFALFA) survey data.
        Training on a subset of the observations, we find that our
        machine learning method can reasonably predict H I richnesses in
        the remaining data (RMSE ̃ 0.28 for RESOLVE and ̃0.25 for
        ALFALFA). Training on mock data from MUFASA to predict observed
        data is worse (RMSE ̃ 0.45 for RESOLVE and 0.31 for ALFALFA),
        with DNN well outperforming other regressors. Our method will be
        useful for making galaxy-by-galaxy survey predictions and
        incompleteness corrections for upcoming H I 21 cm surveys on
        Square Kilometre Array precursors such as MeerKAT, over regions
        where photometry is already available.}",
          doi = {10.1093/mnras/sty1777},
archivePrefix = {arXiv},
       eprint = {1803.08334},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.479.4509R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018GeoRL..4511137S,
       author = {{Sun}, Alexander Y.},
        title = "{Discovering State-Parameter Mappings in Subsurface Models Using Generative Adversarial Networks}",
      journal = {\grl},
     keywords = {pattern recognition, semisupervised learning, deep learning, generative adversarial networks (GANs), groundwater modeling, surrogate models, Physics - Data Analysis, Statistics and Probability, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2018",
        month = "Oct",
       volume = {45},
       number = {20},
        pages = {11,137-11,146},
     abstract = "{A fundamental problem in geophysical modeling is related to the
        identification and approximation of causal structures among
        physical processes. However, resolving the bidirectional
        mappings between physical parameters and model state variables
        (i.e., solving the forward and inverse problems) is challenging,
        especially when parameter dimensionality is high. Deep learning
        has opened a new door toward knowledge representation and
        complex pattern identification. In particular, the recently
        introduced generative adversarial networks (GANs) hold strong
        promises in learning cross-domain mappings for image
        translation. This study presents a state-parameter
        identification GAN (SPID-GAN) for simultaneously learning
        bidirectional mappings between a high-dimensional parameter
        space and the corresponding model state space. SPID-GAN is
        demonstrated using a series of representative problems from
        subsurface flow modeling. Results show that SPID-GAN achieves
        satisfactory performance in identifying the bidirectional state-
        parameter mappings, providing a new deep-learning-based,
        knowledge representation paradigm for a wide array of complex
        geophysical problems.}",
          doi = {10.1029/2018GL080404},
archivePrefix = {arXiv},
       eprint = {1810.12856},
 primaryClass = {physics.data-an},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018GeoRL..4511137S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...866..149Z,
       author = {{Zhang}, Yunfan Gerry and {Gajjar}, Vishal and {Foster}, Griffin and
         {Siemion}, Andrew and {Cordes}, James and {Law}, Casey and {Wang}, Yu},
        title = "{Fast Radio Burst 121102 Pulse Detection and Periodicity: A Machine Learning Approach}",
      journal = {\apj},
     keywords = {methods: data analysis, methods: observational, methods: statistical, pulsars: general, techniques: image processing, Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Oct",
       volume = {866},
       number = {2},
          eid = {149},
        pages = {149},
     abstract = "{We report the detection of 72 new pulses from the repeating fast radio
        burst FRB 121102 in Breakthrough Listen C-band (4-8 GHz)
        observations at the Green Bank Telescope. The new pulses were
        found with a convolutional neural network in data taken on 2017
        August 26, where 21 bursts have been previously detected. Our
        technique combines neural network detection with dedispersion
        verification. For the current application, we demonstrate its
        advantage over a traditional brute-force dedispersion algorithm
        in terms of higher sensitivity, lower false-positive rates, and
        faster computational speed. Together with the 21 previously
        reported pulses, this observation marks the highest number of
        FRB 121102 pulses from a single observation, totaling 93 pulses
        in five hours, including 45 pulses within the first 30 minutes.
        The number of data points reveals trends in pulse fluence, pulse
        detection rate, and pulse frequency structure. We introduce a
        new periodicity search technique, based on the Rayleigh test, to
        analyze the time of arrivals (TOAs), with which we exclude with
        99\% confidence periodicity in TOAs with periods larger than 5.1
        times the model-dependent timestamp uncertainty. In particular,
        we rule out constant periods {\ensuremath{\gtrsim}}10 ms in the
        barycentric arrival times, though intrinsic periodicity in the
        time of emission remains plausible.}",
          doi = {10.3847/1538-4357/aadf31},
archivePrefix = {arXiv},
       eprint = {1809.03043},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...866..149Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&C....25..257K,
       author = {{Kuminski}, E. and {Shamir}, L.},
        title = "{A hybrid approach to machine learning annotation of large galaxy image databases}",
      journal = {Astronomy and Computing},
     keywords = {Image analysis, Galaxies, Galaxy morphology, Pattern recognition, Machine learning, Query-by-example, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Oct",
       volume = {25},
        pages = {257-269},
     abstract = "{Modern astronomy relies on massive databases collected by robotic
        telescopes and digital sky surveys, acquiring data in a much
        faster pace than what manual analysis can support. Among other
        data, these sky surveys collect information about millions and
        sometimes billions of extra-galactic objects. Since the very
        large number of objects makes manual observation impractical,
        automatic methods that can analyze and annotate extra-galactic
        objects are required to fully utilize the discovery power of
        these databases. Machine learning methods for annotation of
        celestial objects can be separated broadly into methods that use
        the photometric information collected by digital sky surveys,
        and methods that analyze the image of the object. Here we
        describe a hybrid method that combines photometry and image data
        to annotate galaxies by their morphology, and a method that uses
        that information to identify objects that are visually similar
        to a query object (query-by-example). The results are compared
        to using just photometric information from SDSS, and to using
        just the morphological descriptors extracted directly from the
        images. The comparison shows that for automatic classification
        the image data provide marginal addition to the information
        provided by the photometry data. For query-by-example, however,
        the analysis of the image data provides more information that
        improves the automatic detection substantially. The source code
        and binaries of the method can be downloaded through the
        Astrophysics Source Code Library.}",
          doi = {10.1016/j.ascom.2018.10.008},
archivePrefix = {arXiv},
       eprint = {1810.11283},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&C....25..257K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&C....25..213C,
       author = {{Cabral}, J.~B. and {S{\'a}nchez}, B. and {Ramos}, F. and
         {Gurovich}, S. and {Granitto}, P.~M. and {Vanderplas}, J.},
        title = "{From FATS to feets: Further improvements to an astronomical feature extraction tool based on machine learning}",
      journal = {Astronomy and Computing},
     keywords = {Astroinformatics, Machine learning algorithm, Feature selection, Software and its engineering, Software post-development issue, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning},
         year = "2018",
        month = "Oct",
       volume = {25},
        pages = {213-220},
     abstract = "{Machine learning algorithms are highly useful for the classification of
        time series data in astronomy in this era of peta-scale public
        survey data releases. These methods can facilitate the discovery
        of new unknown events in most astrophysical areas, as well as
        improving the analysis of samples of known phenomena. Machine
        learning algorithms use features extracted from collected data
        as input predictive variables. A public tool called Feature
        Analysis for Time Series (FATS) has proved an excellent
        workhorse for feature extraction, particularly light curve
        classification for variable objects. In this study, we present a
        major improvement to FATS, which corrects inconvenient design
        choices, minor details, and documentation for the re-engineering
        process. This improvement comprises a new Python package called
        feets, which is important for future code-refactoring for
        astronomical software tools.}",
          doi = {10.1016/j.ascom.2018.09.005},
archivePrefix = {arXiv},
       eprint = {1809.02154},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&C....25..213C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&C....25..103G,
       author = {{Gonz{\'a}lez}, R.~E. and {Mu{\~n}oz}, R.~P. and {Hern{\'a}ndez}, C.~A.},
        title = "{Galaxy detection and identification using deep learning and data augmentation}",
      journal = {Astronomy and Computing},
     keywords = {Galaxies, General, Techniques, Image processing, Computing methodologies, Machine learning, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Oct",
       volume = {25},
        pages = {103-109},
     abstract = "{We present a method for automatic detection and classification of
        galaxies which includes a novel data-augmentation procedure to
        make trained models more robust against the data taken from
        different instruments and contrast-stretching functions. This
        method is shown as part of AstroCV, a growing open source
        computer vision repository for processing and analyzing big
        astronomical datasets, including high performance Python and C++
        algorithms used in the areas of image processing and computer
        vision. The underlying models were trained using convolutional
        neural networks and deep learning techniques, which provide
        better results than methods based on manual feature engineering
        and SVMs in most of the cases where training datasets are large.
        The detection and classification methods were trained end-to-end
        using public datasets such as the Sloan Digital Sky Survey
        (SDSS), the Galaxy Zoo, and private datasets such as the Next
        Generation Virgo (NGVS) and Fornax (NGFS) surveys. Training
        results are strongly bound to the conversion method from raw
        FITS data for each band into a 3-channel color image. Therefore,
        we propose data augmentation for the training using 5 conversion
        methods. This greatly improves the overall galaxy detection and
        classification for images produced from different instruments,
        bands and data reduction procedures. The detection and
        classification methods were trained using the deep learning
        framework DARKNET and the real-time object detection system
        YOLO. These methods are implemented in C language and CUDA
        platform, and makes intensive use of graphical processing units
        (GPU). Using a single high-end Nvidia GPU card, it can process a
        SDSS image in 50 ms and a DECam image in less than 3 s. We
        provide the open source code, documentation, pre-trained
        networks, python tutorials, and how to train your own datasets,
        which can be found in the AstroCV repository.
        https://github.com/astroCV/astroCV.}",
          doi = {10.1016/j.ascom.2018.09.004},
archivePrefix = {arXiv},
       eprint = {1809.01691},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&C....25..103G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...619A..14F,
       author = {{Fotopoulou}, S. and {Paltani}, S.},
        title = "{CPz: Classification-aided photometric-redshift estimation}",
      journal = {\aap},
     keywords = {methods: data analysis, stars: general, galaxies: general, galaxies: active, surveys, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Oct",
       volume = {619},
          eid = {A14},
        pages = {A14},
     abstract = "{Broadband photometry offers a time and cost effective method to
        reconstruct the continuum emission of celestial objects. Thus,
        photometric redshift estimation has supported the scientific
        exploitation of extragalactic multiwavelength surveys for more
        than twenty years. Deep fields have been the backbone of galaxy
        evolution studies and have brought forward a collection of
        various approaches in determining photometric redshifts. In the
        era of precision cosmology, with the upcoming Euclid and LSST
        surveys, very tight constraints are put on the expected
        performance of photometric redshift estimation using broadband
        photometry, thus new methods have to be developed in order to
        reach the required performance. We present a novel automatic
        method of optimizing photometric redshift performance, the
        classification-aided photometric redshift estimation (CPz). The
        main feature of CPz is the unified treatment of all classes of
        objects detected in extragalactic surveys: galaxies of any type
        (passive, starforming and starbursts), active galactic nuclei
        (AGN), quasi-stellar objects (QSO), stars and also includes the
        identification of potential photometric redshift catastrophic
        outliers. The method operates in three stages. First, the
        photometric catalog is confronted with star, galaxy and QSO
        model templates by means of spectral energy distribution
        fitting. Second, three machine-learning classifiers are used to
        identify 1) the probability of each source to be a star, 2) the
        optimal photometric redshift model library set-up for each
        source and 3) the probability to be a photometric redshift
        catastrophic outlier. Lastly, the final sample is assembled by
        identifying the probability thresholds to be applied on the
        outcome of each of the three classifiers. Hence, with the final
        stage we can create a sample appropriate for a given science
        case, for example favoring purity over completeness. We apply
        our method to the near-infrared VISTA public surveys, matched
        with optical photometry from CFHTLS, KIDS and SDSS, mid-infrared
        WISE photometry and ultra-violet photometry from the Galaxy
        Evolution Explorer (GALEX). We show that CPz offers improved
        photometric redshift performance for both normal galaxies and
        AGN without the need for extra X-ray information. The catalog is
        only available at the CDS via anonymous ftp to <A href=``http://
        cdsarc.u-strasbg.fr/''>http://cdsarc.u-strasbg.fr</A>
        (ftp://130.79.128.5) or via <A href=``http://cdsarc.u-strasbg.fr
        /viz-bin/qcat?J/A+A/619/A14''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/619/A14</A>}",
          doi = {10.1051/0004-6361/201730763},
archivePrefix = {arXiv},
       eprint = {1808.04977},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...619A..14F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...619A...6O,
       author = {{Olspert}, N. and {Lehtinen}, J.~J. and {K{\"a}pyl{\"a}}, M.~J. and
         {Pelt}, J. and {Grigorievskiy}, A.},
        title = "{Estimating activity cycles with probabilistic methods. II. The Mount Wilson Ca H\&amp;K data}",
      journal = {\aap},
     keywords = {stars: activity, methods: statistical, Astrophysics - Solar and Stellar Astrophysics, Statistics - Applications, Statistics - Machine Learning},
         year = "2018",
        month = "Oct",
       volume = {619},
          eid = {A6},
        pages = {A6},
     abstract = "{Context. Debate over the existence of branches in the stellar activity-
        rotation diagrams continues. Application of modern time series
        analysis tools to study the mean cycle periods in chromospheric
        activity index is lacking. <BR /> Aims: We develop such models,
        based on Gaussian processes (GPs), for one-dimensional time
        series and apply it to the extended Mount Wilson Ca H\&amp;K
        sample. Our main aim is to study how the previously commonly
        used assumption of strict harmonicity of the stellar cycles as
        well as handling of the linear trends affect the results. <BR />
        Methods: We introduce three methods of different complexity,
        starting with Bayesian harmonic regression model, followed by GP
        regression models with periodic and quasi-periodic covariance
        functions. We also incorporate a linear trend as one of the
        components. We construct rotation to magnetic cycle period
        ratio-activity (RCRA) diagrams and apply a Gaussian mixture
        model to learn the optimal number of clusters explaining the
        data. <BR /> Results: We confirm the existence of two
        populations in the RCRA diagram; this finding is robust with all
        three methods used. We find only one significant trend in the
        inactive population, namely that the cycle periods get shorter
        with increasing rotation, leading to a positive slope in the
        RCRA diagram. This is in contrast with earlier studies, that
        postulate the existence of trends of different types in both of
        the populations. Our data is consistent with only two activity
        branches (inactive, transitional) instead of three (inactive,
        active, transitional) such that the active branch merges
        together with the transitional one. The retrieved stellar cycles
        are uniformly distributed over the R$_{HK}$$^{'}$ activity
        index, indicating that the operation of stellar large-scale
        dynamos carries smoothly over the Vaughan-Preston gap. At around
        the solar activity index, however, indications of a disruption
        in the cyclic dynamo action are seen. <BR /> Conclusions: Our
        study shows that stellar cycle estimates from time series the
        length of which is short in comparison to the searched cycle
        itself depend significantly on the model applied. Such model-
        dependent aspects include the improper treatment of linear
        trends, while the assumption of strict harmonicity can result in
        the appearance of double cyclicities that seem more likely to be
        explained by the quasi-periodicity of the cycles. In the case of
        quasi-periodic GP models, which we regard the most physically
        motivated ones, only 15 stars were found with statistically
        significant cycles against red noise model. The periodicities
        found have to, therefore, be regarded as suggestive.}",
          doi = {10.1051/0004-6361/201732525},
archivePrefix = {arXiv},
       eprint = {1712.08240},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...619A...6O},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...618A..59C,
       author = {{Castro-Ginard}, A. and {Jordi}, C. and {Luri}, X. and {Julbe}, F. and
         {Morvan}, M. and {Balaguer-N{\'u}{\~n}ez}, L. and {Cantat-Gaudin}, T.},
        title = "{A new method for unveiling open clusters in Gaia. New nearby open clusters confirmed by DR2}",
      journal = {\aap},
     keywords = {surveys, open clusters and associations: general, astrometry, methods: data analysis, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Oct",
       volume = {618},
          eid = {A59},
        pages = {A59},
     abstract = "{Context. The publication of the Gaia Data Release 2 (Gaia DR2) opens a
        new era in astronomy. It includes precise astrometric data
        (positions, proper motions, and parallaxes) for more than 1.3
        billion sources, mostly stars. To analyse such a vast amount of
        new data, the use of data-mining techniques and machine-learning
        algorithms is mandatory. <BR /> Aims: A great example of the
        application of such techniques and algorithms is the search for
        open clusters (OCs), groups of stars that were born and move
        together, located in the disc. Our aim is to develop a method to
        automatically explore the data space, requiring minimal manual
        intervention. <BR /> Methods: We explore the performance of a
        density-based clustering algorithm, DBSCAN, to find clusters in
        the data together with a supervised learning method such as an
        artificial neural network (ANN) to automatically distinguish
        between real OCs and statistical clusters. <BR /> Results: The
        development and implementation of this method in a five-
        dimensional space (l, b, {\ensuremath{\varpi}},
        {\ensuremath{\mu}}$_{{\ensuremath{\alpha}}$^{*}$}$,
        {\ensuremath{\mu}}$_{{\ensuremath{\delta}}}$) with the Tycho-
        Gaia Astrometric Solution (TGAS) data, and a posterior
        validation using Gaia DR2 data, lead to the proposal of a set of
        new nearby OCs. <BR /> Conclusions: We have developed a method
        to find OCs in astrometric data, designed to be applied to the
        full Gaia DR2 archive.}",
          doi = {10.1051/0004-6361/201833390},
archivePrefix = {arXiv},
       eprint = {1805.03045},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...618A..59C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...618A..52R,
       author = {{Ruiz}, A. and {Corral}, A. and {Mountrichas}, G. and
         {Georgantopoulos}, I.},
        title = "{XMMPZCAT: A catalogue of photometric redshifts for X-ray sources}",
      journal = {\aap},
     keywords = {catalogs, galaxies: active, X-rays: general, X-rays: galaxies, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Oct",
       volume = {618},
          eid = {A52},
        pages = {A52},
     abstract = "{<BR /> Aims: The third version of the XMM-Newton serendipitous catalogue
        (3XMM), containing almost half million sources, is now the
        largest X-ray catalogue. However, its full scientific potential
        remains untapped due to the lack of distance information (i.e.
        redshifts) for the majority of its sources. Here we present
        XMMPZCAT, a catalogue of photometric redshifts (photo-z) for
        3XMM sources. <BR /> Methods: We searched for optical
        counterparts of 3XMM-DR6 sources outside the Galactic plane in
        the SDSS and Pan-STARRS surveys, with the addition of near-(NIR)
        and mid-infrared (MIR) data whenever possible (2MASS, UKIDSS,
        VISTA-VHS, and AllWISE). We used this photometry data set in
        combination with a training sample of 5157 X-ray selected
        sources and the MLZ-TPZ package, a supervised machine learning
        algorithm based on decision trees and random forests for the
        calculation of photo-z. <BR /> Results: We have estimated
        photo-z for 100 178 X-ray sources, about 50\% of the total
        number of 3XMM sources (205 380) in the XMM-Newton fields
        selected to build this catalogue (4208 out of 9159). The
        accuracy of our results highly depends on the available
        photometric data, with a rate of outliers ranging from 4\% for
        sources with data in the optical + NIR + MIR, up to 40\% for
        sources with only optical data. We also addressed the
        reliability level of our results by studying the shape of the
        photo-z probability density distributions.}",
          doi = {10.1051/0004-6361/201833117},
archivePrefix = {arXiv},
       eprint = {1807.04526},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...618A..52R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...618A..31M,
       author = {{Masoura}, V.~A. and {Mountrichas}, G. and {Georgantopoulos}, I. and
         {Ruiz}, A. and {Magdis}, G. and {Plionis}, M.},
        title = "{Disentangling the AGN and star formation connection using XMM-Newton}",
      journal = {\aap},
     keywords = {galaxies: active, galaxies: evolution, galaxies: star formation, infrared: galaxies, X-rays: galaxies, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Oct",
       volume = {618},
          eid = {A31},
        pages = {A31},
     abstract = "{There is growing evidence supporting the coeval growth of galaxies and
        their resident super-massive black hole (SMBH). Most studies
        also claim a correlation between the activity of the SMBH and
        the star formation of the host galaxy. It is unclear, however,
        whether this correlation extends to all redshifts and X-ray
        luminosities. Some studies find a weaker dependence at lower
        luminosities and/or a suppression of the star formation at high
        luminosities. We here use data from the X-ATLAS and XMM-XXL
        North fields and compile the largest X-ray sample up to date to
        investigate how X-ray selected AGN affect the star formation of
        their host galaxies in a wide redshift and luminosity baseline
        of 0.03 \&lt; z \&lt; 3 and log L$_{X}$(2-10 keV) = (41-45.5)
        erg s$^{-1}$. Our sample consists of 3336 AGN. 1872 of our
        sources have spectroscopic redshifts. For the remaining sources
        we calculate photometric redshifts using TPZ, a machine-learning
        algorithm. We estimate stellar masses
        (M$_{{\ensuremath{\star}}}$) and star formation rates (SFRs) by
        applying spectral energy distribution fitting through the CIGALE
        code, using optical, near-IR, and mid-IR photometry (SDSS,
        VISTA, and WISE). Of our sources, 608 also have far-IR
        photometry (Herschel). We use these sources to calibrate the SFR
        calculations of our remaining X-ray sample. Our results show a
        correlation between the X-ray luminosity (L$_{X}$) and the SFR
        of the host galaxy at all redshifts and luminosities spanned by
        our sample. We also find a dependence of the specific SFR (sSFR)
        on redshift, while there are indications that the X-ray
        luminosity enhances the sSFR even at low redshifts. We then
        disentangle the effects of stellar mass and redshift on the SFR
        and again study its dependence on the X-ray luminosity. Towards
        this end, we estimate the SFR of main-sequence galaxies that
        have the same stellar mass and redshift as our X-ray AGN and
        compare them with the SFR of our X-ray AGN. Our analysis reveals
        that the AGN enhances the star formation of its host galaxy when
        the galaxy lies below the main sequence and quenches the star
        formation of the galaxy it lives in when the host lies above the
        main sequence. Therefore, the effect of AGN on the SFR of the
        host galaxy depends on the location of the galaxy relative to
        the main sequence.}",
          doi = {10.1051/0004-6361/201833397},
archivePrefix = {arXiv},
       eprint = {1807.01723},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...618A..31M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PhRvD..98f3511L,
       author = {{Leclercq}, Florent},
        title = "{Bayesian optimization for likelihood-free cosmological inference}",
      journal = {\prd},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Applications},
         year = "2018",
        month = "Sep",
       volume = {98},
       number = {6},
          eid = {063511},
        pages = {063511},
     abstract = "{Many cosmological models have only a finite number of parameters of
        interest, but a very expensive data-generating process and an
        intractable likelihood function. We address the problem of
        performing likelihood-free Bayesian inference from such black-
        box simulation-based models, under the constraint of a very
        limited simulation budget (typically a few thousand). To do so,
        we adopt an approach based on the likelihood of an alternative
        parametric model. Conventional approaches to approximate
        Bayesian computation such as likelihood-free rejection sampling
        are impractical for the considered problem, due to the lack of
        knowledge about how the parameters affect the discrepancy
        between observed and simulated data. As a response, we make use
        of a strategy previously developed in the machine learning
        literature (Bayesian optimization for likelihood-free inference,
        bolfi), which combines Gaussian process regression of the
        discrepancy to build a surrogate surface with Bayesian
        optimization to actively acquire training data. We extend the
        method by deriving an acquisition function tailored for the
        purpose of minimizing the expected uncertainty in the
        approximate posterior density, in the parametric approach. The
        resulting algorithm is applied to the problems of summarizing
        Gaussian signals and inferring cosmological parameters from the
        joint lightcurve analysis supernovae data. We show that the
        number of required simulations is reduced by several orders of
        magnitude, and that the proposed acquisition function produces
        more accurate posterior approximations, as compared to common
        strategies.}",
          doi = {10.1103/PhysRevD.98.063511},
archivePrefix = {arXiv},
       eprint = {1805.07152},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhRvD..98f3511L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PhRvC..98c4318N,
       author = {{Neufcourt}, L{\'e}o and {Cao}, Yuchen and {Nazarewicz}, Witold and
         {Viens}, Frederi},
        title = "{Bayesian approach to model-based extrapolation of nuclear observables}",
      journal = {\prc},
     keywords = {Nuclear Theory, Statistics - Machine Learning, 62F15, 62P35},
         year = "2018",
        month = "Sep",
       volume = {98},
       number = {3},
          eid = {034318},
        pages = {034318},
     abstract = "{Background: The mass, or binding energy, is the basis property of the
        atomic nucleus. It determines its stability and reaction and
        decay rates. Quantifying the nuclear binding is important for
        understanding the origin of elements in the universe. The
        astrophysical processes responsible for the nucleosynthesis in
        stars often take place far from the valley of stability, where
        experimental masses are not known. In such cases, missing
        nuclear information must be provided by theoretical predictions
        using extreme extrapolations. To take full advantage of the
        information contained in mass model residuals, i.e., deviations
        between experimental and calculated masses, one can utilize
        Bayesian machine-learning techniques to improve predictions.
        Purpose: To improve the quality of model-based predictions of
        nuclear properties of rare isotopes far from stability, we
        consider the information contained in the residuals in the
        regions where the experimental information exist. As a case in
        point, we discuss two-neutron separation energies S$_{2 n}$ of
        even-even nuclei. Through this observable, we assess the
        predictive power of global mass models towards more unstable
        neutron-rich nuclei and provide uncertainty quantification of
        predictions. Methods: We consider 10 global models based on
        nuclear density functional theory with realistic energy density
        functionals as well as two more phenomenological mass models.
        The emulators of S$_{2 n}$ residuals and credibility intervals
        (Bayesian confidence intervals) defining theoretical error bars
        are constructed using Bayesian Gaussian processes and Bayesian
        neural networks. We consider a large training dataset pertaining
        to nuclei whose masses were measured before 2003. For the
        testing datasets, we considered those exotic nuclei whose masses
        have been determined after 2003. By establishing statistical
        methodology and parameters, we carried out extrapolations toward
        the 2 n dripline. Results: While both Gaussian processes and
        Bayesian neural networks reduce the root-mean-square (rms)
        deviation from experiment significantly, GP offers a better and
        much more stable performance. The increase in the predictive
        power of microscopic models aided by the statistical treatment
        is quite astonishing: The resulting rms deviations from
        experiment on the testing dataset are similar to those of more
        phenomenological models. We found that Bayesian neural networks
        results are prone to instabilities caused by the large number of
        parameters in this method. Moreover, since the classical sigmoid
        activation function used in this approach has linear tails that
        do not vanish, it is poorly suited for a bounded extrapolation.
        The empirical coverage probability curves we obtain match very
        well the reference values, in a slightly conservative way in
        most cases, which is highly desirable to ensure honesty of
        uncertainty quantification. The estimated credibility intervals
        on predictions make it possible to evaluate predictive power of
        individual models and also make quantified predictions using
        groups of models. Conclusions: The proposed robust statistical
        approach to extrapolation of nuclear model results can be useful
        for assessing the impact of current and future experiments in
        the context of model developments. The new Bayesian capability
        to evaluate residuals is also expected to impact research in the
        domains where experiments are currently impossible, for
        instance, in simulations of the astrophysical r process.}",
          doi = {10.1103/PhysRevC.98.034318},
archivePrefix = {arXiv},
       eprint = {1806.00552},
 primaryClass = {nucl-th},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhRvC..98c4318N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.479.4153Y,
       author = {{Yong}, Suk Yee and {King}, Anthea L. and {Webster}, Rachel L. and
         {Bate}, Nicholas F. and {O'Dowd}, Matthew J. and {Labrie}, Kathleen},
        title = "{Using the Properties of Broad Absorption Line Quasars to Illuminate Quasar Structure}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, galaxies: active, quasars: absorption lines, quasars: emission lines, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2018",
        month = "Sep",
       volume = {479},
       number = {3},
        pages = {4153-4171},
     abstract = "{A key to understanding quasar unification paradigms is the emission
        properties of broad absorption line quasars (BALQs). The fact
        that only a small fraction of quasar spectra exhibit deep
        absorption troughs blueward of the broad permitted emission
        lines provides a crucial clue to the structure of quasar
        emitting regions. To learn whether it is possible to
        discriminate between the BALQ and non-BALQ populations given the
        observed spectral properties of a quasar, we employ two
        approaches: one based on statistical methods and the other
        supervised machine learning classification, applied to quasar
        samples from the Sloan Digital Sky Survey. The features explored
        include continuum and emission line properties, in particular
        the absolute magnitude, redshift, spectral index, line width,
        asymmetry, strength, and relative velocity offsets of high-
        ionization C IV {\ensuremath{\lambda}}1549 and low-ionization Mg
        II {\ensuremath{\lambda}}2798 lines.}",
          doi = {10.1093/mnras/sty1540},
archivePrefix = {arXiv},
       eprint = {1806.07090},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.479.4153Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.479.3405L,
       author = {{Lucie-Smith}, Luisa and {Peiris}, Hiranya V. and {Pontzen}, Andrew and
         {Lochner}, Michelle},
        title = "{Machine learning cosmological structure formation}",
      journal = {\mnras},
     keywords = {methods: statistical, galaxies: haloes, dark matter, large-scale structure of Universe, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Sep",
       volume = {479},
       number = {3},
        pages = {3405-3414},
     abstract = "{We train a machine learning algorithm to learn cosmological structure
        formation from N-body simulations. The algorithm infers the
        relationship between the initial conditions and the final dark
        matter haloes, without the need to introduce approximate halo
        collapse models. We gain insights into the physics driving halo
        formation by evaluating the predictive performance of the
        algorithm when provided with different types of information
        about the local environment around dark matter particles. The
        algorithm learns to predict whether or not dark matter particles
        will end up in haloes of a given mass range, based on spherical
        overdensities. We show that the resulting predictions match
        those of spherical collapse approximations such as extended
        Press-Schechter theory. Additional information on the shape of
        the local gravitational potential is not able to improve halo
        collapse predictions; the linear density field contains
        sufficient information for the algorithm to also reproduce
        ellipsoidal collapse predictions based on the Sheth-Tormen
        model. We investigate the algorithm's performance in terms of
        halo mass and radial position and perform blind analyses on
        independent initial conditions realizations to demonstrate the
        generality of our results.}",
          doi = {10.1093/mnras/sty1719},
archivePrefix = {arXiv},
       eprint = {1802.04271},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.479.3405L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.479.2389K,
       author = {{Ksoll}, Victor F. and {Gouliermis}, Dimitrios A. and
         {Klessen}, Ralf S. and {Grebel}, Eva K. and {Sabbi}, Elena and
         {Anderson}, Jay and {Lennon}, Daniel J. and {Cignoni}, Michele and
         {de Marchi}, Guido and {Smith}, Linda J. and {Tosi}, Monica and
         {van der Marel}, Roeland P.},
        title = "{Hubble Tarantula Treasury Project - VI. Identification of pre-main-sequence stars using machine-learning techniques}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, Hertzsprung-Russell and colour-magnitude diagrams, stars: pre-main-sequence, Magellanic Clouds, galaxies: star clusters: individual: NGC2060, NGC2070, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Sep",
       volume = {479},
       number = {2},
        pages = {2389-2414},
     abstract = "{The Hubble Tarantula Treasury Project (HTTP) has provided an
        unprecedented photometric coverage of the entire starburst
        region of 30 Doradus down to the half Solar mass limit. We use
        the deep stellar catalogue of HTTP to identify all the pre-main-
        sequence (PMS) stars of the region, i.e. stars that have not
        started their lives on the main-sequence yet. The photometric
        distinction of these stars from the more evolved populations is
        not a trivial task due to several factors that alter their
        colour-magnitude diagram positions. The identification of PMS
        stars requires, thus, sophisticated statistical methods. We
        employ machine-learning classification techniques on the HTTP
        survey of more than 800 000 sources to identify the PMS stellar
        content of the observed field. Our methodology consists of (1)
        carefully selecting the most probable low-mass PMS stellar
        population of the star-forming cluster NGC 2070, (2) using this
        sample to train classification algorithms to build a predictive
        model for PMS stars, and (3) applying this model in order to
        identify the most probable PMS content across the entire
        Tarantula Nebula. We employ decision tree, random forest (RF),
        and support vector machine (SVM) classifiers to categorize the
        stars as PMS and non-PMS. The RF and SVM provided the most
        accurate models, predicting about 20 000 sources with a
        candidateship probability higher than 50 per cent, and almost 10
        000 PMS candidates with a probability higher than 95 per cent.
        This is the richest and most accurate photometric catalogue of
        extragalactic PMS candidates across the extent of a whole star-
        forming complex.}",
          doi = {10.1093/mnras/sty1317},
archivePrefix = {arXiv},
       eprint = {1805.07157},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.479.2389K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.479.1465D,
       author = {{D{\'e}nes}, H. and {McClure-Griffiths}, N.~M. and {Dickey}, J.~M. and
         {Dawson}, J.~R. and {Murray}, C.~E.},
        title = "{Calibrating the HISA temperature: Measuring the temperature of the Riegel-Crutcher cloud}",
      journal = {\mnras},
     keywords = {Galaxy: evolution, (Galaxy): local interstellar matter, radio lines: ISM, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Sep",
       volume = {479},
       number = {2},
        pages = {1465-1490},
     abstract = "{H I self-absorption (HISA) clouds are clumps of cold neutral hydrogen (H
        I) visible in front of warm background gas, which makes them
        ideal places to study the properties of the cold atomic
        component of the interstellar medium. The Riegel-Crutcher (R-C)
        cloud is the most striking HISA feature in the Galaxy. It is one
        of the closest HISA clouds to us and is located in the direction
        of the Galactic Centre, which provides a bright background.
        High-resolution interferometric measurements have revealed the
        filamentary structure of this cloud; however, it is difficult to
        accurately determine the temperature and the density of the gas
        without optical depth measurements. In this paper, we present
        new H I absorption observations with the Australia Telescope
        Compact Array against 46 continuum sources behind the R-C cloud
        to directly measure the optical depth of the cloud. We decompose
        the complex H I absorption spectra into Gaussian components
        using an automated machine learning algorithm. We find 300
        Gaussian components, from which 67 are associated with the R-C
        cloud (0 \&lt; v$_{LSR}$ \&lt; 10 km s$^{-1}$, full width at
        half maximum \&lt; 10 km s$^{-1}$). Combining the new H I
        absorption data with H I emission data from previous surveys, we
        calculate the spin temperature and find it to be between 20 and
        80 K. Our measurements uncover a temperature gradient across the
        cloud with spin temperatures decreasing towards positive
        Galactic latitudes. We also find three new OH absorption lines
        associated with the cloud, which support the presence of
        molecular gas.}",
          doi = {10.1093/mnras/sty1384},
archivePrefix = {arXiv},
       eprint = {1805.09856},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.479.1465D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.479..415A,
       author = {{Ackermann}, Sandro and {Schawinski}, Kevin and {Zhang}, Ce and
         {Weigel}, Anna K. and {Turp}, M. Dennis},
        title = "{Using transfer learning to detect galaxy mergers}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: image processing, galaxies: general, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning},
         year = "2018",
        month = "Sep",
       volume = {479},
       number = {1},
        pages = {415-425},
     abstract = "{We investigate the use of deep convolutional neural networks (deep CNNs)
        for automatic visual detection of galaxy mergers. Moreover, we
        investigate the use of transfer learning in conjunction with
        CNNs by retraining networks first trained on pictures of
        everyday objects. We test the hypothesis that transfer learning
        is useful for improving classification performance for small
        training sets. This would make transfer learning useful for
        finding rare objects in astronomical imaging data sets. We find
        that these deep learning methods perform significantly better
        than current state-of-the-art merger detection methods based on
        non-parametric systems such as CAS and GM$_{20}$. Our method is
        end-to-end and robust to image noise and distortions; it can be
        applied directly without image preprocessing. We also find that
        transfer learning can act as a regularizer in some cases,
        leading to better overall classification accuracy (p = 0.02).
        Transfer learning on our full training set leads to a lowered
        error rate from 0.038 {\ensuremath{\pm}} 1 to 0.032
        {\ensuremath{\pm}} 1, a relative improvement of 15 per cent.
        Finally, we perform a basic sanity-check by creating a merger
        sample with our method, and comparing with an already existing,
        manually created merger catalogue in terms of colour-mass
        distribution and stellar mass function.}",
          doi = {10.1093/mnras/sty1398},
archivePrefix = {arXiv},
       eprint = {1805.10289},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.479..415A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJS..238....4P,
       author = {{Papageorgiou}, Athanasios and {Catelan}, M{\'a}rcio and
         {Christopoulou}, Panagiota-Eleftheria and {Drake}, Andrew J. and
         {Djorgovski}, S.~G.},
        title = "{An Updated Catalog of 4680 Northern Eclipsing Binaries with Algol-type Light-curve Morphology in the Catalina Sky Surveys}",
      journal = {\apjs},
     keywords = {binaries: eclipsing, catalogs, methods: data analysis, surveys, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Sep",
       volume = {238},
       number = {1},
          eid = {4},
        pages = {4},
     abstract = "{We present an updated catalog of 4680 northern eclipsing binaries (EBs)
        with Algol-type light-curve (LC) morphology (i.e., with well-
        defined beginnings and ends of primary and secondary eclipses),
        using data from the Catalina Sky Surveys. Our work includes
        revised period determinations, phenomenological parameters of
        the LCs, and system morphology classifications based on machine-
        learning techniques. While most of the new periods are in
        excellent agreement with those provided in the original Catalina
        catalogs, improved values are now available for ̃10\% of the
        stars. A total of 3456 EBs were classified as detached and 449
        were classified as semi-detached, while 145 could not be
        classified unambiguously into either subtype. The majority of
        the SD systems seem to be comprised of short-period Algols. By
        applying color criteria, we searched for K- and M-type dwarfs in
        these data, and present a subsample of 609 EB candidates for
        further investigation. We report 119 EBs (2.5\% of the total
        sample) that show maximum quadrature light variations over long
        timescales, with periods bracketing the range 4.5-18 years and a
        fractional luminosity variance range of 0.04-0.13. We discuss
        possible causes for this, making use of models of variable
        starspot activity in our interpretation of the results.}",
          doi = {10.3847/1538-4365/aad8a9},
archivePrefix = {arXiv},
       eprint = {1808.09725},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJS..238....4P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018Ap&SS.363..197M,
       author = {{Miettinen}, O.},
        title = "{Protostellar classification using supervised machine learning algorithms}",
      journal = {\apss},
     keywords = {Methods: data analysis, Stars: formation, Stars: protostars, Astrophysics - Astrophysics of Galaxies, Physics - Computational Physics},
         year = "2018",
        month = "Sep",
       volume = {363},
       number = {9},
          eid = {197},
        pages = {197},
     abstract = "{Classification of young stellar objects (YSOs) into different
        evolutionary stages helps us to understand the formation process
        of new stars and planetary systems. Such classification has
        traditionally been based on spectral energy distribution (SED)
        analysis. An alternative approach is provided by supervised
        machine learning algorithms, which can be trained to classify
        large samples of YSOs much faster than via SED analysis. We
        attempt to classify a sample of Orion YSOs (the parent sample
        size is 330) into different classes, where each source has
        already been classified using multiwavelength SED analysis. We
        used eight different learning algorithms to classify the target
        YSOs, namely a decision tree, random forest, gradient boosting
        machine (GBM), logistic regression, na{\"\i}ve Bayes classifier,
        k-nearest neighbour classifier, support vector machine, and
        neural network. The classifiers were trained and tested by using
        a 10-fold cross-validation procedure. As the learning features,
        we employed ten different continuum flux densities spanning from
        the near-infrared to submillimetre wavebands
        ({\ensuremath{\lambda}}= 3.6-870 {\ensuremath{\mu}}m). With a
        classification accuracy of 82\% (with respect to the SED-based
        classes), a GBM algorithm was found to exhibit the best
        performance. The lowest accuracy of 47\% was obtained with a
        na{\"\i}ve Bayes classifier. Our analysis suggests that the
        inclusion of the 3.6 {\ensuremath{\mu}}m and 24
        {\ensuremath{\mu}}m flux densities is useful to maximise the YSO
        classification accuracy. Although machine learning has the
        potential to provide a rapid and fairly reliable way to classify
        YSOs, an SED analysis is still needed to derive the physical
        properties of the sources (e.g. dust temperature and mass), and
        to create the labelled training data. The machine learning
        classification accuracies can be improved with respect to the
        present results by using larger data sets, more detailed missing
        value imputation, and advanced ensemble methods (e.g. extreme
        gradient boosting). Overall, the application of machine learning
        is expected to be very useful in the era of big astronomical
        data, for example to quickly assemble interesting target source
        samples for follow-up studies.}",
          doi = {10.1007/s10509-018-3418-7},
archivePrefix = {arXiv},
       eprint = {1808.08371},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018Ap&SS.363..197M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...617A..70S,
       author = {{Siudek}, M. and {Ma{\l}ek}, K. and {Pollo}, A. and {Krakowski}, T. and
         {Iovino}, A. and {Scodeggio}, M. and {Moutard}, T. and {Zamorani}, G. and
         {Guzzo}, L. and {Garilli}, B. and {Granett}, B.~R. and
         {Bolzonella}, M. and {de la Torre}, S. and {Abbas}, U. and {Adami}, C. and
         {Bottini}, D. and {Cappi}, A. and {Cucciati}, O. and {Davidzon}, I. and
         {Franzetti}, P. and {Fritz}, A. and {Krywult}, J. and {Le Brun}, V. and
         {Le F{\`e}vre}, O. and {Maccagni}, D. and {Marulli}, F. and
         {Polletta}, M. and {Tasca}, L.~A.~M. and {Tojeiro}, R. and
         {Vergani}, D. and {Zanichelli}, A. and {Arnouts}, S. and {Bel}, J. and
         {Branchini}, E. and {Coupon}, J. and {De Lucia}, G. and {Ilbert}, O. and
         {Haines}, C.~P. and {Moscardini}, L. and {Takeuchi}, T.~T.},
        title = "{The VIMOS Public Extragalactic Redshift Survey (VIPERS). The complexity of galaxy populations at 0.4 \&lt; z \&lt; 1.3 revealed with unsupervised machine-learning algorithms}",
      journal = {\aap},
     keywords = {galaxies: evolution, galaxies: star formation, galaxies: stellar content, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Sep",
       volume = {617},
          eid = {A70},
        pages = {A70},
     abstract = "{<BR /> Aims: Various galaxy classification schemes have been developed
        so far to constrain the main physical processes regulating
        evolution of different galaxy types. In the era of a deluge of
        astrophysical information and recent progress in machine
        learning, a new approach to galaxy classification has become
        imperative. <BR /> Methods: In this paper, we employ a Fisher
        Expectation-Maximization (FEM) unsupervised algorithm working in
        a parameter space of 12 rest-frame magnitudes and spectroscopic
        redshift. The model (DBk) and the number of classes (12) were
        established based on the joint analysis of standard statistical
        criteria and confirmed by the analysis of the galaxy
        distribution with respect to a number of classes and their
        properties. This new approach allows us to classify galaxies
        based on only their redshifts and ultraviolet to near-infrared
        (UV-NIR) spectral energy distributions. <BR /> Results: The FEM
        unsupervised algorithm has automatically distinguished 12
        classes: 11 classes of VIPERS galaxies and an additional class
        of broad-line active galactic nuclei (AGNs). After a first broad
        division into blue, green, and red categories, we obtained a
        further sub-division into: three red, three green, and five blue
        galaxy classes. The FEM classes follow the galaxy sequence from
        the earliest to the latest types, which is reflected in their
        colours (which are constructed from rest-frame magnitudes used
        in the classification procedure) but also their morphological,
        physical, and spectroscopic properties (not included in the
        classification scheme). We demonstrate that the members of each
        class share similar physical and spectral properties. In
        particular, we are able to find three different classes of red
        passive galaxy populations. Thus, we demonstrate the potential
        of an unsupervised approach to galaxy classification and we
        retrieve the complexity of galaxy populations at z ̃ 0.7, a task
        that usual, simpler, colour-based approaches cannot fulfil.
        Based on observations collected at the European Southern
        Observatory, Cerro Paranal, Chile, using the Very Large
        Telescope under programs 182.A-0886 and partly 070.A-9007. Also
        based on observations obtained with MegaPrime/MegaCam, a joint
        project of CFHT and CEA/DAPNIA, at the Canada-France-Hawaii
        Telescope (CFHT), which is operated by the National Research
        Council (NRC) of Canada, the Institut National des Sciences de
        l'Univers of the Centre National de la Recherche Scientifique
        (CNRS) of France, and the University of Hawaii. This work is
        based in part on data products produced at TERAPIX and the
        Canadian Astronomy Data Centre as part of the Canada-France-
        Hawaii Telescope Legacy Survey, a collaborative project of NRC
        and CNRS. The VIPERS web site is <A href=``http://www.vipers.ina
        f.it/''>http://www.vipers.inaf.it/</A>}",
          doi = {10.1051/0004-6361/201832784},
archivePrefix = {arXiv},
       eprint = {1805.09904},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...617A..70S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...616A.163V,
       author = {{Vida}, Kriszti{\'a}n and {Roettenbacher}, Rachael M.},
        title = "{Finding flares in Kepler data using machine-learning tools}",
      journal = {\aap},
     keywords = {methods: data analysis, techniques: photometric, stars: activity, stars: flare, stars: late-type, stars: low-mass, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Sep",
       volume = {616},
          eid = {A163},
        pages = {A163},
     abstract = "{Context. Archives of long photometric surveys, such as the Kepler
        database, are a great basis for studying flares. However,
        identifying the flares is a complex task; it is easily done in
        the case of single-target observations by visual inspection, but
        is nearly impossible for several year-long time series for
        several thousand targets. Although automated methods for this
        task exist, several problems are difficult (or impossible) to
        overcome with traditional fitting and analysis approaches. <BR
        /> Aims: We introduce a code for identifying and analyzing
        flares based on machine-learning methods, which are
        intrinsically adept at handling such data sets. <BR /> Methods:
        We used the RANSAC (RANdom SAmple Consensus) algorithm to model
        light curves, as it yields robust fits even in the case of
        several outliers, such as flares. The light curves were divided
        into search windows, approximately on the order of the stellar
        rotation period. This search window was shifted over the data
        set, and a voting system was used to keep false positives to a
        minimum: only those flare candidate points were kept that were
        identified as a flare in several windows. <BR /> Results: The
        code was tested on short-cadence K2 observations of TRAPPIST-1
        and on long-cadence Kepler data of KIC 1722506. The detected
        flare events and flare energies are consistent with earlier
        results from manual inspections.}",
          doi = {10.1051/0004-6361/201833194},
archivePrefix = {arXiv},
       eprint = {1806.00334},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...616A.163V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PASP..130h4203K,
       author = {{Kong}, Xiao and {Luo}, A. -Li and {Li}, Xiang-Ru and {Wang}, You-Fen and
         {Li}, Yin-Bi and {Zhao}, Jing-Kun},
        title = "{Spectral Feature Extraction for DB White Dwarfs Through Machine Learning Applied to New Discoveries in the Sdss DR12 and DR14}",
      journal = {\pasp},
     keywords = {Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Aug",
       volume = {130},
       number = {990},
        pages = {084203},
     abstract = "{Using a machine learning (ML) method, we mine DB white dwarfs (DBWDs)
        from the Sloan Digital Sky Survey (SDSS) Data Release (DR) 12
        and DR14. The ML method consists of two parts: feature
        extraction and classification. The least absolute shrinkage and
        selection operator (LASSO) is used for the spectral feature
        extraction by comparing high quality data of a positive sample
        group with negative sample groups. In both the training and
        testing sets, the positive sample group is composed of a
        selection of 300 known DBWDs, while the negative sample groups
        are obtained from all types of SDSS spectra. In the space of the
        LASSO detected features, a support vector machine is then
        employed to build classifiers that are used to separate the
        DBWDs from the non-DBWDs for each individual type. Depending on
        the classifiers, the DBWD candidates are selected from the
        entire SDSS data set. After visual inspection, 2808 spectra
        (2029 objects) are spectroscopically confirmed. By checking the
        samples with the literature, there are 58 objects with 60
        spectra that are newly identified, including a newly discovered
        AM CVn. Finally, we measure their effective temperatures (T
        $_{eff}$), surface gravities (log g), and radial velocities,
        before compiling them into a catalog.}",
          doi = {10.1088/1538-3873/aac7a8},
archivePrefix = {arXiv},
       eprint = {1806.08473},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PASP..130h4203K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.478.5547A,
       author = {{Alger}, M.~J. and {Banfield}, J.~K. and {Ong}, C.~S. and {Rudnick}, L. and
         {Wong}, O.~I. and {Wolf}, C. and {Andernach}, H. and {Norris}, R.~P. and
         {Shabala}, S.~S.},
        title = "{Radio Galaxy Zoo: machine learning for radio source host galaxy cross-identification}",
      journal = {\mnras},
     keywords = {methods: statistical, techniques: miscellaneous, galaxies: active, infrared: galaxies, radio continuum: galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Aug",
       volume = {478},
       number = {4},
        pages = {5547-5563},
     abstract = "{We consider the problem of determining the host galaxies of radio
        sources by cross-identification. This has traditionally been
        done manually, which will be intractable for wide-area radio
        surveys like the Evolutionary Map of the Universe. Automated
        cross-identification will be critical for these future surveys,
        and machine learning may provide the tools to develop such
        methods. We apply a standard approach from computer vision to
        cross-identification, introducing one possible way of automating
        this problem, and explore the pros and cons of this approach. We
        apply our method to the 1.4 GHz Australian Telescope Large Area
        Survey (ATLAS) observations of the Chandra Deep Field South
        (CDFS) and the ESO Large Area ISO Survey South 1 fields by
        cross-identifying them with the Spitzer Wide-area Infrared
        Extragalactic survey. We train our method with two sets of data:
        expert cross-identifications of CDFS from the initial ATLAS data
        release and crowdsourced cross-identifications of CDFS from
        Radio Galaxy Zoo. We found that a simple strategy of cross-
        identifying a radio component with the nearest galaxy performs
        comparably to our more complex methods, though our estimated
        best-case performance is near 100 per cent. ATLAS contains 87
        complex radio sources that have been cross-identified by
        experts, so there are not enough complex examples to learn how
        to cross-identify them accurately. Much larger data sets are
        therefore required for training methods like ours. We also show
        that training our method on Radio Galaxy Zoo cross-
        identifications gives comparable results to training on expert
        cross-identifications, demonstrating the value of crowdsourced
        training data.}",
          doi = {10.1093/mnras/sty1308},
archivePrefix = {arXiv},
       eprint = {1805.05540},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.478.5547A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.478.4225A,
       author = {{Armstrong}, David J. and {G{\"u}nther}, Maximilian N. and
         {McCormac}, James and {Smith}, Alexis M.~S. and {Bayliss}, Daniel and
         {Bouchy}, Fran{\c{c}}ois and {Burleigh}, Matthew R. and
         {Casewell}, Sarah and {Eigm{\"u}ller}, Philipp and {Gillen}, Edward and
         {Goad}, Michael R. and {Hodgkin}, Simon T. and {Jenkins}, James S. and
         {Louden}, Tom and {Metrailler}, Lionel and {Pollacco}, Don and
         {Poppenhaeger}, Katja and {Queloz}, Didier and {Raynard}, Liam and
         {Rauer}, Heike and {Udry}, St{\'e}phane and {Walker}, Simon R. and
         {Watson}, Christopher A. and {West}, Richard G. and
         {Wheatley}, Peter J.},
        title = "{Automatic vetting of planet candidates from ground-based surveys: machine learning with NGTS}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, planets and satellites: detection, planets and satellites: general, Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Aug",
       volume = {478},
       number = {3},
        pages = {4225-4237},
     abstract = "{State-of-the art exoplanet transit surveys are producing ever increasing
        quantities of data. To make the best use of this resource, in
        detecting interesting planetary systems or in determining
        accurate planetary population statistics, requires new automated
        methods. Here, we describe a machine learning algorithm that
        forms an integral part of the pipeline for the NGTS transit
        survey, demonstrating the efficacy of machine learning in
        selecting planetary candidates from multi-night ground-based
        survey data. Our method uses a combination of random forests and
        self-organizing maps to rank planetary candidates, achieving an
        AUC score of 97.6 per cent in ranking 12368 injected planets
        against 27496 false positives in the NGTS data. We build on past
        examples by using injected transit signals to form a training
        set, a necessary development for applying similar methods to
        upcoming surveys. We also make the autovet code used to
        implement the algorithm publicly accessible. autovet is designed
        to perform machine-learned vetting of planetary candidates, and
        can utilize a variety of methods. The apparent robustness of
        machine learning techniques, whether on space-based or the
        qualitatively different ground-based data, highlights their
        importance to future surveys such as TESS and PLATO and the need
        to better understand their advantages and pitfalls in an
        exoplanetary context.}",
          doi = {10.1093/mnras/sty1313},
archivePrefix = {arXiv},
       eprint = {1805.07089},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.478.4225A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.478.3410A,
       author = {{Agarwal}, Shankar and {Dav{\'e}}, Romeel and {Bassett}, Bruce A.},
        title = "{Painting galaxies into dark matter haloes using machine learning}",
      journal = {\mnras},
     keywords = {galaxies: evolution, cosmology: theory, large-scale structure of Universe, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Aug",
       volume = {478},
       number = {3},
        pages = {3410-3422},
     abstract = "{We develop a machine learning (ML) framework to populate large dark
        matter-only simulations with baryonic galaxies. Our ML framework
        takes input halo properties including halo mass, environment,
        spin, and recent growth history, and outputs central galaxy and
        halo baryonic properties, including stellar mass (M$_{*}$), star
        formation rate (SFR), metallicity (Z), neutral (H I), and
        molecular (H\_2) hydrogen mass. We apply this to the MUFASA
        cosmological hydrodynamic simulation, and show that it recovers
        the mean trends of output quantities with halo mass highly
        accurately, including following the sharp drop in SFR and gas in
        quenched massive galaxies. However, the scatter around the mean
        relations is underpredicted. Examining galaxies individually, at
        z = 0, the stellar mass and metallicity are accurately recovered
        ({\ensuremath{\sigma}} {\ensuremath{\lesssim}} 0.2 dex), but SFR
        and H I show larger scatter ({\ensuremath{\sigma}}
        {\ensuremath{\gtrsim}} 0.3 dex); these values improve somewhat
        at z = 1 and 2. Remarkably, ML quantitatively recovers second
        parameter trends in galaxy properties, e.g. that galaxies with
        higher gas content and lower metallicity have higher SFR at a
        given M$_{*}$. Testing various ML algorithms, we find that none
        perform significantly better than the others, nor does
        ensembling improve performance, likely because none of the
        algorithms reproduce the large observed scatter around the mean
        properties. For the random forest algorithm, we find that halo
        mass and nearby (̃200 kpc) environment are the most important
        predictive variables followed by growth history, while halo spin
        and ̃Mpc-scale environment are not important. Finally, we study
        the impact of additionally inputting key baryonic properties
        M$_{*}$, SFR and Z, as would be available e.g. from an
        equilibrium model, and show that particularly providing the SFR
        enables H I to be recovered substantially more accurately.}",
          doi = {10.1093/mnras/sty1169},
archivePrefix = {arXiv},
       eprint = {1712.03255},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.478.3410A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.478.3177T,
       author = {{Teimoorinia}, H. and {Keown}, J.},
        title = "{The discrimination between star-forming and AGN galaxies in the absence of H {\ensuremath{\alpha}} and [N II]: a machine -learning approach}",
      journal = {\mnras},
     keywords = {methods: data analysis, galaxies: active, galaxies: Seyfert, galaxies: star formation, galaxies: statistics, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Aug",
       volume = {478},
       number = {3},
        pages = {3177-3188},
     abstract = "{In the absence of the two emission lines H {\ensuremath{\alpha}} and [N
        II] (6584 {\r{A}}) in a BPT diagram, we show that other spectral
        information is sufficiently informative to distinguish active
        galactic nucleus (AGN) galaxies from star-forming galaxies. We
        use pattern recognition methods and a sample of galaxy spectra
        from the Sloan Digital Sky Survey to show that, in this survey,
        the flux and equivalent width of [O III] (5007 {\r{A}}) and H
        {\ensuremath{\beta}}, along with the 4000 {\r{A}} break, can be
        used to classify galaxies in a BPT diagram. This method provides
        a higher accuracy of predictions than those which use stellar
        mass and [O III]/H {\ensuremath{\beta}}. First, we use BPT
        diagrams and various physical parameters to re-classify the
        galaxies. Next, using confusion matrices, we determine the
        `correctly' predicted classes as well as confused cases. In this
        way, we investigate the effect of each parameter in the
        confusion matrices and rank the physical parameters used in the
        discrimination of the different classes. We show that in this
        survey, for example, \{g - r\} colour can provide the same
        accuracy as galaxy stellar mass to predict whether or not a
        galaxy hosts an AGN. Finally, with the same information, we also
        rank the parameters involved in the discrimination of Seyfert
        and LINER galaxies.}",
          doi = {10.1093/mnras/sty1331},
archivePrefix = {arXiv},
       eprint = {1805.04069},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.478.3177T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.478.2291C,
       author = {{Cohn}, J.~D.},
        title = "{Approximations to galaxy star formation rate histories: properties and uses of two examples}",
      journal = {\mnras},
     keywords = {galaxies: evolution, galaxies: formation, galaxies: haloes, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Aug",
       volume = {478},
       number = {2},
        pages = {2291-2314},
     abstract = "{Galaxies evolve via a complex interaction of numerous different physical
        processes, scales, and components. In spite of this, overall
        trends often appear. Simplified models for galaxy histories can
        be used to search for and capture such emergent trends, and thus
        to interpret and compare results of galaxy formation models to
        each other and to nature. Here, two approximations are applied
        to galaxy integrated star formation rate histories, drawn from a
        semi-analytic model grafted on to a dark matter simulation. Both
        a lognormal functional form and principal component analysis
        (PCA) approximate the integrated star formation rate histories
        fairly well. Machine learning, based upon simplified galaxy halo
        histories, is somewhat successful at recovering both fits. The
        fits to the histories give fixed time star formation rates which
        have notable scatter from their true final time rates,
        especially for quiescent and `green valley' galaxies, and more
        so for the PCA fit. For classifying galaxies into subfamilies
        sharing similar integrated histories, both approximations are
        better than using final stellar mass or specific star formation
        rate. Several subsamples from the simulation illustrate how
        these simple parametrizations provide points of contact for
        comparisons between different galaxy formation samples, or more
        generally, models. As a side result, the halo masses of
        simulated galaxies with early peak star formation rate
        (according to the lognormal fit) are bimodal. The galaxies with
        a lower halo mass at peak star formation rate appear to stall in
        their halo growth, even though they are central in their host
        haloes.}",
          doi = {10.1093/mnras/sty1148},
archivePrefix = {arXiv},
       eprint = {1802.06197},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.478.2291C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...862..101A,
       author = {{An}, Fang Xia and {Stach}, S.~M. and {Smail}, Ian and
         {Swinbank}, A.~M. and {Almaini}, O. and {Simpson}, C. and
         {Hartley}, W. and {Maltby}, D.~T. and {Ivison}, R.~J. and
         {Arumugam}, V. and {Wardlow}, J.~L. and {Cooke}, E.~A. and
         {Gullberg}, B. and {Thomson}, A.~P. and {Chen}, Chian-Chou and
         {Simpson}, J.~M. and {Geach}, J.~E. and {Scott}, D. and
         {Dunlop}, J.~S. and {Farrah}, D. and {van der Werf}, P. and
         {Blain}, A.~W. and {Conselice}, C. and {Micha{\l}owski}, M. and
         {Chapman}, S.~C. and {Coppin}, K.~E.~K.},
        title = "{A Machine-learning Method for Identifying Multiwavelength Counterparts of Submillimeter Galaxies: Training and Testing Using AS2UDS and ALESS}",
      journal = {\apj},
     keywords = {cosmology: observations, galaxies: evolution, galaxies: formation, galaxies: high-redshift, galaxies: starburst, submillimeter: galaxies, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Aug",
       volume = {862},
       number = {2},
          eid = {101},
        pages = {101},
     abstract = "{We describe the application of supervised machine-learning algorithms to
        identify the likely multiwavelength counterparts to
        submillimeter sources detected in panoramic, single-dish
        submillimeter surveys. As a training set, we employ a sample of
        695 (S $_{870{\ensuremath{\mu}}m}$ {\ensuremath{\gtrsim}} 1 mJy)
        submillimeter galaxies (SMGs) with precise identifications from
        the ALMA follow-up of the SCUBA-2 Cosmology Legacy
        Survey{\textquoteright}s UKIDSS-UDS field (AS2UDS). We show that
        radio emission, near-/mid-infrared colors, photometric redshift,
        and absolute H-band magnitude are effective predictors that can
        distinguish SMGs from submillimeter-faint field galaxies. Our
        combined radio + machine-learning method is able to successfully
        recover ̃85\% of ALMA-identified SMGs that are detected in at
        least three bands from the ultraviolet to radio. We confirm the
        robustness of our method by dividing our training set into
        independent subsets and using these for training and testing,
        respectively, as well as applying our method to an independent
        sample of ̃100 ALMA-identified SMGs from the ALMA/LABOCA ECDF-
        South Survey (ALESS). To further test our methodology, we stack
        the 870 {\ensuremath{\mu}}m ALMA maps at the positions of those
        K-band galaxies that are classified as SMG counterparts by the
        machine learning but do not have a \&gt;4.3{\ensuremath{\sigma}}
        ALMA detection. The median peak flux density of these galaxies
        is S $_{870{\ensuremath{\mu}}m}$ = (0.61 {\ensuremath{\pm}}
        0.03) mJy, demonstrating that our method can recover faint
        and/or diffuse SMGs even when they are below the detection
        threshold of our ALMA observations. In future, we will apply
        this method to samples drawn from panoramic single-dish
        submillimeter surveys that currently lack interferometric
        follow-up observations to address science questions that can
        only be tackled with large statistical samples of SMGs.}",
          doi = {10.3847/1538-4357/aacdaa},
archivePrefix = {arXiv},
       eprint = {1806.06859},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...862..101A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...616L..16S,
       author = {{Schawinski}, Kevin and {Turp}, M. Dennis and {Zhang}, Ce},
        title = "{Exploring galaxy evolution with generative models}",
      journal = {\aap},
     keywords = {methods: data analysis, methods: statistical, galaxies: evolution, Astrophysics - Astrophysics of Galaxies, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2018",
        month = "Aug",
       volume = {616},
          eid = {L16},
        pages = {L16},
     abstract = "{Context. Generative models open up the possibility to interrogate
        scientific data in a more data-driven way. <BR /> Aims: We
        propose a method that uses generative models to explore
        hypotheses in astrophysics and other areas. We use a neural
        network to show how we can independently manipulate physical
        attributes by encoding objects in latent space. <BR /> Methods:
        By learning a latent space representation of the data, we can
        use this network to forward model and explore hypotheses in a
        data-driven way. We train a neural network to generate
        artificial data to test hypotheses for the underlying physical
        processes. <BR /> Results: We demonstrate this process using a
        well-studied process in astrophysics, the quenching of star
        formation in galaxies as they move from low-to high-density
        environments. This approach can help explore astrophysical and
        other phenomena in a way that is different from current methods
        based on simulations and observations.}",
          doi = {10.1051/0004-6361/201833800},
archivePrefix = {arXiv},
       eprint = {1812.01114},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...616L..16S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...616L..11K,
       author = {{Krone-Martins}, A. and {Delchambre}, L. and {Wertz}, O. and
         {Ducourant}, C. and {Mignard}, F. and {Teixeira}, R. and
         {Kl{\"u}ter}, J. and {Le Campion}, J. -F. and {Galluccio}, L. and
         {Surdej}, J. and {Bastian}, U. and {Wambsganss}, J. and
         {Graham}, M.~J. and {Djorgovski}, S.~G. and {Slezak}, E.},
        title = "{Gaia GraL: Gaia DR2 gravitational lens systems. I. New quadruply imaged quasar candidates around known quasars}",
      journal = {\aap},
     keywords = {gravitational lensing: strong, quasars: general, astrometry, methods: data analysis, catalogs, surveys, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Aug",
       volume = {616},
          eid = {L11},
        pages = {L11},
     abstract = "{Context. Multiply imaged gravitationally lensed quasars are among the
        most interesting and useful observable extragalactic phenomena.
        Because their study constitutes a unique tool in various fields
        of astronomy, they are highly sought, but difficult to find.
        Even in this era of all-sky surveys, discovering them remains a
        great challenge, with barely a few hundred systems currently
        known. <BR /> Aims: We aim to discover new multiply imaged
        quasar candidates in the recently published Gaia Data Release 2
        (DR2), which is the astrometric and photometric all-sky survey
        with the highest spatial resolution that achieves effective
        resolutions from 0.4″ to 2.2″. <BR /> Methods: We cross-matched
        a merged list of quasars and candidates with Gaia DR2 and found
        1 839 143 counterparts within 0.5″. We then searched matches
        with more than two Gaia DR2 counterparts within 6″. We further
        narrowed the resulting list using astrometry and photometry
        compatibility criteria between the Gaia DR2 counterparts. A
        supervised machine-learning method, called extremely randomized
        trees, was finally adopted to assign a probability of being
        lensed to each remaining system. <BR /> Results: We report the
        discovery of two quadruply imaged quasar candidates that are
        fully detected in Gaia DR2. These are the most promising new
        quasar lens candidates from Gaia DR2 and a simple singular
        isothermal ellipsoid lens model is able to reproduce their image
        positions to within 1 mas. This Letter demonstrates the
        discovery potential of Gaia for gravitational lenses.}",
          doi = {10.1051/0004-6361/201833337},
archivePrefix = {arXiv},
       eprint = {1804.11051},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...616L..11K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...616A..97D,
       author = {{D'Isanto}, A. and {Cavuoti}, S. and {Gieseke}, F. and
         {Polsterer}, K.~L.},
        title = "{Return of the features. Efficient feature selection and interpretation for photometric redshifts}",
      journal = {\aap},
     keywords = {methods: data analysis, methods: statistical, galaxies: distances and redshifts, quasars: general, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Aug",
       volume = {616},
          eid = {A97},
        pages = {A97},
     abstract = "{Context. The explosion of data in recent years has generated an
        increasing need for new analysis techniques in order to extract
        knowledge from massive data-sets. Machine learning has proved
        particularly useful to perform this task. Fully automatized
        methods (e.g. deep neural networks) have recently gathered great
        popularity, even though those methods often lack physical
        interpretability. In contrast, feature based approaches can
        provide both well-performing models and understandable
        causalities with respect to the correlations found between
        features and physical processes. <BR /> Aims: Efficient feature
        selection is an essential tool to boost the performance of
        machine learning models. In this work, we propose a forward
        selection method in order to compute, evaluate, and characterize
        better performing features for regression and classification
        problems. Given the importance of photometric redshift
        estimation, we adopt it as our case study. <BR /> Methods: We
        synthetically created 4520 features by combining magnitudes,
        errors, radii, and ellipticities of quasars, taken from the
        Sloan Digital Sky Survey (SDSS). We apply a forward selection
        process, a recursive method in which a huge number of feature
        sets is tested through a k-Nearest-Neighbours algorithm, leading
        to a tree of feature sets. The branches of the feature tree are
        then used to perform experiments with the random forest, in
        order to validate the best set with an alternative model. <BR />
        Results: We demonstrate that the sets of features determined
        with our approach improve the performances of the regression
        models significantly when compared to the performance of the
        classic features from the literature. The found features are
        unexpected and surprising, being very different from the classic
        features. Therefore, a method to interpret some of the found
        features in a physical context is presented. <BR /> Conclusions:
        The feature selection methodology described here is very general
        and can be used to improve the performance of machine learning
        models for any regression or classification task. The three
        catalogues are only available at the CDS via anonymous ftp to <A
        href=``http://cdsarc.u-strasbg.fr/''>http://cdsarc.u-strasbg.fr<
        /A> (ftp://130.79.128.5) or via <A
        href=``http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/616/A97''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/616/A97</A>}",
          doi = {10.1051/0004-6361/201833103},
archivePrefix = {arXiv},
       eprint = {1803.10032},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...616A..97D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...616A..69B,
       author = {{Bilicki}, M. and {Hoekstra}, H. and {Brown}, M.~J.~I. and {Amaro}, V. and
         {Blake}, C. and {Cavuoti}, S. and {de Jong}, J.~T.~A. and
         {Georgiou}, C. and {Hildebrandt}, H. and {Wolf}, C. and {Amon}, A. and
         {Brescia}, M. and {Brough}, S. and {Costa-Duarte}, M.~V. and
         {Erben}, T. and {Glazebrook}, K. and {Grado}, A. and {Heymans}, C. and
         {Jarrett}, T. and {Joudaki}, S. and {Kuijken}, K. and {Longo}, G. and
         {Napolitano}, N. and {Parkinson}, D. and {Vellucci}, C. and
         {Verdoes Kleijn}, G.~A. and {Wang}, L.},
        title = "{Photometric redshifts for the Kilo-Degree Survey. Machine-learning analysis with artificial neural networks}",
      journal = {\aap},
     keywords = {galaxies: distances and redshifts, catalogs, large-scale structure of Universe, methods: data analysis, methods: numerical, methods: statistical, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Aug",
       volume = {616},
          eid = {A69},
        pages = {A69},
     abstract = "{We present a machine-learning photometric redshift (ML photo-z) analysis
        of the Kilo-Degree Survey Data Release 3 (KiDS DR3), using two
        neural-network based techniques: ANNz2 and MLPQNA. Despite
        limited coverage of spectroscopic training sets, these ML codes
        provide photo-zs of quality comparable to, if not better than,
        those from the Bayesian Photometric Redshift (BPZ) code, at
        least up to z$_{phot}$ {\ensuremath{\lesssim}} 0.9 and r
        {\ensuremath{\lesssim}} 23.5. At the bright end of r
        {\ensuremath{\lesssim}} 20, where very complete spectroscopic
        data overlapping with KiDS are available, the performance of the
        ML photo-zs clearly surpasses that of BPZ, currently the primary
        photo-z method for KiDS. Using the Galaxy And Mass Assembly
        (GAMA) spectroscopic survey as calibration, we furthermore study
        how photo-zs improve for bright sources when photometric
        parameters additional to magnitudes are included in the photo-z
        derivation, as well as when VIKING and WISE infrared (IR) bands
        are added. While the fiducial four-band ugri setup gives a
        photo-z bias \&lt;{\ensuremath{\delta}}z/(1 + z)\&gt; = -2
        {\texttimes} 10$^{-4}$ and scatter
        {\ensuremath{\sigma}}$_{{\ensuremath{\delta}}z/(1+z)}$ \&lt;
        0.022 at mean \&lt;z\&gt; = 0.23, combining magnitudes, colours,
        and galaxy sizes reduces the scatter by 7\% and the bias by an
        order of magnitude. Once the ugri and IR magnitudes are joined
        into 12-band photometry spanning up to 12 {\ensuremath{\mu}}m,
        the scatter decreases by more than 10\% over the fiducial case.
        Finally, using the 12 bands together with optical colours and
        linear sizes gives \&lt;{\ensuremath{\delta}}z/(1 + z)\&gt;
        \&lt; 4 {\texttimes} 10$^{-5}$ and
        {\ensuremath{\sigma}}$_{{\ensuremath{\delta}}z/(1+z)}$ \&lt;
        0.019. This paper also serves as a reference for two public
        photo-z catalogues accompanying KiDS DR3, both obtained using
        the ANNz2 code. The first one, of general purpose, includes all
        the 39 million KiDS sources with four-band ugri measurements in
        DR3. The second dataset, optimised for low-redshift studies such
        as galaxy-galaxy lensing, is limited to r
        {\ensuremath{\lesssim}} 20, and provides photo-zs of much better
        quality than in the full-depth case thanks to incorporating
        optical magnitudes, colours, and sizes in the GAMA-calibrated
        photo-z derivation.}",
          doi = {10.1051/0004-6361/201731942},
archivePrefix = {arXiv},
       eprint = {1709.04205},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...616A..69B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018SpWea..16..777M,
       author = {{Murray}, Sophie A.},
        title = "{The Importance of Ensemble Techniques for Operational Space Weather Forecasting}",
      journal = {Space Weather},
     keywords = {space weather, operational forecasting, ensembles, frontier techniques, research to operations, Physics - Space Physics, Astrophysics - Earth and Planetary Astrophysics, Physics - Atmospheric and Oceanic Physics},
         year = "2018",
        month = "Jul",
       volume = {16},
       number = {7},
        pages = {777-783},
     abstract = "{The space weather community has begun to use frontier methods such as
        data assimilation, machine learning, and ensemble modeling to
        advance current operational forecasting efforts. This was
        highlighted by a multidisciplinary session at the 2017 American
        Geophysical Union Meeting, Frontier Solar-Terrestrial Science
        Enabled by the Combination of Data-Driven Techniques and
        Physics-Based Understanding, with considerable discussion
        surrounding ensemble techniques. Here ensemble methods are
        described in detail, using a set of predictions to improve on a
        single-model output, for example, taking a simple average of
        multiple models, or using more complex techniques for data
        assimilation. They have been used extensively in fields such as
        numerical weather prediction and data science, for both
        improving model accuracy and providing a measure of model
        uncertainty. Researchers in the space weather community have
        found them to be similarly useful, and some examples of success
        stories are highlighted in this commentary. Future developments
        are also encouraged to transition these basic research efforts
        to operational forecasting as well as providing prediction
        errors to aid end-user understanding.}",
          doi = {10.1029/2018SW001861},
archivePrefix = {arXiv},
       eprint = {1806.09861},
 primaryClass = {physics.space-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018SpWea..16..777M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018RPPh...81g4001D,
       author = {{Dunjko}, Vedran and {Briegel}, Hans J.},
        title = "{Machine learning \&amp; artificial intelligence in the quantum domain: a review of recent progress}",
      journal = {Reports on Progress in Physics},
         year = "2018",
        month = "Jul",
       volume = {81},
       number = {7},
          eid = {074001},
        pages = {074001},
     abstract = "{Quantum information technologies, on the one hand, and intelligent
        learning systems, on the other, are both emergent technologies
        that are likely to have a transformative impact on our society
        in the future. The respective underlying fields of basic
        research{\textemdash}quantum information versus machine learning
        (ML) and artificial intelligence (AI){\textemdash}have their own
        specific questions and challenges, which have hitherto been
        investigated largely independently. However, in a growing body
        of recent work, researchers have been probing the question of
        the extent to which these fields can indeed learn and benefit
        from each other. Quantum ML explores the interaction between
        quantum computing and ML, investigating how results and
        techniques from one field can be used to solve the problems of
        the other. Recently we have witnessed significant breakthroughs
        in both directions of influence. For instance, quantum computing
        is finding a vital application in providing speed-ups for ML
        problems, critical in our {\textquoteleft}big
        data{\textquoteright} world. Conversely, ML already permeates
        many cutting-edge technologies and may become instrumental in
        advanced quantum technologies. Aside from quantum speed-up in
        data analysis, or classical ML optimization used in quantum
        experiments, quantum enhancements have also been (theoretically)
        demonstrated for interactive learning tasks, highlighting the
        potential of quantum-enhanced learning agents. Finally, works
        exploring the use of AI for the very design of quantum
        experiments and for performing parts of genuine research
        autonomously, have reported their first successes. Beyond the
        topics of mutual enhancement{\textemdash}exploring what ML/AI
        can do for quantum physics and vice
        versa{\textemdash}researchers have also broached the fundamental
        issue of quantum generalizations of learning and AI concepts.
        This deals with questions of the very meaning of learning and
        intelligence in a world that is fully described by quantum
        mechanics. In this review, we describe the main ideas, recent
        developments and progress in a broad spectrum of research
        investigating ML and AI in the quantum domain.}",
          doi = {10.1088/1361-6633/aab406},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018RPPh...81g4001D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PhRvD..98b3019F,
       author = {{Fujimoto}, Yuki and {Fukushima}, Kenji and {Murase}, Koichi},
        title = "{Methodology study of machine learning for the neutron star equation of state}",
      journal = {\prd},
     keywords = {Nuclear Theory, Astrophysics - High Energy Astrophysical Phenomena, High Energy Physics - Phenomenology},
         year = "2018",
        month = "Jul",
       volume = {98},
       number = {2},
          eid = {023019},
        pages = {023019},
     abstract = "{We discuss a methodology of machine learning to deduce the neutron star
        equation of state from a set of mass-radius observational data.
        We propose an efficient procedure to deal with a mapping from
        finite data points with observational errors onto an equation of
        state. We generate training data and optimize the neural
        network. Using independent validation data (mock observational
        data) we confirm that the equation of state is correctly
        reconstructed with precision surpassing observational errors. We
        finally discuss the relation between our method and Bayesian
        analysis with an emphasis put on generality of our method for
        underdetermined problems.}",
          doi = {10.1103/PhysRevD.98.023019},
archivePrefix = {arXiv},
       eprint = {1711.06748},
 primaryClass = {nucl-th},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhRvD..98b3019F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.478.1209F,
       author = {{Farah}, W. and {Flynn}, C. and {Bailes}, M. and {Jameson}, A. and
         {Bannister}, K.~W. and {Barr}, E.~D. and {Bateman}, T. and {Bhand
        ari}, S. and {Caleb}, M. and {Campbell-Wilson}, D. and {Chang}, S. -W. and
         {Deller}, A. and {Green}, A.~J. and {Hunstead}, R. and {Jankowski}, F. and
         {Keane}, E. and {Macquart}, J. -P. and {M{\"o}ller}, A. and
         {Onken}, C.~A. and {Os{\l}owski}, S. and {Parthasarathy}, A. and
         {Plant}, K. and {Ravi}, V. and {Shannon}, R.~M. and {Tucker}, B.~E. and
         {Venkatraman Krishnan}, V. and {Wolf}, C.},
        title = "{FRB microstructure revealed by the real-time detection of FRB170827}",
      journal = {\mnras},
     keywords = {instrumentation: interferometers, methods: data analysis, radio continuum: transients, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2018",
        month = "Jul",
       volume = {478},
       number = {1},
        pages = {1209-1217},
     abstract = "{We report a new fast radio burst (FRB) discovered in real-time as part
        of the UTMOST project at the Molonglo Observatory Synthesis
        Radio Telescope. FRB170827 was first detected with our low-
        latency (\&lt;24 s) and machine-learning based FRB detection
        system. The FRB discovery was accompanied by the capture of
        voltage data at the native time and frequency resolution of the
        observing system, enabling coherent dedispersion and detailed
        off-line analysis that have unveiled fine temporal and frequency
        structure. The dispersion measure (DM) of 176.80
        {\ensuremath{\pm}} 0.04 pc cm$^{-3}$ is the lowest of the FRB
        population. The Milky Way contribution along the line of sight
        is ̃40 pc cm$^{-3}$, leaving an excess DM of ̃145 pc cm$^{-3}$.
        The FRB has a fluence \&gt;20 {\ensuremath{\pm}} 7 Jy ms, and is
        narrow with a width of ̃400 \{{\ensuremath{\mu}}\}s at 10 per
        cent of its maximum amplitude. However, the burst shows three
        temporal components, the narrowest of which is ̃30
        \{{\ensuremath{\mu}}\}s, and a scattering time-scale of 4.1
        {\ensuremath{\pm}} 2.7 \{{\ensuremath{\mu}}\}s. The FRB shows
        spectral modulations on frequency scales of 1.5 MHz and 0.1 MHz.
        Both are prominent in the dynamic spectrum, which shows a very
        bright region of emission between 841 and 843 MHz, and weaker
        and patchy emission across the entire band. We show that the
        fine spectral structure could arise in the FRB host galaxy, or
        its immediate vicinity.}",
          doi = {10.1093/mnras/sty1122},
archivePrefix = {arXiv},
       eprint = {1803.05697},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.478.1209F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.478.1132V,
       author = {{Vafaei Sadr}, A. and {Farhang}, M. and {Movahed}, S.~M.~S. and
         {Bassett}, B. and {Kunz}, M.},
        title = "{Cosmic string detection with tree-based machine learning}",
      journal = {\mnras},
     keywords = {methods: data analysis, observational, statistical, cosmic background radiation, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
         year = "2018",
        month = "Jul",
       volume = {478},
       number = {1},
        pages = {1132-1140},
     abstract = "{We explore the use of random forest and gradient boosting, two powerful
        tree-based machine learning algorithms, for the detection of
        cosmic strings in maps of the cosmic microwave background (CMB),
        through their unique Gott-Kaiser-Stebbins effect on the
        temperature anisotropies. The information in the maps is
        compressed into feature vectors before being passed to the
        learning units. The feature vectors contain various statistical
        measures of the processed CMB maps that boost cosmic string
        detectability. Our proposed classifiers, after training, give
        results similar to or better than claimed detectability levels
        from other methods for string tension, G{\ensuremath{\mu}}. They
        can make 3{\ensuremath{\sigma}} detection of strings with
        G{\ensuremath{\mu}} {\ensuremath{\gtrsim}} 2.1 {\texttimes}
        10$^{-10}$ for noise-free, 0.9$^{'}$-resolution CMB
        observations. The minimum detectable tension increases to
        G{\ensuremath{\mu}} {\ensuremath{\gtrsim}} 3.0 {\texttimes}
        10$^{-8}$ for a more realistic, CMB S4-like (II) strategy,
        improving over previous results.}",
          doi = {10.1093/mnras/sty1055},
archivePrefix = {arXiv},
       eprint = {1801.04140},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.478.1132V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.477.3433E,
       author = {{Eriksen}, Martin and {Hoekstra}, Henk},
        title = "{Implications of a wavelength-dependent PSF for weak lensing measurements}",
      journal = {\mnras},
     keywords = {gravitational lensing: weak, methods: data analysis, space vehicles: instruments, cosmological parameters, cosmology: observations, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2018",
        month = "Jul",
       volume = {477},
       number = {3},
        pages = {3433-3448},
     abstract = "{The convolution of galaxy images by the point spread function (PSF) is
        the dominant source of bias for weak gravitational lensing
        studies, and an accurate estimate of the PSF is required to
        obtain unbiased shape measurements. The PSF estimate for a
        galaxy depends on its spectral energy distribution (SED),
        because the instrumental PSF is generally a function of the
        wavelength. In this paper we explore various approaches to
        determine the resulting `effective' PSF using broad-band data.
        Considering the Euclid mission as a reference, we find that
        standard SED template fitting methods result in biases that
        depend on source redshift, although this may be remedied if the
        algorithms can be optimized for this purpose. Using a machine
        learning algorithm we show that, at least in principle, the
        required accuracy can be achieved with the current survey
        parameters. It is also possible to account for the correlations
        between photometric redshift and PSF estimates that arise from
        the use of the same photometry. We explore the impact of errors
        in photometric calibration, errors in the assumed wavelength
        dependence of the PSF model, and limitations of the adopted
        template libraries. Our results indicate that the required
        accuracy for Euclid can be achieved using the data that are
        planned to determine photometric redshifts.}",
          doi = {10.1093/mnras/sty830},
archivePrefix = {arXiv},
       eprint = {1707.04334},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.477.3433E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018JGRA..123.5640C,
       author = {{Clausen}, Lasse B.~N. and {Nickisch}, Hannes},
        title = "{Automatic Classification of Auroral Images From the Oslo Auroral THEMIS (OATH) Data Set Using Machine Learning}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {aurora, auroral imaging, machine learning},
         year = "2018",
        month = "Jul",
       volume = {123},
       number = {7},
        pages = {5640-5647},
     abstract = "{Based on their salient features we manually label 5,824 images from
        various Time History of Events and Macroscale Interactions
        during Substorms (THEMIS) all-sky imagers; the labels we use are
        clear/no aurora, cloudy, moon, arc, diffuse, and discrete. We
        then use a pretrained deep neural network to automatically
        extract a 1,001-dimensional feature vector from these images.
        Together, the labels and feature vectors are used to train a
        ridge classifier that is then able to correctly predict the
        category of unseen auroral images based on extracted features
        with 82\% accuracy. If we only distinguish between a binary
        classification aurora and no aurora, the true positive rate
        increases to 96\%. While this study paves the way for easy
        automatic classification of all auroral images from the THEMIS
        all-sky imager chain, we believe that the methodology shown here
        is readily applied to all images from any other auroral imager
        as long as the data are available in digital form. Both the
        neural network and the ridge classifier are free, off-the-shelf
        computer codes; the simplicity of our approach is demonstrated
        by the fact that our entire analysis comprises about 50 lines of
        Python code. Automatically attaching labels to all available
        all-sky imager data would enable statistical studies of
        unprecedented scope.}",
          doi = {10.1029/2018JA025274},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018JGRA..123.5640C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018JCAP...07..054H,
       author = {{Herbel}, J{\"o}rg and {Kacprzak}, Tomasz and {Amara}, Adam and
         {Refregier}, Alexandre and {Lucchi}, Aurelien},
        title = "{Fast point spread function modeling with deep learning}",
      journal = {Journal of Cosmology and Astro-Particle Physics},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Machine Learning},
         year = "2018",
        month = "Jul",
       volume = {2018},
       number = {7},
          eid = {054},
        pages = {054},
     abstract = "{Modeling the Point Spread Function (PSF) of wide-field surveys is vital
        for many astrophysical applications and cosmological probes
        including weak gravitational lensing. The PSF smears the image
        of any recorded object and therefore needs to be taken into
        account when inferring properties of galaxies from astronomical
        images. In the case of cosmic shear, the PSF is one of the
        dominant sources of systematic errors and must be treated
        carefully to avoid biases in cosmological parameters. Recently,
        forward modeling approaches to calibrate shear measurements
        within the Monte-Carlo Control Loops (MCCL) framework have been
        developed. These methods typically require simulating a large
        amount of wide-field images, thus, the simulations need to be
        very fast yet have realistic properties in key features such as
        the PSF pattern. Hence, such forward modeling approaches require
        a very flexible PSF model, which is quick to evaluate and whose
        parameters can be estimated reliably from survey data. We
        present a PSF model that meets these requirements based on a
        fast deep-learning method to estimate its free parameters. We
        demonstrate our approach on publicly available SDSS data. We
        extract the most important features of the SDSS sample via
        principal component analysis. Next, we construct our model based
        on perturbations of a fixed base profile, ensuring that it
        captures these features. We then train a Convolutional Neural
        Network to estimate the free parameters of the model from noisy
        images of the PSF. This allows us to render a model image of
        each star, which we compare to the SDSS stars to evaluate the
        performance of our method. We find that our approach is able to
        accurately reproduce the SDSS PSF at the pixel level, which, due
        to the speed of both the model evaluation and the parameter
        estimation, offers good prospects for incorporating our method
        into the MCCL framework.}",
          doi = {10.1088/1475-7516/2018/07/054},
archivePrefix = {arXiv},
       eprint = {1801.07615},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018JCAP...07..054H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018JATIS...4c8001I,
       author = {{Ibrahim}, Rehab Ali and {Elaziz}, Mohamed Abd and {Ewees}, Ahmed A. and
         {Selim}, Ibrahim M. and {Lu}, Songfeng},
        title = "{Galaxy images classification using hybrid brain storm optimization with moth flame optimization}",
      journal = {Journal of Astronomical Telescopes, Instruments, and Systems},
         year = "2018",
        month = "Jul",
       volume = {4},
          eid = {038001},
        pages = {038001},
     abstract = "{Galaxy classification has an important role in understanding the
        formation of galaxies and the evaluation of our universe. Most
        of the machine learning methods were used to improve galaxy
        image classification. However, these methods suffer from some
        limitations, such as getting stuck in local point and slow
        convergence. Therefore, an alternative method to enhance the
        performance of galaxy images classification and avoid the
        limitations of other methods is proposed. The proposed method
        for galaxy classification (called BSOMFOG) is based on an
        improvement in the brain storm optimization (BSO) through
        combining it with the moth flame optimization (MFO). In this
        modified version of BSO (called BSOMFO), the MFO algorithm works
        as a local search operator to enhance the exploitation ability
        of BSO. The performance of the BSOMFO algorithm is compared
        against other algorithms through two experiments. In the first
        one, a set of 15 global optimization problems is used to
        evaluate the ability of the BSOMFO algorithm to find the
        solution for these problems. Meanwhile, in the second
        experiment, the BSOMFO is included in the BSOMFOG framework to
        improve the classification of the galaxy images into three
        classes, namely, spiral, lenticular, and elliptical. BSOMFOG
        consists of three phases: the first phase is to extract the
        shape, color, and texture features from the galaxy images, while
        the second phase used the BSOMFO algorithm to select the
        relevant features from the extracted features. The last phase is
        to evaluate the selected features through classification using
        the k-nearest neighbor classifier. The experimental results show
        that the BSOMFO algorithm provides better results than the
        traditional BSO algorithm and other metaheuristic algorithms to
        solve the optimization problem. Moreover, it makes the proposed
        BSOMFOG framework improves the classification accuracy (̃97 \% )
        for galaxy images, and its general purpose makes it suitable for
        automatic classification of galaxies.}",
          doi = {10.1117/1.JATIS.4.3.038001},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018JATIS...4c8001I},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJS..237...19E,
       author = {{Erasmus}, N. and {McNeill}, A. and {Mommert}, M. and {Trilling}, D.~E. and
         {Sickafoose}, A.~A. and {van Gend}, C.},
        title = "{Taxonomy and Light-curve Data of 1000 Serendipitously Observed Main-belt Asteroids}",
      journal = {\apjs},
     keywords = {minor planets, asteroids: individual: Main-Belt Asteroids, surveys, techniques: photometric, Astrophysics - Earth and Planetary Astrophysics},
         year = "2018",
        month = "Jul",
       volume = {237},
       number = {1},
          eid = {19},
        pages = {19},
     abstract = "{We present VRI spectrophotometry of 1003 main-belt asteroids (MBAs)
        observed with the Sutherland, South Africa node of the Korea
        Microlensing Telescope Network (KMTNet). All of the observed
        MBAs were serendipitously captured in KMTNet{\textquoteright}s
        large 2{\textdegree} {\texttimes} 2{\textdegree} field of view
        during a separate targeted near-Earth Asteroid study. Our
        broadband spectrophotometry is reliable enough to distinguish
        among four asteroid taxonomies and we confidently categorize 836
        of the 1003 observed targets as either a S-, C-, X-, or D-type
        asteroid by means of a machine learning algorithm approach. Our
        data show that the ratio between S-type MBAs and (C+X+D)-type
        MBAs, with H magnitudes between 12 and 18 (12 km
        {\ensuremath{\gtrsim}} diameter {\ensuremath{\gtrsim}} 0.75 km),
        is almost exactly 1:1. Additionally, we report 0.5-3 hr (median:
        1.3 hr) light-curve data for each MBA and we resolve the
        complete rotation periods and amplitudes for 59 targets. Of the
        59 targets, 2 have rotation periods potentially below the
        theoretical zero-cohesion boundary limit of 2.2 hr. We report
        lower limits for the rotation periods and amplitudes for the
        remaining targets. Using the resolved and unresolved light
        curves we determine the shape distribution for this population
        using a Monte Carlo simulation. Our model suggests a population
        with an average elongation b/a = 0.74 {\ensuremath{\pm}} 0.07
        and also shows that this is independent of asteroid size and
        taxonomy.}",
          doi = {10.3847/1538-4365/aac38f},
archivePrefix = {arXiv},
       eprint = {1805.04478},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJS..237...19E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...861..128I,
       author = {{Inceoglu}, Fadil and {Jeppesen}, Jacob H. and {Kongstad}, Peter and
         {Hern{\'a}ndez Marcano}, N{\'e}stor J. and {Jacobsen}, Rune H. and
         {Karoff}, Christoffer},
        title = "{Using Machine Learning Methods to Forecast if Solar Flares Will Be Associated with CMEs and SEPs}",
      journal = {\apj},
     keywords = {Sun: activity, Sun: coronal mass ejections: CMEs, Sun: flares, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Jul",
       volume = {861},
       number = {2},
          eid = {128},
        pages = {128},
     abstract = "{Among the eruptive activity phenomena observed on the Sun, those that
        threaten human technology the most are flares with associated
        coronal mass ejections (CMEs) and solar energetic particles
        (SEPs). Flares with associated CMEs and SEPs are produced by
        magnetohydrodynamical processes in magnetically active regions
        (ARs) on the Sun. However, these ARs do not only produce flares
        with associated CMEs and SEPs, they also lead to flares and
        CMEs, which are not associated with any other event. In an
        attempt to distinguish flares with associated CMEs and SEPs from
        flares and CMEs, which are unassociated with any other event, we
        investigate the performances of two machine learning algorithms.
        To achieve this objective, we employ support vector machines
        (SVMs) and multilayer perceptrons (MLPs) using data from the
        Space Weather Database of Notification, Knowledge, Information
        of NASA Space Weather Center, the Geostationary Operational
        Environmental Satellite, and the Space-Weather Heliospheric and
        Magnetic Imager Active Region Patches. We show that True Skill
        Statistics (TSS) and Heidke Skill Scores (HSS) calculated for
        SVMs are slightly better than those from the MLPs. We also show
        that the forecasting time frame of 96 hr provides the best
        results in predicting if a flare will be associated with CMEs
        and SEPs (TSS = 0.92 {\ensuremath{\pm}} 0.09 and HSS = 0.92
        {\ensuremath{\pm}} 0.08). Additionally, we obtain the maximum
        TSS and HSS values of 0.91 {\ensuremath{\pm}} 0.06 for
        predicting that a flare will not be associated with CMEs and
        SEPs for the 36 hr forecast window, while the 108 hr forecast
        window gives the maximum TSS and HSS values for predicting that
        CMEs will not be accompanying any events (TSS = HSS = 0.98
        {\ensuremath{\pm}} 0.02).}",
          doi = {10.3847/1538-4357/aac81e},
archivePrefix = {arXiv},
       eprint = {1806.07117},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...861..128I},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...861...62P,
       author = {{Panos}, Brandon and {Kleint}, Lucia and {Huwyler}, Cedric and
         {Krucker}, S{\"a}m and {Melchior}, Martin and {Ullmann}, Denis and
         {Voloshynovskiy}, Sviatoslav},
        title = "{Identifying Typical Mg II Flare Spectra Using Machine Learning}",
      journal = {\apj},
     keywords = {Sun: chromosphere, Sun: flares, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Jul",
       volume = {861},
       number = {1},
          eid = {62},
        pages = {62},
     abstract = "{The Interface Region Imaging Spectrograph (IRIS) performs solar
        observations over a large range of atmospheric heights,
        including the chromosphere where the majority of flare energy is
        dissipated. The strong Mg II h\&amp;k spectral lines are capable
        of providing excellent atmospheric diagnostics, but have not
        been fully utilized for flaring atmospheres. We aim to
        investigate whether the physics of the chromosphere is identical
        for all flare observations by analyzing if there are certain
        spectra that occur in all flares. To achieve this, we
        automatically analyze hundreds of thousands of Mg II
        h\&amp;k-line profiles from a set of 33 flares and use a machine
        learning technique, which we call supervised hierarchical
        k-means, to cluster all profile shapes. We identify a single
        peaked Mg II profile, in contrast to the double-peaked quiet Sun
        profiles, appearing in every flare. Additionally, we find
        extremely broad profiles with characteristic blueshifted central
        reversals appearing at the front of fast-moving flare ribbons.
        These profiles occur during the impulsive phase of the flare,
        and we present results of their temporal and spatial correlation
        with non-thermal hard X-ray signatures, suggesting that flare-
        accelerated electrons play an important role in the formation of
        these profiles. The ratio of the integrated Mg II h\&amp;k lines
        can also serve as an opacity diagnostic, and we find higher
        opacities during each flare maximum. Our study shows that
        machine learning is a powerful tool for large scale statistical
        solar analyses.}",
          doi = {10.3847/1538-4357/aac779},
archivePrefix = {arXiv},
       eprint = {1805.10494},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...861...62P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018AJ....156....7H,
       author = {{Hinners}, Trisha A. and {Tat}, Kevin and {Thorp}, Rachel},
        title = "{Machine Learning Techniques for Stellar Light Curve Classification}",
      journal = {\aj},
     keywords = {methods: data analysis, planetary systems, planets and satellites: detection, stars: general, techniques: image processing, Astrophysics - Instrumentation and Methods for Astrophysics, 85},
         year = "2018",
        month = "Jul",
       volume = {156},
       number = {1},
          eid = {7},
        pages = {7},
     abstract = "{We apply machine learning techniques in an attempt to predict and
        classify stellar properties from noisy and sparse time-series
        data. We preprocessed over 94 GB of Kepler light curves from the
        Mikulski Archive for Space Telescopes (MAST) to classify
        according to 10 distinct physical properties using both
        representation learning and feature engineering approaches.
        Studies using machine learning in the field have been primarily
        done on simulated data, making our study one of the first to use
        real light-curve data for machine learning approaches. We tuned
        our data using previous work with simulated data as a template
        and achieved mixed results between the two approaches.
        Representation learning using a long short-term memory recurrent
        neural network produced no successful predictions, but our work
        with feature engineering was successful for both classification
        and regression. In particular, we were able to achieve values
        for stellar density, stellar radius, and effective temperature
        with low error (̃2\%-4\%) and good accuracy (̃75\%) for
        classifying the number of transits for a given star. The results
        show promise for improvement for both approaches upon using
        larger data sets with a larger minority class. This work has the
        potential to provide a foundation for future tools and
        techniques to aid in the analysis of astrophysical data.}",
          doi = {10.3847/1538-3881/aac16d},
archivePrefix = {arXiv},
       eprint = {1710.06804},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018AJ....156....7H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018AdSpR..62..288G,
       author = {{Ghasemian}, Nafiseh and {Akhoondzadeh}, Mehdi},
        title = "{Introducing two Random Forest based methods for cloud detection in remote sensing images}",
      journal = {Advances in Space Research},
     keywords = {Cloud detection, Feature Level Fusion Random Forest, Decision Level Fusion Random Forest, Remote sensing imagery},
         year = "2018",
        month = "Jul",
       volume = {62},
       number = {2},
        pages = {288-303},
     abstract = "{Cloud detection is a necessary phase in satellite images processing to
        retrieve the atmospheric and lithospheric parameters. Currently,
        some cloud detection methods based on Random Forest (RF) model
        have been proposed but they do not consider both spectral and
        textural characteristics of the image. Furthermore, they have
        not been tested in the presence of snow/ice. In this paper, we
        introduce two RF based algorithms, Feature Level Fusion Random
        Forest (FLFRF) and Decision Level Fusion Random Forest (DLFRF)
        to incorporate visible, infrared (IR) and thermal spectral and
        textural features (FLFRF) including Gray Level Co-occurrence
        Matrix (GLCM) and Robust Extended Local Binary Pattern
        (RELBP\_CI) or visible, IR and thermal classifiers (DLFRF) for
        highly accurate cloud detection on remote sensing images. FLFRF
        first fuses visible, IR and thermal features. Thereafter, it
        uses the RF model to classify pixels to cloud, snow/ice and
        background or thick cloud, thin cloud and background. DLFRF
        considers visible, IR and thermal features (both spectral and
        textural) separately and inserts each set of features to RF
        model. Then, it holds vote matrix of each run of the model.
        Finally, it fuses the classifiers using the majority vote
        method. To demonstrate the effectiveness of the proposed
        algorithms, 10 Terra MODIS and 15 Landsat 8 OLI/TIRS images with
        different spatial resolutions are used in this paper.
        Quantitative analyses are based on manually selected ground
        truth data. Results show that after adding RELBP\_CI to input
        feature set cloud detection accuracy improves. Also, the average
        cloud kappa values of FLFRF and DLFRF on MODIS images (1 and
        0.99) are higher than other machine learning methods, Linear
        Discriminate Analysis (LDA), Classification And Regression Tree
        (CART), K Nearest Neighbor (KNN) and Support Vector Machine
        (SVM) (0.96). The average snow/ice kappa values of FLFRF and
        DLFRF on MODIS images (1 and 0.85) are higher than other
        traditional methods. The quantitative values on Landsat 8 images
        show similar trend. Consequently, while SVM and K-nearest
        neighbor show overestimation in predicting cloud and snow/ice
        pixels, our Random Forest (RF) based models can achieve higher
        cloud, snow/ice kappa values on MODIS and thin cloud, thick
        cloud and snow/ice kappa values on Landsat 8 images. Our
        algorithms predict both thin and thick cloud on Landsat 8 images
        while the existing cloud detection algorithm, Fmask cannot
        discriminate them. Compared to the state-of-the-art methods, our
        algorithms have acquired higher average cloud and snow/ice kappa
        values for different spatial resolutions.}",
          doi = {10.1016/j.asr.2018.04.030},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018AdSpR..62..288G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&C....24...25A,
       author = {{Araya}, M. and {Mendoza}, M. and {Solar}, M. and {Mardones}, D. and
         {Bayo}, A.},
        title = "{Unsupervised learning of structure in spectroscopic cubes}",
      journal = {Astronomy and Computing},
     keywords = {Astronomical imaging, Image analysis, Homogeneous representations, Machine learning, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Jul",
       volume = {24},
        pages = {25-35},
     abstract = "{We consider the problem of analyzing the structure of spectroscopic
        cubes using unsupervised machine learning techniques. We propose
        representing the target's signal as a homogeneous set of volumes
        through an iterative algorithm that separates the structured
        emission from the background while not overestimating the flux.
        Besides verifying some basic theoretical properties, the
        algorithm is designed to be tuned by domain experts, because its
        parameters have meaningful values in the astronomical context.
        Nevertheless, we propose a heuristic to automatically estimate
        the signal-to-noise ratio parameter of the algorithm directly
        from data. The resulting light-weighted set of samples
        ({\ensuremath{\leq}} 1\% compared to the original data) offer
        several advantages. For instance, it is statistically correct
        and computationally inexpensive to apply well-established
        techniques of the pattern recognition and machine learning
        domains; such as clustering and dimensionality reduction
        algorithms. We use ALMA science verification data to validate
        our method, and present examples of the operations that can be
        performed by using the proposed representation. Even though this
        approach is focused on providing faster and better analysis
        tools for the end-user astronomer, it also opens the possibility
        of content-aware data discovery by applying our algorithm to big
        data.}",
          doi = {10.1016/j.ascom.2018.06.001},
archivePrefix = {arXiv},
       eprint = {1806.05650},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&C....24...25A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...615A.111O,
       author = {{Olspert}, N. and {Pelt}, J. and {K{\"a}pyl{\"a}}, M.~J. and
         {Lehtinen}, J.},
        title = "{Estimating activity cycles with probabilistic methods. I. Bayesian generalised Lomb-Scargle periodogram with trend}",
      journal = {\aap},
     keywords = {methods: statistical, methods: numerical, stars: activity, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Applications, Statistics - Machine Learning},
         year = "2018",
        month = "Jul",
       volume = {615},
          eid = {A111},
        pages = {A111},
     abstract = "{Context. Period estimation is one of the central topics in astronomical
        time series analysis, in which data is often unevenly sampled.
        Studies of stellar magnetic cycles are especially challenging,
        as the periods expected in those cases are approximately the
        same length as the datasets themselves. The datasets often
        contain trends, the origin of which is either a real long-term
        cycle or an instrumental effect. But these effects cannot be
        reliably separated, while they can lead to erroneous period
        determinations if not properly handled. <BR /> Aims: In this
        study we aim at developing a method that can handle the trends
        properly. By performing an extensive set of testing, we show
        that this is the optimal procedure when contrasted with methods
        that do not include the trend directly in the model. The effect
        of the form of the noise (whether constant or heteroscedastic)
        on the results is also investigated. <BR /> Methods: We
        introduced a Bayesian generalised Lomb-Scargle periodogram with
        trend (BGLST), which is a probabilistic linear regression model
        using Gaussian priors for the coefficients of the fit and a
        uniform prior for the frequency parameter. <BR /> Results: We
        show, using synthetic data, that when there is no prior
        information on whether and to what extent the true model of the
        data contains a linear trend, the introduced BGLST method is
        preferable to the methods that either detrend the data or opt
        not to detrend the data before fitting the periodic model.
        Whether to use noise with other than constant variance in the
        model depends on the density of the data sampling and on the
        true noise type of the process.}",
          doi = {10.1051/0004-6361/201732524},
archivePrefix = {arXiv},
       eprint = {1712.08235},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...615A.111O},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018RAA....18...71Z,
       author = {{Zhao}, Hao and {Peng}, Wen-Xi and {Wang}, Huan-Yu and {Qiao}, Rui and
         {Guo}, Dong-Ya and {Xiao}, Hong and {Wang}, Zhao-Min},
        title = "{A machine learning method to separate cosmic ray electrons from protons from 10 to 100 GeV using DAMPE data}",
      journal = {Research in Astronomy and Astrophysics},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Physics - Instrumentation and Detectors},
         year = "2018",
        month = "Jun",
       volume = {18},
       number = {6},
          eid = {071},
        pages = {071},
     abstract = "{DArk Matter Particle Explorer (DAMPE) is a general purpose high energy
        cosmic ray and gamma ray observatory, aiming to detect high
        energy electrons and gammas in the energy range 5 GeV to 10 TeV
        and hundreds of TeV for nuclei. This paper provides a method
        using machine learning to identify electrons and separate them
        from gammas, protons, helium and heavy nuclei with the DAMPE
        data acquired from 2016 January 1 to 2017 June 30, in the energy
        range from 10 to 100 GeV.}",
          doi = {10.1088/1674-4527/18/6/71},
archivePrefix = {arXiv},
       eprint = {1803.06628},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018RAA....18...71Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018NatAs...2..719M,
       author = {{M{\'a}rquez-Neila}, Pablo and {Fisher}, Chloe and {Sznitman}, Raphael and
         {Heng}, Kevin},
        title = "{Supervised machine learning for analysing spectra of exoplanetary atmospheres}",
      journal = {Nature Astronomy},
     keywords = {Astrophysics - Earth and Planetary Astrophysics, Physics - Atmospheric and Oceanic Physics, Physics - Data Analysis, Statistics and Probability},
         year = "2018",
        month = "Jun",
       volume = {2},
        pages = {719-724},
     abstract = "{The use of machine learning is becoming ubiquitous in astronomy$^{1-3}$,
        but remains rare in the study of the atmospheres of exoplanets.
        Given the spectrum of an exoplanetary atmosphere, a multi-
        parameter space is swept through in real time to find the best-
        fit model$^{4-6}$. Known as atmospheric retrieval, this
        technique originates in the Earth and planetary sciences$^{7}$.
        Such methods are very time-consuming, and by necessity there is
        a compromise between physical and chemical realism and
        computational feasibility. Machine learning has previously been
        used to determine which molecules to include in the model, but
        the retrieval itself was still performed using standard
        methods$^{8}$. Here, we report an adaptation of the `random
        forest' method of supervised machine learning$^{9,10}$, trained
        on a precomputed grid of atmospheric models, which retrieves
        full posterior distributions of the abundances of molecules and
        the cloud opacity. The use of a precomputed grid allows a large
        part of the computational burden to be shifted offline. We
        demonstrate our technique on a transmission spectrum of the hot
        gas-giant exoplanet WASP-12b using a five-parameter model
        (temperature, a constant cloud opacity and the volume mixing
        ratios or relative abundances of molecules of water, ammonia and
        hydrogen cyanide)$^{11}$. We obtain results consistent with the
        standard nested-sampling retrieval method. We also estimate the
        sensitivity of the measured spectrum to the model parameters,
        and we are able to quantify the information content of the
        spectrum. Our method can be straightforwardly applied using more
        sophisticated atmospheric models to interpret an ensemble of
        spectra without having to retrain the random forest.}",
          doi = {10.1038/s41550-018-0504-2},
archivePrefix = {arXiv},
       eprint = {1806.03944},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018NatAs...2..719M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.477.1484U,
       author = {{Ucci}, G. and {Ferrara}, A. and {Pallottini}, A. and {Gallerani}, S.},
        title = "{GAME: GAlaxy Machine learning for Emission lines}",
      journal = {\mnras},
     keywords = {methods: data analysis, galaxies: ISM, ISM: general, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Jun",
       volume = {477},
       number = {2},
        pages = {1484-1494},
     abstract = "{We present an updated, optimized version of GAME (GAlaxy Machine
        learning for Emission lines), a code designed to infer key
        interstellar medium physical properties from emission line
        intensities of ultraviolet /optical/far-infrared galaxy spectra.
        The improvements concern (a) an enlarged spectral library
        including Pop III stars, (b) the inclusion of spectral noise in
        the training procedure, and (c) an accurate evaluation of
        uncertainties. We extensively validate the optimized code and
        compare its performance against empirical methods and other
        available emission line codes (PYQZ and HII-CHI-MISTRY) on a
        sample of 62 SDSS stacked galaxy spectra and 75 observed HII
        regions. Very good agreement is found for metallicity. However,
        ionization parameters derived by GAME tend to be higher. We show
        that this is due to the use of too limited libraries in the
        other codes. The main advantages of GAME are the simultaneous
        use of all the measured spectral lines and the extremely short
        computational times. We finally discuss the code potential and
        limitations.}",
          doi = {10.1093/mnras/sty804},
archivePrefix = {arXiv},
       eprint = {1803.10236},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.477.1484U},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.476.5692L,
       author = {{Lam}, Christopher and {Kipping}, David},
        title = "{A machine learns to predict the stability of circumbinary planets}",
      journal = {\mnras},
     keywords = {methods: numerical, methods: statistical, planets and satellites: dynamical evolution and stability, binaries: general, planetary systems, Astrophysics - Earth and Planetary Astrophysics},
         year = "2018",
        month = "Jun",
       volume = {476},
       number = {4},
        pages = {5692-5697},
     abstract = "{Long-period circumbinary planets appear to be as common as those
        orbiting single stars and have been found to frequently have
        orbital radii just beyond the critical distance for dynamical
        stability. Assessing the stability is typically done either
        through N-body simulations or using the classic stability
        criterion first considered by Dvorak and later developed by
        Holman and Wiegert: a second-order polynomial calibrated to
        broadly match numerical simulations. However, the polynomial is
        unable to capture islands of instability introduced by mean
        motion resonances, causing the accuracy of the criterion to
        approach that of a random coin-toss when close to the boundary.
        We show how a deep neural network (DNN) trained on N-body
        simulations generated with REBOUND is able to significantly
        improve stability predictions for circumbinary planets on
        initially coplanar, circular orbits. Specifically, we find that
        the accuracy of our DNN never drops below 86 per cent, even when
        tightly surrounding the boundary of instability, and is fast
        enough to be practical for on-the-fly calls during likelihood
        evaluations typical of modern Bayesian inference. Our binary
        classifier DNN is made publicly available at
        https://github.com/CoolWorlds/orbital-stability.}",
          doi = {10.1093/mnras/sty022},
archivePrefix = {arXiv},
       eprint = {1801.03955},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.476.5692L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.476.5516B,
       author = {{Beck}, Melanie R. and {Scarlata}, Claudia and {Fortson}, Lucy F. and
         {Lintott}, Chris J. and {Simmons}, B.~D. and {Galloway}, Melanie A. and
         {Willett}, Kyle W. and {Dickinson}, Hugh and {Masters}, Karen L. and
         {Marshall}, Philip J. and {Wright}, Darryl},
        title = "{Integrating human and machine intelligence in galaxy morphology classification tasks}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, galaxies: statistics, galaxies: structure, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Jun",
       volume = {476},
       number = {4},
        pages = {5516-5534},
     abstract = "{Quantifying galaxy morphology is a challenging yet scientifically
        rewarding task. As the scale of data continues to increase with
        upcoming surveys, traditional classification methods will
        struggle to handle the load. We present a solution through an
        integration of visual and automated classifications, preserving
        the best features of both human and machine. We demonstrate the
        effectiveness of such a system through a re-analysis of visual
        galaxy morphology classifications collected during the Galaxy
        Zoo 2 (GZ2) project. We reprocess the top-level question of the
        GZ2 decision tree with a Bayesian classification aggregation
        algorithm dubbed SWAP, originally developed for the Space Warps
        gravitational lens project. Through a simple binary
        classification scheme, we increase the classification rate
        nearly 5-fold classifying 226 124 galaxies in 92 d of GZ2
        project time while reproducing labels derived from GZ2
        classification data with 95.7 per cent accuracy. We next combine
        this with a Random Forest machine learning algorithm that learns
        on a suite of non-parametric morphology indicators widely used
        for automated morphologies. We develop a decision engine that
        delegates tasks between human and machine and demonstrate that
        the combined system provides at least a factor of 8 increase in
        the classification rate, classifying 210 803 galaxies in just 32
        d of GZ2 project time with 93.1 per cent accuracy. As the Random
        Forest algorithm requires a minimal amount of computational
        cost, this result has important implications for galaxy
        morphology identification tasks in the era of Euclid and other
        large-scale surveys.}",
          doi = {10.1093/mnras/sty503},
archivePrefix = {arXiv},
       eprint = {1802.08713},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.476.5516B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018JGRA..123.5068K,
       author = {{Kodikara}, Timothy and {Carter}, Brett and {Zhang}, Kefei},
        title = "{The First Comparison Between Swarm-C Accelerometer-Derived Thermospheric Densities and Physical and Empirical Model Estimates}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {Swarm-C accelerometer-derived density, physics-based thermospheric density, TIE-GCM NRLMSISE-00 DTM-2013, thermosphere machine learning, Taylor diagram, thermosphere upper atmosphere, Physics - Atmospheric and Oceanic Physics, Physics - Space Physics},
         year = "2018",
        month = "Jun",
       volume = {123},
       number = {6},
        pages = {5068-5086},
     abstract = "{The first systematic comparison between Swarm-C accelerometer-derived
        thermospheric density and both empirical and physics-based model
        results using multiple model performance metrics is presented.
        This comparison is performed at the satellite's high temporal
        10-s resolution, which provides a meaningful evaluation of the
        models' fidelity for orbit prediction and other space weather
        forecasting applications. The comparison against the physical
        model is influenced by the specification of the lower
        atmospheric forcing, the high-latitude ionospheric plasma
        convection, and solar activity. Some insights into the model
        response to thermosphere-driving mechanisms are obtained through
        a machine learning exercise. The results of this analysis show
        that the short-timescale variations observed by Swarm-C during
        periods of high solar and geomagnetic activity were better
        captured by the physics-based model than the empirical models.
        It is concluded that Swarm-C data agree well with the
        climatologies inherent within the models and are, therefore, a
        useful data set for further model validation and scientific
        research.}",
          doi = {10.1029/2017JA025118},
archivePrefix = {arXiv},
       eprint = {1712.01961},
 primaryClass = {physics.ao-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018JGRA..123.5068K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018GeoRL..45.6289B,
       author = {{Brenowitz}, N.~D. and {Bretherton}, C.~S.},
        title = "{Prognostic Validation of a Neural Network Unified Physics Parameterization}",
      journal = {\grl},
     keywords = {cumulus parameterization, cloud-resolving model, single-column model, neural network, machine learning},
         year = "2018",
        month = "Jun",
       volume = {45},
       number = {12},
        pages = {6289-6298},
     abstract = "{Weather and climate models approximate diabatic and sub-grid-scale
        processes in terms of grid-scale variables using
        parameterizations. Current parameterizations are designed by
        humans based on physical understanding, observations, and
        process modeling. As a result, they are numerically efficient
        and interpretable, but potentially oversimplified. However, the
        advent of global high-resolution simulations and observations
        enables a more robust approach based on machine learning. In
        this letter, a neural network-based parameterization is trained
        using a near-global aqua-planet simulation with a 4-km
        resolution (NG-Aqua). The neural network predicts the apparent
        sources of heat and moisture averaged onto (160 km)$^{2}$ grid
        boxes. A numerically stable scheme is obtained by minimizing the
        prediction error over multiple time steps rather than single
        one. In prognostic single-column model tests, this scheme
        matches both the fluctuations and equilibrium of NG-Aqua
        simulation better than the Community Atmosphere Model does.}",
          doi = {10.1029/2018GL078510},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018GeoRL..45.6289B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018GeoRL..45.5742G,
       author = {{Gentine}, P. and {Pritchard}, M. and {Rasp}, S. and {Reinaudi}, G. and
         {Yacalis}, G.},
        title = "{Could Machine Learning Break the Convection Parameterization Deadlock?}",
      journal = {\grl},
     keywords = {convection, machine learning, clouds},
         year = "2018",
        month = "Jun",
       volume = {45},
       number = {11},
        pages = {5742-5751},
     abstract = "{Representing unresolved moist convection in coarse-scale climate models
        remains one of the main bottlenecks of current climate
        simulations. Many of the biases present with parameterized
        convection are strongly reduced when convection is explicitly
        resolved (i.e., in cloud resolving models at high spatial
        resolution approximately a kilometer or so). We here present a
        novel approach to convective parameterization based on machine
        learning, using an aquaplanet with prescribed sea surface
        temperatures as a proof of concept. A deep neural network is
        trained with a superparameterized version of a climate model in
        which convection is resolved by thousands of embedded 2-D cloud
        resolving models. The machine learning representation of
        convection, which we call the Cloud Brain (CBRAIN), can
        skillfully predict many of the convective heating, moistening,
        and radiative features of superparameterization that are most
        important to climate simulation, although an unintended side
        effect is to reduce some of the superparameterization's inherent
        variance. Since as few as three months' high-frequency global
        training data prove sufficient to provide this skill, the
        approach presented here opens up a new possibility for a future
        class of convection parameterizations in climate models that are
        built ``top-down,'' that is, by learning salient features of
        convection from unusually explicit simulations.}",
          doi = {10.1029/2018GL078202},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018GeoRL..45.5742G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018FoPh...48..636V,
       author = {{Vanchurin}, Vitaly},
        title = "{Information Graph Flow: A Geometric Approximation of Quantum and Statistical Systems}",
      journal = {Foundations of Physics},
     keywords = {Foundations of quantum gravity, Information theory, Graph theory, High Energy Physics - Theory, General Relativity and Quantum Cosmology, Quantum Physics},
         year = "2018",
        month = "Jun",
       volume = {48},
       number = {6},
        pages = {636-653},
     abstract = "{Given a quantum (or statistical) system with a very large number of
        degrees of freedom and a preferred tensor product factorization
        of the Hilbert space (or of a space of distributions) we
        describe how it can be approximated with a very low-dimensional
        field theory with geometric degrees of freedom. The geometric
        approximation procedure consists of three steps. The first step
        is to construct weighted graphs (we call information graphs)
        with vertices representing subsystems (e.g., qubits or random
        variables) and edges representing mutual information (or the
        flow of information) between subsystems. The second step is to
        deform the adjacency matrices of the information graphs to that
        of a (locally) low-dimensional lattice using the graph flow
        equations introduced in the paper. (Note that the graph flow
        produces very sparse adjacency matrices and thus might also be
        used, for example, in machine learning or network science where
        the task of graph sparsification is of a central importance.)
        The third step is to define an emergent metric and to derive an
        effective description of the metric and possibly other degrees
        of freedom. To illustrate the procedure we analyze (numerically
        and analytically) two information graph flows with geometric
        attractors (towards locally one- and two-dimensional lattices)
        and metric perturbations obeying a geometric flow equation. Our
        analysis also suggests a possible approach to (a non-
        perturbative) quantum gravity in which the geometry (a secondary
        object) emerges directly from a quantum state (a primary object)
        due to the flow of the information graphs.}",
          doi = {10.1007/s10701-018-0166-z},
archivePrefix = {arXiv},
       eprint = {1706.02229},
 primaryClass = {hep-th},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018FoPh...48..636V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...859..129N,
       author = {{Nadler}, Ethan O. and {Mao}, Yao-Yuan and {Wechsler}, Risa H. and
         {Garrison-Kimmel}, Shea and {Wetzel}, Andrew},
        title = "{Modeling the Impact of Baryons on Subhalo Populations with Machine Learning}",
      journal = {\apj},
     keywords = {dark matter, galaxies: abundances, galaxies: halos, methods: numerical, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2018",
        month = "Jun",
       volume = {859},
       number = {2},
          eid = {129},
        pages = {129},
     abstract = "{We identify subhalos in dark matter-only (DMO) zoom-in simulations that
        are likely to be disrupted due to baryonic effects by using a
        random forest classifier trained on two hydrodynamic simulations
        of Milky Way (MW)-mass host halos from the Latte suite of the
        Feedback in Realistic Environments (FIRE) project. We train our
        classifier using five properties of each disrupted and surviving
        subhalo: pericentric distance and scale factor at first
        pericentric passage after accretion and scale factor, virial
        mass, and maximum circular velocity at accretion. Our five-
        property classifier identifies disrupted subhalos in the FIRE
        simulations with an 85\% out-of-bag classification score. We
        predict surviving subhalo populations in DMO simulations of the
        FIRE host halos, finding excellent agreement with the
        hydrodynamic results; in particular, our classifier outperforms
        DMO zoom-in simulations that include the gravitational potential
        of the central galactic disk in each hydrodynamic simulation,
        indicating that it captures both the dynamical effects of a
        central disk and additional baryonic physics. We also predict
        surviving subhalo populations for a suite of DMO zoom-in
        simulations of MW-mass host halos, finding that baryons impact
        each system consistently and that the predicted amount of
        subhalo disruption is larger than the host-to-host scatter among
        the subhalo populations. Although the small size and specific
        baryonic physics prescription of our training set limits the
        generality of our results, our work suggests that machine-
        learning classification algorithms trained on hydrodynamic zoom-
        in simulations can efficiently predict realistic subhalo
        populations.}",
          doi = {10.3847/1538-4357/aac266},
archivePrefix = {arXiv},
       eprint = {1712.04467},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...859..129N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018AJ....155..266D,
       author = {{Deitrick}, Russell and {Barnes}, Rory and {Bitz}, Cecilia and
         {Fleming}, David and {Charnay}, Benjamin and {Meadows}, Victoria and
         {Wilhelm}, Caitlyn and {Armstrong}, John and {Quinn}, Thomas R.},
        title = "{Exo-Milankovitch Cycles. II. Climates of G-dwarf Planets in Dynamically Hot Systems}",
      journal = {\aj},
     keywords = {planetary systems, planets and satellites: atmospheres, planets and satellites: dynamical evolution and stability, Astrophysics - Earth and Planetary Astrophysics},
         year = "2018",
        month = "Jun",
       volume = {155},
       number = {6},
          eid = {266},
        pages = {266},
     abstract = "{Using an energy balance model with ice sheets, we examine the climate
        response of an Earth-like planet orbiting a G-dwarf star and
        experiencing large orbital and obliquity variations. We find
        that ice caps couple strongly to the orbital forcing, leading to
        extreme ice ages. In contrast with previous studies, we find
        that such exo-Milankovitch cycles tend to impair habitability by
        inducing snowball states within the habitable zone. The large
        amplitude changes in obliquity and eccentricity cause the ice
        edge, the lowest-latitude extent of the ice caps, to become
        unstable and grow to the equator. We apply an analytical theory
        of the ice edge latitude to show that obliquity is the primary
        driver of the instability. The thermal inertia of the ice sheets
        and the spectral energy distribution of the G-dwarf star
        increase the sensitivity of the model to triggering runaway
        glaciation. Finally, we apply a machine learning algorithm to
        demonstrate how this technique can be used to extend the power
        of climate models. This work illustrates the importance of
        orbital evolution for habitability in dynamically rich planetary
        systems. We emphasize that as potentially habitable planets are
        discovered around G dwarfs, we need to consider orbital
        dynamics.}",
          doi = {10.3847/1538-3881/aac214},
archivePrefix = {arXiv},
       eprint = {1805.00283},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018AJ....155..266D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...613A..71G,
       author = {{Gomez Gonzalez}, C.~A. and {Absil}, O. and {Van Droogenbroeck}, M.},
        title = "{Supervised detection of exoplanets in high-contrast imaging sequences}",
      journal = {\aap},
     keywords = {methods: data analysis, techniques: high angular resolution, techniques: imaging spectroscopy, planetary systems, planets and satellites: detection, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Jun",
       volume = {613},
          eid = {A71},
        pages = {A71},
     abstract = "{Context. Post-processing algorithms play a key role in pushing the
        detection limits of high-contrast imaging (HCI) instruments.
        State-of-the-art image processing approaches for HCI enable the
        production of science-ready images relying on unsupervised
        learning techniques, such as low-rank approximations, for
        generating a model point spread function (PSF) and subtracting
        the residual starlight and speckle noise. <BR /> Aims: In order
        to maximize the detection rate of HCI instruments and survey
        campaigns, advanced algorithms with higher sensitivities to
        faint companions are needed, especially for the speckle-
        dominated innermost region of the images. <BR /> Methods: We
        propose a reformulation of the exoplanet detection task (for ADI
        sequences) that builds on well-established machine learning
        techniques to take HCI post-processing from an unsupervised to a
        supervised learning context. In this new framework, we present
        algorithmic solutions using two different discriminative models:
        SODIRF (random forests) and SODINN (neural networks). We test
        these algorithms on real ADI datasets from VLT/NACO and
        VLT/SPHERE HCI instruments. We then assess their performances by
        injecting fake companions and using receiver operating
        characteristic analysis. This is done in comparison with state-
        of-the-art ADI algorithms, such as ADI principal component
        analysis (ADI-PCA). <BR /> Results: This study shows the
        improved sensitivity versus specificity trade-off of the
        proposed supervised detection approach. At the diffraction
        limit, SODINN improves the true positive rate by a factor
        ranging from 2 to 10 (depending on the dataset and angular
        separation) with respect to ADI-PCA when working at the same
        false-positive level. <BR /> Conclusions: The proposed
        supervised detection framework outperforms state-of-the-art
        techniques in the task of discriminating planet signal from
        speckles. In addition, it offers the possibility of re-
        processing existing HCI databases to maximize their scientific
        return and potentially improve the demographics of directly
        imaged exoplanets.}",
          doi = {10.1051/0004-6361/201731961},
archivePrefix = {arXiv},
       eprint = {1712.02841},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...613A..71G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PhRvD..97j3515G,
       author = {{Gupta}, Arushi and {Matilla}, Jos{\'e} Manuel Zorrilla and
         {Hsu}, Daniel and {Haiman}, Zolt{\'a}n},
        title = "{Non-Gaussian information from weak lensing data via deep learning}",
      journal = {\prd},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2018",
        month = "May",
       volume = {97},
       number = {10},
          eid = {103515},
        pages = {103515},
     abstract = "{Weak lensing maps contain information beyond two-point statistics on
        small scales. Much recent work has tried to extract this
        information through a range of different observables or via
        nonlinear transformations of the lensing field. Here we train
        and apply a two-dimensional convolutional neural network to
        simulated noiseless lensing maps covering 96 different
        cosmological models over a range of
        \{{\ensuremath{\Omega}}$_{m}$,{\ensuremath{\sigma}}$_{8}$\} .
        Using the area of the confidence contour in the
        \{{\ensuremath{\Omega}}$_{m}$,{\ensuremath{\sigma}}$_{8}$\}
        plane as a figure of merit, derived from simulated convergence
        maps smoothed on a scale of 1.0 arcmin, we show that the neural
        network yields {\ensuremath{\approx}}5 {\texttimes} tighter
        constraints than the power spectrum, and {\ensuremath{\approx}}4
        {\texttimes} tighter than the lensing peaks. Such gains
        illustrate the extent to which weak lensing data encode
        cosmological information not accessible to the power spectrum or
        even other, non-Gaussian statistics such as lensing peaks.}",
          doi = {10.1103/PhysRevD.97.103515},
archivePrefix = {arXiv},
       eprint = {1802.01212},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhRvD..97j3515G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PhRvD..97j1501G,
       author = {{George}, Daniel and {Shen}, Hongyu and {Huerta}, E.~A.},
        title = "{Classification and unsupervised clustering of LIGO data with Deep Transfer Learning}",
      journal = {\prd},
     keywords = {General Relativity and Quantum Cosmology, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
         year = "2018",
        month = "May",
       volume = {97},
       number = {10},
          eid = {101501},
        pages = {101501},
     abstract = "{Gravitational wave detection requires a detailed understanding of the
        response of the LIGO and Virgo detectors to true signals in the
        presence of environmental and instrumental noise. Of particular
        interest is the study of anomalous non-Gaussian transients, such
        as glitches, since their occurrence rate in LIGO and Virgo data
        can obscure or even mimic true gravitational wave signals.
        Therefore, successfully identifying and excising these anomalies
        from gravitational wave data is of utmost importance for the
        detection and characterization of true signals and for the
        accurate computation of their significance. To facilitate this
        work, we present the first application of deep learning combined
        with transfer learning to show that knowledge from pretrained
        models for real-world object recognition can be transferred for
        classifying spectrograms of glitches. To showcase this new
        method, we use a data set of twenty-two classes of glitches,
        curated and labeled by the Gravity Spy project using data
        collected during LIGO's first discovery campaign. We demonstrate
        that our Deep Transfer Learning method enables an optimal use of
        very deep convolutional neural networks for glitch
        classification given small and unbalanced training data sets,
        significantly reduces the training time, and achieves state-of-
        the-art accuracy above 98.8\%, lowering the previous error rate
        by over 60\%. More importantly, once trained via transfer
        learning on the known classes, we show that our neural networks
        can be truncated and used as feature extractors for unsupervised
        clustering to automatically group together new unknown classes
        of glitches and anomalous signals. This novel capability is of
        paramount importance to identify and remove new types of
        glitches which will occur as the LIGO/Virgo detectors gradually
        attain design sensitivity.}",
          doi = {10.1103/PhysRevD.97.101501},
archivePrefix = {arXiv},
       eprint = {1706.07446},
 primaryClass = {gr-qc},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhRvD..97j1501G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2018PhDT........36G,
       author = {{Grasha}, Kathryn},
        title = "{The Clustering Of Young Stellar Clusters In Nearby Galaxies}",
     keywords = {Astronomy, Astrophysics, Young star clusters, Extragalactic galaxies, Star formation, Interstellar medium, Ultraviolet observations},
       school = {University of Massachusetts},
         year = "2018",
        month = "May",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhDT........36G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.476.3981Y,
       author = {{Yan}, Qing-Zeng and {Xu}, Ye and {Walsh}, A.~J. and {Macquart}, J.~P. and
         {MacLeod}, G.~C. and {Zhang}, Bo and {Hancock}, P.~J. and {Chen}, Xi and
         {Tang}, Zheng-Hong},
        title = "{Improved selection criteria for H II regions, based on IRAS sources}",
      journal = {\mnras},
     keywords = {stars: evolution, stars: massive, stars: statistics, H \&lt;sc\&gt;ii\&lt;/sc\&gt; regions, infrared: ISM, infrared: stars, Astrophysics - Astrophysics of Galaxies, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "May",
       volume = {476},
       number = {3},
        pages = {3981-3990},
     abstract = "{We present new criteria for selecting H II regions from the Infrared
        Astronomical Satellite (IRAS) Point Source Catalogue (PSC),
        based on an H II region catalogue derived manually from the all-
        sky Wide-field Infrared Survey Explorer (WISE). The criteria are
        used to augment the number of H II region candidates in the
        Milky Way. The criteria are defined by the linear decision
        boundary of two samples: IRAS point sources associated with
        known H II regions, which serve as the H II region sample, and
        IRAS point sources at high Galactic latitudes, which serve as
        the non-H II region sample. A machine learning classifier,
        specifically a support vector machine, is used to determine the
        decision boundary. We investigate all combinations of four IRAS
        bands and suggest that the optimal criterion is
        log(F\_\{60\}/F\_\{12\}){\ensuremath{\geq}} ( -0.19 {\texttimes}
        log(F\_\{100\}/F\_\{25\})+ 1.52), with detections at 60 and 100
        \{{\ensuremath{\mu}}\}m. This selects 3041 H II region
        candidates from the IRAS PSC. We find that IRAS H II region
        candidates show evidence of evolution on the two-colour diagram.
        Merging the WISE H II catalogue with IRAS H II region
        candidates, we estimate a lower limit of approximately 10 200
        for the number of H II regions in the Milky Way.}",
          doi = {10.1093/mnras/sty518},
archivePrefix = {arXiv},
       eprint = {1802.08354},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.476.3981Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.476.3661D,
       author = {{Dom{\'\i}nguez S{\'a}nchez}, H. and {Huertas-Company}, M. and
         {Bernardi}, M. and {Tuccillo}, D. and {Fischer}, J.~L.},
        title = "{Improving galaxy morphologies for SDSS with Deep Learning}",
      journal = {\mnras},
     keywords = {methods: observational, catalogues, galaxies: structure, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "May",
       volume = {476},
       number = {3},
        pages = {3661-3676},
     abstract = "{We present a morphological catalogue for ̃670 000 galaxies in the Sloan
        Digital Sky Survey in two flavours: T-type, related to the
        Hubble sequence, and Galaxy Zoo 2 (GZ2 hereafter) classification
        scheme. By combining accurate existing visual classification
        catalogues with machine learning, we provide the largest and
        most accurate morphological catalogue up to date. The
        classifications are obtained with Deep Learning algorithms using
        Convolutional Neural Networks (CNNs). We use two visual
        classification catalogues, GZ2 and Nair \&amp; Abraham (2010),
        for training CNNs with colour images in order to obtain T-types
        and a series of GZ2 type questions (disc/features, edge-on
        galaxies, bar signature, bulge prominence, roundness, and
        mergers). We also provide an additional probability enabling a
        separation between pure elliptical (E) from S0, where the T-type
        model is not so efficient. For the T-type, our results show
        smaller offset and scatter than previous models trained with
        support vector machines. For the GZ2 type questions, our models
        have large accuracy (\&gt;97 per cent), precision and recall
        values (\&gt;90 per cent), when applied to a test sample with
        the same characteristics as the one used for training. The
        catalogue is publicly released with the paper.}",
          doi = {10.1093/mnras/sty338},
archivePrefix = {arXiv},
       eprint = {1711.05744},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.476.3661D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.476.2968H,
       author = {{Hedges}, Christina and {Hodgkin}, Simon and {Kennedy}, Grant},
        title = "{Discovery of new dipper stars with K2: a window into the inner disc region of T Tauri stars}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: photometric, stars: variables: T Tauri, Herbig Ae/Be, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "May",
       volume = {476},
       number = {3},
        pages = {2968-2998},
     abstract = "{In recent years, a new class of young stellar object (YSO) has been
        defined, referred to as dippers, where large transient drops in
        flux are observed. These dips are too large to be attributed to
        stellar variability, last from hours to days and can reduce the
        flux of a star by 10-50 per cent. This variability has been
        attributed to occultations by warps or accretion columns near
        the inner edge of circumstellar discs. Here, we present 95
        dippers in the Upper Scorpius association and
        {\ensuremath{\rho}} Ophiuchus cloud complex found in K2 Campaign
        2 data using supervised machine learning with a random forest
        classifier. We also present 30 YSOs that exhibit brightening
        events on the order of days, known as bursters. Not all dippers
        and bursters are known members, but all exhibit infrared
        excesses and are consistent with belonging to either of the two
        young star-forming regions. We find 21.0 {\ensuremath{\pm}} 5.5
        per cent of stars with discs are dippers for both regions
        combined. Our entire dipper sample consists only of late-type
        (KM) stars, but we show that biases limit dipper discovery for
        earlier spectral types. Using the dipper properties as a proxy,
        we find that the temperature at the inner disc edge is
        consistent with interferometric results for similar and earlier
        type stars.}",
          doi = {10.1093/mnras/sty328},
archivePrefix = {arXiv},
       eprint = {1802.00409},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.476.2968H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.476.2117R,
       author = {{Reis}, Itamar and {Poznanski}, Dovi and {Baron}, Dalya and
         {Zasowski}, Gail and {Shahaf}, Sahar},
        title = "{Detecting outliers and learning complex structures with large spectroscopic surveys - a case study with APOGEE stars}",
      journal = {\mnras},
     keywords = {methods: data analysis, stars: general, stars: peculiar, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "May",
       volume = {476},
       number = {2},
        pages = {2117-2136},
     abstract = "{In this work, we apply and expand on a recently introduced outlier
        detection algorithm that is based on an unsupervised random
        forest. We use the algorithm to calculate a similarity measure
        for stellar spectra from the Apache Point Observatory Galactic
        Evolution Experiment (APOGEE). We show that the similarity
        measure traces non-trivial physical properties and contains
        information about complex structures in the data. We use it for
        visualization and clustering of the data set, and discuss its
        ability to find groups of highly similar objects, including
        spectroscopic twins. Using the similarity matrix to search the
        data set for objects allows us to find objects that are
        impossible to find using their best-fitting model parameters.
        This includes extreme objects for which the models fail, and
        rare objects that are outside the scope of the model. We use the
        similarity measure to detect outliers in the data set, and find
        a number of previously unknown Be-type stars, spectroscopic
        binaries, carbon rich stars, young stars, and a few that we
        cannot interpret. Our work further demonstrates the potential
        for scientific discovery when combining machine learning methods
        with modern survey data.}",
          doi = {10.1093/mnras/sty348},
archivePrefix = {arXiv},
       eprint = {1711.00022},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.476.2117R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.476.1120S,
       author = {{Sarro}, L.~M. and {Ordieres-Mer{\'e}}, J. and {Bello-Garc{\'\i}a}, A. and
         {Gonz{\'a}lez-Marcos}, A. and {Solano}, E.},
        title = "{Estimates of the atmospheric parameters of M-type stars: a machine-learning perspective}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, techniques: spectroscopic, stars: atmospheres, stars: fundamental parameters, stars: late-type, stars: statistics},
         year = "2018",
        month = "May",
       volume = {476},
       number = {1},
        pages = {1120-1139},
     abstract = "{Estimating the atmospheric parameters of M-type stars has been a
        difficult task due to the lack of simple diagnostics in the
        stellar spectra. We aim at uncovering good sets of predictive
        features of stellar atmospheric parameters (T$_{eff}$, log (g),
        [M/H]) in spectra of M-type stars. We define two types of
        potential features (equivalent widths and integrated flux
        ratios) able to explain the atmospheric physical parameters. We
        search the space of feature sets using a genetic algorithm that
        evaluates solutions by their prediction performance in the
        framework of the BT-Settl library of stellar spectra.
        Thereafter, we construct eight regression models using different
        machine-learning techniques and compare their performances with
        those obtained using the classical {\ensuremath{\chi}}$^{2}$
        approach and independent component analysis (ICA) coefficients.
        Finally, we validate the various alternatives using two sets of
        real spectra from the NASA Infrared Telescope Facility (IRTF)
        and Dwarf Archives collections. We find that the cross-
        validation errors are poor measures of the performance of
        regression models in the context of physical parameter
        prediction in M-type stars. For R ̃ 2000 spectra with signal-to-
        noise ratios typical of the IRTF and Dwarf Archives, feature
        selection with genetic algorithms or alternative techniques
        produces only marginal advantages with respect to representation
        spaces that are unconstrained in wavelength (full spectrum or
        ICA). We make available the atmospheric parameters for the two
        collections of observed spectra as online material.}",
          doi = {10.1093/mnras/sty165},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.476.1120S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.476..246L,
       author = {{Lukic}, V. and {Br{\"u}ggen}, M. and {Banfield}, J.~K. and
         {Wong}, O.~I. and {Rudnick}, L. and {Norris}, R.~P. and {Simmons}, B.},
        title = "{Radio Galaxy Zoo: compact and extended radio source classification with deep learning}",
      journal = {\mnras},
     keywords = {instrumentation: miscellaneous, methods: miscellaneous, techniques: miscellaneous, radio continuum: galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "May",
       volume = {476},
       number = {1},
        pages = {246-260},
     abstract = "{Machine learning techniques have been increasingly useful in
        astronomical applications over the last few years, for example
        in the morphological classification of galaxies. Convolutional
        neural networks have proven to be highly effective in
        classifying objects in image data. In the context of radio-
        interferometric imaging in astronomy, we looked for ways to
        identify multiple components of individual sources. To this
        effect, we design a convolutional neural network to
        differentiate between different morphology classes using sources
        from the Radio Galaxy Zoo (RGZ) citizen science project. In this
        first step, we focus on exploring the factors that affect the
        performance of such neural networks, such as the amount of
        training data, number and nature of layers, and the
        hyperparameters. We begin with a simple experiment in which we
        only differentiate between two extreme morphologies, using
        compact and multiple-component extended sources. We found that a
        three-convolutional layer architecture yielded very good
        results, achieving a classification accuracy of 97.4 per cent on
        a test data set. The same architecture was then tested on a
        four-class problem where we let the network classify sources
        into compact and three classes of extended sources, achieving a
        test accuracy of 93.5 per cent. The best-performing
        convolutional neural network set-up has been verified against
        RGZ Data Release 1 where a final test accuracy of 94.8 per cent
        was obtained, using both original and augmented images. The use
        of sigma clipping does not offer a significant benefit overall,
        except in cases with a small number of training images.}",
          doi = {10.1093/mnras/sty163},
archivePrefix = {arXiv},
       eprint = {1801.04861},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.476..246L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018GeoRL..45.4773L,
       author = {{Li}, Zefeng and {Meier}, Men-Andrin and {Hauksson}, Egill and
         {Zhan}, Zhongwen and {Andrews}, Jennifer},
        title = "{Machine Learning Seismic Wave Discrimination: Application to Earthquake Early Warning}",
      journal = {\grl},
     keywords = {machine learning, earthquake early warning, seismic waves},
         year = "2018",
        month = "May",
       volume = {45},
       number = {10},
        pages = {4773-4779},
     abstract = "{Performance of earthquake early warning systems suffers from false
        alerts caused by local impulsive noise from natural or
        anthropogenic sources. To mitigate this problem, we train a
        generative adversarial network (GAN) to learn the
        characteristics of first-arrival earthquake P waves, using
        300,000 waveforms recorded in southern California and Japan. We
        apply the GAN critic as an automatic feature extractor and train
        a Random Forest classifier with about 700,000 earthquake and
        noise waveforms. We show that the discriminator can recognize
        99.2\% of the earthquake P waves and 98.4\% of the noise
        signals. This state-of-the-art performance is expected to reduce
        significantly the number of false triggers from local impulsive
        noise. Our study demonstrates that GANs can discover a compact
        and effective representation of seismic waves, which has the
        potential for wide applications in seismology.}",
          doi = {10.1029/2018GL077870},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018GeoRL..45.4773L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018GeoRL..45.4273A,
       author = {{Anderson}, Gemma J. and {Lucas}, Donald D.},
        title = "{Machine Learning Predictions of a Multiresolution Climate Model Ensemble}",
      journal = {\grl},
         year = "2018",
        month = "May",
       volume = {45},
       number = {9},
        pages = {4273-4280},
     abstract = "{Statistical models of high-resolution climate models are useful for many
        purposes, including sensitivity and uncertainty analyses, but
        building them can be computationally prohibitive. We generated a
        unique multiresolution perturbed parameter ensemble of a global
        climate model. We use a novel application of a machine learning
        technique known as random forests to train a statistical model
        on the ensemble to make high-resolution model predictions of two
        important quantities: global mean top-of-atmosphere energy flux
        and precipitation. The random forests leverage cheaper low-
        resolution simulations, greatly reducing the number of high-
        resolution simulations required to train the statistical model.
        We demonstrate that high-resolution predictions of these
        quantities can be obtained by training on an ensemble that
        includes only a small number of high-resolution simulations. We
        also find that global annually averaged precipitation is more
        sensitive to resolution changes than to any of the model
        parameters considered.}",
          doi = {10.1029/2018GL077049},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018GeoRL..45.4273A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJS..236....9N,
       author = {{Narayan}, Gautham and {Zaidi}, Tayeb and {Soraisam}, Monika D. and
         {Wang}, Zhe and {Lochner}, Michelle and {Matheson}, Thomas and
         {Saha}, Abhijit and {Yang}, Shuo and {Zhao}, Zhenge and
         {Kececioglu}, John and {Scheidegger}, Carlos and
         {Snodgrass}, Richard T. and {Axelrod}, Tim and {Jenness}, Tim and
         {Maier}, Robert S. and {Ridgway}, Stephen T. and {Seaman}, Robert L. and
         {Evans}, Eric Michael and {Singh}, Navdeep and {Taylor}, Clark and
         {Toeniskoetter}, Jackson and {Welch}, Eric and {Zhu}, Songzhe and
         {ANTARES Collaboration}},
        title = "{Machine-learning-based Brokers for Real-time Classification of the LSST Alert Stream}",
      journal = {\apjs},
     keywords = {methods: data analysis, methods: statistical, stars: variables: general, supernovae: general, surveys, virtual observatory tools, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2018",
        month = "May",
       volume = {236},
       number = {1},
          eid = {9},
        pages = {9},
     abstract = "{The unprecedented volume and rate of transient events that will be
        discovered by the Large Synoptic Survey Telescope (LSST) demand
        that the astronomical community update its follow-up paradigm.
        Alert-brokers{\textemdash}automated software system to sift
        through, characterize, annotate, and prioritize events for
        follow-up{\textemdash}will be critical tools for managing alert
        streams in the LSST era. The Arizona-NOAO Temporal Analysis and
        Response to Events System (ANTARES) is one such broker. In this
        work, we develop a machine learning pipeline to characterize and
        classify variable and transient sources only using the available
        multiband optical photometry. We describe three illustrative
        stages of the pipeline, serving the three goals of early,
        intermediate, and retrospective classification of alerts. The
        first takes the form of variable versus transient
        categorization, the second a multiclass typing of the combined
        variable and transient data set, and the third a purity-driven
        subtyping of a transient class. Although several similar
        algorithms have proven themselves in simulations, we validate
        their performance on real observations for the first time. We
        quantitatively evaluate our pipeline on sparse, unevenly
        sampled, heteroskedastic data from various existing
        observational campaigns, and demonstrate very competitive
        classification performance. We describe our progress toward
        adapting the pipeline developed in this work into a real-time
        broker working on live alert streams from time-domain surveys.}",
          doi = {10.3847/1538-4365/aab781},
archivePrefix = {arXiv},
       eprint = {1801.07323},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJS..236....9N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...859...20T,
       author = {{Timlin}, John D. and {Ross}, Nicholas P. and {Richards}, Gordon T. and
         {Myers}, Adam D. and {Pellegrino}, Andrew and {Bauer}, Franz E. and
         {Lacy}, Mark and {Schneider}, Donald P. and {Wollack}, Edward J. and
         {Zakamska}, Nadia L.},
        title = "{The Clustering of High-redshift (2.9 {\ensuremath{\leq}} z {\ensuremath{\leq}} 5.1) Quasars in SDSS Stripe 82}",
      journal = {\apj},
     keywords = {large-scale structure of universe, quasars: general, quasars: supermassive black holes, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "May",
       volume = {859},
       number = {1},
          eid = {20},
        pages = {20},
     abstract = "{We present a measurement of the two-point autocorrelation function of
        photometrically selected high-z quasars over ̃100 deg$^{2}$ on
        the Sloan Digital Sky Survey Stripe 82 field. Selection is
        performed using three machine-learning algorithms in a six-
        dimensional optical/mid-infrared color space. Optical data from
        the Sloan Digital Sky Survey are combined with overlapping deep
        mid-infrared data from the Spitzer IRAC Equatorial Survey and
        the Spitzer-HETDEX Exploratory Large-Area survey. Our selection
        algorithms are trained on the colors of known high-z quasars.
        The selected quasar sample consists of 1378 objects and contains
        both spectroscopically confirmed quasars and photometrically
        selected quasar candidates. These objects span a redshift range
        of 2.9 {\ensuremath{\leq}} z {\ensuremath{\leq}} 5.1 and are
        generally fainter than i = 20.2, a regime that has lacked
        sufficient number density to perform autocorrelation function
        measurements of photometrically classified quasars. We compute
        the angular correlation function of these data, marginally
        detecting quasar clustering. We fit a single power law with an
        index of {\ensuremath{\delta}} = 1.39 {\ensuremath{\pm}} 0.618
        and amplitude of {\ensuremath{\theta}} $_{0}$ = 0.′71
        {\ensuremath{\pm}} 0.′546 . A dark matter model is fit to the
        angular correlation function to estimate the linear bias. At the
        average redshift of our survey (\&lt; z\&gt; =3.38), the bias is
        b = 6.78 {\ensuremath{\pm}} 1.79. Using this bias, we calculate
        a characteristic dark matter halo mass of 1.70-9.83{\texttimes}
        \{10\}$^{12}$\{h\}$^{-1}$ \{M\}$_{☉ }$. Our bias estimate
        suggests that quasar feedback intermittently shuts down the
        accretion of gas onto the central supermassive black hole at
        early times. If confirmed, these results hint at a level of
        luminosity dependence in the clustering of quasars at high-z.}",
          doi = {10.3847/1538-4357/aab9ac},
archivePrefix = {arXiv},
       eprint = {1712.03128},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...859...20T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...858..114H,
       author = {{Huertas-Company}, M. and {Primack}, J.~R. and {Dekel}, A. and
         {Koo}, D.~C. and {Lapiner}, S. and {Ceverino}, D. and {Simons}, R.~C. and
         {Snyder}, G.~F. and {Bernardi}, M. and {Chen}, Z. and
         {Dom{\'\i}nguez-S{\'a}nchez}, H. and {Lee}, C.~T. and
         {Margalef-Bentabol}, B. and {Tuccillo}, D.},
        title = "{Deep Learning Identifies High-z Galaxies in a Central Blue Nugget Phase in a Characteristic Mass Range}",
      journal = {\apj},
     keywords = {galaxies: bulges, galaxies: fundamental parameters, galaxies: high-redshift, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "May",
       volume = {858},
       number = {2},
          eid = {114},
        pages = {114},
     abstract = "{We use machine learning to identify in color images of high-redshift
        galaxies an astrophysical phenomenon predicted by cosmological
        simulations. This phenomenon, called the blue nugget (BN) phase,
        is the compact star-forming phase in the central regions of many
        growing galaxies that follows an earlier phase of gas compaction
        and is followed by a central quenching phase. We train a
        convolutional neural network (CNN) with mock
        {\textquotedblleft}observed{\textquotedblright} images of
        simulated galaxies at three phases of evolution{\textemdash}
        pre-BN, BN, and post-BN{\textemdash}and demonstrate that the CNN
        successfully retrieves the three phases in other simulated
        galaxies. We show that BNs are identified by the CNN within a
        time window of ̃0.15 Hubble times. When the trained CNN is
        applied to observed galaxies from the CANDELS survey at z = 1-3,
        it successfully identifies galaxies at the three phases. We find
        that the observed BNs are preferentially found in galaxies at a
        characteristic stellar mass range, 10$^{9.2-10.3}$ M $_{☉}$ at
        all redshifts. This is consistent with the characteristic galaxy
        mass for BNs as detected in the simulations and is meaningful
        because it is revealed in the observations when the direct
        information concerning the total galaxy luminosity has been
        eliminated from the training set. This technique can be applied
        to the classification of other astrophysical phenomena for
        improved comparison of theory and observations in the era of
        large imaging surveys and cosmological simulations.}",
          doi = {10.3847/1538-4357/aabfed},
archivePrefix = {arXiv},
       eprint = {1804.07307},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...858..114H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018AdSpR..61.2628P,
       author = {{Peng}, Hao and {Bai}, Xiaoli},
        title = "{Improving orbit prediction accuracy through supervised machine learning}",
      journal = {Advances in Space Research},
     keywords = {Orbit prediction, Resident space object, Supervised machine learning, Support vector machine, Astrophysics - Earth and Planetary Astrophysics, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2018",
        month = "May",
       volume = {61},
       number = {10},
        pages = {2628-2646},
     abstract = "{Due to the lack of information such as the space environment condition
        and resident space objects' (RSOs') body characteristics,
        current orbit predictions that are solely grounded on physics-
        based models may fail to achieve required accuracy for collision
        avoidance and have led to satellite collisions already. This
        paper presents a methodology to predict RSOs' trajectories with
        higher accuracy than that of the current methods. Inspired by
        the machine learning (ML) theory through which the models are
        learned based on large amounts of observed data and the
        prediction is conducted without explicitly modeling space
        objects and space environment, the proposed ML approach
        integrates physics-based orbit prediction algorithms with a
        learning-based process that focuses on reducing the prediction
        errors. Using a simulation-based space catalog environment as
        the test bed, the paper demonstrates three types of
        generalization capability for the proposed ML approach: (1) the
        ML model can be used to improve the same RSO's orbit information
        that is not available during the learning process but shares the
        same time interval as the training data; (2) the ML model can be
        used to improve predictions of the same RSO at future epochs;
        and (3) the ML model based on a RSO can be applied to other RSOs
        that share some common features.}",
          doi = {10.1016/j.asr.2018.03.001},
archivePrefix = {arXiv},
       eprint = {1801.04856},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018AdSpR..61.2628P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018AcAau.146...33N,
       author = {{Nesvold}, E.~R. and {Greenberg}, A. and {Erasmus}, N. and
         {van Heerden}, E. and {Galache}, J.~L. and {Dahlstrom}, E. and
         {Marchis}, F.},
        title = "{The Deflector Selector: A machine learning framework for prioritizing hazardous object deflection technology development}",
      journal = {Acta Astronautica},
     keywords = {Planetary defense, Orbital mechanics, Machine learning, Astrophysics - Earth and Planetary Astrophysics},
         year = "2018",
        month = "May",
       volume = {146},
        pages = {33-45},
     abstract = "{Several technologies have been proposed for deflecting a hazardous Solar
        System object on a trajectory that would otherwise impact the
        Earth. The effectiveness of each technology depends on several
        characteristics of the given object, including its orbit and
        size. The distribution of these parameters in the likely
        population of Earth-impacting objects can thus determine which
        of the technologies are most likely to be useful in preventing a
        collision with the Earth. None of the proposed deflection
        technologies has been developed and fully tested in space.
        Developing every proposed technology is currently prohibitively
        expensive, so determining now which technologies are most likely
        to be effective would allow us to prioritize a subset of
        proposed deflection technologies for funding and development. We
        present a new model, the Deflector Selector, that takes as its
        input the characteristics of a hazardous object or population of
        such objects and predicts which technology would be able to
        perform a successful deflection. The model consists of a
        machine-learning algorithm trained on data produced by N-body
        integrations simulating the deflections. We describe the model
        and present the results of tests of the effectiveness of nuclear
        explosives, kinetic impactors, and gravity tractors on three
        simulated populations of hazardous objects.}",
          doi = {10.1016/j.actaastro.2018.01.049},
archivePrefix = {arXiv},
       eprint = {1802.00458},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018AcAau.146...33N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...612A..98G,
       author = {{Garcia-Dias}, Rafael and {Allende Prieto}, Carlos and
         {S{\'a}nchez Almeida}, Jorge and {Ordov{\'a}s-Pascual}, Ignacio},
        title = "{Machine learning in APOGEE. Unsupervised spectral classification with K-means}",
      journal = {\aap},
     keywords = {methods: data analysis, methods: numerical, catalogs, surveys, techniques: spectroscopic, Galaxy: stellar content, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Astrophysics - Solar and Stellar Astrophysics, Computer Science - Machine Learning},
         year = "2018",
        month = "May",
       volume = {612},
          eid = {A98},
        pages = {A98},
     abstract = "{Context. The volume of data generated by astronomical surveys is growing
        rapidly. Traditional analysis techniques in spectroscopy either
        demand intensive human interaction or are computationally
        expensive. In this scenario, machine learning, and unsupervised
        clustering algorithms in particular, offer interesting
        alternatives. The Apache Point Observatory Galactic Evolution
        Experiment (APOGEE) offers a vast data set of near-infrared
        stellar spectra, which is perfect for testing such alternatives.
        <BR /> Aims: Our research applies an unsupervised classification
        scheme based on K-means to the massive APOGEE data set. We
        explore whether the data are amenable to classification into
        discrete classes. <BR /> Methods: We apply the K-means algorithm
        to 153 847 high resolution spectra (R {\ensuremath{\approx}} 22
        500). We discuss the main virtues and weaknesses of the
        algorithm, as well as our choice of parameters. <BR /> Results:
        We show that a classification based on normalised spectra
        captures the variations in stellar atmospheric parameters,
        chemical abundances, and rotational velocity, among other
        factors. The algorithm is able to separate the bulge and halo
        populations, and distinguish dwarfs, sub-giants, RC, and RGB
        stars. However, a discrete classification in flux space does not
        result in a neat organisation in the parameters' space.
        Furthermore, the lack of obvious groups in flux space causes the
        results to be fairly sensitive to the initialisation, and
        disrupts the efficiency of commonly-used methods to select the
        optimal number of clusters. Our classification is publicly
        available, including extensive online material associated with
        the APOGEE Data Release 12 (DR12). <BR /> Conclusions: Our
        description of the APOGEE database can help greatly with the
        identification of specific types of targets for various
        applications. We find a lack of obvious groups in flux space,
        and identify limitations of the K-means algorithm in dealing
        with this kind of data. Full Tables B.1-B.4 are only available
        at the CDS via anonymous ftp to <A href=``http://cdsarc.u-strasb
        g.fr''>http://cdsarc.u-strasbg.fr</A> (<A
        href=``http://130.79.128.5''>http://130.79.128.5</A>) or via <A
        href=``http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/612/A98''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/612/A98</A>}",
          doi = {10.1051/0004-6361/201732134},
archivePrefix = {arXiv},
       eprint = {1801.07912},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...612A..98G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PhRvD..97h3004C,
       author = {{Charnock}, Tom and {Lavaux}, Guilhem and {Wandelt}, Benjamin D.},
        title = "{Automatic physical inference with information maximizing neural networks}",
      journal = {\prd},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Apr",
       volume = {97},
       number = {8},
          eid = {083004},
        pages = {083004},
     abstract = "{Compressing large data sets to a manageable number of summaries that are
        informative about the underlying parameters vastly simplifies
        both frequentist and Bayesian inference. When only simulations
        are available, these summaries are typically chosen
        heuristically, so they may inadvertently miss important
        information. We introduce a simulation-based machine learning
        technique that trains artificial neural networks to find
        nonlinear functionals of data that maximize Fisher information:
        information maximizing neural networks (IMNNs). In test cases
        where the posterior can be derived exactly, likelihood-free
        inference based on automatically derived IMNN summaries produces
        nearly exact posteriors, showing that these summaries are good
        approximations to sufficient statistics. In a series of
        numerical examples of increasing complexity and astrophysical
        relevance we show that IMNNs are robustly capable of
        automatically finding optimal, nonlinear summaries of the data
        even in cases where linear compression fails: inferring the
        variance of Gaussian signal in the presence of noise, inferring
        cosmological parameters from mock simulations of the
        Lyman-{\ensuremath{\alpha}} forest in quasar spectra, and
        inferring frequency-domain parameters from LISA-like detections
        of gravitational waveforms. In this final case, the IMNN summary
        outperforms linear data compression by avoiding the introduction
        of spurious likelihood maxima. We anticipate that the automatic
        physical inference method described in this paper will be
        essential to obtain both accurate and precise cosmological
        parameter estimates from complex and large astronomical data
        sets, including those from LSST and Euclid.}",
          doi = {10.1103/PhysRevD.97.083004},
archivePrefix = {arXiv},
       eprint = {1802.03537},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhRvD..97h3004C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.475.4494H,
       author = {{Hui}, Jianan and {Aragon}, Miguel and {Cui}, Xinping and
         {Flegal}, James M.},
        title = "{A machine learning approach to galaxy-LSS classification - I. Imprints on halo merger trees}",
      journal = {\mnras},
     keywords = {methods: data analysis, galaxies: evolution, large-scale structure of Universe, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Apr",
       volume = {475},
       number = {4},
        pages = {4494-4503},
     abstract = "{The cosmic web plays a major role in the formation and evolution of
        galaxies and defines, to a large extent, their properties.
        However, the relation between galaxies and environment is still
        not well understood. Here, we present a machine learning
        approach to study imprints of environmental effects on the mass
        assembly of haloes. We present a galaxy-LSS machine learning
        classifier based on galaxy properties sensitive to the
        environment. We then use the classifier to assess the relevance
        of each property. Correlations between galaxy properties and
        their cosmic environment can be used to predict galaxy
        membership to void/wall or filament/cluster with an accuracy of
        93 per cent. Our study unveils environmental information encoded
        in properties of haloes not normally considered directly
        dependent on the cosmic environment such as merger history and
        complexity. Understanding the physical mechanism by which the
        cosmic web is imprinted in a halo can lead to significant
        improvements in galaxy formation models. This is accomplished by
        extracting features from galaxy properties and merger trees,
        computing feature scores for each feature and then applying
        support vector machine (SVM) to different feature sets. To this
        end, we have discovered that the shape and depth of the merger
        tree, formation time, and density of the galaxy are strongly
        associated with the cosmic environment. We describe a
        significant improvement in the original classification algorithm
        by performing LU decomposition of the distance matrix computed
        by the feature vectors and then using the output of the
        decomposition as input vectors for SVM.}",
          doi = {10.1093/mnras/stx3235},
archivePrefix = {arXiv},
       eprint = {1803.11156},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.475.4494H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.475.3633W,
       author = {{Wu}, Yaqian and {Xiang}, Maosheng and {Bi}, Shaolan and {Liu}, Xiaowei and
         {Yu}, Jie and {Hon}, Marc and {Sharma}, Sanjib and {Li}, Tanda and
         {Huang}, Yang and {Liu}, Kang and {Zhang}, Xianfei and {Li}, Yaguang and
         {Ge}, Zhishuai and {Tian}, Zhijia and {Zhang}, Jinghua and
         {Zhang}, Jianwei},
        title = "{Mass and age of red giant branch stars observed with LAMOST and Kepler}",
      journal = {\mnras},
     keywords = {asteroseismology, stars: evolution, stars: fundamental parameters, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Apr",
       volume = {475},
       number = {3},
        pages = {3633-3643},
     abstract = "{Obtaining accurate and precise masses and ages for large numbers of
        giant stars is of great importance for unraveling the assemblage
        history of the Galaxy. In this paper, we estimate masses and
        ages of 6940 red giant branch (RGB) stars with asteroseismic
        parameters deduced from Kepler photometry and stellar
        atmospheric parameters derived from LAMOST spectra. The typical
        uncertainties of mass is a few per cent, and that of age is ̃20
        per cent. The sample stars reveal two separate sequences in the
        age-[{\ensuremath{\alpha}}/Fe] relation - a
        high-{\ensuremath{\alpha}} sequence with stars older than ̃8 Gyr
        and a low-{\ensuremath{\alpha}} sequence composed of stars with
        ages ranging from younger than 1 Gyr to older than 11 Gyr. We
        further investigate the feasibility of deducing ages and masses
        directly from LAMOST spectra with a machine learning method
        based on kernel based principal component analysis, taking a
        sub-sample of these RGB stars as a training data set. We
        demonstrate that ages thus derived achieve an accuracy of ̃24
        per cent. We also explored the feasibility of estimating ages
        and masses based on the spectroscopically measured carbon and
        nitrogen abundances. The results are quite satisfactory and
        significantly improved compared to the previous studies.}",
          doi = {10.1093/mnras/stx3296},
archivePrefix = {arXiv},
       eprint = {1712.09779},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.475.3633W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.475.3613S,
       author = {{Soo}, John Y.~H. and {Moraes}, Bruno and {Joachimi}, Benjamin and
         {Hartley}, William and {Lahav}, Ofer and {Charbonnier}, Ald{\'e}e and
         {Makler}, Mart{\'\i}n and {Pereira}, Maria E.~S. and {Comparat}, Johan and
         {Erben}, Thomas and {Leauthaud}, Alexie and {Shan}, Huanyuan and
         {Van Waerbeke}, Ludovic},
        title = "{Morpho-z: improving photometric redshifts with galaxy morphology}",
      journal = {\mnras},
     keywords = {methods: statistical, catalogues, galaxies: distances and redshifts, galaxies: structure, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Apr",
       volume = {475},
       number = {3},
        pages = {3613-3632},
     abstract = "{We conduct a comprehensive study of the effects of incorporating galaxy
        morphology information in photometric redshift estimation. Using
        machine learning methods, we assess the changes in the scatter
        and outlier fraction of photometric redshifts when galaxy size,
        ellipticity, S{\'e}rsic index, and surface brightness are
        included in training on galaxy samples from the SDSS and the
        CFHT Stripe-82 Survey (CS82). We show that by adding galaxy
        morphological parameters to full ugriz photometry, only mild
        improvements are obtained, while the gains are substantial in
        cases where fewer passbands are available. For instance, the
        combination of grz photometry and morphological parameters
        almost fully recovers the metrics of 5-band photometric
        redshifts. We demonstrate that with morphology it is possible to
        determine useful redshift distribution N(z) of galaxy samples
        without any colour information. We also find that the inclusion
        of quasar redshifts and associated object sizes in training
        improves the quality of photometric redshift catalogues,
        compensating for the lack of a good star-galaxy separator. We
        further show that morphological information can mitigate biases
        and scatter due to bad photometry. As an application, we derive
        both point estimates and posterior distributions of redshifts
        for the official CS82 catalogue, training on morphology and SDSS
        Stripe-82 ugriz bands when available. Our redshifts yield a 68th
        percentile error of 0.058(1 + z), and a outlier fraction of 5.2
        per cent. We further include a deep extension trained on
        morphology and single i-band CS82 photometry.}",
          doi = {10.1093/mnras/stx3201},
archivePrefix = {arXiv},
       eprint = {1707.03169},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.475.3613S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.475.2560V,
       author = {{van Roestel}, J. and {Kupfer}, T. and {Ruiz-Carmona}, R. and
         {Groot}, P.~J. and {Prince}, T.~A. and {Burdge}, K. and {Laher}, R. and
         {Shupe}, D.~L. and {Bellm}, E.},
        title = "{Discovery of 36 eclipsing EL CVn binaries found by the Palomar Transient Factory}",
      journal = {\mnras},
     keywords = {binaries: close, binaries: eclipsing, stars: individual: EL CVn, white dwarfs, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Apr",
       volume = {475},
       number = {2},
        pages = {2560-2590},
     abstract = "{We report on the discovery and analysis of 36 new eclipsing EL CVn-type
        binaries, consisting of a core helium-composition pre-white
        dwarf (pre-He-WD) and an early-type main-sequence companion.
        This more than doubles the known population of these systems. We
        have used supervised machine learning methods to search 0.8
        million light curves from the Palomar Transient Factory (PTF),
        combined with Sloan Digital Sky Survey (SDSS), Panoramic Survey
        Telescope and Rapid Response System (Pan-STARRS) and Two-Micron
        All-Sky Survey (2MASS) colours. The new systems range in orbital
        periods from 0.46 to 3.8 d and in apparent brightness from ̃14
        to 16 mag in the PTF R or g$^{΄}$ filters. For 12 of the
        systems, we obtained radial velocity curves with the
        Intermediate Dispersion Spectrograph at the Isaac Newton
        Telescope. We modelled the light curves, radial velocity curves
        and spectral energy distributions to determine the system
        parameters. The radii (0.3-0.7 R$_{☉}$) and effective
        temperatures (8000-17 000 K) of the pre-He-WDs are consistent
        with stellar evolution models, but the masses (0.12-0.28
        M$_{☉}$) show more variance than models have predicted. This
        study shows that using machine learning techniques on large
        synoptic survey data is a powerful way to discover substantial
        samples of binary systems in short-lived evolutionary stages.}",
          doi = {10.1093/mnras/stx3291},
archivePrefix = {arXiv},
       eprint = {1712.06507},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.475.2560V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.475.2326P,
       author = {{Pashchenko}, Ilya N. and {Sokolovsky}, Kirill V. and
         {Gavras}, Panagiotis},
        title = "{Machine learning search for variable stars}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, stars: variables: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Apr",
       volume = {475},
       number = {2},
        pages = {2326-2343},
     abstract = "{Photometric variability detection is often considered as a hypothesis
        testing problem: an object is variable if the null hypothesis
        that its brightness is constant can be ruled out given the
        measurements and their uncertainties. The practical
        applicability of this approach is limited by uncorrected
        systematic errors. We propose a new variability detection
        technique sensitive to a wide range of variability types while
        being robust to outliers and underestimated measurement
        uncertainties. We consider variability detection as a
        classification problem that can be approached with machine
        learning. Logistic Regression (LR), Support Vector Machines
        (SVM), k Nearest Neighbours (kNN), Neural Nets (NN), Random
        Forests (RF), and Stochastic Gradient Boosting classifier (SGB)
        are applied to 18 features (variability indices) quantifying
        scatter and/or correlation between points in a light curve. We
        use a subset of Optical Gravitational Lensing Experiment phase
        two (OGLE-II) Large Magellanic Cloud (LMC) photometry (30 265
        light curves) that was searched for variability using
        traditional methods (168 known variable objects) as the training
        set and then apply the NN to a new test set of 31 798 OGLE-II
        LMC light curves. Among 205 candidates selected in the test set,
        178 are real variables, while 13 low-amplitude variables are new
        discoveries. The machine learning classifiers considered are
        found to be more efficient (select more variables and fewer
        false candidates) compared to traditional techniques using
        individual variability indices or their linear combination. The
        NN, SGB, SVM, and RF show a higher efficiency compared to LR and
        kNN.}",
          doi = {10.1093/mnras/stx3222},
archivePrefix = {arXiv},
       eprint = {1710.07290},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.475.2326P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018GeoRL..45.3706J,
       author = {{Jiang}, Guo-Qing and {Xu}, Jing and {Wei}, Jun},
        title = "{A Deep Learning Algorithm of Neural Network for the Parameterization of Typhoon-Ocean Feedback in Typhoon Forecast Models}",
      journal = {\grl},
     keywords = {neural network, typhoon, tropical cyclone, air-sea interaction, parameterization},
         year = "2018",
        month = "Apr",
       volume = {45},
       number = {8},
        pages = {3706-3716},
     abstract = "{Two algorithms based on machine learning neural networks are
        proposed{\textemdash}the shallow learning (S-L) and deep
        learning (D-L) algorithms{\textemdash}that can potentially be
        used in atmosphere-only typhoon forecast models to provide flow-
        dependent typhoon-induced sea surface temperature cooling (SSTC)
        for improving typhoon predictions. The major challenge of
        existing SSTC algorithms in forecast models is how to accurately
        predict SSTC induced by an upcoming typhoon, which requires
        information not only from historical data but more importantly
        also from the target typhoon itself. The S-L algorithm composes
        of a single layer of neurons with mixed atmospheric and oceanic
        factors. Such a structure is found to be unable to represent
        correctly the physical typhoon-ocean interaction. It tends to
        produce an unstable SSTC distribution, for which any
        perturbations may lead to changes in both SSTC pattern and
        strength. The D-L algorithm extends the neural network to a 4
        {\texttimes} 5 neuron matrix with atmospheric and oceanic
        factors being separated in different layers of neurons, so that
        the machine learning can determine the roles of atmospheric and
        oceanic factors in shaping the SSTC. Therefore, it produces a
        stable crescent-shaped SSTC distribution, with its large-scale
        pattern determined mainly by atmospheric factors (e.g., winds)
        and small-scale features by oceanic factors (e.g., eddies).
        Sensitivity experiments reveal that the D-L algorithms improve
        maximum wind intensity errors by 60-70\% for four case study
        simulations, compared to their atmosphere-only model runs.}",
          doi = {10.1002/2018GL077004},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018GeoRL..45.3706J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018GeoRL..45.3136G,
       author = {{Gentine}, P. and {Alemohammad}, S.~H.},
        title = "{Reconstructed Solar-Induced Fluorescence: A Machine Learning Vegetation Product Based on MODIS Surface Reflectance to Reproduce GOME-2 Solar-Induced Fluorescence}",
      journal = {\grl},
     keywords = {Photosynthesis, Solar-induced fluorescence, Remote Sensing, MODIS},
         year = "2018",
        month = "Apr",
       volume = {45},
       number = {7},
        pages = {3136-3146},
     abstract = "{Solar-induced fluorescence (SIF) observations from space have resulted
        in major advancements in estimating gross primary productivity
        (GPP). However, current SIF observations remain spatially
        coarse, infrequent, and noisy. Here we develop a machine
        learning approach using surface reflectances from Moderate
        Resolution Imaging Spectroradiometer (MODIS) channels to
        reproduce SIF normalized by clear sky surface irradiance from
        the Global Ozone Monitoring Experiment-2 (GOME-2). The resulting
        product is a proxy for ecosystem photosynthetically active
        radiation absorbed by chlorophyll (fAPAR$_{Ch}$). Multiplying
        this new product with a MODIS estimate of photosynthetically
        active radiation provides a new MODIS-only reconstruction of SIF
        called Reconstructed SIF (RSIF). RSIF exhibits much higher
        seasonal and interannual correlation than the original SIF when
        compared with eddy covariance estimates of GPP and two reference
        global GPP products, especially in dry and cold regions. RSIF
        also reproduces intense productivity regions such as the U.S.
        Corn Belt contrary to typical vegetation indices and similarly
        to SIF.}",
          doi = {10.1002/2017GL076294},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018GeoRL..45.3136G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJS..235...42A,
       author = {{Abolfathi}, Bela and {Aguado}, D.~S. and {Aguilar}, Gabriela and
         {Allende Prieto}, Carlos and {Almeida}, Andres and
         {Ananna}, Tonima Tasnim and {Anders}, Friedrich and
         {Anderson}, Scott F. and {Andrews}, Brett H. and {Anguiano}, Borja and
         {Arag{\'o}n-Salamanca}, Alfonso and {Argudo-Fern{\'a}ndez}, Maria and
         {Armengaud}, Eric and {Ata}, Metin and {Aubourg}, Eric and
         {Avila-Reese}, Vladimir and {Badenes}, Carles and {Bailey}, Stephen and
         {Balland}, Christophe and {Barger}, Kathleen A. and
         {Barrera-Ballesteros}, Jorge and {Bartosz}, Curtis and
         {Bastien}, Fabienne and {Bates}, Dominic and {Baumgarten}, Falk and
         {Bautista}, Julian and {Beaton}, Rachael and {Beers}, Timothy C. and
         {Belfiore}, Francesco and {Bender}, Chad F. and {Bernardi}, Mariangela and
         {Bershady}, Matthew A. and {Beutler}, Florian and {Bird}, Jonathan C. and
         {Bizyaev}, Dmitry and {Blanc}, Guillermo A. and {Blanton}, Michael R. and
         {Blomqvist}, Michael and {Bolton}, Adam S. and
         {Boquien}, M{\'e}d{\'e}ric and {Borissova}, Jura and {Bovy}, Jo and
         {Bradna Diaz}, Christian Andres and {Brandt}, William Nielsen and
         {Brinkmann}, Jonathan and {Brownstein}, Joel R. and {Bundy}, Kevin and
         {Burgasser}, Adam J. and {Burtin}, Etienne and {Busca}, Nicol{\'a}s G. and
         {Ca{\~n}as}, Caleb I. and {Cano-D{\'\i}az}, Mariana and
         {Cappellari}, Michele and {Carrera}, Ricardo and {Casey}, Andrew R. and
         {Cervantes Sodi}, Bernardo and {Chen}, Yanping and {Cherinka}, Brian and
         {Chiappini}, Cristina and {Choi}, Peter Doohyun and {Chojnowski}, Drew and
         {Chuang}, Chia-Hsun and {Chung}, Haeun and {Clerc}, Nicolas and
         {Cohen}, Roger E. and {Comerford}, Julia M. and {Comparat}, Johan and
         {Correa do Nascimento}, Janaina and {da Costa}, Luiz and
         {Cousinou}, Marie-Claude and {Covey}, Kevin and {Crane}, Jeffrey D. and
         {Cruz-Gonzalez}, Irene and {Cunha}, Katia and
         {da Silva Ilha}, Gabriele and {Damke}, Guillermo J. and
         {Darling}, Jeremy and {Davidson}, James W., Jr. and {Dawson}, Kyle and
         {de Icaza Lizaola}, Miguel Angel C. and {de la Macorra}, Axel and
         {de la Torre}, Sylvain and {De Lee}, Nathan and
         {de Sainte Agathe}, Victoria and {Deconto Machado}, Alice and
         {Dell'Agli}, Flavia and {Delubac}, Timoth{\'e}e and
         {Diamond-Stanic}, Aleksandar M. and {Donor}, John and
         {Downes}, Juan Jos{\'e} and {Drory}, Niv and
         {du Mas des Bourboux}, H{\'e}lion and {Duckworth}, Christopher J. and
         {Dwelly}, Tom and {Dyer}, Jamie and {Ebelke}, Garrett and
         {Davis Eigenbrot}, Arthur and {Eisenstein}, Daniel J. and
         {Elsworth}, Yvonne P. and {Emsellem}, Eric and {Eracleous}, Michael and
         {Erfanianfar}, Ghazaleh and {Escoffier}, Stephanie and {Fan}, Xiaohui and
         {Fern{\'a}ndez Alvar}, Emma and {Fernandez-Trincado}, J.~G. and {Fernand
        o Cirolini}, Rafael and {Feuillet}, Diane and {Finoguenov}, Alexis and
         {Fleming}, Scott W. and {Font-Ribera}, Andreu and {Freischlad}, Gordon and
         {Frinchaboy}, Peter and {Fu}, Hai and {G{\'o}mez Maqueo Chew}, Yilen and
         {Galbany}, Llu{\'\i}s and {Garc{\'\i}a P{\'e}rez}, Ana E. and
         {Garcia-Dias}, R. and {Garc{\'\i}a-Hern{\'a}ndez}, D.~A. and
         {Garma Oehmichen}, Luis Alberto and {Gaulme}, Patrick and {Gelfand
        }, Joseph and {Gil-Mar{\'\i}n}, H{\'e}ctor and {Gillespie}, Bruce A. and
         {Goddard}, Daniel and {Gonz{\'a}lez Hern{\'a}ndez}, Jonay I. and
         {Gonzalez-Perez}, Violeta and {Grabowski}, Kathleen and
         {Green}, Paul J. and {Grier}, Catherine J. and {Gueguen}, Alain and
         {Guo}, Hong and {Guy}, Julien and {Hagen}, Alex and {Hall}, Patrick and
         {Harding}, Paul and {Hasselquist}, Sten and {Hawley}, Suzanne and
         {Hayes}, Christian R. and {Hearty}, Fred and {Hekker}, Saskia and {Hernand
        ez}, Jesus and {Hernandez Toledo}, Hector and {Hogg}, David W. and
         {Holley-Bockelmann}, Kelly and {Holtzman}, Jon A. and {Hou}, Jiamin and
         {Hsieh}, Bau-Ching and {Hunt}, Jason A.~S. and
         {Hutchinson}, Timothy A. and {Hwang}, Ho Seong and
         {Jimenez Angel}, Camilo Eduardo and {Johnson}, Jennifer A. and
         {Jones}, Amy and {J{\"o}nsson}, Henrik and {Jullo}, Eric and
         {Khan}, Fahim Sakil and {Kinemuchi}, Karen and {Kirkby}, David and
         {Kirkpatrick}, Charles C., IV and {Kitaura}, Francisco-Shu and
         {Knapp}, Gillian R. and {Kneib}, Jean-Paul and {Kollmeier}, Juna A. and
         {Lacerna}, Ivan and {Lane}, Richard R. and {Lang}, Dustin and
         {Law}, David R. and {Le Goff}, Jean-Marc and {Lee}, Young-Bae and
         {Li}, Hongyu and {Li}, Cheng and {Lian}, Jianhui and {Liang}, Yu and
         {Lima}, Marcos and {Lin}, Lihwai and {Long}, Dan and {Lucatello}, Sara and
         {Lundgren}, Britt and {Mackereth}, J. Ted and {MacLeod}, Chelsea L. and
         {Mahadevan}, Suvrath and {Maia}, Marcio Antonio Geimba and
         {Majewski}, Steven and {Manchado}, Arturo and {Maraston}, Claudia and
         {Mariappan}, Vivek and {Marques-Chaves}, Rui and {Masseron}, Thomas and
         {Masters}, Karen L. and {McDermid}, Richard M. and {McGreer}, Ian D. and
         {Melendez}, Matthew and {Meneses-Goytia}, Sofia and {Merloni}, Andrea and
         {Merrifield}, Michael R. and {Meszaros}, Szabolcs and {Meza}, Andres and
         {Minchev}, Ivan and {Minniti}, Dante and {Mueller}, Eva-Maria and
         {Muller-Sanchez}, Francisco and {Muna}, Demitri and
         {Mu{\~n}oz}, Ricardo R. and {Myers}, Adam D. and {Nair}, Preethi and {Nand
        ra}, Kirpal and {Ness}, Melissa and {Newman}, Jeffrey A. and
         {Nichol}, Robert C. and {Nidever}, David L. and {Nitschelm}, Christian and
         {Noterdaeme}, Pasquier and {O'Connell}, Julia and
         {Oelkers}, Ryan James and {Oravetz}, Audrey and {Oravetz}, Daniel and
         {Ort{\'\i}z}, Erik Aquino and {Osorio}, Yeisson and {Pace}, Zach and
         {Padilla}, Nelson and {Palanque-Delabrouille}, Nathalie and
         {Palicio}, Pedro Alonso and {Pan}, Hsi-An and {Pan}, Kaike and
         {Parikh}, Taniya and {P{\^a}ris}, Isabelle and {Park}, Changbom and
         {Peirani}, Sebastien and {Pellejero-Ibanez}, Marcos and
         {Penny}, Samantha and {Percival}, Will J. and {Perez-Fournon}, Ismael and
         {Petitjean}, Patrick and {Pieri}, Matthew M. and {Pinsonneault}, Marc and
         {Pisani}, Alice and {Prada}, Francisco and {Prakash}, Abhishek and
         {Queiroz}, Anna B{\'a}rbara de Andrade and {Raddick}, M. Jordan and
         {Raichoor}, Anand and {Barboza Rembold}, Sandro and
         {Richstein}, Hannah and {Riffel}, Rogemar A. and {Riffel}, Rog{\'e}rio and
         {Rix}, Hans-Walter and {Robin}, Annie C. and
         {Rodr{\'\i}guez Torres}, Sergio and {Rom{\'a}n-Z{\'u}{\~n}iga}, Carlos and
         {Ross}, Ashley J. and {Rossi}, Graziano and {Ruan}, John and
         {Ruggeri}, Rossana and {Ruiz}, Jose and {Salvato}, Mara and
         {S{\'a}nchez}, Ariel G. and {S{\'a}nchez}, Sebasti{\'a}n F. and
         {Sanchez Almeida}, Jorge and {S{\'a}nchez-Gallego}, Jos{\'e} R. and
         {Santana Rojas}, Felipe Antonio and {Santiago}, Bas{\'\i}lio Xavier and
         {Schiavon}, Ricardo P. and {Schimoia}, Jaderson S. and
         {Schlafly}, Edward and {Schlegel}, David and {Schneider}, Donald P. and
         {Schuster}, William J. and {Schwope}, Axel and {Seo}, Hee-Jong and
         {Serenelli}, Aldo and {Shen}, Shiyin and {Shen}, Yue and
         {Shetrone}, Matthew and {Shull}, Michael and
         {Silva Aguirre}, V{\'\i}ctor and {Simon}, Joshua D. and
         {Skrutskie}, Mike and {Slosar}, An{\v{z}}e and {Smethurst}, Rebecca and
         {Smith}, Verne and {Sobeck}, Jennifer and {Somers}, Garrett and
         {Souter}, Barbara J. and {Souto}, Diogo and {Spindler}, Ashley and
         {Stark}, David V. and {Stassun}, Keivan and {Steinmetz}, Matthias and
         {Stello}, Dennis and {Storchi-Bergmann}, Thaisa and
         {Streblyanska}, Alina and {Stringfellow}, Guy S. and
         {Su{\'a}rez}, Genaro and {Sun}, Jing and {Szigeti}, Laszlo and
         {Taghizadeh-Popp}, Manuchehr and {Talbot}, Michael S. and
         {Tang}, Baitian and {Tao}, Charling and {Tayar}, Jamie and
         {Tembe}, Mita and {Teske}, Johanna and {Thakar}, Aniruddha R. and
         {Thomas}, Daniel and {Tissera}, Patricia and {Tojeiro}, Rita and
         {Tremonti}, Christy and {Troup}, Nicholas W. and {Urry}, Meg and
         {Valenzuela}, O. and {van den Bosch}, Remco and
         {Vargas-Gonz{\'a}lez}, Jaime and {Vargas-Maga{\~n}a}, Mariana and
         {Vazquez}, Jose Alberto and {Villanova}, Sandro and {Vogt}, Nicole and
         {Wake}, David and {Wang}, Yuting and {Weaver}, Benjamin Alan and
         {Weijmans}, Anne-Marie and {Weinberg}, David H. and
         {Westfall}, Kyle B. and {Whelan}, David G. and {Wilcots}, Eric and
         {Wild}, Vivienne and {Williams}, Rob A. and {Wilson}, John and
         {Wood-Vasey}, W.~M. and {Wylezalek}, Dominika and {Xiao}, Ting and
         {Yan}, Renbin and {Yang}, Meng and {Ybarra}, Jason E. and
         {Y{\`e}che}, Christophe and {Zakamska}, Nadia and {Zamora}, Olga and
         {Zarrouk}, Pauline and {Zasowski}, Gail and {Zhang}, Kai and
         {Zhao}, Cheng and {Zhao}, Gong-Bo and {Zheng}, Zheng and
         {Zheng}, Zheng and {Zhou}, Zhi-Min and {Zhu}, Guangtun and
         {Zinn}, Joel C. and {Zou}, Hu},
        title = "{The Fourteenth Data Release of the Sloan Digital Sky Survey: First Spectroscopic Data from the Extended Baryon Oscillation Spectroscopic Survey and from the Second Phase of the Apache Point Observatory Galactic Evolution Experiment}",
      journal = {\apjs},
     keywords = {atlases, catalogs, surveys, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Apr",
       volume = {235},
       number = {2},
          eid = {42},
        pages = {42},
     abstract = "{The fourth generation of the Sloan Digital Sky Survey (SDSS-IV) has been
        in operation since 2014 July. This paper describes the second
        data release from this phase, and the 14th from SDSS overall
        (making this Data Release Fourteen or DR14). This release makes
        the data taken by SDSS-IV in its first two years of operation
        (2014-2016 July) public. Like all previous SDSS releases, DR14
        is cumulative, including the most recent reductions and
        calibrations of all data taken by SDSS since the first phase
        began operations in 2000. New in DR14 is the first public
        release of data from the extended Baryon Oscillation
        Spectroscopic Survey; the first data from the second phase of
        the Apache Point Observatory (APO) Galactic Evolution Experiment
        (APOGEE-2), including stellar parameter estimates from an
        innovative data-driven machine-learning algorithm known as
        {\textquotedblleft}The Cannon{\textquotedblright} and almost
        twice as many data cubes from the Mapping Nearby Galaxies at APO
        (MaNGA) survey as were in the previous release (N = 2812 in
        total). This paper describes the location and format of the
        publicly available data from the SDSS-IV surveys. We provide
        references to the important technical papers describing how
        these data have been taken (both targeting and observation
        details) and processed for scientific use. The SDSS web site (<A
        href=``http://www.sdss.org''>www.sdss.org</A>) has been updated
        for this release and provides links to data downloads, as well
        as tutorials and examples of data use. SDSS-IV is planning to
        continue to collect astronomical data until 2020 and will be
        followed by SDSS-V.}",
          doi = {10.3847/1538-4365/aa9e8a},
archivePrefix = {arXiv},
       eprint = {1707.09322},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJS..235...42A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...857...55H,
       author = {{Hajdu}, Gergely and {D{\'e}k{\'a}ny}, Istv{\'a}n and
         {Catelan}, M{\'a}rcio and {Grebel}, Eva K. and {Jurcsik}, Johanna},
        title = "{A Data-driven Study of RR Lyrae Near-IR Light Curves: Principal Component Analysis, Robust Fits, and Metallicity Estimates}",
      journal = {\apj},
     keywords = {methods: data analysis, methods: observational, methods: statistical, stars: variables: RR Lyrae, techniques: photometric, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Apr",
       volume = {857},
       number = {1},
          eid = {55},
        pages = {55},
     abstract = "{RR Lyrae variables are widely used tracers of Galactic halo structure
        and kinematics, but they can also serve to constrain the
        distribution of the old stellar population in the Galactic
        bulge. With the aim of improving their near-infrared photometric
        characterization, we investigate their near-infrared light
        curves, as well as the empirical relationships between their
        light curve and metallicities using machine learning methods. We
        introduce a new, robust method for the estimation of the light-
        curve shapes, hence the average magnitudes of RR Lyrae variables
        in the K $_{S}$ band, by utilizing the first few principal
        components (PCs) as basis vectors, obtained from the PC analysis
        of a training set of light curves. Furthermore, we use the
        amplitudes of these PCs to predict the light-curve shape of each
        star in the J-band, allowing us to precisely determine their
        average magnitudes (hence colors), even in cases where only one
        J measurement is available. Finally, we demonstrate that the K
        $_{S}$-band light-curve parameters of RR Lyrae variables,
        together with the period, allow the estimation of the
        metallicity of individual stars with an accuracy of ̃0.2-0.25
        dex, providing valuable chemical information about old stellar
        populations bearing RR Lyrae variables. The methods presented
        here can be straightforwardly adopted for other classes of
        variable stars, bands, or for the estimation of other physical
        quantities.}",
          doi = {10.3847/1538-4357/aab4fd},
archivePrefix = {arXiv},
       eprint = {1804.01456},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...857...55H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...857...54D,
       author = {{D{\'e}k{\'a}ny}, Istv{\'a}n and {Hajdu}, Gergely and {Grebel}, Eva K. and
         {Catelan}, M{\'a}rcio and {Elorrieta}, Felipe and
         {Eyheramendy}, Susana and {Majaess}, Daniel and
         {Jord{\'a}n}, Andr{\'e}s},
        title = "{A Near-infrared RR Lyrae Census along the Southern Galactic Plane: The Milky Way{\textquoteright}s Stellar Fossil Brought to Light}",
      journal = {\apj},
     keywords = {catalogs, Galaxy: abundances, Galaxy: disk, stars: variables: RR Lyrae, surveys, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Apr",
       volume = {857},
       number = {1},
          eid = {54},
        pages = {54},
     abstract = "{RR Lyrae stars (RRLs) are tracers of the Milky Way{\textquoteright}s
        fossil record, holding valuable information on its formation and
        early evolution. Owing to the high interstellar extinction
        endemic to the Galactic plane, distant RRLs lying at low
        Galactic latitudes have been elusive. We attained a census of
        1892 high-confidence RRLs by exploiting the near-infrared
        photometric database of the VVV survey{\textquoteright}s disk
        footprint spanning ̃70{\textdegree} of Galactic longitude, using
        a machine-learned classifier. Novel data-driven methods were
        employed to accurately characterize their spatial distribution
        using sparsely sampled multi-band photometry. The RRL
        metallicity distribution function (MDF) was derived from their K
        $_{ s }$-band light-curve parameters using machine-learning
        methods. The MDF shows remarkable structural similarities to
        both the spectroscopic MDF of red clump giants and the MDF of
        bulge RRLs. We model the MDF with a multi-component density
        distribution and find that the number density of stars
        associated with the different model components systematically
        changes with both the Galactocentric radius and vertical
        distance from the Galactic plane, equivalent to weak metallicity
        gradients. Based on the consistency with results from the ARGOS
        survey, three MDF modes are attributed to the old disk
        populations, while the most metal-poor RRLs are probably halo
        interlopers. We propose that the dominant [Fe/H] component with
        a mean of -1 dex might correspond to the outskirts of an ancient
        Galactic spheroid or classical bulge component residing in the
        central Milky Way. The physical origins of the RRLs in this
        study need to be verified by kinematical information.}",
          doi = {10.3847/1538-4357/aab4fa},
archivePrefix = {arXiv},
       eprint = {1804.01457},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...857...54D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&C....23..141S,
       author = {{Saha}, S. and {Basak}, S. and {Safonova}, M. and {Bora}, K. and
         {Agrawal}, S. and {Sarkar}, P. and {Murthy}, J.},
        title = "{Theoretical validation of potential habitability via analytical and boosted tree methods: An optimistic study on recently discovered exoplanets}",
      journal = {Astronomy and Computing},
     keywords = {Astrophysics - Earth and Planetary Astrophysics},
         year = "2018",
        month = "Apr",
       volume = {23},
          eid = {141},
        pages = {141},
     abstract = "{Seven Earth-sized planets, known as the TRAPPIST-1 system, was
        discovered with great fanfare in the last week of February 2017.
        Three of these planets are in the habitable zone of their star,
        making them potentially habitable planets (PHPs) a mere 40 light
        years away. The discovery of the closest potentially habitable
        planet to us just a year before - Proxima b and a realization
        that Earth-type planets in circumstellar habitable zones are a
        common occurrence provides the impetus to the existing pursuit
        for life outside the Solar System. The search for life has two
        goals essentially: looking for planets with Earth-like
        conditions (Earth similarity) and looking for the possibility of
        life in some form (habitability). An index was recently
        developed, the Cobb-Douglas Habitability Score (CDHS), based on
        Cobb-Douglas habitability production function (CD-HPF), which
        computes the habitability score by using measured and estimated
        planetary parameters. As an initial set, radius, density, escape
        velocity and surface temperature of a planet were used. The
        proposed metric, with exponents accounting for metric
        elasticity, is endowed with analytical properties that ensure
        global optima and can be scaled to accommodate a finite number
        of input parameters. We show here that the model is elastic, and
        the conditions on elasticity to ensure global maxima can scale
        as the number of predictor parameters increase. K-NN (K-Nearest
        Neighbor) classification algorithm, embellished with
        probabilistic herding and thresholding restriction, utilizes
        CDHS scores and labels exoplanets into appropriate classes via
        feature-learning methods yielding granular clusters of
        habitability. The algorithm works on top of a decision-
        theoretical model using the power of convex optimization and
        machine learning. The goal is to characterize the recently
        discovered exoplanets into an ``Earth League'' and several other
        classes based on their CDHS values. A second approach, based on
        a novel feature-learning and tree-building method classifies the
        same planets without computing the CDHS of the planets and
        produces a similar outcome. For this, we use XGBoosted trees.
        The convergence of the outcome of the two different approaches
        indicates the strength of the proposed solution scheme and the
        likelihood of the potential habitability of the recently
        announced discoveries.}",
          doi = {10.1016/j.ascom.2018.03.003},
archivePrefix = {arXiv},
       eprint = {1712.01040},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&C....23..141S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&C....23...72N,
       author = {{Nguyen}, T. and {Pankratius}, V. and {Eckman}, L. and {Seager}, S.},
        title = "{Computer-aided discovery of debris disk candidates: A case study using the Wide-Field Infrared Survey Explorer (WISE) catalog}",
      journal = {Astronomy and Computing},
     keywords = {Debris disk, WISE, Machine learning, Classification},
         year = "2018",
        month = "Apr",
       volume = {23},
          eid = {72},
        pages = {72},
     abstract = "{Debris disks around stars other than the Sun have received significant
        attention in studies of exoplanets, specifically exoplanetary
        system formation. Since debris disks are major sources of
        infrared emissions, infrared survey data such as the Wide-Field
        Infrared Survey (WISE) catalog potentially harbors numerous
        debris disk candidates. However, it is currently challenging to
        perform disk candidate searches for over 747 million sources in
        the WISE catalog due to the high probability of false positives
        caused by interstellar matter, galaxies, and other background
        artifacts. Crowdsourcing techniques have thus started to harness
        citizen scientists for debris disk identification since humans
        can be easily trained to distinguish between desired artifacts
        and irrelevant noises. With a limited number of citizen
        scientists, however, increasing data volumes from large surveys
        will inevitably lead to analysis bottlenecks. To overcome this
        scalability problem and push the current limits of automated
        debris disk candidate identification, we present a novel
        approach that uses citizen science results as a seed to train
        machine learning based classification. In this paper, we detail
        a case study with a computer-aided discovery pipeline
        demonstrating such feasibility based on WISE catalog data and
        NASA's Disk Detective project. Our approach of debris disk
        candidates classification was shown to be robust under a wide
        range of image quality and features. Our hybrid approach of
        citizen science with algorithmic scalability can facilitate big
        data processing for future detections as envisioned in future
        missions such as the Transiting Exoplanet Survey Satellite
        (TESS) and the Wide-Field Infrared Survey Telescope (WFIRST).}",
          doi = {10.1016/j.ascom.2018.02.004},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&C....23...72N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&C....23...15B,
       author = {{Bethapudi}, S. and {Desai}, S.},
        title = "{Separation of pulsar signals from noise using supervised machine learning algorithms}",
      journal = {Astronomy and Computing},
     keywords = {Methods, Data analysis stars, Neutron, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2018",
        month = "Apr",
       volume = {23},
          eid = {15},
        pages = {15},
     abstract = "{We evaluate the performance of four different machine learning (ML)
        algorithms: an Artificial Neural Network Multi-Layer Perceptron
        (ANN MLP), Adaboost, Gradient Boosting Classifier (GBC), and
        XGBoost, for the separation of pulsars from radio frequency
        interference (RFI) and other sources of noise, using a dataset
        obtained from the post-processing of a pulsar search pipeline.
        This dataset was previously used for the cross-validation of the
        SPINN-based machine learning engine, obtained from the
        reprocessing of the HTRU-S survey data (Morello et al., 2014).
        We have used the Synthetic Minority Over-sampling Technique
        (SMOTE) to deal with high-class imbalance in the dataset. We
        report a variety of quality scores from all four of these
        algorithms on both the non-SMOTE and SMOTE datasets. For all the
        above ML methods, we report high accuracy and G-mean for both
        the non-SMOTE and SMOTE cases. We study the feature importances
        using Adaboost, GBC, and XGBoost and also from the minimum
        Redundancy Maximum Relevance approach to report algorithm-
        agnostic feature ranking. From these methods, we find that the
        signal to noise of the folded profile to be the best feature. We
        find that all the ML algorithms report FPRs about an order of
        magnitude lower than the corresponding FPRs obtained in Morello
        et al. (2014), for the same recall value.}",
          doi = {10.1016/j.ascom.2018.02.002},
archivePrefix = {arXiv},
       eprint = {1704.04659},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&C....23...15B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&C....23....1M,
       author = {{Mahmoud}, E. and {Shoukry}, A. and {Takey}, A.},
        title = "{Clustering by reordering of similarity and Laplacian matrices: Application to galaxy clusters}",
      journal = {Astronomy and Computing},
     keywords = {Clustering algorithms, Block diagonal form of a matrix, Dulmage-Mendelsohn decomposition, Cuthill McKee algorithm, Gap statistics, Galaxy clusters},
         year = "2018",
        month = "Apr",
       volume = {23},
          eid = {1},
        pages = {1},
     abstract = "{Similarity metrics, kernels and similarity-based algorithms have gained
        much attention due to their increasing applications in
        information retrieval, data mining, pattern recognition and
        machine learning. Similarity Graphs are often adopted as the
        underlying representation of similarity matrices and are at the
        origin of known clustering algorithms such as spectral
        clustering. Similarity matrices offer the advantage of working
        in object-object (two-dimensional) space where visualization of
        clusters similarities is available instead of object-features
        (multi-dimensional) space. In this paper, sparse
        {\ensuremath{\in}}-similarity graphs are constructed and
        decomposed into strong components using appropriate methods such
        as Dulmage-Mendelsohn permutation (DMperm) and/or Reverse
        Cuthill-McKee (RCM) algorithms. The obtained strong components
        correspond to groups (clusters) in the input (feature) space.
        Parameter {\ensuremath{\in}}$_{i}$ is estimated locally, at each
        data point i from a corresponding narrow range of the number of
        nearest neighbors. Although more advanced clustering techniques
        are available, our method has the advantages of simplicity,
        better complexity and direct visualization of the clusters
        similarities in a two-dimensional space. Also, no prior
        information about the number of clusters is needed. We conducted
        our experiments on two and three dimensional, low and high-sized
        synthetic datasets as well as on an astronomical real-dataset.
        The results are verified graphically and analyzed using gap
        statistics over a range of neighbors to verify the robustness of
        the algorithm and the stability of the results. Combining the
        proposed algorithm with gap statistics provides a promising tool
        for solving clustering problems. An astronomical application is
        conducted for confirming the existence of 45 galaxy clusters
        around the X-ray positions of galaxy clusters in the redshift
        range [0.1..0.8]. We re-estimate the photometric redshifts of
        the identified galaxy clusters and obtain acceptable values
        compared to published spectroscopic redshifts with a 0.029
        standard deviation of their differences.}",
          doi = {10.1016/j.ascom.2018.02.001},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&C....23....1M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PhRvD..97f3001G,
       author = {{Gonz{\'a}lez}, J.~A. and {Guzm{\'a}n}, F.~S.},
        title = "{Characterizing the velocity of a wandering black hole and properties of the surrounding medium using convolutional neural networks}",
      journal = {\prd},
     keywords = {Astrophysics - High Energy Astrophysical Phenomena},
         year = "2018",
        month = "Mar",
       volume = {97},
       number = {6},
          eid = {063001},
        pages = {063001},
     abstract = "{We present a method for estimating the velocity of a wandering black
        hole and the equation of state for the gas around it based on a
        catalog of numerical simulations. The method uses machine-
        learning methods based on convolutional neural networks applied
        to the classification of images resulting from numerical
        simulations. Specifically we focus on the supersonic velocity
        regime and choose the direction of the black hole to be parallel
        to its spin. We build a catalog of 900 simulations by
        numerically solving Euler's equations onto the fixed space-time
        background of a black hole, for two parameters: the adiabatic
        index {\ensuremath{\Gamma}} with values in the range [1.1, 5 /3
        ], and the asymptotic relative velocity of the black hole with
        respect to the surroundings v$_{{\ensuremath{\infty}}}$, with
        values within [0.2 ,0.8 ]c . For each simulation we produce a 2D
        image of the gas density once the process of accretion has
        approached a stationary regime. The results obtained show that
        the implemented convolutional neural networks are able to
        correctly classify the adiabatic index 87.78\% of the time
        within an uncertainty of {\ensuremath{\pm}}0.0284 , while the
        prediction of the velocity is correct 96.67\% of the time within
        an uncertainty of {\ensuremath{\pm}}0.03 c . We expect that this
        combination of a massive number of numerical simulations and
        machine-learning methods will help us analyze more complicated
        scenarios related to future high-resolution observations of
        black holes, like those from the Event Horizon Telescope.}",
          doi = {10.1103/PhysRevD.97.063001},
archivePrefix = {arXiv},
       eprint = {1803.06060},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhRvD..97f3001G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PhLB..778...64G,
       author = {{George}, Daniel and {Huerta}, E.~A.},
        title = "{Deep Learning for real-time gravitational wave detection and parameter estimation: Results with Advanced LIGO data}",
      journal = {Physics Letters B},
     keywords = {Deep Learning, Convolutional neural networks, Gravitational waves, LIGO, Time-series signal processing, Classification and regression, General Relativity and Quantum Cosmology, Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
         year = "2018",
        month = "Mar",
       volume = {778},
        pages = {64-70},
     abstract = "{The recent Nobel-prize-winning detections of gravitational waves from
        merging black holes and the subsequent detection of the
        collision of two neutron stars in coincidence with
        electromagnetic observations have inaugurated a new era of
        multimessenger astrophysics. To enhance the scope of this
        emergent field of science, we pioneered the use of deep learning
        with convolutional neural networks, that take time-series
        inputs, for rapid detection and characterization of
        gravitational wave signals. This approach, Deep Filtering, was
        initially demonstrated using simulated LIGO noise. In this
        article, we present the extension of Deep Filtering using real
        data from LIGO, for both detection and parameter estimation of
        gravitational waves from binary black hole mergers using
        continuous data streams from multiple LIGO detectors. We
        demonstrate for the first time that machine learning can detect
        and estimate the true parameters of real events observed by
        LIGO. Our results show that Deep Filtering achieves similar
        sensitivities and lower errors compared to matched-filtering
        while being far more computationally efficient and more
        resilient to glitches, allowing real-time processing of weak
        time-series signals in non-stationary non-Gaussian noise with
        minimal resources, and also enables the detection of new classes
        of gravitational wave sources that may go unnoticed with
        existing detection algorithms. This unified framework for data
        analysis is ideally suited to enable coincident detection
        campaigns of gravitational waves and their multimessenger
        counterparts in real-time.}",
          doi = {10.1016/j.physletb.2017.12.053},
archivePrefix = {arXiv},
       eprint = {1711.03121},
 primaryClass = {gr-qc},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhLB..778...64G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.474.4571T,
       author = {{Tan}, C.~M. and {Lyon}, R.~J. and {Stappers}, B.~W. and {Cooper}, S. and
         {Hessels}, J.~W.~T. and {Kondratiev}, V.~I. and {Michilli}, D. and
         {Sanidas}, S.},
        title = "{Ensemble candidate classification for the LOTAAS pulsar survey}",
      journal = {\mnras},
     keywords = {pulsars: general, methods: data analysis, methods: statistical},
         year = "2018",
        month = "Mar",
       volume = {474},
       number = {4},
        pages = {4571-4583},
     abstract = "{One of the biggest challenges arising from modern large-scale pulsar
        surveys is the number of candidates generated. Here, we
        implemented several improvements to the machine learning (ML)
        classifier previously used by the LOFAR Tied-Array All-Sky
        Survey (LOTAAS) to look for new pulsars via filtering the
        candidates obtained during periodicity searches. To assist the
        ML algorithm, we have introduced new features which capture the
        frequency and time evolution of the signal and improved the
        signal-to-noise calculation accounting for broad profiles. We
        enhanced the ML classifier by including a third class
        characterizing RFI instances, allowing candidates arising from
        RFI to be isolated, reducing the false positive return rate. We
        also introduced a new training data set used by the ML algorithm
        that includes a large sample of pulsars misclassified by the
        previous classifier. Lastly, we developed an ensemble classifier
        comprised of five different Decision Trees. Taken together these
        updates improve the pulsar recall rate by 2.5 per cent, while
        also improving the ability to identify pulsars with wide pulse
        profiles, often misclassified by the previous classifier. The
        new ensemble classifier is also able to reduce the percentage of
        false positive candidates identified from each LOTAAS pointing
        from 2.5 per cent (̃500 candidates) to 1.1 per cent (̃220
        candidates).}",
          doi = {10.1093/mnras/stx3047},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.474.4571T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.474.3259V,
       author = {{Valenzuela}, Lucas and {Pichara}, Karim},
        title = "{Unsupervised classification of variable stars}",
      journal = {\mnras},
     keywords = {astronomical data bases: miscellaneous, Surveys, stars: general, stars: statistics, stars: variables: general, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Mar",
       volume = {474},
       number = {3},
        pages = {3259-3272},
     abstract = "{During the past 10 years, a considerable amount of effort has been made
        to develop algorithms for automatic classification of variable
        stars. That has been primarily achieved by applying machine
        learning methods to photometric data sets where objects are
        represented as light curves. Classifiers require training sets
        to learn the underlying patterns that allow the separation among
        classes. Unfortunately, building training sets is an expensive
        process that demands a lot of human efforts. Every time data
        come from new surveys; the only available training instances are
        the ones that have a cross-match with previously labelled
        objects, consequently generating insufficient training sets
        compared with the large amounts of unlabelled sources. In this
        work, we present an algorithm that performs unsupervised
        classification of variable stars, relying only on the similarity
        among light curves. We tackle the unsupervised classification
        problem by proposing an untraditional approach. Instead of
        trying to match classes of stars with clusters found by a
        clustering algorithm, we propose a query-based method where
        astronomers can find groups of variable stars ranked by
        similarity. We also develop a fast similarity function specific
        for light curves, based on a novel data structure that allows
        scaling the search over the entire data set of unlabelled
        objects. Experiments show that our unsupervised model achieves
        high accuracy in the classification of different types of
        variable stars and that the proposed algorithm scales up to
        massive amounts of light curves.}",
          doi = {10.1093/mnras/stx2913},
archivePrefix = {arXiv},
       eprint = {1801.09723},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.474.3259V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018JCAP...03..026B,
       author = {{Bertone}, Gianfranco and {Bozorgnia}, Nassim and {Kim}, Jong Soo and
         {Liem}, Sebastian and {McCabe}, Christopher and {Otten}, Sydney and
         {Ruiz de Austri}, Roberto},
        title = "{Identifying WIMP dark matter from particle and astroparticle data}",
      journal = {Journal of Cosmology and Astro-Particle Physics},
     keywords = {High Energy Physics - Phenomenology, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2018",
        month = "Mar",
       volume = {2018},
       number = {3},
          eid = {026},
        pages = {026},
     abstract = "{One of the most promising strategies to identify the nature of dark
        matter consists in the search for new particles at accelerators
        and with so-called direct detection experiments. Working within
        the framework of simplified models, and making use of machine
        learning tools to speed up statistical inference, we address the
        question of what we can learn about dark matter from a detection
        at the LHC and a forthcoming direct detection experiment. We
        show that with a combination of accelerator and direct detection
        data, it is possible to identify newly discovered particles as
        dark matter, by reconstructing their relic density assuming they
        are weakly interacting massive particles (WIMPs) thermally
        produced in the early Universe, and demonstrating that it is
        consistent with the measured dark matter abundance. An
        inconsistency between these two quantities would instead point
        either towards additional physics in the dark sector, or towards
        a non-standard cosmology, with a thermal history substantially
        different from that of the standard cosmological model.}",
          doi = {10.1088/1475-7516/2018/03/026},
archivePrefix = {arXiv},
       eprint = {1712.04793},
 primaryClass = {hep-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018JCAP...03..026B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJS..235...22M,
       author = {{Mukund}, Nikhil and {Thakur}, Saurabh and {Abraham}, Sheelu and
         {Aniyan}, A.~K. and {Mitra}, Sanjit and {Sajeeth Philip}, Ninan and
         {Vaghmare}, Kaustubh and {Acharjya}, D.~P.},
        title = "{An Information Retrieval and Recommendation System for Astronomical Observatories}",
      journal = {\apjs},
     keywords = {gravitational waves, instrumentation: interferometers, methods: data analysis, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Mar",
       volume = {235},
       number = {1},
          eid = {22},
        pages = {22},
     abstract = "{We present a machine-learning-based information retrieval system for
        astronomical observatories that tries to address user-defined
        queries related to an instrument. In the modern instrumentation
        scenario where heterogeneous systems and talents are
        simultaneously at work, the ability to supply people with the
        right information helps speed up the tasks for detector
        operation, maintenance, and upgradation. The proposed method
        analyzes existing documented efforts at the site to
        intelligently group related information to a query and to
        present it online to the user. The user in response can probe
        the suggested content and explore previously developed solutions
        or probable ways to address the present situation optimally. We
        demonstrate natural language-processing-backed knowledge
        rediscovery by making use of the open source logbook data from
        the Laser Interferometric Gravitational Observatory (LIGO). We
        implement and test a web application that incorporates the above
        idea for LIGO Livingston, LIGO Hanford, and Virgo observatories.}",
          doi = {10.3847/1538-4365/aaadb2},
archivePrefix = {arXiv},
       eprint = {1710.05350},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJS..235...22M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...856...68P,
       author = {{Pourrahmani}, Milad and {Nayyeri}, Hooshang and {Cooray}, Asantha},
        title = "{LensFlow: A Convolutional Neural Network in Search of Strong Gravitational Lenses}",
      journal = {\apj},
     keywords = {gravitational lensing: strong, methods: data analysis, techniques: image processing, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Mar",
       volume = {856},
       number = {1},
          eid = {68},
        pages = {68},
     abstract = "{In this work, we present our machine learning classification algorithm
        for identifying strong gravitational lenses from wide-area
        surveys using convolutional neural networks; LENSFLOW. We train
        and test the algorithm using a wide variety of strong
        gravitational lens configurations from simulations of lensing
        events. Images are processed through multiple convolutional
        layers that extract feature maps necessary to assign a lens
        probability to each image. LENSFLOW provides a ranking scheme
        for all sources that could be used to identify potential
        gravitational lens candidates by significantly reducing the
        number of images that have to be visually inspected. We apply
        our algorithm to the HST/ACS i-band observations of the COSMOS
        field and present our sample of identified lensing candidates.
        The developed machine learning algorithm is more computationally
        efficient and complimentary to classical lens identification
        algorithms and is ideal for discovering such events across wide
        areas from current and future surveys such as LSST and WFIRST.}",
          doi = {10.3847/1538-4357/aaae6a},
archivePrefix = {arXiv},
       eprint = {1705.05857},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...856...68P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...855..109L,
       author = {{Liu}, Jiajia and {Ye}, Yudong and {Shen}, Chenglong and {Wang}, Yuming and
         {Erd{\'e}lyi}, Robert},
        title = "{A New Tool for CME Arrival Time Prediction using Machine Learning Algorithms: CAT-PUMA}",
      journal = {\apj},
     keywords = {solar─terrestrial relations, Sun: coronal mass ejections: CMEs, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Mar",
       volume = {855},
       number = {2},
          eid = {109},
        pages = {109},
     abstract = "{Coronal mass ejections (CMEs) are arguably the most violent eruptions in
        the solar system. CMEs can cause severe disturbances in
        interplanetary space and can even affect human activities in
        many aspects, causing damage to infrastructure and loss of
        revenue. Fast and accurate prediction of CME arrival time is
        vital to minimize the disruption that CMEs may cause when
        interacting with geospace. In this paper, we propose a new
        approach for partial-/full halo CME Arrival Time Prediction
        Using Machine learning Algorithms (CAT-PUMA). Via detailed
        analysis of the CME features and solar-wind parameters, we build
        a prediction engine taking advantage of 182 previously observed
        geo-effective partial-/full halo CMEs and using algorithms of
        the Support Vector Machine. We demonstrate that CAT-PUMA is
        accurate and fast. In particular, predictions made after
        applying CAT-PUMA to a test set unknown to the engine show a
        mean absolute prediction error of ̃5.9 hr within the CME arrival
        time, with 54\% of the predictions having absolute errors less
        than 5.9 hr. Comparisons with other models reveal that CAT-PUMA
        has a more accurate prediction for 77\% of the events
        investigated that can be carried out very quickly, i.e., within
        minutes of providing the necessary input parameters of a CME. A
        practical guide containing the CAT-PUMA engine and the source
        code of two examples are available in the Appendix, allowing the
        community to perform their own applications for prediction using
        CAT-PUMA.}",
          doi = {10.3847/1538-4357/aaae69},
archivePrefix = {arXiv},
       eprint = {1802.02803},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...855..109L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018AJ....155..135P,
       author = {{Pe{\~n}a}, J. and {Fuentes}, C. and {F{\"o}rster}, F. and
         {Maureira}, J.~C. and {San Mart{\'\i}n}, J. and {Litt{\'\i}n}, J. and
         {Huijse}, P. and {Cabrera-Vives}, G. and {Est{\'e}vez}, P.~A. and
         {Galbany}, L. and {Gonz{\'a}lez-Gait{\'a}n}, S. and
         {Mart{\'\i}nez}, J. and {de Jaeger}, Th. and {Hamuy}, M.},
        title = "{Asteroids in the High Cadence Transient Survey}",
      journal = {\aj},
     keywords = {astrometry, minor planets, asteroids: general, surveys, Astrophysics - Earth and Planetary Astrophysics},
         year = "2018",
        month = "Mar",
       volume = {155},
       number = {3},
          eid = {135},
        pages = {135},
     abstract = "{We report on the serendipitous observations of solar system objects
        imaged during the High cadence Transient Survey 2014 observation
        campaign. Data from this high-cadence wide-field survey was
        originally analyzed for finding variable static sources using
        machine learning to select the most-likely candidates. In this
        work, we search for moving transients consistent with solar
        system objects and derive their orbital parameters. We use a
        simple, custom motion detection algorithm to link trajectories
        and assume Keplerian motion to derive the
        asteroid{\textquoteright}s orbital parameters. We use known
        asteroids from the Minor Planet Center database to assess the
        detection efficiency of the survey and our search algorithm.
        Trajectories have an average of nine detections spread over two
        days, and our fit yields typical errors of
        \{{\ensuremath{\sigma}} \}$_{a}$̃ 0.07 \{au\},
        {\ensuremath{\sigma}} $_{e}$ ̃ 0.07 and {\ensuremath{\sigma}}
        $_{ i }$ ̃ 0.{\textdegree}5 in semimajor axis, eccentricity, and
        inclination, respectively, for known asteroids in our sample. We
        extract 7700 orbits from our trajectories, identifying 19 near-
        Earth objects, 6687 asteroids, 14 Centaurs, and 15 trans-
        Neptunian objects. This highlights the complementarity of
        supernova wide-field surveys for solar system research and the
        significance of machine learning to clean data of false
        detections. It is a good example of the data-driven science that
        Large Synoptic Survey Telescope will deliver.}",
          doi = {10.3847/1538-3881/aaaaed},
archivePrefix = {arXiv},
       eprint = {1806.03352},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018AJ....155..135P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018AJ....155..108Z,
       author = {{Zhang}, Jingyi and {Zhang}, Yanxia and {Zhao}, Yongheng},
        title = "{Imbalanced Learning for RR Lyrae Stars Based on SDSS and GALEX Databases}",
      journal = {\aj},
     keywords = {astronomical databases: miscellaneous, methods: data analysis, methods: statistical, stars: general, stars: variables: RR Lyrae},
         year = "2018",
        month = "Mar",
       volume = {155},
       number = {3},
          eid = {108},
        pages = {108},
     abstract = "{We apply machine learning and Convex-Hull algorithms to separate RR
        Lyrae stars from other stars like main-sequence stars, white
        dwarf stars, carbon stars, CVs, and carbon-lines stars, based on
        the Sloan Digital Sky Survey and Galaxy Evolution Explorer
        (GALEX). In low-dimensional spaces, the Convex-Hull algorithm is
        applied to select RR Lyrae stars. Given different input patterns
        of (u - g, g - r), (g - r, r - i), (r - i, i - z), (u - g, g -
        r, r - i), (g - r, r - i, i - z), (u - g, g - r, i - z), and (u
        - g, r - i, i - z), different convex hulls can be built for RR
        Lyrae stars. Comparing the performance of different input
        patterns, u - g, g - r, i - z is the best input pattern. For
        this input pattern, the efficiency (the fraction of true RR
        Lyrae stars in the predicted RR Lyrae sample) is 4.2\% with a
        completeness (the fraction of recovered RR Lyrae stars in the
        whole RR Lyrae sample) of 100\%, increases to 9.9\% with 97\%
        completeness and to 16.1\% with 53\% completeness by removing
        some outliers. In high-dimensional spaces, machine learning
        algorithms are used with input patterns (u - g, g - r, r - i, i
        - z), (u - g, g - r, r - i, i - z, r), (NUV - u, u - g, g - r, r
        - i, i - z), and (NUV - u, u - g, g - r, r - i, i - z, r). RR
        Lyrae stars, which belong to the class of interest in our paper,
        are rare compared to other stars. For the highly imbalanced
        data, cost-sensitive Support Vector Machine, cost-sensitive
        Random Forest, and Fast Boxes is used. The results show that
        information from GALEX is helpful for identifying RR Lyrae
        stars, and Fast Boxes is the best performer on the skewed data
        in our case.}",
          doi = {10.3847/1538-3881/aaa5b1},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018AJ....155..108Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...611A..53J,
       author = {{Jamal}, S. and {Le Brun}, V. and {Le F{\`e}vre}, O. and {Vibert}, D. and
         {Schmitt}, A. and {Surace}, C. and {Copin}, Y. and {Garilli}, B. and
         {Moresco}, M. and {Pozzetti}, L.},
        title = "{Automated reliability assessment for spectroscopic redshift measurements}",
      journal = {\aap},
     keywords = {methods: data analysis, methods: statistical, techniques: spectroscopic, galaxies: distances and redshifts, surveys, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Mar",
       volume = {611},
          eid = {A53},
        pages = {A53},
     abstract = "{Context. Future large-scale surveys, such as the ESA Euclid mission,
        will produce a large set of galaxy redshifts
        ({\ensuremath{\geq}}{}10$^{6}$) that will require fully
        automated data-processing pipelines to analyze the data, extract
        crucial information and ensure that all requirements are met. A
        fundamental element in these pipelines is to associate to each
        galaxy redshift measurement a quality, or reliability, estimate.
        Aim. In this work, we introduce a new approach to automate the
        spectroscopic redshift reliability assessment based on machine
        learning (ML) and characteristics of the redshift probability
        density function. <BR /> Methods: We propose to rephrase the
        spectroscopic redshift estimation into a Bayesian framework, in
        order to incorporate all sources of information and
        uncertainties related to the redshift estimation process and
        produce a redshift posterior probability density function (PDF).
        To automate the assessment of a reliability flag, we exploit key
        features in the redshift posterior PDF and machine learning
        algorithms. <BR /> Results: As a working example, public data
        from the VIMOS VLT Deep Survey is exploited to present and test
        this new methodology. We first tried to reproduce the existing
        reliability flags using supervised classification in order to
        describe different types of redshift PDFs, but due to the
        subjective definition of these flags (classification accuracy
        58\%), we soon opted for a new homogeneous partitioning of the
        data into distinct clusters via unsupervised classification.
        After assessing the accuracy of the new clusters via
        resubstitution and test predictions (classification accuracy
        98\%), we projected unlabeled data from preliminary mock
        simulations for the Euclid space mission into this mapping to
        predict their redshift reliability labels. <BR /> Conclusions:
        Through the development of a methodology in which a system can
        build its own experience to assess the quality of a parameter,
        we are able to set a preliminary basis of an automated
        reliability assessment for spectroscopic redshift measurements.
        This newly-defined method is very promising for next-generation
        large spectroscopic surveys from the ground and in space, such
        as Euclid and WFIRST. A table of the reclassified VVDS redshifts
        and reliability is only available at the CDS via anonymous ftp
        to <A href=``http://cdsarc.u-strasbg.fr''>http://cdsarc.u-strasb
        g.fr</A> (<A
        href=``http://cdsarc.u-strasbg.fr''>http://130.79.128.5</A>) or
        via <A href=``http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/611/A53''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/611/A53</A>}",
          doi = {10.1051/0004-6361/201731305},
archivePrefix = {arXiv},
       eprint = {1706.01103},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...611A..53J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018SoPh..293...28F,
       author = {{Florios}, Kostas and {Kontogiannis}, Ioannis and {Park}, Sung-Hong and
         {Guerra}, Jordan A. and {Benvenuto}, Federico and
         {Bloomfield}, D. Shaun and {Georgoulis}, Manolis K.},
        title = "{Forecasting Solar Flares Using Magnetogram-based Predictors and Machine Learning}",
      journal = {\solphys},
     keywords = {Flares, forecasting, relation to magnetic field, Active regions, magnetic fields, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Feb",
       volume = {293},
       number = {2},
          eid = {28},
        pages = {28},
     abstract = "{We propose a forecasting approach for solar flares based on data from
        Solar Cycle 24, taken by the Helioseismic and Magnetic Imager
        (HMI) on board the Solar Dynamics Observatory (SDO) mission. In
        particular, we use the Space-weather HMI Active Region Patches
        (SHARP) product that facilitates cut-out magnetograms of solar
        active regions (AR) in the Sun in near-realtime (NRT), taken
        over a five-year interval (2012 - 2016). Our approach utilizes a
        set of thirteen predictors, which are not included in the SHARP
        metadata, extracted from line-of-sight and vector photospheric
        magnetograms. We exploit several machine learning (ML) and
        conventional statistics techniques to predict flares of peak
        magnitude \{\&gt;\} M1 and \{\&gt;\} C1 within a 24 h forecast
        window. The ML methods used are multi-layer perceptrons (MLP),
        support vector machines (SVM), and random forests (RF). We
        conclude that random forests could be the prediction technique
        of choice for our sample, with the second-best method being
        multi-layer perceptrons, subject to an entropy objective
        function. A Monte Carlo simulation showed that the best-
        performing method gives accuracy ACC=0.93(0.00), true skill
        statistic TSS=0.74(0.02), and Heidke skill score HSS=0.49(0.01)
        for \{\&gt;\} M1 flare prediction with probability threshold
        15\% and ACC=0.84(0.00), TSS=0.60(0.01), and HSS=0.59(0.01) for
        \{\&gt;\} C1 flare prediction with probability threshold 35\%.}",
          doi = {10.1007/s11207-018-1250-4},
archivePrefix = {arXiv},
       eprint = {1801.05744},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018SoPh..293...28F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PhRvD..97d4039G,
       author = {{George}, Daniel and {Huerta}, E.~A.},
        title = "{Deep neural networks to enable real-time multimessenger astrophysics}",
      journal = {\prd},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Astrophysics - High Energy Astrophysical Phenomena, Computer Science - Machine Learning, General Relativity and Quantum Cosmology},
         year = "2018",
        month = "Feb",
       volume = {97},
       number = {4},
          eid = {044039},
        pages = {044039},
     abstract = "{Gravitational wave astronomy has set in motion a scientific revolution.
        To further enhance the science reach of this emergent field of
        research, there is a pressing need to increase the depth and
        speed of the algorithms used to enable these ground-breaking
        discoveries. We introduce Deep Filtering{\textemdash}a new
        scalable machine learning method for end-to-end time-series
        signal processing. Deep Filtering is based on deep learning with
        two deep convolutional neural networks, which are designed for
        classification and regression, to detect gravitational wave
        signals in highly noisy time-series data streams and also
        estimate the parameters of their sources in real time.
        Acknowledging that some of the most sensitive algorithms for the
        detection of gravitational waves are based on implementations of
        matched filtering, and that a matched filter is the optimal
        linear filter in Gaussian noise, the application of Deep
        Filtering using whitened signals in Gaussian noise is
        investigated in this foundational article. The results indicate
        that Deep Filtering outperforms conventional machine learning
        techniques, achieves similar performance compared to matched
        filtering, while being several orders of magnitude faster,
        allowing real-time signal processing with minimal resources.
        Furthermore, we demonstrate that Deep Filtering can detect and
        characterize waveform signals emitted from new classes of
        eccentric or spin-precessing binary black holes, even when
        trained with data sets of only quasicircular binary black hole
        waveforms. The results presented in this article, and the recent
        use of deep neural networks for the identification of optical
        transients in telescope data, suggests that deep learning can
        facilitate real-time searches of gravitational wave sources and
        their electromagnetic and astroparticle counterparts. In the
        subsequent article, the framework introduced herein is directly
        applied to identify and characterize gravitational wave events
        in real LIGO data.}",
          doi = {10.1103/PhysRevD.97.044039},
archivePrefix = {arXiv},
       eprint = {1701.00008},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhRvD..97d4039G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PhRvB..97d5153Y,
       author = {{You}, Yi-Zhuang and {Yang}, Zhao and {Qi}, Xiao-Liang},
        title = "{Machine learning spatial geometry from entanglement features}",
      journal = {\prb},
     keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Strongly Correlated Electrons, General Relativity and Quantum Cosmology, High Energy Physics - Theory, Quantum Physics},
         year = "2018",
        month = "Feb",
       volume = {97},
       number = {4},
          eid = {045153},
        pages = {045153},
     abstract = "{Motivated by the close relations of the renormalization group with both
        the holography duality and the deep learning, we propose that
        the holographic geometry can emerge from deep learning the
        entanglement feature of a quantum many-body state. We develop a
        concrete algorithm, call the entanglement feature learning
        (EFL), based on the random tensor network (RTN) model for the
        tensor network holography. We show that each RTN can be mapped
        to a Boltzmann machine, trained by the entanglement entropies
        over all subregions of a given quantum many-body state. The goal
        is to construct the optimal RTN that best reproduce the
        entanglement feature. The RTN geometry can then be interpreted
        as the emergent holographic geometry. We demonstrate the EFL
        algorithm on a 1D free fermion system and observe the emergence
        of the hyperbolic geometry (AdS$_{3}$ spatial geometry) as we
        tune the fermion system towards the gapless critical point
        (CFT$_{2}$ point).}",
          doi = {10.1103/PhysRevB.97.045153},
archivePrefix = {arXiv},
       eprint = {1709.01223},
 primaryClass = {cond-mat.dis-nn},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhRvB..97d5153Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.474..478P,
       author = {{Pearson}, Kyle A. and {Palafox}, Leon and {Griffith}, Caitlin A.},
        title = "{Searching for exoplanets using artificial intelligence}",
      journal = {\mnras},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Earth and Planetary Astrophysics},
         year = "2018",
        month = "Feb",
       volume = {474},
       number = {1},
        pages = {478-491},
     abstract = "{In the last decade, over a million stars were monitored to detect
        transiting planets. Manual interpretation of potential exoplanet
        candidates is labor intensive and subject to human error, the
        results of which are difficult to quantify. Here we present a
        new method of detecting exoplanet candidates in large planetary
        search projects which, unlike current methods uses a neural
        network. Neural networks, also called ``deep learning'' or
        ``deep nets'' are designed to give a computer perception into a
        specific problem by training it to recognize patterns. Unlike
        past transit detection algorithms deep nets learn to recognize
        planet features instead of relying on hand-coded metrics that
        humans perceive as the most representative. Our convolutional
        neural network is capable of detecting Earth-like exoplanets in
        noisy time-series data with a greater accuracy than a least-
        squares method. Deep nets are highly generalizable allowing data
        to be evaluated from different time series after interpolation
        without compromising performance. As validated by our deep net
        analysis of Kepler light curves, we detect periodic transits
        consistent with the true period without any model fitting. Our
        study indicates that machine learning will facilitate the
        characterization of exoplanets in future analysis of large
        astronomy data sets.}",
          doi = {10.1093/mnras/stx2761},
archivePrefix = {arXiv},
       eprint = {1706.04319},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.474..478P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018JCAP...02..042K,
       author = {{Kacprzak}, T. and {Herbel}, J. and {Amara}, A. and
         {R{\'e}fr{\'e}gier}, A.},
        title = "{Accelerating Approximate Bayesian Computation with Quantile Regression: application to cosmological redshift distributions}",
      journal = {Journal of Cosmology and Astro-Particle Physics},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Statistics - Machine Learning},
         year = "2018",
        month = "Feb",
       volume = {2018},
       number = {2},
          eid = {042},
        pages = {042},
     abstract = "{Approximate Bayesian Computation (ABC) is a method to obtain a posterior
        distribution without a likelihood function, using simulations
        and a set of distance metrics. For that reason, it has recently
        been gaining popularity as an analysis tool in cosmology and
        astrophysics. Its drawback, however, is a slow convergence rate.
        We propose a novel method, which we call qABC, to accelerate ABC
        with Quantile Regression. In this method, we create a model of
        quantiles of distance measure as a function of input parameters.
        This model is trained on a small number of simulations and
        estimates which regions of the prior space are likely to be
        accepted into the posterior. Other regions are then immediately
        rejected. This procedure is then repeated as more simulations
        are available. We apply it to the practical problem of
        estimation of redshift distribution of cosmological samples,
        using forward modelling developed in previous work. The qABC
        method converges to nearly same posterior as the basic ABC. It
        uses, however, only 20\% of the number of simulations compared
        to basic ABC, achieving a fivefold gain in execution time for
        our problem. For other problems the acceleration rate may vary;
        it depends on how close the prior is to the final posterior. We
        discuss possible improvements and extensions to this method.}",
          doi = {10.1088/1475-7516/2018/02/042},
archivePrefix = {arXiv},
       eprint = {1707.07498},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018JCAP...02..042K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018GeoRL..45.1786P,
       author = {{Pawley}, Steven and {Schultz}, Ryan and {Playter}, Tiffany and
         {Corlett}, Hilary and {Shipman}, Todd and {Lyster}, Steven and
         {Hauck}, Tyler},
        title = "{The Geological Susceptibility of Induced Earthquakes in the Duvernay Play}",
      journal = {\grl},
     keywords = {induced seismicity, hydraulic fracturing, machine learning, earthquake susceptibility},
         year = "2018",
        month = "Feb",
       volume = {45},
       number = {4},
        pages = {1786-1793},
     abstract = "{Presently, consensus on the incorporation of induced earthquakes into
        seismic hazard has yet to be established. For example, the
        nonstationary, spatiotemporal nature of induced earthquakes is
        not well understood. Specific to the Western Canada Sedimentary
        Basin, geological bias in seismogenic activation potential has
        been suggested to control the spatial distribution of induced
        earthquakes regionally. In this paper, we train a machine
        learning algorithm to systemically evaluate tectonic,
        geomechanical, and hydrological proxies suspected to control
        induced seismicity. Feature importance suggests that proximity
        to basement, in situ stress, proximity to fossil reef margins,
        lithium concentration, and rate of natural seismicity are among
        the strongest model predictors. Our derived seismogenic
        potential map faithfully reproduces the current distribution of
        induced seismicity and is suggestive of other regions which may
        be prone to induced earthquakes. The refinement of induced
        seismicity geological susceptibility may become an important
        technique to identify significant underlying geological features
        and address induced seismic hazard forecasting issues.}",
          doi = {10.1002/2017GL076100},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018GeoRL..45.1786P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018GeoRL..45.1321R,
       author = {{Rouet-Leduc}, Bertrand and {Hulbert}, Claudia and {Bolton}, David C. and
         {Ren}, Christopher X. and {Riviere}, Jacques and {Marone}, Chris and
         {Guyer}, Robert A. and {Johnson}, Paul A.},
        title = "{Estimating Fault Friction From Seismic Signals in the Laboratory}",
      journal = {\grl},
     keywords = {machine learning, seismic signal identification, laboratory earthquake, fault friction, earthquake hazard, Physics - Geophysics},
         year = "2018",
        month = "Feb",
       volume = {45},
       number = {3},
        pages = {1321-1329},
     abstract = "{Nearly all aspects of earthquake rupture are controlled by the friction
        along the fault that progressively increases with tectonic
        forcing but in general cannot be directly measured. We show that
        fault friction can be determined at any time, from the
        continuous seismic signal. In a classic laboratory experiment of
        repeating earthquakes, we find that the seismic signal follows a
        specific pattern with respect to fault friction, allowing us to
        determine the fault's position within its failure cycle. Using
        machine learning, we show that instantaneous statistical
        characteristics of the seismic signal are a fingerprint of the
        fault zone shear stress and frictional state. Further analysis
        of this fingerprint leads to a simple equation of state
        quantitatively relating the seismic signal power and the
        friction on the fault. These results show that fault zone
        frictional characteristics and the state of stress in the
        surroundings of the fault can be inferred from seismic waves, at
        least in the laboratory.}",
          doi = {10.1002/2017GL076708},
archivePrefix = {arXiv},
       eprint = {1710.04172},
 primaryClass = {physics.geo-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018GeoRL..45.1321R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJS..234...31L,
       author = {{Li}, Yin-Bi and {Luo}, A. -Li and {Du}, Chang-De and {Zuo}, Fang and
         {Wang}, Meng-Xin and {Zhao}, Gang and {Jiang}, Bi-Wei and
         {Zhang}, Hua-Wei and {Liu}, Chao and {Qin}, Li and {Wang}, Rui and
         {Du}, Bing and {Guo}, Yan-Xin and {Wang}, Bo and {Han}, Zhan-Wen and
         {Xiang}, Mao-Sheng and {Huang}, Yang and {Chen}, Bing-Qiu and
         {Chen}, Jian-Jun and {Kong}, Xiao and {Hou}, Wen and {Song}, Yi-Han and
         {Wang}, You-Fen and {Wu}, Ke-Fei and {Zhang}, Jian-Nan and
         {Zhang}, Yong and {Wang}, Yue-Fei and {Cao}, Zi-Huang and
         {Hou}, Yong-Hui and {Zhao}, Yong-Heng},
        title = "{Carbon Stars Identified from LAMOST DR4 Using Machine Learning}",
      journal = {\apjs},
     keywords = {catalogs, methods: data analysis, methods: statistical, stars: carbon, surveys, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Feb",
       volume = {234},
       number = {2},
          eid = {31},
        pages = {31},
     abstract = "{In this work, we present a catalog of 2651 carbon stars from the fourth
        Data Release (DR4) of the Large Sky Area Multi-Object Fiber
        Spectroscopy Telescope (LAMOST). Using an efficient machine-
        learning algorithm, we find these stars from more than 7 million
        spectra. As a by-product, 17 carbon-enhanced metal-poor turnoff
        star candidates are also reported in this paper, and they are
        preliminarily identified by their atmospheric parameters. Except
        for 176 stars that could not be given spectral types, we
        classify the other 2475 carbon stars into five subtypes: 864
        C-H, 226 C-R, 400 C-J, 266 C-N, and 719 barium stars based on a
        series of spectral features. Furthermore, we divide the C-J
        stars into three subtypes, C-J(H), C-J(R), and C-J(N), and about
        90\% of them are cool N-type stars as expected from previous
        literature. Besides spectroscopic classification, we also match
        these carbon stars to multiple broadband photometries. Using
        ultraviolet photometry data, we find that 25 carbon stars have
        FUV detections and that they are likely to be in binary systems
        with compact white dwarf companions.}",
          doi = {10.3847/1538-4365/aaa415},
archivePrefix = {arXiv},
       eprint = {1712.07784},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJS..234...31L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...854...99W,
       author = {{Wu}, J. and {Clark}, C.~J. and {Pletsch}, H.~J. and {Guillemot}, L. and
         {Johnson}, T.~J. and {Torne}, P. and {Champion}, D.~J. and
         {Deneva}, J. and {Ray}, P.~S. and {Salvetti}, D. and {Kramer}, M. and
         {Aulbert}, C. and {Beer}, C. and {Bhattacharyya}, B. and {Bock}, O. and
         {Camilo}, F. and {Cognard}, I. and {Cu{\'e}llar}, A. and
         {Eggenstein}, H.~B. and {Fehrmann}, H. and {Ferrara}, E.~C. and
         {Kerr}, M. and {Machenschalk}, B. and {Ransom}, S.~M. and
         {Sanpa-Arsa}, S. and {Wood}, K.},
        title = "{The Einstein@Home Gamma-ray Pulsar Survey. II. Source Selection, Spectral Analysis, and Multiwavelength Follow-up}",
      journal = {\apj},
     keywords = {gamma rays: stars, pulsars: individual: PSR J0002+6216, PSR J0631+0646, PSR J1624─4041, PSR J2017+3625, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2018",
        month = "Feb",
       volume = {854},
       number = {2},
          eid = {99},
        pages = {99},
     abstract = "{We report on the analysis of 13 gamma-ray pulsars discovered in the
        Einstein@Home blind search survey using Fermi Large Area
        Telescope (LAT) Pass 8 data. The 13 new gamma-ray pulsars were
        discovered by searching 118 unassociated LAT sources from the
        third LAT source catalog (3FGL), selected using the Gaussian
        Mixture Model machine-learning algorithm on the basis of their
        gamma-ray emission properties being suggestive of pulsar
        magnetospheric emission. The new gamma-ray pulsars have pulse
        profiles and spectral properties similar to those of previously
        detected young gamma-ray pulsars. Follow-up radio observations
        have revealed faint radio pulsations from two of the newly
        discovered pulsars and enabled us to derive upper limits on the
        radio emission from the others, demonstrating that they are
        likely radio-quiet gamma-ray pulsars. We also present results
        from modeling the gamma-ray pulse profiles and radio profiles,
        if available, using different geometric emission models of
        pulsars. The high discovery rate of this survey, despite the
        increasing difficulty of blind pulsar searches in gamma rays,
        suggests that new systematic surveys such as presented in this
        article should be continued when new LAT source catalogs become
        available.}",
          doi = {10.3847/1538-4357/aaa411},
archivePrefix = {arXiv},
       eprint = {1712.05395},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...854...99W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018AJ....155...94S,
       author = {{Shallue}, Christopher J. and {Vanderburg}, Andrew},
        title = "{Identifying Exoplanets with Deep Learning: A Five-planet Resonant Chain around Kepler-80 and an Eighth Planet around Kepler-90}",
      journal = {\aj},
     keywords = {methods: data analysis, planets and satellites: detection, techniques: photometric, Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Feb",
       volume = {155},
       number = {2},
          eid = {94},
        pages = {94},
     abstract = "{NASA{\textquoteright}s Kepler Space Telescope was designed to determine
        the frequency of Earth-sized planets orbiting Sun-like stars,
        but these planets are on the very edge of the
        mission{\textquoteright}s detection sensitivity. Accurately
        determining the occurrence rate of these planets will require
        automatically and accurately assessing the likelihood that
        individual candidates are indeed planets, even at low signal-to-
        noise ratios. We present a method for classifying potential
        planet signals using deep learning, a class of machine learning
        algorithms that have recently become state-of-the-art in a wide
        variety of tasks. We train a deep convolutional neural network
        to predict whether a given signal is a transiting exoplanet or a
        false positive caused by astrophysical or instrumental
        phenomena. Our model is highly effective at ranking individual
        candidates by the likelihood that they are indeed planets:
        98.8\% of the time it ranks plausible planet signals higher than
        false-positive signals in our test set. We apply our model to a
        new set of candidate signals that we identified in a search of
        known Kepler multi-planet systems. We statistically validate two
        new planets that are identified with high confidence by our
        model. One of these planets is part of a five-planet resonant
        chain around Kepler-80, with an orbital period closely matching
        the prediction by three-body Laplace relations. The other planet
        orbits Kepler-90, a star that was previously known to host seven
        transiting planets. Our discovery of an eighth planet brings
        Kepler-90 into a tie with our Sun as the star known to host the
        most planets.}",
          doi = {10.3847/1538-3881/aa9e09},
archivePrefix = {arXiv},
       eprint = {1712.05044},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018AJ....155...94S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018A&A...610A..12B,
       author = {{Bron}, Emeric and {Daudon}, Chlo{\'e} and {Pety}, J{\'e}r{\^o}me and
         {Levrier}, Fran{\c{c}}ois and {Gerin}, Maryvonne and {Gratier}, Pierre and
         {Orkisz}, Jan H. and {Guzman}, Viviana and {Bardeau}, S{\'e}bastien and
         {Goicoechea}, Javier R. and {Liszt}, Harvey and {{\"O}berg}, Karin and
         {Peretto}, Nicolas and {Sievers}, Albrecht and {Tremblin}, Pascal},
        title = "{Clustering the Orion B giant molecular cloud based on its molecular emission}",
      journal = {\aap},
     keywords = {astrochemistry, ISM: molecules, ISM: clouds, ISM: structure, methods: statistical, ISM: individual objects: Orion B, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2018",
        month = "Feb",
       volume = {610},
          eid = {A12},
        pages = {A12},
     abstract = "{Context. Previous attempts at segmenting molecular line maps of
        molecular clouds have focused on using position-position-
        velocity data cubes of a single molecular line to separate the
        spatial components of the cloud. In contrast, wide field
        spectral imaging over a large spectral bandwidth in the (sub)mm
        domain now allows one to combine multiple molecular tracers to
        understand the different physical and chemical phases that
        constitute giant molecular clouds (GMCs). <BR /> Aims: We aim at
        using multiple tracers (sensitive to different physical
        processes and conditions) to segment a molecular cloud into
        physically/chemically similar regions (rather than spatially
        connected components), thus disentangling the different
        physical/chemical phases present in the cloud. <BR /> Methods:
        We use a machine learning clustering method, namely the
        Meanshift algorithm, to cluster pixels with similar molecular
        emission, ignoring spatial information. Clusters are defined
        around each maximum of the multidimensional probability density
        function (PDF) of the line integrated intensities. Simple
        radiative transfer models were used to interpret the
        astrophysical information uncovered by the clustering analysis.
        <BR /> Results: A clustering analysis based only on the J = 1-0
        lines of three isotopologues of CO proves sufficient to reveal
        distinct density/column density regimes (n$_{H}$ 100 cm$^{-3}$,
        500 cm$^{-3}$, and \&gt;1000 cm$^{-3}$), closely related to the
        usual definitions of diffuse, translucent and high-column-
        density regions. Adding two UV-sensitive tracers, the J = 1-0
        line of HCO$^{+}$ and the N = 1-0 line of CN, allows us to
        distinguish two clearly distinct chemical regimes,
        characteristic of UV-illuminated and UV-shielded gas. The UV-
        illuminated regime shows overbright HCO$^{+}$ and CN emission,
        which we relate to a photochemical enrichment effect. We also
        find a tail of high CN/HCO$^{+}$ intensity ratio in UV-
        illuminated regions. Finer distinctions in density classes
        (n$_{H}$ 7 {\texttimes} {}10$^{3}$ cm$^{-3}$, 4 {\texttimes}
        {}10$^{4}$ cm$^{-3}$) for the densest regions are also
        identified, likely related to the higher critical density of the
        CN and HCO$^{+}$ (1-0) lines. These distinctions are only
        possible because the high-density regions are spatially
        resolved. <BR /> Conclusions: Molecules are versatile tracers of
        GMCs because their line intensities bear the signature of the
        physics and chemistry at play in the gas. The association of
        simultaneous multi-line, wide-field mapping and powerful machine
        learning methods such as the Meanshift clustering algorithm
        reveals how to decode the complex information available in these
        molecular tracers. Data products associated with this paper are
        available at the CDS via anonymous ftp to <A href=``http://cdsar
        c.u-strasbg.fr''>http://cdsarc.u-strasbg.fr</A> (<A
        href=``http://130.79.128.5''>http://130.79.128.5</A>) or via <A
        href=``http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/610/A12''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/610/A12</A> and at <A href=``http://www.iram.fr/
        pety/ORION-B''>http://www.iram.fr/ pety/ORION-B</A>}",
          doi = {10.1051/0004-6361/201731833},
archivePrefix = {arXiv},
       eprint = {1710.07288},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018A&A...610A..12B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PhRvD..97b4031H,
       author = {{Huerta}, E.~A. and {Moore}, C.~J. and {Kumar}, Prayush and
         {George}, Daniel and {Chua}, Alvin J.~K. and {Haas}, Roland and
         {Wessel}, Erik and {Johnson}, Daniel and {Glennon}, Derek and
         {Rebei}, Adam and {Holgado}, A. Miguel and {Gair}, Jonathan R. and
         {Pfeiffer}, Harald P.},
        title = "{Eccentric, nonspinning, inspiral, Gaussian-process merger approximant for the detection and characterization of eccentric binary black hole mergers}",
      journal = {\prd},
     keywords = {General Relativity and Quantum Cosmology, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - High Energy Astrophysical Phenomena, Computer Science - Computational Engineering, Finance, and Science, J.2},
         year = "2018",
        month = "Jan",
       volume = {97},
       number = {2},
          eid = {024031},
        pages = {024031},
     abstract = "{We present ENIGMA, a time domain, inspiral-merger-ringdown waveform
        model that describes nonspinning binary black holes systems that
        evolve on moderately eccentric orbits. The inspiral evolution is
        described using a consistent combination of post-Newtonian
        theory, self-force and black hole perturbation theory. Assuming
        eccentric binaries that circularize prior to coalescence, we
        smoothly match the eccentric inspiral with a stand-alone,
        quasicircular merger, which is constructed using machine
        learning algorithms that are trained with quasicircular
        numerical relativity waveforms. We show that ENIGMA reproduces
        with excellent accuracy the dynamics of quasicircular compact
        binaries. We validate ENIGMA using a set of Einstein Toolkit
        eccentric numerical relativity waveforms, which describe
        eccentric binary black hole mergers with mass-ratios between 1
        {\ensuremath{\leq}}q {\ensuremath{\leq}}5.5 , and eccentricities
        e$_{0}${\ensuremath{\lesssim}}0.2 ten orbits before merger. We
        use this model to explore in detail the physics that can be
        extracted with moderately eccentric, nonspinning binary black
        hole mergers. In particular, we use ENIGMA to show that the
        gravitational wave transients GW150914, GW151226, GW170104,
        GW170814 and GW170608 can be effectively recovered with
        spinning, quasicircular templates if the eccentricity of these
        events at a gravitational wave frequency of 10 Hz satisfies
        e$_{0}${\ensuremath{\leq}}\{0.175 ,0.125 ,0.175 ,0.175 ,0.125
        \}, respectively. We show that if these systems have
        eccentricities e$_{0}$̃0.1 at a gravitational wave frequency of
        10 Hz, they can be misclassified as quasicircular binaries due
        to parameter space degeneracies between eccentricity and spin
        corrections. Using our catalog of eccentric numerical relativity
        simulations, we discuss the importance of including higher-order
        waveform multipoles in gravitational wave searches of eccentric
        binary black hole mergers.}",
          doi = {10.1103/PhysRevD.97.024031},
archivePrefix = {arXiv},
       eprint = {1711.06276},
 primaryClass = {gr-qc},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhRvD..97b4031H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2018PhDT.......151S,
       author = {{Schindler}, Jan-Torge},
        title = "{The Formation and Evolution of Supermassive Black Holes: The Extremely Luminous Quasar Survey}",
     keywords = {Astronomy;Astrophysics},
       school = {The University of Arizona},
         year = "2018",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhDT.......151S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2018PhDT.......117L,
       author = {{Lum}, Michael Gavin},
        title = "{A Differential Study of Nucleosynthesis in Open Star Clusters}",
     keywords = {Astrophysics;Astronomy;Artificial intelligence},
       school = {University of Hawai'i at Manoa},
         year = "2018",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhDT.......117L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2018PhDT........17N,
       author = {{Niederhausen}, Hans},
        title = "{Measurement of the High Energy Astrophysical Neutrino Flux Using Electron and Tau Neutrinos Observed in Four Years of IceCube Data}",
     keywords = {Physics;Astrophysics;High energy physics},
       school = {State University of New York at Stony Brook},
         year = "2018",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhDT........17N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2018PhDT........16S,
       author = {{Sullivan}, Christopher James},
        title = "{Constraining Nuclear Weak Interactions in Astrophysics and New Many-Core Algorithms for Neuroevolution}",
     keywords = {Physics;Artificial intelligence;Computer science;Astrophysics},
       school = {Michigan State University},
         year = "2018",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhDT........16S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2018PhDT........16B,
       author = {{Beck}, Melanie Renee},
        title = "{Integrating Human and Machine Intelligence in Galaxy Morphology Classification Tasks}",
     keywords = {Astrophysics;Artificial intelligence;Information science},
       school = {University of Minnesota},
         year = "2018",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhDT........16B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2018PhDT.........7T,
       author = {{Timlin}, John D., III},
        title = "{Clustering of High-Redshift Quasars}",
     keywords = {Physics;Astrophysics;Astronomy},
       school = {Drexel University},
         year = "2018",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PhDT.........7T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PASJ...70S..39L,
       author = {{Lin}, Hsing-Wen and {Chen}, Ying-Tung and {Wang}, Jen-Hung and
         {Wang}, Shiang-Yu and {Yoshida}, Fumi and {Ip}, Wing-Huen and
         {Miyazaki}, Satoshi and {Terai}, Tsuyoshi},
        title = "{Machine-learning-based real-bogus system for the HSC-SSP moving object detection pipeline}",
      journal = {\pasj},
     keywords = {Kuiper belt: general, methods: data analysis, surveys, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Earth and Planetary Astrophysics},
         year = "2018",
        month = "Jan",
       volume = {70},
          eid = {S39},
        pages = {S39},
     abstract = "{Machine-learning techniques are widely applied in many modern optical
        sky surveys, e.g., Pan-STARRS1, PTF/iPTF, and the Subaru/Hyper
        Suprime-Cam survey, to reduce human intervention in data
        verification. In this study, we have established a machine-
        learning-based real-bogus system to reject false detections in
        the Subaru/Hyper-Suprime-Cam Strategic Survey Program (HSC-SSP)
        source catalog. Therefore, the HSC-SSP moving object detection
        pipeline can operate more effectively due to the reduction of
        false positives. To train the real-bogus system, we use
        stationary sources as the real training set and ``flagged'' data
        as the bogus set. The training set contains 47 features, most of
        which are photometric measurements and shape moments generated
        from the HSC image reduction pipeline (hscPipe). Our system can
        reach a true positive rate (tpr) ̃96\% with a false positive
        rate (fpr) ̃1\% or tpr ̃99\% at fpr ̃5\%. Therefore, we conclude
        that stationary sources are decent real training samples, and
        using photometry measurements and shape moments can reject false
        positives effectively.}",
          doi = {10.1093/pasj/psx082},
archivePrefix = {arXiv},
       eprint = {1704.06413},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PASJ...70S..39L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PASJ...70S..38C,
       author = {{Chen}, Ying-Tung and {Lin}, Hsing-Wen and {Alexandersen}, Mike and
         {Lehner}, Matthew J. and {Wang}, Shiang-Yu and {Wang}, Jen-Hung and
         {Yoshida}, Fumi and {Komiyama}, Yutaka and {Miyazaki}, Satoshi},
        title = "{Searching for moving objects in HSC-SSP: Pipeline and preliminary results}",
      journal = {\pasj},
     keywords = {Kuiper belt: general, methods: data analysis, minor planets, asteroids: general, Astrophysics - Earth and Planetary Astrophysics},
         year = "2018",
        month = "Jan",
       volume = {70},
          eid = {S38},
        pages = {S38},
     abstract = "{The Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP) is currently
        the deepest wide-field survey in progress. The 8.2 m aperture of
        the Subaru telescope is very powerful in detecting faint/small
        moving objects, including near-Earth objects, asteroids,
        centaurs and Tran-Neptunian objects (TNOs). However, the cadence
        and dithering pattern of the HSC-SSP are not designed for
        detecting moving objects, making it difficult to do so
        systematically. In this paper, we introduce a new pipeline for
        detecting moving objects (specifically TNOs) in a non-dedicated
        survey. The HSC-SSP catalogs are sliced into HEALPix partitions.
        Then, the stationary detections and false positives are removed
        with a machine-learning algorithm to produce a list of moving
        object candidates. An orbit linking algorithm and visual
        inspections are executed to generate the final list of detected
        TNOs. The preliminary results of a search for TNOs using this
        new pipeline on data from the first HSC-SSP data release (2014
        March to 2015 November) present 231 TNO/Centaurs candidates. The
        bright candidates with H$_{r}$ \&lt; 7.7 and i \&gt; 5 show that
        the best-fitting slope of a single power law to absolute
        magnitude distribution is 0.77. The g - r color distribution of
        hot HSC-SSP TNOs indicates a bluer peak at g - r = 0.9, which is
        consistent with the bluer peak of the bimodal color distribution
        in literature.}",
          doi = {10.1093/pasj/psx145},
archivePrefix = {arXiv},
       eprint = {1705.01722},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PASJ...70S..38C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018PASJ...70S..37G,
       author = {{Goulding}, Andy D. and {Greene}, Jenny E. and {Bezanson}, Rachel and
         {Greco}, Johnny and {Johnson}, Sean and {Leauthaud}, Alexie and
         {Matsuoka}, Yoshiki and {Medezinski}, Elinor and
         {Price-Whelan}, Adrian M.},
        title = "{Galaxy interactions trigger rapid black hole growth: An unprecedented view from the Hyper Suprime-Cam survey}",
      journal = {\pasj},
     keywords = {galaxies: active, galaxies: evolution, galaxies: interacting, Astrophysics - Astrophysics of Galaxies, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2018",
        month = "Jan",
       volume = {70},
          eid = {S37},
        pages = {S37},
     abstract = "{Collisions and interactions between gas-rich galaxies are thought to be
        pivotal stages in their formation and evolution, causing the
        rapid production of new stars, and possibly serving as a
        mechanism for fueling supermassive black holes (BHs). Harnessing
        the exquisite spatial resolution (̃0\{\^''$_{.}$\}5) afforded by
        the first ̃170 deg$^{2}$ of the Hyper Suprime-Cam (HSC) survey,
        we present our new constraints on the importance of galaxy-
        galaxy major mergers (1 : 4) in growing BHs throughout the last
        ̃8 Gyr. Utilizing mid-infrared observations in the WISE all-sky
        survey, we robustly select active galactic nuclei (AGN) and
        mass-matched control galaxy samples, totaling ̃140000
        spectroscopically confirmed systems at i \&lt; 22 mag. We
        identify galaxy interaction signatures using a novel machine-
        learning random forest decision tree technique allowing us to
        select statistically significant samples of major mergers, minor
        mergers / irregular systems, and non-interacting galaxies. We
        use these samples to show that galaxies undergoing mergers are a
        factor of ̃2-7 more likely to contain luminous obscured AGN than
        non-interacting galaxies, and this is independent of both
        stellar mass and redshift to z \&lt; 0.9. Furthermore, based on
        our comparison of AGN fractions in mass-matched samples, we
        determine that the most luminous AGN population (L$_{AGN}$
        {\ensuremath{\gtrsim}} {}10$^{45}$ erg s$^{-1}$) systematically
        reside in merging systems over non-interacting galaxies. Our
        findings show that galaxy-galaxy interactions do, on average,
        trigger luminous AGN activity substantially more often than in
        secularly evolving non-interacting galaxies, and we further
        suggest that the BH growth rate may be closely tied to the
        dynamical time of the merger system.}",
          doi = {10.1093/pasj/psx135},
archivePrefix = {arXiv},
       eprint = {1706.07436},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018PASJ...70S..37G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.473.3895L,
       author = {{Lanusse}, Fran{\c{c}}ois and {Ma}, Quanbin and {Li}, Nan and
         {Collett}, Thomas E. and {Li}, Chun-Liang and {Ravanbakhsh}, Siamak and
         {Mandelbaum}, Rachel and {P{\'o}czos}, Barnab{\'a}s},
        title = "{CMU DeepLens: deep learning for automatic image-based galaxy-galaxy strong lens finding}",
      journal = {\mnras},
     keywords = {gravitational lensing: strong, methods: statistical, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Jan",
       volume = {473},
       number = {3},
        pages = {3895-3906},
     abstract = "{Galaxy-scale strong gravitational lensing can not only provide a
        valuable probe of the dark matter distribution of massive
        galaxies, but also provide valuable cosmological constraints,
        either by studying the population of strong lenses or by
        measuring time delays in lensed quasars. Due to the rarity of
        galaxy-scale strongly lensed systems, fast and reliable
        automated lens finding methods will be essential in the era of
        large surveys such as Large Synoptic Survey Telescope, Euclid
        and Wide-Field Infrared Survey Telescope. To tackle this
        challenge, we introduce CMU DeepLens, a new fully automated
        galaxy-galaxy lens finding method based on deep learning. This
        supervised machine learning approach does not require any tuning
        after the training step which only requires realistic image
        simulations of strongly lensed systems. We train and validate
        our model on a set of 20 000 LSST-like mock observations
        including a range of lensed systems of various sizes and signal-
        to-noise ratios (S/N). We find on our simulated data set that
        for a rejection rate of non-lenses of 99 per cent, a
        completeness of 90 per cent can be achieved for lenses with
        Einstein radii larger than 1.4 arcsec and S/N larger than 20 on
        individual g-band LSST exposures. Finally, we emphasize the
        importance of realistically complex simulations for training
        such machine learning methods by demonstrating that the
        performance of models of significantly different complexities
        cannot be distinguished on simpler simulations. We make our code
        publicly available at
        https://github.com/McWilliamsCenter/CMUDeepLens.}",
          doi = {10.1093/mnras/stx1665},
archivePrefix = {arXiv},
       eprint = {1703.02642},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.473.3895L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.473.2565M,
       author = {{Morello}, Giuseppe and {Morris}, P.~W. and {Van Dyk}, S.~D. and
         {Marston}, A.~P. and {Mauerhan}, J.~C.},
        title = "{Applications of machine-learning algorithms for infrared colour selection of Galactic Wolf-Rayet stars}",
      journal = {\mnras},
     keywords = {methods: observational, methods: statistical, stars: evolution, stars: massive, stars: Wolf-Rayet, infrared: stars, Astrophysics - Solar and Stellar Astrophysics},
         year = "2018",
        month = "Jan",
       volume = {473},
       number = {2},
        pages = {2565-2574},
     abstract = "{We have investigated and applied machine-learning algorithms for
        infrared colour selection of Galactic Wolf-Rayet (WR)
        candidates. Objects taken from the Spitzer Galactic Legacy
        Infrared Midplane Survey Extraordinaire (GLIMPSE) catalogue of
        the infrared objects in the Galactic plane can be classified
        into different stellar populations based on the colours inferred
        from their broad-band photometric magnitudes [J, H and K$_{s}$
        from 2 Micron All Sky Survey (2MASS), and the four Spitzer/IRAC
        bands]. The algorithms tested in this pilot study are variants
        of the k-nearest neighbours approach, which is ideal for
        exploratory studies of classification problems where
        interrelations between variables and classes are complicated.
        The aims of this study are (1) to provide an automated tool to
        select reliable WR candidates and potentially other classes of
        objects, (2) to measure the efficiency of infrared colour
        selection at performing these tasks and (3) to lay the
        groundwork for statistically inferring the total number of WR
        stars in our Galaxy. We report the performance results obtained
        over a set of known objects and selected candidates for which we
        have carried out follow-up spectroscopic observations, and
        confirm the discovery of four new WR stars.}",
          doi = {10.1093/mnras/stx2474},
archivePrefix = {arXiv},
       eprint = {1712.01409},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.473.2565M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018MNRAS.473.1108H,
       author = {{Hocking}, Alex and {Geach}, James E. and {Sun}, Yi and {Davey}, Neil},
        title = "{An automatic taxonomy of galaxy morphology using unsupervised machine learning}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: observational, methods: statistical, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2018",
        month = "Jan",
       volume = {473},
       number = {1},
        pages = {1108-1129},
     abstract = "{We present an unsupervised machine learning technique that automatically
        segments and labels galaxies in astronomical imaging surveys
        using only pixel data. Distinct from previous unsupervised
        machine learning approaches used in astronomy we use no pre-
        selection or pre-filtering of target galaxy type to identify
        galaxies that are similar. We demonstrate the technique on the
        Hubble Space Telescope (HST) Frontier Fields. By training the
        algorithm using galaxies from one field (Abell 2744) and
        applying the result to another (MACS 0416.1-2403), we show how
        the algorithm can cleanly separate early and late type galaxies
        without any form of pre-directed training for what an 'early' or
        'late' type galaxy is. We then apply the technique to the HST
        Cosmic Assembly Near-infrared Deep Extragalactic Legacy Survey
        (CANDELS) fields, creating a catalogue of approximately 60 000
        classifications. We show how the automatic classification groups
        galaxies of similar morphological (and photometric) type and
        make the classifications public via a catalogue, a visual
        catalogue and galaxy similarity search. We compare the CANDELS
        machine-based classifications to human-classifications from the
        Galaxy Zoo: CANDELS project. Although there is not a direct
        mapping between Galaxy Zoo and our hierarchical labelling, we
        demonstrate a good level of concordance between human and
        machine classifications. Finally, we show how the technique can
        be used to identify rarer objects and present lensed galaxy
        candidates from the CANDELS imaging.}",
          doi = {10.1093/mnras/stx2351},
archivePrefix = {arXiv},
       eprint = {1709.05834},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018MNRAS.473.1108H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...853...90B,
       author = {{Benvenuto}, Federico and {Piana}, Michele and {Campi}, Cristina and
         {Massone}, Anna Maria},
        title = "{A Hybrid Supervised/Unsupervised Machine Learning Approach to Solar Flare Prediction}",
      journal = {\apj},
     keywords = {methods: data analysis, methods: statistical, Sun: flares, sunspots, Astrophysics - Solar and Stellar Astrophysics, Computer Science - Machine Learning, 85-08},
         year = "2018",
        month = "Jan",
       volume = {853},
       number = {1},
          eid = {90},
        pages = {90},
     abstract = "{This paper introduces a novel method for flare forecasting, combining
        prediction accuracy with the ability to identify the most
        relevant predictive variables. This result is obtained by means
        of a two-step approach: first, a supervised regularization
        method for regression, namely, LASSO is applied, where a
        sparsity-enhancing penalty term allows the identification of the
        significance with which each data feature contributes to the
        prediction; then, an unsupervised fuzzy clustering technique for
        classification, namely, Fuzzy C-Means, is applied, where the
        regression outcome is partitioned through the minimization of a
        cost function and without focusing on the optimization of a
        specific skill score. This approach is therefore hybrid, since
        it combines supervised and unsupervised learning; realizes
        classification in an automatic, skill-score-independent way; and
        provides effective prediction performances even in the case of
        imbalanced data sets. Its prediction power is verified against
        NOAA Space Weather Prediction Center data, using as a test set,
        data in the range between 1996 August and 2010 December and as
        training set, data in the range between 1988 December and 1996
        June. To validate the method, we computed several skill scores
        typically utilized in flare prediction and compared the values
        provided by the hybrid approach with the ones provided by
        several standard (non-hybrid) machine learning methods. The
        results showed that the hybrid approach performs classification
        better than all other supervised methods and with an
        effectiveness comparable to the one of clustering methods; but,
        in addition, it provides a reliable ranking of the weights with
        which the data properties contribute to the forecast.}",
          doi = {10.3847/1538-4357/aaa23c},
archivePrefix = {arXiv},
       eprint = {1706.07103},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...853...90B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018ApJ...852..104R,
       author = {{Ren}, Bin and {Pueyo}, Laurent and {Ben Zhu}, Guangtun and
         {Debes}, John and {Duch{\^e}ne}, Gaspard},
        title = "{Non-negative Matrix Factorization: Robust Extraction of Extended Structures}",
      journal = {\apj},
     keywords = {protoplanetary disks, stars: imaging, stars: individual: HD 181327, techniques: image processing, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Earth and Planetary Astrophysics, Computer Science - Machine Learning},
         year = "2018",
        month = "Jan",
       volume = {852},
       number = {2},
          eid = {104},
        pages = {104},
     abstract = "{We apply the vectorized non-negative matrix factorization (NMF) method
        to the post-processing of the direct imaging data of
        exoplanetary systems such as circumstellar disks. NMF is an
        iterative approach, which first creates a nonorthogonal and non-
        negative basis of components using the given reference images
        and then models a target with the components. The constructed
        model is then rescaled with a factor to compensate for the
        contribution from the disks. We compare NMF with existing
        methods (classical reference differential imaging method, and
        the Karhunen-Lo{\`e}ve image projection algorithm) using
        synthetic circumstellar disks and demonstrate the superiority of
        NMF: with no need of prior selection of references, NMF not only
        can detect fainter circumstellar disks but also better preserves
        their morphology and does not require forward modeling. As an
        application to a well-known disk example, we process the
        archival Hubble Space Telescope STIS coronagraphic observations
        of HD 181327 with different methods and compare them, and NMF is
        able to extract some circumstellar materials inside the primary
        ring for the first time. In an appendix, we mathematically
        investigate the stability of NMF components during the iteration
        and the linearity of NMF modeling.}",
          doi = {10.3847/1538-4357/aaa1f2},
archivePrefix = {arXiv},
       eprint = {1712.10317},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018ApJ...852..104R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.472.3101G,
       author = {{Gieseke}, Fabian and {Bloemen}, Steven and {van den Bogaard}, Cas and
         {Heskes}, Tom and {Kindler}, Jonas and {Scalzo}, Richard A. and
         {Ribeiro}, Val{\'e}rio A.~R.~M. and {van Roestel}, Jan and
         {Groot}, Paul J. and {Yuan}, Fang and {M{\"o}ller}, Anais and
         {Tucker}, Brad E.},
        title = "{Convolutional neural networks for transient candidate vetting in large-scale surveys}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: image processing, surveys - supernovae: general, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Dec",
       volume = {472},
       number = {3},
        pages = {3101-3114},
     abstract = "{Current synoptic sky surveys monitor large areas of the sky to find
        variable and transient astronomical sources. As the number of
        detections per night at a single telescope easily exceeds
        several thousand, current detection pipelines make intensive use
        of machine learning algorithms to classify the detected objects
        and to filter out the most interesting candidates. A number of
        upcoming surveys will produce up to three orders of magnitude
        more data, which renders high-precision classification systems
        essential to reduce the manual and, hence, expensive vetting by
        human experts. We present an approach based on convolutional
        neural networks to discriminate between true astrophysical
        sources and artefacts in reference-subtracted optical images. We
        show that relatively simple networks are already competitive
        with state-of-the-art systems and that their quality can further
        be improved via slightly deeper networks and additional pre-
        processing steps - eventually yielding models outperforming
        state-of-the-art systems. In particular, our best model
        correctly classifies about 97.3 per cent of all 'real' and 99.7
        per cent of all 'bogus' instances on a test set containing 1942
        'bogus' and 227 'real' instances in total. Furthermore, the
        networks considered in this work can also successfully classify
        these objects at hand without relying on difference images,
        which might pave the way for future detection pipelines not
        containing image subtraction steps at all.}",
          doi = {10.1093/mnras/stx2161},
archivePrefix = {arXiv},
       eprint = {1708.08947},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.472.3101G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.472.2808D,
       author = {{de Souza}, R.~S. and {Dantas}, M.~L.~L. and {Costa-Duarte}, M.~V. and
         {Feigelson}, E.~D. and {Killedar}, M. and {Lablanche}, P. -Y. and
         {Vilalta}, R. and {Krone-Martins}, A. and {Beck}, R. and {Gieseke}, F.},
        title = "{A probabilistic approach to emission-line galaxy classification}",
      journal = {\mnras},
     keywords = {methods: data analysis, galaxies: evolution, galaxies: general, galaxies: nuclei, galaxies: star formation, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Machine Learning},
         year = "2017",
        month = "Dec",
       volume = {472},
       number = {3},
        pages = {2808-2822},
     abstract = "{We invoke a Gaussian mixture model (GMM) to jointly analyse two
        traditional emission-line classification schemes of galaxy
        ionization sources: the Baldwin-Phillips-Terlevich (BPT) and
        W$_{H {\ensuremath{\alpha}}}$ versus [N II]/H
        {\ensuremath{\alpha}} (WHAN) diagrams, using spectroscopic data
        from the Sloan Digital Sky Survey Data Release 7 and
        SEAGal/STARLIGHT data sets. We apply a GMM to empirically define
        classes of galaxies in a three-dimensional space spanned by the
        log [O III]/H {\ensuremath{\beta}}, log [N II]/H
        {\ensuremath{\alpha}} and log EW(H {\ensuremath{\alpha}})
        optical parameters. The best-fitting GMM based on several
        statistical criteria suggests a solution around four Gaussian
        components (GCs), which are capable to explain up to 97 per cent
        of the data variance. Using elements of information theory, we
        compare each GC to their respective astronomical counterpart.
        GC1 and GC4 are associated with star-forming galaxies,
        suggesting the need to define a new starburst subgroup. GC2 is
        associated with BPT's active galactic nuclei (AGN) class and
        WHAN's weak AGN class. GC3 is associated with BPT's composite
        class and WHAN's strong AGN class. Conversely, there is no
        statistical evidence - based on four GCs - for the existence of
        a Seyfert/low-ionization nuclear emission-line region (LINER)
        dichotomy in our sample. Notwithstanding, the inclusion of an
        additional GC5 unravels it. The GC5 appears associated with the
        LINER and passive galaxies on the BPT and WHAN diagrams,
        respectively. This indicates that if the Seyfert/LINER dichotomy
        is there, it does not account significantly to the global data
        variance and may be overlooked by standard metrics of goodness
        of fit. Subtleties aside, we demonstrate the potential of our
        methodology to recover/unravel different objects inside the
        wilderness of astronomical data sets, without lacking the
        ability to convey physically interpretable results. The
        probabilistic classifications from the GMM analysis are publicly
        available within the COINtoolbox at
        https://cointoolbox.github.io/GMM\_Catalogue/.}",
          doi = {10.1093/mnras/stx2156},
archivePrefix = {arXiv},
       eprint = {1703.07607},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.472.2808D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017JHEP...12..149L,
       author = {{Liu}, Junyu},
        title = "{Artificial neural network in cosmic landscape}",
      journal = {Journal of High Energy Physics},
     keywords = {Cosmology of Theories beyond the SM, Models of Quantum Gravity, Random Systems, High Energy Physics - Theory, Astrophysics - Cosmology and Nongalactic Astrophysics, General Relativity and Quantum Cosmology},
         year = "2017",
        month = "Dec",
       volume = {2017},
       number = {12},
          eid = {149},
        pages = {149},
     abstract = "{In this paper we propose that artificial neural network, the basis of
        machine learning, is useful to generate the inflationary
        landscape from a cosmological point of view. Traditional
        numerical simulations of a global cosmic landscape typically
        need an exponential complexity when the number of fields is
        large. However, a basic application of artificial neural network
        could solve the problem based on the universal approximation
        theorem of the multilayer perceptron. A toy model in inflation
        with multiple light fields is investigated numerically as an
        example of such an application.}",
          doi = {10.1007/JHEP12(2017)149},
archivePrefix = {arXiv},
       eprint = {1707.02800},
 primaryClass = {hep-th},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017JHEP...12..149L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017JGRA..12212586M,
       author = {{McGranaghan}, Ryan M. and {Bhatt}, Asti and {Matsuo}, Tomoko and
         {Mannucci}, Anthony J. and {Semeter}, Joshua L. and
         {Datta-Barua}, Seebany},
        title = "{Ushering in a New Frontier in Geospace Through Data Science}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {geospace, space weather, data science, data-driven science, machine learning},
         year = "2017",
        month = "Dec",
       volume = {122},
       number = {12},
        pages = {12,586-12,590},
     abstract = "{Our understanding and specification of solar-terrestrial interactions
        benefit from taking advantage of comprehensive data-intensive
        approaches. These data-driven methods are taking on new
        importance in light of the shifting data landscape of the
        geospace system, which extends from the near Earth space
        environment, through the magnetosphere and interplanetary space,
        to the Sun. The space physics community faces both an exciting
        opportunity and an important imperative to create a new frontier
        built at the intersection of traditional approaches and state-
        of-the-art data-driven sciences and technologies. This brief
        commentary addresses the current paradigm of geospace science
        and the emerging need for data science innovation, discusses the
        meaning of data science in the context of geospace, and
        highlights community efforts to respond to the changing
        landscape.}",
          doi = {10.1002/2017JA024835},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017JGRA..12212586M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017JCAP...12..034M,
       author = {{McLeod}, M. and {Libeskind}, N. and {Lahav}, O. and {Hoffman}, Y.},
        title = "{Estimating the mass of the Local Group using machine learning applied to numerical simulations}",
      journal = {Journal of Cosmology and Astro-Particle Physics},
     keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Dec",
       volume = {2017},
       number = {12},
          eid = {034},
        pages = {034},
     abstract = "{We present a new approach to calculating the combined mass of the Milky
        Way (MW) and Andromeda (M31), which together account for the
        bulk of the mass of the Local Group (LG). We base our work on an
        ensemble of 30,190 halo pairs from the Small MultiDark
        simulation, assuming a {\ensuremath{\Lambda}}CDM (Cosmological
        Constant and Cold Dark Matter) cosmology. This is used in
        conjunction with machine learning methods (artificial neural
        networks, ANN) to investigate the relationship between the mass
        and selected parameters characterising the orbit and local
        environment of the binary. ANN are employed to take account of
        additional physics arising from interactions with larger
        structures or dynamical effects which are not analytically well
        understood. Results from the ANN are most successful when the
        velocity shear is provided, which demonstrates the flexibility
        of machine learning to model physical phenomena and readily
        incorporate new information. The resulting estimate for the
        Local Group mass, when shear information is included, is
        4.9{\texttimes}{}10$^{12}$M$_{solar}$, with an error of
        {\ensuremath{\pm}}0.8{\texttimes}{}10$^{12}$M$_{solar}$ from the
        68\% uncertainty in observables, and a r.m.s. scatter interval
        of $^{+1.7}$$_{-1.3}${\texttimes}{}10$^{12}$M$_{solar}$
        estimated scatter from the differences between the model
        estimates and simulation masses for a testing sample of halo
        pairs. We also consider a recently reported large relative
        transverse velocity of M31 and the Milky Way, and produce an
        alternative mass estimate of 3.6{\ensuremath{\pm}}0.3$^{+2.1}$$_
        {-1.3}${\texttimes}{}10$^{12}$M$_{solar}$. Although the methods
        used predict similar values for the most likely mass of the LG,
        application of ANN compared to the traditional Timing Argument
        reduces the scatter in the log mass by approximately half when
        tested on samples from the simulation.}",
          doi = {10.1088/1475-7516/2017/12/034},
archivePrefix = {arXiv},
       eprint = {1606.02694},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017JCAP...12..034M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017GeoRL..4412396S,
       author = {{Schneider}, Tapio and {Lan}, Shiwei and {Stuart}, Andrew and
         {Teixeira}, Jo{\~a}o.},
        title = "{Earth System Modeling 2.0: A Blueprint for Models That Learn From Observations and Targeted High-Resolution Simulations}",
      journal = {\grl},
     keywords = {Earth system models, parameterizations, data assimilation, machine learning, Kalman inversion, Markov chain Monte Carlo, Statistics - Machine Learning, Physics - Atmospheric and Oceanic Physics},
         year = "2017",
        month = "Dec",
       volume = {44},
       number = {24},
        pages = {12,396-12,417},
     abstract = "{Climate projections continue to be marred by large uncertainties, which
        originate in processes that need to be parameterized, such as
        clouds, convection, and ecosystems. But rapid progress is now
        within reach. New computational tools and methods from data
        assimilation and machine learning make it possible to integrate
        global observations and local high-resolution simulations in an
        Earth system model (ESM) that systematically learns from both
        and quantifies uncertainties. Here we propose a blueprint for
        such an ESM. We outline how parameterization schemes can learn
        from global observations and targeted high-resolution
        simulations, for example, of clouds and convection, through
        matching low-order statistics between ESMs, observations, and
        high-resolution simulations. We illustrate learning algorithms
        for ESMs with a simple dynamical system that shares
        characteristics of the climate system; and we discuss the
        opportunities the proposed framework presents and the challenges
        that remain to realize it.}",
          doi = {10.1002/2017GL076101},
archivePrefix = {arXiv},
       eprint = {1709.00037},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017GeoRL..4412396S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017GeoRL..4412271R,
       author = {{Rezvanbehbahani}, Soroush and {Stearns}, Leigh A. and {Kadivar}, Amir and
         {Walker}, J. Doug and {van{\^A} der{\^A} Veen}, C.~J.},
        title = "{Predicting the Geothermal Heat Flux in Greenland: A Machine Learning Approach}",
      journal = {\grl},
     keywords = {Greenland ice sheet, geothermal heat flux, machine learning},
         year = "2017",
        month = "Dec",
       volume = {44},
       number = {24},
        pages = {12,271-12,279},
     abstract = "{Geothermal heat flux (GHF) is a crucial boundary condition for making
        accurate predictions of ice sheet mass loss, yet it is poorly
        known in Greenland due to inaccessibility of the bedrock. Here
        we use a machine learning algorithm on a large collection of
        relevant geologic features and global GHF measurements and
        produce a GHF map of Greenland that we argue is within ̃15\%
        accuracy. The main features of our predicted GHF map include a
        large region with high GHF in central-north Greenland
        surrounding the NorthGRIP ice core site, and hot spots in the
        Jakobshavn Isbr{\ae} catchment, upstream of Petermann Gletscher,
        and near the terminus of Nioghalvfjerdsfjorden glacier. Our
        model also captures the trajectory of Greenland movement over
        the Icelandic plume by predicting a stripe of elevated GHF in
        central-east Greenland. Finally, we show that our model can
        produce substantially more accurate predictions if additional
        measurements of GHF in Greenland are provided.}",
          doi = {10.1002/2017GL075661},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017GeoRL..4412271R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017Ge&Ae..57.1086K,
       author = {{Knyazeva}, I.~S. and {Urtiev}, F.~A. and {Makarenko}, N.~G.},
        title = "{On the Prognostic Efficiency of Topological Descriptors for Magnetograms of Active Regions}",
      journal = {Geomagnetism and Aeronomy},
         year = "2017",
        month = "Dec",
       volume = {57},
       number = {8},
        pages = {1086-1091},
     abstract = "{Solar flare prediction remains an important practical task of space
        weather. An increase in the amount and quality of observational
        data and the development of machine-learning methods has led to
        an improvement in prediction techniques. Additional information
        has been retrieved from the vector magnetograms; these have been
        recently supplemented by traditional line-of-sight (LOS)
        magnetograms. In this work, the problem of the comparative
        prognostic efficiency of features obtained on the basis of
        vector data and LOS magnetograms is discussed. Invariants
        obtained from a topological analysis of LOS magnetograms are
        used as complexity characteristics of magnetic patterns.
        Alternatively, the so-called SHARP parameters were used; they
        were calculated by the data analysis group of the Stanford
        University Laboratory on the basis of HMI/SDO vector
        magnetograms and are available online at the website
        (http://jsoc.stanford.edu/) with the solar dynamics observatory
        (SDO) database for the entire history of SDO observations. It
        has been found that the efficiency of large-flare prediction
        based on topological descriptors of LOS magnetograms in
        epignosis mode is at least s no worse than the results of
        prognostic schemes based on vector features. The advantages of
        the use of topological invariants based on LOS data are
        discussed.}",
          doi = {10.1134/S0016793217080126},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017Ge&Ae..57.1086K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...851..149X,
       author = {{Xu}, Duo and {Offner}, Stella S.~R.},
        title = "{Assessing the Performance of a Machine Learning Algorithm in Identifying Bubbles in Dust Emission}",
      journal = {\apj},
     keywords = {ISM: bubbles, ISM: clouds, methods: data analysis, stars: formation, Astrophysics - Astrophysics of Galaxies},
         year = "2017",
        month = "Dec",
       volume = {851},
       number = {2},
          eid = {149},
        pages = {149},
     abstract = "{Stellar feedback created by radiation and winds from massive stars plays
        a significant role in both physical and chemical evolution of
        molecular clouds. This energy and momentum leaves an
        identifiable signature
        ({\textquotedblleft}bubbles{\textquotedblright}) that affects
        the dynamics and structure of the cloud. Most bubble searches
        are performed {\textquotedblleft}by eye,{\textquotedblright}
        which is usually time-consuming, subjective, and difficult to
        calibrate. Automatic classifications based on machine learning
        make it possible to perform systematic, quantifiable, and
        repeatable searches for bubbles. We employ a previously
        developed machine learning algorithm, Brut, and quantitatively
        evaluate its performance in identifying bubbles using synthetic
        dust observations. We adopt magnetohydrodynamics simulations,
        which model stellar winds launching within turbulent molecular
        clouds, as an input to generate synthetic images. We use a
        publicly available three-dimensional dust continuum Monte Carlo
        radiative transfer code, HYPERION, to generate synthetic images
        of bubbles in three Spitzer bands (4.5, 8, and 24
        {\ensuremath{\mu}}m). We designate half of our synthetic bubbles
        as a training set, which we use to train Brut along with
        citizen-science data from the Milky Way Project (MWP). We then
        assess Brut{\textquoteright}s accuracy using the remaining
        synthetic observations. We find that Brut{\textquoteright}s
        performance after retraining increases significantly, and it is
        able to identify yellow bubbles, which are likely associated
        with B-type stars. Brut continues to perform well on previously
        identified high-score bubbles, and over 10\% of the MWP bubbles
        are reclassified as high-confidence bubbles, which were
        previously marginal or ambiguous detections in the MWP data. We
        also investigate the influence of the size of the training set,
        dust model, evolutionary stage, and background noise on bubble
        identification.}",
          doi = {10.3847/1538-4357/aa9a42},
archivePrefix = {arXiv},
       eprint = {1711.03480},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...851..149X},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...851..139L,
       author = {{Lin}, Yen-Ting and {Hsieh}, Bau-Ching and {Lin}, Sheng-Chieh and
         {Oguri}, Masamune and {Chen}, Kai-Feng and {Tanaka}, Masayuki and
         {Chiu}, I. -Non and {Huang}, Song and {Kodama}, Tadayuki and
         {Leauthaud}, Alexie and {More}, Surhud and {Nishizawa}, Atsushi J. and
         {Bundy}, Kevin and {Lin}, Lihwai and {Miyazaki}, Satoshi},
        title = "{First Results on the Cluster Galaxy Population from the Subaru Hyper Suprime-Cam Survey. III. Brightest Cluster Galaxies, Stellar Mass Distribution, and Active Galaxies}",
      journal = {\apj},
     keywords = {galaxies: active, galaxies: clusters: general, galaxies: elliptical and lenticular, cD, galaxies: luminosity function, mass function, Astrophysics - Astrophysics of Galaxies},
         year = "2017",
        month = "Dec",
       volume = {851},
       number = {2},
          eid = {139},
        pages = {139},
     abstract = "{The unprecedented depth and area surveyed by the Subaru Strategic
        Program with the Hyper Suprime-Cam (HSC-SSP) have enabled us to
        construct and publish the largest distant cluster sample out to
        z̃ 1 to date. In this exploratory study of cluster galaxy
        evolution from z = 1 to z = 0.3, we investigate the stellar mass
        assembly history of brightest cluster galaxies (BCGs), the
        evolution of stellar mass and luminosity distributions, the
        stellar mass surface density profile, as well as the population
        of radio galaxies. Our analysis is the first high-redshift
        application of the top N richest cluster selection, which is
        shown to allow us to trace the cluster galaxy evolution
        faithfully. Over the 230 deg$^{2}$ area of the current HSC-SSP
        footprint, selecting the top 100 clusters in each of the four
        redshift bins allows us to observe the buildup of galaxy
        population in descendants of clusters whose
        z{\ensuremath{\approx}} 1 mass is about 2{\texttimes}
        \{10\}$^{14}$ \{M\}$_{☉ }$. Our stellar mass is derived from a
        machine-learning algorithm, which is found to be unbiased and
        accurate with respect to the COSMOS data. We find very mild
        stellar mass growth in BCGs (about 35\% between z = 1 and 0.3),
        and no evidence for evolution in both the total stellar mass-
        cluster mass correlation and the shape of the stellar mass
        surface density profile. We also present the first measurement
        of the radio luminosity distribution in clusters out to z̃ 1,
        and show hints of changes in the dominant accretion mode
        powering the cluster radio galaxies at z̃ 0.8.}",
          doi = {10.3847/1538-4357/aa9bf5},
archivePrefix = {arXiv},
       eprint = {1709.04484},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...851..139L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...851...13S,
       author = {{Schindler}, Jan-Torge and {Fan}, Xiaohui and {McGreer}, Ian D. and
         {Yang}, Qian and {Wu}, Jin and {Jiang}, Linhua and {Green}, Richard},
        title = "{The Extremely Luminous Quasar Survey in the SDSS Footprint. I. Infrared-based Candidate Selection}",
      journal = {\apj},
     keywords = {galaxies: nuclei, quasars: general, Astrophysics - Astrophysics of Galaxies},
         year = "2017",
        month = "Dec",
       volume = {851},
       number = {1},
          eid = {13},
        pages = {13},
     abstract = "{Studies of the most luminous quasars at high redshift directly probe the
        evolution of the most massive black holes in the early universe
        and their connection to massive galaxy formation. However,
        extremely luminous quasars at high redshift are very rare
        objects. Only wide-area surveys have a chance to constrain their
        population. The Sloan Digital Sky Survey (SDSS) has so far
        provided the most widely adopted measurements of the quasar
        luminosity function at z\&gt; 3. However, a careful re-
        examination of the SDSS quasar sample revealed that the SDSS
        quasar selection is in fact missing a significant fraction of
        z{\ensuremath{\gtrsim}} 3 quasars at the brightest end. We
        identified the purely optical-color selection of SDSS, where
        quasars at these redshifts are strongly contaminated by late-
        type dwarfs, and the spectroscopic incompleteness of the SDSS
        footprint as the main reasons. Therefore, we designed the
        Extremely Luminous Quasar Survey (ELQS), based on a novel near-
        infrared JKW2 color cut using Wide-field Infrared Survey
        Explorer mission (WISE) AllWISE and 2MASS all-sky photometry, to
        yield high completeness for very bright
        (\{m\}$_{\{\{i}$\}\}\&lt; 18.0) quasars in the redshift range of
        3.0{\ensuremath{\leq}}slant z{\ensuremath{\leq}}slant 5.0. It
        effectively uses random forest machine-learning algorithms on
        SDSS and WISE photometry for quasar-star classification and
        photometric redshift estimation. The ELQS will spectroscopically
        follow-up ̃230 new quasar candidates in an area of ̃12,000
        deg$^{2}$ in the SDSS footprint to obtain a well-defined and
        complete quasar sample for an accurate measurement of the
        bright-end quasar luminosity function (QLF) at
        3.0{\ensuremath{\leq}}slant z{\ensuremath{\leq}}slant 5.0. In
        this paper, we present the quasar selection algorithm and the
        quasar candidate catalog.}",
          doi = {10.3847/1538-4357/aa9929},
archivePrefix = {arXiv},
       eprint = {1712.01205},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...851...13S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017A&A...608A..39M,
       author = {{Mountrichas}, G. and {Corral}, A. and {Masoura}, V.~A. and
         {Georgantopoulos}, I. and {Ruiz}, A. and {Georgakakis}, A. and
         {Carrera}, F.~J. and {Fotopoulou}, S.},
        title = "{Estimating photometric redshifts for X-ray sources in the X-ATLAS field using machine-learning techniques}",
      journal = {\aap},
     keywords = {X-rays: general, galaxies: active, catalogs, techniques: photometric, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2017",
        month = "Dec",
       volume = {608},
          eid = {A39},
        pages = {A39},
     abstract = "{We present photometric redshifts for 1031 X-ray sources in the X-ATLAS
        field using the machine-learning technique TPZ. X-ATLAS covers
        7.1 deg$^{2}$ observed with XMM-Newton within the Science
        Demonstration Phase of the H-ATLAS field, making it one of the
        largest contiguous areas of the sky with both XMM-Newton and
        Herschel coverage. All of the sources have available SDSS
        photometry, while 810 additionally have mid-IR and/or near-IR
        photometry. A spectroscopic sample of 5157 sources primarily in
        the XMM/XXL field, but also from several X-ray surveys and the
        SDSS DR13 redshift catalogue, was used to train the algorithm.
        Our analysis reveals that the algorithm performs best when the
        sources are split, based on their optical morphology, into
        point-like and extended sources. Optical photometry alone is not
        enough to estimate accurate photometric redshifts, but the
        results greatly improve when at least mid-IR photometry is added
        in the training process. In particular, our measurements show
        that the estimated photometric redshifts for the X-ray sources
        of the training sample have a normalized absolute median
        deviation, nmad {\ensuremath{\approx}} 0.06, and a percentage of
        outliers, {\ensuremath{\eta}} = 10-14\%, depending upon whether
        the sources are extended or point like. Our final catalogue
        contains photometric redshifts for 933 out of the 1031 X-ray
        sources with a median redshift of 0.9. The table of the
        photometric redshifts is only available at the CDS via anonymous
        ftp to <A href=``http://cdsarc.u-strasbg.fr''>http://cdsarc.u-st
        rasbg.fr</A> (<A
        href=``http://130.79.128.5''>http://130.79.128.5</A>) or via <A
        href=``http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/608/A39''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/608/A39</A>}",
          doi = {10.1051/0004-6361/201731762},
archivePrefix = {arXiv},
       eprint = {1710.01313},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017A&A...608A..39M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.471.3378H,
       author = {{Hartley}, P. and {Flamary}, R. and {Jackson}, N. and {Tagore}, A.~S. and
         {Metcalf}, R.~B.},
        title = "{Support vector machine classification of strong gravitational lenses}",
      journal = {\mnras},
     keywords = {gravitational lensing: strong, methods: data analysis, methods: statistical, surveys, galaxies: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2017",
        month = "Nov",
       volume = {471},
       number = {3},
        pages = {3378-3397},
     abstract = "{The imminent advent of very large-scale optical sky surveys, such as
        Euclid and the Large Synoptic Survey Telescope (LSST), makes it
        important to find efficient ways of discovering rare objects
        such as strong gravitational lens systems, where a background
        object is multiply gravitationally imaged by a foreground mass.
        As well as finding the lens systems, it is important to reject
        false positives due to intrinsic structure in galaxies, and much
        work is in progress with machine learning algorithms such as
        neural networks in order to achieve both these aims. We present
        and discuss a support vector machine (SVM) algorithm which makes
        use of a Gabor filter bank in order to provide learning criteria
        for separation of lenses and non-lenses, and demonstrate using
        blind challenges that under certain circumstances, it is a
        particularly efficient algorithm for rejecting false positives.
        We compare the SVM engine with a large-scale human examination
        of 100 000 simulated lenses in a challenge data set, and also
        apply the SVM method to survey images from the Kilo Degree
        Survey.}",
          doi = {10.1093/mnras/stx1733},
archivePrefix = {arXiv},
       eprint = {1705.08949},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.471.3378H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017JGRA..12211227Z,
       author = {{Zhelavskaya}, Irina S. and {Shprits}, Yuri Y. and
         {Spasojevi{\'c}}, Maria},
        title = "{Empirical Modeling of the Plasmasphere Dynamics Using Neural Networks}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {plasmasphere, inner magnetosphere, models, neural networks, machine learning},
         year = "2017",
        month = "Nov",
       volume = {122},
       number = {11},
        pages = {11,227-11,244},
     abstract = "{We present the PINE (Plasma density in the Inner magnetosphere Neural
        network-based Empirical) model - a new empirical model for
        reconstructing the global dynamics of the cold plasma density
        distribution based only on solar wind data and geomagnetic
        indices. Utilizing the density database obtained using the NURD
        (Neural-network-based Upper hybrid Resonance Determination)
        algorithm for the period of 1 October 2012 to 1 July 2016, in
        conjunction with solar wind data and geomagnetic indices, we
        develop a neural network model that is capable of globally
        reconstructing the dynamics of the cold plasma density
        distribution for 2{\ensuremath{\leq}}L{\ensuremath{\leq}}6 and
        all local times. We validate and test the model by measuring its
        performance on independent data sets withheld from the training
        set and by comparing the model-predicted global evolution with
        global images of He$^{+}$ distribution in the Earth's
        plasmasphere from the IMAGE Extreme UltraViolet (EUV)
        instrument. We identify the parameters that best quantify the
        plasmasphere dynamics by training and comparing multiple neural
        networks with different combinations of input parameters
        (geomagnetic indices, solar wind data, and different durations
        of their time history). The optimal model is based on the 96 h
        time history of Kp, AE, SYM-H, and F$_{10.7}$ indices. The model
        successfully reproduces erosion of the plasmasphere on the
        nightside and plume formation and evolution. We demonstrate
        results of both local and global plasma density reconstruction.
        This study illustrates how global dynamics can be reconstructed
        from local in situ observations by using machine learning
        techniques.}",
          doi = {10.1002/2017JA024406},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017JGRA..12211227Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017JGRA..12210910C,
       author = {{Camporeale}, Enrico and {Car{\`e}}, Algo and {Borovsky}, Joseph E.},
        title = "{Classification of Solar Wind With Machine Learning}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {solar wind, classification, machine learning, Physics - Space Physics},
         year = "2017",
        month = "Nov",
       volume = {122},
       number = {11},
        pages = {10,910-10,920},
     abstract = "{We present a four-category classification algorithm for the solar wind,
        based on Gaussian Process. The four categories are the ones
        previously adopted in Xu and Borovsky (2015): ejecta, coronal
        hole origin plasma, streamer belt origin plasma, and sector
        reversal origin plasma. The algorithm is trained and tested on a
        labeled portion of the OMNI data set. It uses seven inputs: the
        solar wind speed V$_{sw}$, the temperature standard deviation
        {\ensuremath{\sigma}}$_{T}$, the sunspot number R, the
        F$_{10.7}$ index, the Alfven speed v$_{A}$, the proton specific
        entropy S$_{p}$, and the proton temperature T$_{p}$ compared to
        a velocity-dependent expected temperature. The output of the
        Gaussian Process classifier is a four-element vector containing
        the probabilities that an event (one reading from the hourly
        averaged OMNI database) belongs to each category. The
        probabilistic nature of the prediction allows for a more
        informative and flexible interpretation of the results, for
        instance, being able to classify events as ``undecided.'' The
        new method has a median accuracy larger than 90\% for all
        categories, even using a small set of data for training. The
        Receiver Operating Characteristic curve and the reliability
        diagram also demonstrate the excellent quality of this new
        method. Finally, we use the algorithm to classify a large
        portion of the OMNI data set, and we present for the first time
        transition probabilities between different solar wind
        categories. Such probabilities represent the ``climatological''
        statistics that determine the solar wind baseline.}",
          doi = {10.1002/2017JA024383},
archivePrefix = {arXiv},
       eprint = {1710.02313},
 primaryClass = {physics.space-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017JGRA..12210910C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJS..233....2B,
       author = {{Bu}, Yude and {Lei}, Zhenxin and {Zhao}, Gang and {Bu}, Jingde and
         {Pan}, Jingchang},
        title = "{Searching for Hot Subdwarf Stars from the LAMOST Spectra. I. Method}",
      journal = {\apjs},
     keywords = {methods: statistical, stars: statistics, subdwarfs},
         year = "2017",
        month = "Nov",
       volume = {233},
       number = {1},
          eid = {2},
        pages = {2},
     abstract = "{Hot subdwarf stars are core He burning stars located at the blue end of
        the horizontal branch, which is also known as the extreme
        horizontal branch. The study of hot subdwarf stars is important
        for understanding stellar astrophysics, globular clusters, and
        galaxies. Presently, some problems associated with hot subdwarf
        stars are still unclear. To better study the properties of these
        stars, we should find more hot subdwarf stars to enlarge the
        sample size. The traditional method of searching for hot
        subdwarfs from the large data sets is based on the color cuts
        followed by visual inspection. This method is not suitable for
        the data set without homogeneous colors, such as the spectra
        obtained by the Large Sky Area Multi-object Fiber Spectroscopic
        Telescope (LAMOST). In this paper, we present a new method of
        searching for hot subdwarf stars in large spectroscopic surveys
        using a machine learning algorithm, the hierarchical extreme
        learning machine (HELM) algorithm. We have applied the HELM
        algorithm to the spectra from the LAMOST survey, and
        classification errors are considerably small: for the single hot
        subdwarf stars, accuracy = 0.92 and efficiency - 0.96; and for
        the hot subdwarf binaries, accuracy = 0.80 and efficiency =
        0.71. A comparison of the HELM and other popular algorithms
        shows that HELM is accurate and efficient in classifying hot
        subdwarf stars. This method provides a new tool for searching
        for hot subdwarf stars in large spectroscopic surveys.}",
          doi = {10.3847/1538-4365/aa91cd},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJS..233....2B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...849..150C,
       author = {{Cohen}, Judith G. and {Sesar}, Branimir and {Bahnolzer}, Sophianna and
         {He}, Kevin and {Kulkarni}, Shrinivas R. and {Prince}, Thomas A. and
         {Bellm}, Eric and {Laher}, Russ R.},
        title = "{The Outer Halo of the Milky Way as Probed by RR Lyr Variables from the Palomar Transient Facility}",
      journal = {\apj},
     keywords = {Galaxy: halo, Galaxy: kinematics and dynamics, stars: variables: RR Lyrae, Astrophysics - Astrophysics of Galaxies},
         year = "2017",
        month = "Nov",
       volume = {849},
       number = {2},
          eid = {150},
        pages = {150},
     abstract = "{RR Lyrae stars are ideal massless tracers that can be used to study the
        total mass and dark matter content of the outer halo of the
        Milky Way (MW). This is because they are easy to find in the
        light-curve databases of large stellar surveys and their
        distances can be determined with only knowledge of the light
        curve. We present here a sample of 112 RR Lyr stars beyond 50
        kpc in the outer halo of the MW, excluding the Sgr streams, for
        which we have obtained moderate-resolution spectra with Deimos
        on the Keck II Telescope. Four of these have distances exceeding
        100 kpc. These were selected from a much larger set of 447
        candidate RR Lyr stars that were data-mined using machine-
        learning techniques applied to the light curves of variable
        stars in the Palomar Transient Facility database. The observed
        radial velocities taken at the phase of the variable
        corresponding to the time of observation were converted to
        systemic radial velocities in the Galactic standard of rest.
        From our sample of 112 RR Lyr stars we determine the radial
        velocity dispersion in the outer halo of the MW to be ̃90 km
        s$^{-1}$ at 50 kpc, falling to about 65 km s$^{-1}$ near 100 kpc
        once a small number of major outliers are removed. With
        reasonable estimates of the completeness of our sample of 447
        candidates and assuming a spherical halo, we find that the
        stellar density in the outer halo declines as \{r\}$^{-4}$.
        Based in part on observations obtained at the W. M. Keck
        Observatory, which is operated jointly by the California
        Institute of Technology, the University of California, and the
        National Aeronautics and Space Administration.}",
          doi = {10.3847/1538-4357/aa9120},
archivePrefix = {arXiv},
       eprint = {1710.01276},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...849..150C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2017PhDT........33O,
       author = {{Osborn}, Hugh},
        title = "{Long-Period Exoplanets from Photometric Transit Surveys}",
     keywords = {Exoplanets, Transit surveys, Photometry, Machine learning, K2, WASP, NGTS, Single Transits, Monotransits},
       school = {University of Warwick},
         year = "2017",
        month = "Oct",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017PhDT........33O},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.471..167J,
       author = {{Jacobs}, C. and {Glazebrook}, K. and {Collett}, T. and {More}, A. and
         {McCarthy}, C.},
        title = "{Finding strong lenses in CFHTLS using convolutional neural networks}",
      journal = {\mnras},
     keywords = {gravitational lensing: strong, methods: statistical, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2017",
        month = "Oct",
       volume = {471},
       number = {1},
        pages = {167-181},
     abstract = "{We train and apply convolutional neural networks, a machine learning
        technique developed to learn from and classify image data, to
        Canada-France-Hawaii Telescope Legacy Survey (CFHTLS) imaging
        for the identification of potential strong lensing systems. An
        ensemble of four convolutional neural networks was trained on
        images of simulated galaxy-galaxy lenses. The training sets
        consisted of a total of 62 406 simulated lenses and 64 673 non-
        lens negative examples generated with two different
        methodologies. An ensemble of trained networks was applied to
        all of the 171 deg$^{2}$ of the CFHTLS wide field image data,
        identifying 18 861 candidates including 63 known and 139 other
        potential lens candidates. A second search of 1.4 million early-
        type galaxies selected from the survey catalogue as potential
        deflectors, identified 2465 candidates including 117 previously
        known lens candidates, 29 confirmed lenses/high-quality lens
        candidates, 266 novel probable or potential lenses and 2097
        candidates we classify as false positives. For the catalogue-
        based search we estimate a completeness of 21-28 per cent with
        respect to detectable lenses and a purity of 15 per cent, with a
        false-positive rate of 1 in 671 images tested. We predict a
        human astronomer reviewing candidates produced by the system
        would identify 20 probable lenses and 100 possible lenses per
        hour in a sample selected by the robot. Convolutional neural
        networks are therefore a promising tool for use in the search
        for lenses in current and forthcoming surveys such as the Dark
        Energy Survey and the Large Synoptic Survey Telescope.}",
          doi = {10.1093/mnras/stx1492},
archivePrefix = {arXiv},
       eprint = {1704.02744},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.471..167J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017EPJC...77..692A,
       author = {{Aartsen}, M.~G. and {Ackermann}, M. and {Adams}, J. and
         {Aguilar}, J.~A. and {Ahlers}, M. and {Ahrens}, M. and
         {Al Samarai}, I. and {Altmann}, D. and {Andeen}, K. and {Anderson}, T. and
         {Ansseau}, I. and {Anton}, G. and {Archinger}, M. and
         {Arg{\"u}elles}, C. and {Auffenberg}, J. and {Axani}, S. and
         {Bagherpour}, H. and {Bai}, X. and {Barwick}, S.~W. and {Baum}, V. and
         {Bay}, R. and {Beatty}, J.~J. and {Becker Tjus}, J. and
         {Becker}, K. -H. and {BenZvi}, S. and {Berley}, D. and
         {Bernardini}, E. and {Besson}, D.~Z. and {Binder}, G. and {Bindig}, D. and
         {Blaufuss}, E. and {Blot}, S. and {Bohm}, C. and {B{\"o}rner}, M. and
         {Bos}, F. and {Bose}, D. and {B{\"o}ser}, S. and {Botner}, O. and
         {Bradascio}, F. and {Braun}, J. and {Brayeur}, L. and {Bretz}, H. -P. and
         {Bron}, S. and {Burgman}, A. and {Carver}, T. and {Casier}, M. and
         {Cheung}, E. and {Chirkin}, D. and {Christov}, A. and {Clark}, K. and
         {Classen}, L. and {Coenders}, S. and {Collin}, G.~H. and
         {Conrad}, J.~M. and {Cowen}, D.~F. and {Cross}, R. and {Day}, M. and
         {de Andr{\'e}}, J.~P.~A.~M. and {De Clercq}, C. and
         {del Pino Rosendo}, E. and {Dembinski}, H. and {De Ridder}, S. and
         {Desiati}, P. and {de Vries}, K.~D. and {de Wasseige}, G. and
         {de With}, M. and {DeYoung}, T. and {D{\'\i}az-V{\'e}lez}, J.~C. and
         {di Lorenzo}, V. and {Dujmovic}, H. and {Dumm}, J.~P. and
         {Dunkman}, M. and {Eberhardt}, B. and {Ehrhardt}, T. and
         {Eichmann}, B. and {Eller}, P. and {Euler}, S. and {Evenson}, P.~A. and
         {Fahey}, S. and {Fazely}, A.~R. and {Feintzeig}, J. and {Felde}, J. and
         {Filimonov}, K. and {Finley}, C. and {Flis}, S. and
         {F{\"o}sig}, C. -C. and {Franckowiak}, A. and {Friedman}, E. and
         {Fuchs}, T. and {Gaisser}, T.~K. and {Gallagher}, J. and
         {Gerhardt}, L. and {Ghorbani}, K. and {Giang}, W. and {Gladstone}, L. and
         {Glauch}, T. and {Gl{\"u}senkamp}, T. and {Goldschmidt}, A. and
         {Gonzalez}, J.~G. and {Grant}, D. and {Griffith}, Z. and {Haack}, C. and
         {Hallgren}, A. and {Halzen}, F. and {Hansen}, E. and {Hansmann}, T. and
         {Hanson}, K. and {Hebecker}, D. and {Heereman}, D. and {Helbing}, K. and
         {Hellauer}, R. and {Hickford}, S. and {Hignight}, J. and {Hill}, G.~C. and
         {Hoffman}, K.~D. and {Hoffmann}, R. and {Hoshina}, K. and {Huang}, F. and
         {Huber}, M. and {Hultqvist}, K. and {In}, S. and {Ishihara}, A. and
         {Jacobi}, E. and {Japaridze}, G.~S. and {Jeong}, M. and {Jero}, K. and
         {Jones}, B.~J.~P. and {Kang}, W. and {Kappes}, A. and {Karg}, T. and
         {Karle}, A. and {Katz}, U. and {Kauer}, M. and {Keivani}, A. and
         {Kelley}, J.~L. and {Kheirandish}, A. and {Kim}, J. and {Kim}, M. and
         {Kintscher}, T. and {Kiryluk}, J. and {Kittler}, T. and {Klein}, S.~R. and
         {Kohnen}, G. and {Koirala}, R. and {Kolanoski}, H. and {Konietz}, R. and
         {K{\"o}pke}, L. and {Kopper}, C. and {Kopper}, S. and
         {Koskinen}, D.~J. and {Kowalski}, M. and {Krings}, K. and {Kroll}, M. and
         {Kr{\"u}ckl}, G. and {Kr{\"u}ger}, C. and {Kunnen}, J. and
         {Kunwar}, S. and {Kurahashi}, N. and {Kuwabara}, T. and {Kyriacou}, A. and
         {Labare}, M. and {Lanfranchi}, J.~L. and {Larson}, M.~J. and
         {Lauber}, F. and {Lennarz}, D. and {Lesiak-Bzdak}, M. and
         {Leuermann}, M. and {Lu}, L. and {L{\"u}nemann}, J. and {Madsen}, J. and
         {Maggi}, G. and {Mahn}, K.~B.~M. and {Mancina}, S. and {Maruyama}, R. and
         {Mase}, K. and {Maunu}, R. and {McNally}, F. and {Meagher}, K. and
         {Medici}, M. and {Meier}, M. and {Menne}, T. and {Merino}, G. and
         {Meures}, T. and {Miarecki}, S. and {Micallef}, J. and
         {Moment{\'e}}, G. and {Montaruli}, T. and {Moulai}, M. and
         {Nahnhauer}, R. and {Naumann}, U. and {Neer}, G. and
         {Niederhausen}, H. and {Nowicki}, S.~C. and {Nygren}, D.~R. and
         {Obertacke Pollmann}, A. and {Olivas}, A. and {O'Murchadha}, A. and
         {Palczewski}, T. and {Pandya}, H. and {Pankova}, D.~V. and
         {Peiffer}, P. and {Penek}, {\"O}. and {Pepper}, J.~A. and
         {P{\'e}rez de los Heros}, C. and {Pieloth}, D. and {Pinat}, E. and
         {Price}, P.~B. and {Przybylski}, G.~T. and {Quinnan}, M. and
         {Raab}, C. and {R{\"a}del}, L. and {Rameez}, M. and {Rawlins}, K. and
         {Reimann}, R. and {Relethford}, B. and {Relich}, M. and {Resconi}, E. and
         {Rhode}, W. and {Richman}, M. and {Riedel}, B. and {Robertson}, S. and
         {Rongen}, M. and {Rott}, C. and {Ruhe}, T. and {Ryckbosch}, D. and
         {Rysewyk}, D. and {Sabbatini}, L. and {Sanchez Herrera}, S.~E. and {Sand
        rock}, A. and {Sandroos}, J. and {Sarkar}, S. and {Satalecka}, K. and
         {Schlunder}, P. and {Schmidt}, T. and {Schoenen}, S. and
         {Sch{\"o}neberg}, S. and {Schumacher}, L. and {Seckel}, D. and
         {Seunarine}, S. and {Soldin}, D. and {Song}, M. and {Spiczak}, G.~M. and
         {Spiering}, C. and {Stachurska}, J. and {Stanev}, T. and {Stasik}, A. and
         {Stettner}, J. and {Steuer}, A. and {Stezelberger}, T. and
         {Stokstad}, R.~G. and {St{\"o}{\ss}l}, A. and {Str{\"o}m}, R. and
         {Strotjohann}, N.~L. and {Sullivan}, G.~W. and {Sutherland}, M. and
         {Taavola}, H. and {Taboada}, I. and {Tatar}, J. and {Tenholt}, F. and
         {Ter-Antonyan}, S. and {Terliuk}, A. and {Te{\v{s}}i{\'c}}, G. and
         {Tilav}, S. and {Toale}, P.~A. and {Tobin}, M.~N. and {Toscano}, S. and
         {Tosi}, D. and {Tselengidou}, M. and {Tung}, C.~F. and {Turcati}, A. and
         {Unger}, E. and {Usner}, M. and {Vandenbroucke}, J. and
         {van Eijndhoven}, N. and {Vanheule}, S. and {van Rossem}, M. and
         {van Santen}, J. and {Vehring}, M. and {Voge}, M. and {Vogel}, E. and
         {Vraeghe}, M. and {Walck}, C. and {Wallace}, A. and {Wallraff}, M. and
         {Wandkowsky}, N. and {Waza}, A. and {Weaver}, Ch. and {Weiss}, M.~J. and
         {Wendt}, C. and {Westerhoff}, S. and {Whelan}, B.~J. and
         {Wickmann}, S. and {Wiebe}, K. and {Wiebusch}, C.~H. and {Wille}, L. and
         {Williams}, D.~R. and {Wills}, L. and {Wolf}, M. and {Wood}, T.~R. and
         {Woolsey}, E. and {Woschnagg}, K. and {Xu}, D.~L. and {Xu}, X.~W. and
         {Xu}, Y. and {Yanez}, J.~P. and {Yodh}, G. and {Yoshida}, S. and
         {Zoll}, M.},
        title = "{Measurement of the {\ensuremath{\nu}} \_\{{\ensuremath{\mu}} \} energy spectrum with IceCube-79}",
      journal = {European Physical Journal C},
     keywords = {Astrophysics - High Energy Astrophysical Phenomena},
         year = "2017",
        month = "Oct",
       volume = {77},
       number = {10},
          eid = {692},
        pages = {692},
     abstract = "{IceCube is a neutrino observatory deployed in the glacial ice at the
        geographic South Pole. The {\ensuremath{\nu}}
        \_{\ensuremath{\mu}} energy unfolding described in this paper is
        based on data taken with IceCube in its 79-string configuration.
        A sample of muon neutrino charged-current interactions with a
        purity of 99.5\% was selected by means of a multivariate
        classification process based on machine learning. The subsequent
        unfolding was performed using the software Truee. The resulting
        spectrum covers an E\_{\ensuremath{\nu}} -range of more than
        four orders of magnitude from 125 GeV to 3.2 PeV. Compared to
        the Honda atmospheric neutrino flux model, the energy spectrum
        shows an excess of more than 1.9 {\ensuremath{\sigma}} in four
        adjacent bins for neutrino energies E\_{\ensuremath{\nu}}
        {\ensuremath{\geq}} 177.8 \{TeV\}. The obtained spectrum is
        fully compatible with previous measurements of the atmospheric
        neutrino flux and recent IceCube measurements of a flux of high-
        energy astrophysical neutrinos.}",
          doi = {10.1140/epjc/s10052-017-5261-3},
archivePrefix = {arXiv},
       eprint = {1705.07780},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017EPJC...77..692A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...848...23K,
       author = {{Kern}, Nicholas S. and {Liu}, Adrian and {Parsons}, Aaron R. and
         {Mesinger}, Andrei and {Greig}, Bradley},
        title = "{Emulating Simulations of Cosmic Dawn for 21 cm Power Spectrum Constraints on Cosmology, Reionization, and X-Ray Heating}",
      journal = {\apj},
     keywords = {dark ages, reionization, first stars, methods: numerical, methods: statistical, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2017",
        month = "Oct",
       volume = {848},
       number = {1},
          eid = {23},
        pages = {23},
     abstract = "{Current and upcoming radio interferometric experiments are aiming to
        make a statistical characterization of the high-redshift 21 cm
        fluctuation signal spanning the hydrogen reionization and X-ray
        heating epochs of the universe. However, connecting 21 cm
        statistics to the underlying physical parameters is complicated
        by the theoretical challenge of modeling the relevant physics at
        computational speeds quick enough to enable exploration of the
        high-dimensional and weakly constrained parameter space. In this
        work, we use machine learning algorithms to build a fast
        emulator that can accurately mimic an expensive simulation of
        the 21 cm signal across a wide parameter space. We embed our
        emulator within a Markov Chain Monte Carlo framework in order to
        perform Bayesian parameter constraints over a large number of
        model parameters, including those that govern the Epoch of
        Reionization, the Epoch of X-ray Heating, and cosmology. As a
        worked example, we use our emulator to present an updated
        parameter constraint forecast for the Hydrogen Epoch of
        Reionization Array experiment, showing that its characterization
        of a fiducial 21 cm power spectrum will considerably narrow the
        allowed parameter space of reionization and heating parameters,
        and could help strengthen Planck's constraints on
        \{{\ensuremath{\sigma}} \}$_{8}$. We provide both our
        generalized emulator code and its implementation specifically
        for 21 cm parameter constraints as publicly available software.}",
          doi = {10.3847/1538-4357/aa8bb4},
archivePrefix = {arXiv},
       eprint = {1705.04688},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...848...23K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017AJ....154..162E,
       author = {{Erasmus}, N. and {Mommert}, M. and {Trilling}, D.~E. and
         {Sickafoose}, A.~A. and {van Gend}, C. and {Hora}, J.~L.},
        title = "{Characterization of Near-Earth Asteroids Using KMTNET-SAAO}",
      journal = {\aj},
     keywords = {minor planets, asteroids: individual: near-Earth objects, surveys, techniques: photometric, Astrophysics - Earth and Planetary Astrophysics},
         year = "2017",
        month = "Oct",
       volume = {154},
       number = {4},
          eid = {162},
        pages = {162},
     abstract = "{We present here VRI spectrophotometry of 39 near-Earth asteroids (NEAs)
        observed with the Sutherland, South Africa, node of the Korea
        Microlensing Telescope Network (KMTNet). Of the 39 NEAs, 19 were
        targeted, but because of KMTNet{\textquoteright}s large
        2{\textdegree} {\texttimes} 2{\textdegree} field of view, 20
        serendipitous NEAs were also captured in the observing fields.
        Targeted observations were performed within 44 days (median: 16
        days, min: 4 days) of each NEA{\textquoteright}s discovery date.
        Our broadband spectrophotometry is reliable enough to
        distinguish among four asteroid taxonomies and we were able to
        confidently categorize 31 of the 39 observed targets as either
        an S-, C-, X-, or D-type asteroid by means of a Machine Learning
        algorithm approach. Our data suggest that the ratio between
        {\textquotedblleft}stony{\textquotedblright} S-type NEAs and
        {\textquotedblleft}not-stony{\textquotedblright} (C+X+D)-type
        NEAs, with H magnitudes between 15 and 25, is roughly 1:1.
        Additionally, we report ̃1 hr light curve data for each NEA, and
        of the 39 targets, we were able to resolve the complete rotation
        period and amplitude for six targets and report lower limits for
        the remaining targets.}",
          doi = {10.3847/1538-3881/aa88be},
archivePrefix = {arXiv},
       eprint = {1709.03305},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017AJ....154..162E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017AdSpR..60.1557X,
       author = {{Xin}, Xin and {Di}, Kaichang and {Wang}, Yexin and {Wan}, Wenhui and
         {Yue}, Zongyu},
        title = "{Automated detection of new impact sites on Martian surface from HiRISE images}",
      journal = {Advances in Space Research},
     keywords = {New impact sites, Martian surface, HiRISE, Automatic detection, AdaBoost, Machine learning},
         year = "2017",
        month = "Oct",
       volume = {60},
       number = {7},
        pages = {1557-1569},
     abstract = "{In this study, an automated method for Martian new impact site detection
        from single images is presented. It first extracts dark areas in
        full high resolution image, then detects new impact craters
        within dark areas using a cascade classifier which combines
        local binary pattern features and Haar-like features trained by
        an AdaBoost machine learning algorithm. Experimental results
        using 100 HiRISE images show that the overall detection rate of
        proposed method is 84.5\%, with a true positive rate of 86.9\%.
        The detection rate and true positive rate in the flat regions
        are 93.0\% and 91.5\%, respectively.}",
          doi = {10.1016/j.asr.2017.06.044},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017AdSpR..60.1557X},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017A&A...606A.119K,
       author = {{Kuntzer}, T. and {Courbin}, F.},
        title = "{Detecting unresolved binary stars in Euclid VIS images}",
      journal = {\aap},
     keywords = {methods: data analysis, methods: statistical, binaries: close, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Oct",
       volume = {606},
          eid = {A119},
        pages = {A119},
     abstract = "{Measuring a weak gravitational lensing signal to the level required by
        the next generation of space-based surveys demands exquisite
        reconstruction of the point-spread function (PSF). However,
        unresolved binary stars can significantly distort the PSF shape.
        In an effort to mitigate this bias, we aim at detecting
        unresolved binaries in realistic Euclid stellar populations. We
        tested methods in numerical experiments where (I) the PSF shape
        is known to Euclid requirements across the field of view; and
        (II) the PSF shape is unknown. We drew simulated catalogues of
        PSF shapes for this proof-of-concept paper. Following the Euclid
        survey plan, the objects were observed four times. We propose
        three methods to detect unresolved binary stars. The detection
        is based on the systematic and correlated biases between
        exposures of the same object. One method is a simple correlation
        analysis, while the two others use supervised machine-learning
        algorithms (random forest and artificial neural network). In
        both experiments, we demonstrate the ability of our methods to
        detect unresolved binary stars in simulated catalogues. The
        performance depends on the level of prior knowledge of the PSF
        shape and the shape measurement errors. Good detection
        performances are observed in both experiments. Full complexity,
        in terms of the images and the survey design, is not included,
        but key aspects of a more mature pipeline are discussed. Finding
        unresolved binaries in objects used for PSF reconstruction
        increases the quality of the PSF determination at arbitrary
        positions. We show, using different approaches, that we are able
        to detect at least binary stars that are most damaging for the
        PSF reconstruction process. The code corresponding to the
        algorithms used in this work and all scripts to reproduce the
        results are publicly available from a GitHub repository
        accessible via <A href=``http://lastro.epfl.ch/software''>http:/
        /lastro.epfl.ch/software</A>}",
          doi = {10.1051/0004-6361/201730792},
archivePrefix = {arXiv},
       eprint = {1708.08468},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017A&A...606A.119K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.470.1388M,
       author = {{Marchetti}, T. and {Rossi}, E.~M. and {Kordopatis}, G. and
         {Brown}, A.~G.~A. and {Rimoldi}, A. and {Starkenburg}, E. and
         {Youakim}, K. and {Ashley}, R.},
        title = "{An artificial neural network to discover hypervelocity stars: candidates in Gaia DR1/TGAS}",
      journal = {\mnras},
     keywords = {Galaxy: centre, Galaxy: kinematics and dynamics, Galaxy: stellar content, Astrophysics - Astrophysics of Galaxies, Astrophysics - Solar and Stellar Astrophysics},
         year = "2017",
        month = "Sep",
       volume = {470},
       number = {2},
        pages = {1388-1403},
     abstract = "{The paucity of hypervelocity stars (HVSs) known to date has severely
        hampered their potential to investigate the stellar population
        of the Galactic Centre and the Galactic potential. The first
        Gaia data release (DR1, 2016 September 14) gives an opportunity
        to increase the current sample. The challenge is the disparity
        between the expected number of HVSs and that of bound background
        stars. We have applied a novel data mining algorithm based on
        machine learning techniques, an artificial neural network, to
        the Tycho-Gaia astrometric solution catalogue. With no pre-
        selection of data, we could exclude immediately ̃99 per cent of
        the stars in the catalogue and find 80 candidates with more than
        90 per cent predicted probability to be HVSs, based only on
        their position, proper motions and parallax. We have cross-
        checked our findings with other spectroscopic surveys,
        determining radial velocities for 30 and spectroscopic distances
        for five candidates. In addition, follow-up observations have
        been carried out at the Isaac Newton Telescope for 22 stars, for
        which we obtained radial velocities and distance estimates. We
        discover 14 stars with a total velocity in the Galactic rest
        frame \&gt;400 km s$^{-1}$, and five of these have a probability
        of \&gt;50 per cent of being unbound from the Milky Way. Tracing
        back their orbits in different Galactic potential models, we
        find one possible unbound HVS with v ̃ 520 km s$^{-1}$, five
        bound HVSs and, notably, five runaway stars with median velocity
        between 400 and 780 km s$^{-1}$. At the moment, uncertainties in
        the distance estimates and ages are too large to confirm the
        nature of our candidates by narrowing down their ejection
        location, and we wait for future Gaia releases to validate the
        quality of our sample. This test successfully demonstrates the
        feasibility of our new data-mining routine.}",
          doi = {10.1093/mnras/stx1304},
archivePrefix = {arXiv},
       eprint = {1704.07990},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.470.1388M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017JGRA..122.9183C,
       author = {{Chu}, X. and {Bortnik}, J. and {Li}, W. and {Ma}, Q. and {Denton}, R. and
         {Yue}, C. and {Angelopoulos}, V. and {Thorne}, R.~M. and
         {Darrouzet}, F. and {Ozhogin}, P. and {Kletzing}, C.~A. and {Wang}, Y. and
         {Menietti}, J.},
        title = "{A neural network model of three-dimensional dynamic electron density in the inner magnetosphere}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {neural network, machine learning, plasma density model, erosion, refilling, plume},
         year = "2017",
        month = "Sep",
       volume = {122},
       number = {9},
        pages = {9183-9197},
     abstract = "{A plasma density model of the inner magnetosphere is important for a
        variety of applications including the study of wave-particle
        interactions, and wave excitation and propagation. Previous
        empirical models have been developed under many limiting
        assumptions and do not resolve short-term variations, which are
        especially important during storms. We present a three-
        dimensional dynamic electron density (DEN3D) model developed
        using a feedforward neural network with electron densities
        obtained from four satellite missions. The DEN3D model takes
        spacecraft location and time series of solar and geomagnetic
        indices (F$_{10.7}$, SYM-H, and AL) as inputs. It can reproduce
        the observed density with a correlation coefficient of 0.95 and
        predict test data set with error less than a factor of 2. Its
        predictive ability on out-of-sample data is tested on field-
        aligned density profiles from the IMAGE satellite. DEN3D's
        predictive ability provides unprecedented opportunities to gain
        insight into the 3-D behavior of the inner magnetospheric plasma
        density at any time and location. As an example, we apply DEN3D
        to a storm that occurred on 1 June 2013. It successfully
        reproduces various well-known dynamic features in three
        dimensions, such as plasmaspheric erosion and recovery, as well
        as plume formation. Storm time long-term density variations are
        consistent with expectations; short-term variations appear to be
        modulated by substorm activity or enhanced convection, an effect
        that requires further study together with multispacecraft in
        situ or imaging measurements. Investigating plasmaspheric
        refilling with the model, we find that it is not monotonic in
        time and is more complex than expected from previous studies,
        deserving further attention.}",
          doi = {10.1002/2017JA024464},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017JGRA..122.9183C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017GeoRL..44.9276R,
       author = {{Rouet-Leduc}, Bertrand and {Hulbert}, Claudia and {Lubbers}, Nicholas and
         {Barros}, Kipton and {Humphreys}, Colin J. and {Johnson}, Paul A.},
        title = "{Machine Learning Predicts Laboratory Earthquakes}",
      journal = {\grl},
     keywords = {machine learning, earthquake prediction, laboratory earthquakes, acoustic signal identification, earthquake precursors, Physics - Geophysics},
         year = "2017",
        month = "Sep",
       volume = {44},
       number = {18},
        pages = {9276-9282},
     abstract = "{We apply machine learning to data sets from shear laboratory
        experiments, with the goal of identifying hidden signals that
        precede earthquakes. Here we show that by listening to the
        acoustic signal emitted by a laboratory fault, machine learning
        can predict the time remaining before it fails with great
        accuracy. These predictions are based solely on the
        instantaneous physical characteristics of the acoustical signal
        and do not make use of its history. Surprisingly, machine
        learning identifies a signal emitted from the fault zone
        previously thought to be low-amplitude noise that enables
        failure forecasting throughout the laboratory quake cycle. We
        infer that this signal originates from continuous grain motions
        of the fault gouge as the fault blocks displace. We posit that
        applying this approach to continuous seismic data may lead to
        significant advances in identifying currently unknown signals,
        in providing new insights into fault physics, and in placing
        bounds on fault failure times.}",
          doi = {10.1002/2017GL074677},
archivePrefix = {arXiv},
       eprint = {1702.05774},
 primaryClass = {physics.geo-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017GeoRL..44.9276R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017GeoRL..44.9084H,
       author = {{Hosking}, J. Scott and {Fogt}, Ryan and {Thomas}, Elizabeth R. and
         {Moosavi}, Vahid and {Phillips}, Tony and {Coggins}, Jack and
         {Reusch}, David},
        title = "{Accumulation in coastal West Antarctic ice core records and the role of cyclone activity}",
      journal = {\grl},
     keywords = {cyclones, West Antarctic, ice cores, SOMs, accumulation},
         year = "2017",
        month = "Sep",
       volume = {44},
       number = {17},
        pages = {9084-9092},
     abstract = "{Cyclones are an important component of Antarctic climate variability,
        yet quantifying their impact on the polar environment is
        challenging. We assess how cyclones which pass through the
        Bellingshausen Sea affect accumulation over Ellsworth Land, West
        Antarctica, where we have two ice core records. We use self-
        organizing maps (SOMs), an unsupervised machine learning
        technique, to group cyclones into nine SOM nodes differing by
        their trajectories (1980-2015). The annual frequency of cyclones
        associated with the first SOM node (SOM1, which generally
        originate from lower latitudes over the South Pacific Ocean) is
        significantly (\&lt;fi\&gt;p \&lt;\&lt;/fi\&gt; 0.001)
        correlated with annual accumulation, with the highest seasonal
        correlations (\&lt;fi\&gt;p \&lt;\&lt;/fi\&gt; 0.001) found
        during autumn. While significant (\&lt;fi\&gt;p\&lt;/fi\&gt;
        \&lt; 0.01) increases in vertically integrated water vapor over
        the South Pacific Ocean coincide with this same group of
        cyclones, we find no indication that this has led to an increase
        in moisture advection into, nor accumulation over, Ellsworth
        Land over this short time period.}",
          doi = {10.1002/2017GL074722},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017GeoRL..44.9084H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017AdSpR..60.1271D,
       author = {{Das}, Saurabh and {Chakraborty}, Rohit and {Maitra}, Animesh},
        title = "{A random forest algorithm for nowcasting of intense precipitation events}",
      journal = {Advances in Space Research},
     keywords = {Nowcasting, Machine learning, Random forest, Convective rain, Microwave radiometer},
         year = "2017",
        month = "Sep",
       volume = {60},
       number = {6},
        pages = {1271-1282},
     abstract = "{Automatic nowcasting of convective initiation and thunderstorms has
        potential applications in several sectors including aviation
        planning and disaster management. In this paper, random forest
        based machine learning algorithm is tested for nowcasting of
        convective rain with a ground based radiometer. Brightness
        temperatures measured at 14 frequencies (7 frequencies in 22-31
        GHz band and 7 frequencies in 51-58 GHz bands) are utilized as
        the inputs of the model. The lower frequency band is associated
        to the water vapor absorption whereas the upper frequency band
        relates to the oxygen absorption and hence, provide information
        on the temperature and humidity of the atmosphere. Synthetic
        minority over-sampling technique is used to balance the data set
        and 10-fold cross validation is used to assess the performance
        of the model. Results indicate that random forest algorithm with
        fixed alarm generation time of 30 min and 60 min performs quite
        well (probability of detection of all types of weather condition
        {\ensuremath{\sim}}90\%) with low false alarms. It is, however,
        also observed that reducing the alarm generation time improves
        the threat score significantly and also decreases false alarms.
        The proposed model is found to be very sensitive to the boundary
        layer instability as indicated by the variable importance
        measure. The study shows the suitability of a random forest
        algorithm for nowcasting application utilizing a large number of
        input parameters from diverse sources and can be utilized in
        other forecasting problems.}",
          doi = {10.1016/j.asr.2017.03.026},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017AdSpR..60.1271D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017A&A...605A.123P,
       author = {{P{\'e}rez-Ortiz}, M.~F. and {Garc{\'\i}a-Varela}, A. and
         {Quiroz}, A.~J. and {Sabogal}, B.~E. and {Hern{\'a}ndez}, J.},
        title = "{Machine learning techniques to select Be star candidates. An application in the OGLE-IV Gaia south ecliptic pole field}",
      journal = {\aap},
     keywords = {methods: statistical, stars: variables: general, stars: emission-line, Be, catalogs, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Sep",
       volume = {605},
          eid = {A123},
        pages = {A123},
     abstract = "{Context. Optical and infrared variability surveys produce a large number
        of high quality light curves. Statistical pattern recognition
        methods have provided competitive solutions for variable star
        classification at a relatively low computational cost. In order
        to perform supervised classification, a set of features is
        proposed and used to train an automatic classification system.
        Quantities related to the magnitude density of the light curves
        and their Fourier coefficients have been chosen as features in
        previous studies. However, some of these features are not robust
        to the presence of outliers and the calculation of Fourier
        coefficients is computationally expensive for large data sets.
        <BR /> Aims: We propose and evaluate the performance of a new
        robust set of features using supervised classifiers in order to
        look for new Be star candidates in the OGLE-IV Gaia south
        ecliptic pole field. <BR /> Methods: We calculated the proposed
        set of features on six types of variable stars and also on a set
        of Be star candidates reported in the literature. We evaluated
        the performance of these features using classification trees and
        random forests along with the K-nearest neighbours, support
        vector machines, and gradient boosted trees methods. We tuned
        the classifiers with a 10-fold cross-validation and grid search.
        We then validated the performance of the best classifier on a
        set of OGLE-IV light curves and applied this to find new Be star
        candidates. <BR /> Results: The random forest classifier
        outperformed the others. By using the random forest classifier
        and colours criteria we found 50 Be star candidates in the
        direction of the Gaia south ecliptic pole field, four of which
        have infrared colours that are consistent with Herbig Ae/Be
        stars. <BR /> Conclusions: Supervised methods are very useful in
        order to obtain preliminary samples of variable stars extracted
        from large databases. As usual, the stars classified as Be stars
        candidates must be checked for the colours and spectroscopic
        characteristics expected for them.}",
          doi = {10.1051/0004-6361/201628937},
archivePrefix = {arXiv},
       eprint = {1707.04560},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017A&A...605A.123P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2017PhDT........46W,
       author = {{Watson}, Andrew William},
        title = "{Transverse Position Reconstruction in a Liquid Argon Time Projection Chamber using Principal Component Analysis and Multi-Dimensional Fitting}",
     keywords = {High energy physics;Astrophysics},
       school = {Temple University},
         year = "2017",
        month = "Aug",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017PhDT........46W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.469.2024S,
       author = {{Smirnov}, Evgeny A. and {Markov}, Alexey B.},
        title = "{Identification of asteroids trapped inside three-body mean motion resonances: a machine-learning approach}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: numerical, minor planets, asteroids: general},
         year = "2017",
        month = "Aug",
       volume = {469},
       number = {2},
        pages = {2024-2031},
     abstract = "{In this paper, we apply the following machine learning methods that do
        not require numerical integration - namely, k-nearest
        neighbours, decision tree, gradient boosting and logistic
        regression - for identifying three-body resonant asteroids in
        the main belt. It is shown that the results of the
        identification by machine learning methods are accurate and take
        significantly less time than numerical integration (seconds
        versus days). We have identified 404 new asteroids subjected to
        the three-body resonance 4J-2S-1 using a machine learning
        methodology.}",
          doi = {10.1093/mnras/stx999},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.469.2024S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017JCAP...08..028C,
       author = {{Ciuca}, Razvan and {Hern{\'a}ndez}, Oscar F.},
        title = "{A Bayesian framework for cosmic string searches in CMB maps}",
      journal = {Journal of Cosmology and Astro-Particle Physics},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, General Relativity and Quantum Cosmology, High Energy Physics - Phenomenology, High Energy Physics - Theory},
         year = "2017",
        month = "Aug",
       volume = {2017},
       number = {8},
          eid = {028},
        pages = {028},
     abstract = "{There exists various proposals to detect cosmic strings from Cosmic
        Microwave Background (CMB) or 21 cm temperature maps. Current
        proposals do not aim to find the location of strings on sky
        maps, all of these approaches can be thought of as a statistic
        on a sky map. We propose a Bayesian interpretation of cosmic
        string detection and within that framework, we derive a
        connection between estimates of cosmic string locations and
        cosmic string tension G{\ensuremath{\mu}}. We use this Bayesian
        framework to develop a machine learning framework for detecting
        strings from sky maps and outline how to implement this
        framework with neural networks. The neural network we trained
        was able to detect and locate cosmic strings on noiseless CMB
        temperature map down to a string tension of
        G{\ensuremath{\mu}}=5 {\texttimes}10$^{-9}$ and when analyzing a
        CMB temperature map that does not contain strings, the neural
        network gives a 0.95 probability that
        G{\ensuremath{\mu}}\&lt;=2.3{\texttimes}10$^{-9}$.}",
          doi = {10.1088/1475-7516/2017/08/028},
archivePrefix = {arXiv},
       eprint = {1706.04131},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017JCAP...08..028C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...845..147B,
       author = {{Benavente}, Patricio and {Protopapas}, Pavlos and {Pichara}, Karim},
        title = "{Automatic Survey-invariant Classification of Variable Stars}",
      journal = {\apj},
     keywords = {methods: data analysis, methods: statistical, stars: statistics, stars: variables: general, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Aug",
       volume = {845},
       number = {2},
          eid = {147},
        pages = {147},
     abstract = "{Machine learning techniques have been successfully used to classify
        variable stars on widely studied astronomical surveys. These
        data sets have been available to astronomers long enough, thus
        allowing them to perform deep analysis over several variable
        sources and generating useful catalogs with identified variable
        stars. The products of these studies are labeled data that
        enable supervised learning models to be trained successfully.
        However, when these models are blindly applied to data from new
        sky surveys, their performance drops significantly. Furthermore,
        unlabeled data become available at a much higher rate than their
        labeled counterpart, since labeling is a manual and time-
        consuming effort. Domain adaptation techniques aim to learn from
        a domain where labeled data are available, the source domain,
        and through some adaptation perform well on a different domain,
        the target domain. We propose a full probabilistic model that
        represents the joint distribution of features from two surveys,
        as well as a probabilistic transformation of the features from
        one survey to the other. This allows us to transfer labeled data
        to a study where they are not available and to effectively run a
        variable star classification model in a new survey. Our model
        represents the features of each domain as a Gaussian mixture and
        models the transformation as a translation, rotation, and
        scaling of each separate component. We perform tests using three
        different variability catalogs, EROS, MACHO, and HiTS,
        presenting differences among them, such as the number of
        observations per star, cadence, observational time, and optical
        bands observed, among others.}",
          doi = {10.3847/1538-4357/aa7f2d},
archivePrefix = {arXiv},
       eprint = {1801.09737},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...845..147B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017A&A...604A.134D,
       author = {{de Jong}, Jelte T.~A. and {Verdoes Kleijn}, Gijs A. and
         {Erben}, Thomas and {Hildebrandt}, Hendrik and {Kuijken}, Konrad and
         {Sikkema}, Gert and {Brescia}, Massimo and {Bilicki}, Maciej and
         {Napolitano}, Nicola R. and {Amaro}, Valeria and {Begeman}, Kor G. and
         {Boxhoorn}, Danny R. and {Buddelmeijer}, Hugo and {Cavuoti}, Stefano and
         {Getman}, Fedor and {Grado}, Aniello and {Helmich}, Ewout and
         {Huang}, Zhuoyi and {Irisarri}, Nancy and {La Barbera}, Francesco and
         {Longo}, Giuseppe and {McFarland}, John P. and {Nakajima}, Reiko and
         {Paolillo}, Maurizio and {Puddu}, Emanuella and {Radovich}, Mario and
         {Rifatto}, Agatino and {Tortora}, Crescenzo and {Valentijn}, Edwin A. and
         {Vellucci}, Civita and {Vriend}, Willem-Jan and {Amon}, Alexandra and
         {Blake}, Chris and {Choi}, Ami and {Conti}, Ian Fenech and
         {Gwyn}, Stephen D.~J. and {Herbonnet}, Ricardo and
         {Heymans}, Catherine and {Hoekstra}, Henk and {Klaes}, Dominik and
         {Merten}, Julian and {Miller}, Lance and {Schneider}, Peter and
         {Viola}, Massimo},
        title = "{The third data release of the Kilo-Degree Survey and associated data products}",
      journal = {\aap},
     keywords = {surveys, catalogs, techniques: photometric, techniques: image processing, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Aug",
       volume = {604},
          eid = {A134},
        pages = {A134},
     abstract = "{Context. The Kilo-Degree Survey (KiDS) is an ongoing optical wide-field
        imaging survey with the OmegaCAM camera at the VLT Survey
        Telescope. It aims to image 1500 square degrees in four filters
        (ugri). The core science driver is mapping the large-scale
        matter distribution in the Universe, using weak lensing shear
        and photometric redshift measurements. Further science cases
        include galaxy evolution, Milky Way structure, detection of
        high-redshift clusters, and finding rare sources such as strong
        lenses and quasars. <BR /> Aims: Here we present the third
        public data release and several associated data products, adding
        further area, homogenized photometric calibration, photometric
        redshifts and weak lensing shear measurements to the first two
        releases. <BR /> Methods: A dedicated pipeline embedded in the
        Astro-WISE information system is used for the production of the
        main release. Modifications with respect to earlier releases are
        described in detail. Photometric redshifts have been derived
        using both Bayesian template fitting, and machine-learning
        techniques. For the weak lensing measurements, optimized
        procedures based on the THELI data reduction and lensfit shear
        measurement packages are used. <BR /> Results: In this third
        data release an additional 292 new survey tiles
        ({\ensuremath{\approx}}300 deg$^{2}$) stacked ugri images are
        made available, accompanied by weight maps, masks, and source
        lists. The multi-band catalogue, including homogenized
        photometry and photometric redshifts, covers the combined DR1,
        DR2 and DR3 footprint of 440 survey tiles (44 deg$^{2}$).
        Limiting magnitudes are typically 24.3, 25.1, 24.9, 23.8
        (5{\ensuremath{\sigma}} in a 2`` aperture) in ugri,
        respectively, and the typical r-band PSF size is less than
        0.7''. The photometric homogenization scheme ensures accurate
        colours and an absolute calibration stable to
        {\ensuremath{\approx}}2\% for gri and {\ensuremath{\approx}}3\%
        in u. Separately released for the combined area of all KiDS
        releases to date are a weak lensing shear catalogue and
        photometric redshifts based on two different machine-learning
        techniques.}",
          doi = {10.1051/0004-6361/201730747},
archivePrefix = {arXiv},
       eprint = {1703.02991},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017A&A...604A.134D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.469.1205S,
       author = {{Speagle}, Joshua S. and {Eisenstein}, Daniel J.},
        title = "{Deriving photometric redshifts using fuzzy archetypes and self-organizing maps - II. Implementation}",
      journal = {\mnras},
     keywords = {methods: statistical, techniques: photometric, galaxies: distances and redshifts},
         year = "2017",
        month = "Jul",
       volume = {469},
       number = {1},
        pages = {1205-1224},
     abstract = "{With an eye towards the computational requirements of future large-scale
        surveys such as Euclid and Large Synoptic Survey Telescope
        (LSST) that will require photometric redshifts (photo-z's) for
        {\ensuremath{\gtrsim}} {}10$^{9}$ objects, we investigate a
        variety of ways that 'fuzzy archetypes' can be used to improve
        photometric redshifts and explore their respective statistical
        interpretations. We characterize their relative performance
        using an idealized LSST ugrizY and Euclid YJH mock catalogue of
        10 000 objects spanning z = 0-6 at Y = 24 mag. We find most
        schemes are able to robustly identify redshift probability
        distribution functions that are multimodal and/or poorly
        constrained. Once these objects are flagged and removed, the
        results are generally in good agreement with the strict accuracy
        requirements necessary to meet Euclid weak lensing goals for
        most redshifts between 0.8 {\ensuremath{\lesssim}} z
        {\ensuremath{\lesssim}} 2. These results demonstrate the
        statistical robustness and flexibility that can be gained by
        combining template-fitting and machine-learning methods and
        provide useful insights into how astronomers can further exploit
        the colour-redshift relation.}",
          doi = {10.1093/mnras/stx510},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.469.1205S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.468.4323B,
       author = {{Beck}, R. and {Lin}, C. -A. and {Ishida}, E.~E.~O. and {Gieseke}, F. and
         {de Souza}, R.~S. and {Costa-Duarte}, M.~V. and {Hattab}, M.~W. and
         {Krone-Martins}, A.},
        title = "{On the realistic validation of photometric redshifts}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, techniques: photometric, catalogues, galaxies: distances and redshifts, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Jul",
       volume = {468},
       number = {4},
        pages = {4323-4339},
     abstract = "{Two of the main problems encountered in the development and accurate
        validation of photometric redshift (photo-z) techniques are the
        lack of spectroscopic coverage in the feature space (e.g.
        colours and magnitudes) and the mismatch between the photometric
        error distributions associated with the spectroscopic and
        photometric samples. Although these issues are well known, there
        is currently no standard benchmark allowing a quantitative
        analysis of their impact on the final photo-z estimation. In
        this work, we present two galaxy catalogues, Teddy and Happy,
        built to enable a more demanding and realistic test of photo-z
        methods. Using photometry from the Sloan Digital Sky Survey and
        spectroscopy from a collection of sources, we constructed data
        sets that mimic the biases between the underlying probability
        distribution of the real spectroscopic and photometric sample.
        We demonstrate the potential of these catalogues by submitting
        them to the scrutiny of different photo-z methods, including
        machine learning (ML) and template fitting approaches. Beyond
        the expected bad results from most ML algorithms for cases with
        missing coverage in the feature space, we were able to recognize
        the superiority of global models in the same situation and the
        general failure across all types of methods when incomplete
        coverage is convoluted with the presence of photometric errors -
        a data situation which photo-z methods were not trained to deal
        with up to now and which must be addressed by future large-scale
        surveys. Our catalogues represent the first controlled
        environment allowing a straightforward implementation of such
        tests. The data are publicly available within the COINtoolbox
        (https://github.com/COINtoolbox/photoz\_catalogues).}",
          doi = {10.1093/mnras/stx687},
archivePrefix = {arXiv},
       eprint = {1701.08748},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.468.4323B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017JGRA..122.7118C,
       author = {{Chu}, X.~N. and {Bortnik}, J. and {Li}, W. and {Ma}, Q. and
         {Angelopoulos}, V. and {Thorne}, R.~M.},
        title = "{Erosion and refilling of the plasmasphere during a geomagnetic storm modeled by a neural network}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {neural network, machine learning, plasma density model, erosion, refilling, plume},
         year = "2017",
        month = "Jul",
       volume = {122},
       number = {7},
        pages = {7118-7129},
     abstract = "{We present a history-dependent model of the equatorial plasma density of
        the inner magnetosphere using a feedforward neural network with
        two hidden layers. As the model inputs, we take locations and
        time series of SYM-H, AL, and F$_{10.7}$ indices. By considering
        not only the instantaneous values but also the past values of
        geomagnetic and solar indices, the model is history dependent on
        levels of geomagnetic and solar activity. The modeled electron
        density is continuous both spatially and temporally so that the
        evolution of the density can be studied (such as plasmaspheric
        refilling). The model is trained using the electron density
        inferred from the spacecraft potential from three THEMIS probes.
        The equatorial electron density is shown to be accurately
        reconstructed with a correlation coefficient of r 0.953 between
        data and model target. Since the model is history dependent, it
        succeeds in reconstructing various density features and dynamic
        behaviors, such as the quiet time plasmasphere, erosion and
        recovery of the plasmasphere, as well as the plume formation
        during a storm on 4 February 2011. Our model may provide
        unprecedented insight into the behavior of the equatorial
        density at any time and location; as an example we show the
        inferred refilling rate from our model and compare it to
        previous estimates.}",
          doi = {10.1002/2017JA023948},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017JGRA..122.7118C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017JASTP.160...11D,
       author = {{da Silva}, Maur{\'\i}cio Bruno Prado and
         {Francisco Escobedo}, Jo{\~a}o and {Juliana Rossi}, Taiza and
         {dos Santos}, C{\'\i}cero Manoel and
         {da Silva}, S{\'\i}lvia Helena Modenese Gorla},
        title = "{Performance of the Angstrom-Prescott Model (A-P) and SVM and ANN techniques to estimate daily global solar irradiation in Botucatu/SP/Brazil}",
      journal = {Journal of Atmospheric and Solar-Terrestrial Physics},
     keywords = {Solar radiation, Angstrom-Prescott, Statistical modeling, Artificial intelligence, Meteorological variables},
         year = "2017",
        month = "Jul",
       volume = {160},
        pages = {11-23},
     abstract = "{This study describes the comparative study of different methods for
        estimating daily global solar irradiation (H): Angstrom-Prescott
        (A-P) model and two Machine Learning techniques (ML) - Support
        Vector Machine (SVM) and Artificial Neural Network (ANN). The H
        database was measured from 1996 to 2011 in Botucatu/SP/Brazil.
        Different combinations of input variables were adopted. MBE,
        RMSE, d Willmott, r and r$^{2}$ statistical indicators obtained
        in the validation of A-P and SVM and ANN models showed that: SVM
        technique has better performance in estimating H than A-P and
        ANN models. A-P model has better performance in estimating H
        than ANN.}",
          doi = {10.1016/j.jastp.2017.04.001},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017JASTP.160...11D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ChA&A..41..331G,
       author = {{Gao}, Wei and {Li}, Xiang-ru},
        title = "{Application of Multi-task Sparse Lasso Feature Extraction and Support Vector Machine Regression in the Stellar Atmospheric Parameterization}",
      journal = {\caa},
     keywords = {stars: fundamental parameters, methods: data analysis, methods: statistical, methods: miscellaneous},
         year = "2017",
        month = "Jul",
       volume = {41},
       number = {3},
        pages = {331-346},
     abstract = "{The multi-task learning takes the multiple tasks together to make
        analysis and calculation, so as to dig out the correlations
        among them, and therefore to improve the accuracy of the
        analyzed results. This kind of methods have been widely applied
        to the machine learning, pattern recognition, computer vision,
        and other related fields. This paper investigates the
        application of multi-task learning in estimating the stellar
        atmospheric parameters, including the surface temperature
        (T$_{eff}$), surface gravitational acceleration (lg g), and
        chemical abundance ([Fe/H]). Firstly, the spectral features of
        the three stellar atmospheric parameters are extracted by using
        the multi-task sparse group Lasso algorithm, then the support
        vector machine is used to estimate the atmospheric physical
        parameters. The proposed scheme is evaluated on both the Sloan
        stellar spectra and the theoretical spectra computed from the
        Kurucz's New Opacity Distribution Function (NEWODF) model. The
        mean absolute errors (MAEs) on the Sloan spectra are: 0.0064 for
        lg (T$_{eff}$ /K), 0.1622 for lg (g/(cm {\textperiodcentered}
        s$^{-2}$)), and 0.1221 dex for [Fe/H]; the MAEs on the synthetic
        spectra are 0.0006 for lg (T$_{eff}$ /K), 0.0098 for lg (g/(cm
        {\textperiodcentered} s$^{-2}$)), and 0.0082 dex for [Fe/H].
        Experimental results show that the proposed scheme has a rather
        high accuracy for the estimation of stellar atmospheric
        parameters.}",
          doi = {10.1016/j.chinastron.2017.08.004},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ChA&A..41..331G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ChA&A..41..318P,
       author = {{Pan}, Ru-yang and {Li}, Xiang-ru},
        title = "{Stellar Atmospheric Parameterization Based on Deep Learning}",
      journal = {\caa},
     keywords = {stars: fundamental parameters, stars: atmospheres, stars: abundances, methods: data analysis, methods: statistical},
         year = "2017",
        month = "Jul",
       volume = {41},
       number = {3},
        pages = {318-330},
     abstract = "{Deep learning is a typical learning method widely studied in the fields
        of machine learning, pattern recognition, and artificial
        intelligence. This work investigates the problem of stellar
        atmospheric parameterization by constructing a deep neural
        network with five layers, and the node number in each layer of
        the network is respectively 3821-500-100-50-1. The proposed
        scheme is verified on both the real spectra measured by the
        Sloan Digital Sky Survey (SDSS) and the theoretic spectra
        computed with the Kurucz's New Opacity Distribution Function
        (NEWODF) model, to make an automatic estimation for three
        physical parameters: the effective temperature (T$_{eff}$),
        surface gravitational acceleration (lg g), and metallic
        abundance (Fe/H). The results show that the stacked autoencoder
        deep neural network has a better accuracy for the estimation. On
        the SDSS spectra, the mean absolute errors (MAEs) are 79.95 for
        T$_{eff}$/K, 0.0058 for (lg T$_{eff}$/K), 0.1706 for lg
        (g/(cm{\textperiodcentered}s$^{-2}$)), and 0.1294 dex for the
        [Fe/H], respectively; On the theoretic spectra, the MAEs are
        15.34 for T$_{eff}$/K, 0.0011 for lg (T$_{eff}$/K), 0.0214 for
        lg(g/(cm {\textperiodcentered} s$^{-2}$)), and 0.0121 dex for
        [Fe/H], respectively.}",
          doi = {10.1016/j.chinastron.2017.08.003},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ChA&A..41..318P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...843..104L,
       author = {{Liu}, Chang and {Deng}, Na and {Wang}, Jason T.~L. and {Wang}, Haimin},
        title = "{Predicting Solar Flares Using SDO/HMI Vector Magnetic Data Products and the Random Forest Algorithm}",
      journal = {\apj},
     keywords = {magnetic fields, Sun: activity, Sun: flares, Astrophysics - Solar and Stellar Astrophysics},
         year = "2017",
        month = "Jul",
       volume = {843},
       number = {2},
          eid = {104},
        pages = {104},
     abstract = "{Adverse space-weather effects can often be traced to solar flares, the
        prediction of which has drawn significant research interests.
        The Helioseismic and Magnetic Imager (HMI) produces full-disk
        vector magnetograms with continuous high cadence, while flare
        prediction efforts utilizing this unprecedented data source are
        still limited. Here we report results of flare prediction using
        physical parameters provided by the Space-weather HMI Active
        Region Patches (SHARP) and related data products. We survey
        X-ray flares that occurred from 2010 May to 2016 December and
        categorize their source regions into four classes (B, C, M, and
        X) according to the maximum GOES magnitude of flares they
        generated. We then retrieve SHARP-related parameters for each
        selected region at the beginning of its flare date to build a
        database. Finally, we train a machine-learning algorithm, called
        random forest (RF), to predict the occurrence of a certain class
        of flares in a given active region within 24 hr, evaluate the
        classifier performance using the 10-fold cross-validation
        scheme, and characterize the results using standard performance
        metrics. Compared to previous works, our experiments indicate
        that using the HMI parameters and RF is a valid method for flare
        forecasting with fairly reasonable prediction performance. To
        our knowledge, this is the first time that RF has been used to
        make multiclass predictions of solar flares. We also find that
        the total unsigned quantities of vertical current, current
        helicity, and flux near the polarity inversion line are among
        the most important parameters for classifying flaring regions
        into different classes.}",
          doi = {10.3847/1538-4357/aa789b},
archivePrefix = {arXiv},
       eprint = {1706.02422},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...843..104L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017A&A...603A.117S,
       author = {{S{\"u}veges}, M. and {Barblan}, F. and {Lecoeur-Ta{\"\i}bi}, I. and
         {Pr{\v{s}}a}, A. and {Holl}, B. and {Eyer}, L. and {Kochoska}, A. and
         {Mowlavi}, N. and {Rimoldini}, L.},
        title = "{Gaia eclipsing binary and multiple systems. Supervised classification and self-organizing maps}",
      journal = {\aap},
     keywords = {methods: data analysis, methods: statistical, binaries: eclipsing, surveys, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2017",
        month = "Jul",
       volume = {603},
          eid = {A117},
        pages = {A117},
     abstract = "{Context. Large surveys producing tera- and petabyte-scale databases
        require machine-learning and knowledge discovery methods to deal
        with the overwhelming quantity of data and the difficulties of
        extracting concise, meaningful information with reliable
        assessment of its uncertainty. This study investigates the
        potential of a few machine-learning methods for the automated
        analysis of eclipsing binaries in the data of such surveys. <BR
        /> Aims: We aim to aid the extraction of samples of eclipsing
        binaries from such databases and to provide basic information
        about the objects. We intend to estimate class labels according
        to two different, well-known classification systems, one based
        on the light curve morphology (EA/EB/EW classes) and the other
        based on the physical characteristics of the binary system
        (system morphology classes; detached through overcontact
        systems). Furthermore, we explore low-dimensional surfaces along
        which the light curves of eclipsing binaries are concentrated,
        and consider their use in the characterization of the binary
        systems and in the exploration of biases of the full unknown
        Gaia data with respect to the training sets. <BR /> Methods: We
        have explored the performance of principal component analysis
        (PCA), linear discriminant analysis (LDA), Random Forest
        classification and self-organizing maps (SOM) for the above
        aims. We pre-processed the photometric time series by combining
        a double Gaussian profile fit and a constrained smoothing
        spline, in order to de-noise and interpolate the observed light
        curves. We achieved further denoising, and selected the most
        important variability elements from the light curves using PCA.
        Supervised classification was performed using Random Forest and
        LDA based on the PC decomposition, while SOM gives a continuous
        2-dimensional manifold of the light curves arranged by a few
        important features. We estimated the uncertainty of the
        supervised methods due to the specific finite training set using
        ensembles of models constructed on randomized training sets. <BR
        /> Results: We obtain excellent results (about 5\% global error
        rate) with classification into light curve morphology classes on
        the Hipparcos data. The classification into system morphology
        classes using the Catalog and Atlas of Eclipsing binaries
        (CALEB) has a higher error rate (about 10.5\%), most importantly
        due to the (sometimes strong) similarity of the photometric
        light curves originating from physically different systems. When
        trained on CALEB and then applied to Kepler-detected eclipsing
        binaries subsampled according to Gaia observing times, LDA and
        SOM provide tractable, easy-to-visualize subspaces of the full
        (functional) space of light curves that summarize the most
        important phenomenological elements of the individual light
        curves. The sequence of light curves ordered by their first
        linear discriminant coefficient is compared to results obtained
        using local linear embedding. The SOM method proves able to find
        a 2-dimensional embedded surface in the space of the light
        curves which separates the system morphology classes in its
        different regions, and also identifies a few other phenomena,
        such as the asymmetry of the light curves due to spots,
        eccentric systems, and systems with a single eclipse.
        Furthermore, when data from other surveys are projected to the
        same SOM surface, the resulting map yields a good overview of
        the general biases and distortions due to differences in time
        sampling or population.}",
          doi = {10.1051/0004-6361/201629710},
archivePrefix = {arXiv},
       eprint = {1702.06296},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017A&A...603A.117S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017A&A...603A..60F,
       author = {{Frontera-Pons}, J. and {Sureau}, F. and {Bobin}, J. and {Le Floc'h}, E.},
        title = "{Unsupervised feature-learning for galaxy SEDs with denoising autoencoders}",
      journal = {\aap},
     keywords = {methods: statistical, galaxies: star formation, methods: data analysis, galaxies: photometry, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2017",
        month = "Jul",
       volume = {603},
          eid = {A60},
        pages = {A60},
     abstract = "{With the increasing number of deep multi-wavelength galaxy surveys, the
        spectral energy distribution (SED) of galaxies has become an
        invaluable tool for studying the formation of their structures
        and their evolution. In this context, standard analysis relies
        on simple spectro-photometric selection criteria based on a few
        SED colors. If this fully supervised classification already
        yielded clear achievements, it is not optimal to extract
        relevant information from the data. In this article, we propose
        to employ very recent advances in machine learning, and more
        precisely in feature learning, to derive a data-driven diagram.
        We show that the proposed approach based on denoising
        autoencoders recovers the bi-modality in the galaxy population
        in an unsupervised manner, without using any prior knowledge on
        galaxy SED classification. This technique has been compared to
        principal component analysis (PCA) and to standard color/color
        representations. In addition, preliminary results illustrate
        that this enables the capturing of extra physically meaningful
        information, such as redshift dependence, galaxy mass evolution
        and variation over the specific star formation rate. PCA also
        results in an unsupervised representation with physical
        properties, such as mass and sSFR, although this representation
        separates out less other characteristics (bimodality, redshift
        evolution) than denoising autoencoders.}",
          doi = {10.1051/0004-6361/201630240},
archivePrefix = {arXiv},
       eprint = {1705.05620},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017A&A...603A..60F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJS..230...20A,
       author = {{Aniyan}, A.~K. and {Thorat}, K.},
        title = "{Classifying Radio Galaxies with the Convolutional Neural Network}",
      journal = {\apjs},
     keywords = {methods: miscellaneous, methods: observational, radio continuum: galaxies, techniques: miscellaneous, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Jun",
       volume = {230},
       number = {2},
          eid = {20},
        pages = {20},
     abstract = "{We present the application of a deep machine learning technique to
        classify radio images of extended sources on a morphological
        basis using convolutional neural networks (CNN). In this study,
        we have taken the case of the Fanaroff-Riley (FR) class of radio
        galaxies as well as radio galaxies with bent-tailed morphology.
        We have used archival data from the Very Large Array
        (VLA){\textemdash}Faint Images of the Radio Sky at Twenty
        Centimeters survey and existing visually classified samples
        available in the literature to train a neural network for
        morphological classification of these categories of radio
        sources. Our training sample size for each of these categories
        is ̃200 sources, which has been augmented by rotated versions of
        the same. Our study shows that CNNs can classify images of the
        FRI and FRII and bent-tailed radio galaxies with high accuracy
        (maximum precision at 95\%) using well-defined samples and a
        {\textquotedblleft}fusion classifier,{\textquotedblright} which
        combines the results of binary classifications, while allowing
        for a mechanism to find sources with unusual morphologies. The
        individual precision is highest for bent-tailed radio galaxies
        at 95\% and is 91\% and 75\% for the FRI and FRII classes,
        respectively, whereas the recall is highest for FRI and FRIIs at
        91\% each, while the bent-tailed class has a recall of 79\%.
        These results show that our results are comparable to that of
        manual classification, while being much faster. Finally, we
        discuss the computational and data-related challenges associated
        with the morphological classification of radio galaxies with
        CNNs.}",
          doi = {10.3847/1538-4365/aa7333},
archivePrefix = {arXiv},
       eprint = {1705.03413},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJS..230...20A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017AJ....153..249H,
       author = {{Holoien}, Thomas W. -S. and {Marshall}, Philip J. and
         {Wechsler}, Risa H.},
        title = "{EmpiriciSN: Re-sampling Observed Supernova/Host Galaxy Populations Using an XD Gaussian Mixture Model}",
      journal = {\aj},
     keywords = {methods: statistical, supernovae: general, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Jun",
       volume = {153},
       number = {6},
          eid = {249},
        pages = {249},
     abstract = "{We describe two new open-source tools written in Python for performing
        extreme deconvolution Gaussian mixture modeling (XDGMM) and
        using a conditioned model to re-sample observed supernova and
        host galaxy populations. XDGMM is new program that uses Gaussian
        mixtures to perform density estimation of noisy data using
        extreme deconvolution (XD) algorithms. Additionally, it has
        functionality not available in other XD tools. It allows the
        user to select between the AstroML and Bovy et al. fitting
        methods and is compatible with scikit-learn machine learning
        algorithms. Most crucially, it allows the user to condition a
        model based on the known values of a subset of parameters. This
        gives the user the ability to produce a tool that can predict
        unknown parameters based on a model that is conditioned on known
        values of other parameters. EmpiriciSN is an exemplary
        application of this functionality, which can be used to fit an
        XDGMM model to observed supernova/host data sets and predict
        likely supernova parameters using a model conditioned on
        observed host properties. It is primarily intended to simulate
        realistic supernovae for LSST data simulations based on
        empirical galaxy properties.}",
          doi = {10.3847/1538-3881/aa68a1},
archivePrefix = {arXiv},
       eprint = {1611.00363},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017AJ....153..249H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017A&A...602A..80D,
       author = {{Drews}, Ainar and {Rouppe van der Voort}, Luc},
        title = "{Microjets in the penumbra of a sunspot}",
      journal = {\aap},
     keywords = {Sun: atmosphere, Sun: chromosphere, Sun: photosphere, sunspots, Sun: magnetic fields, Astrophysics - Solar and Stellar Astrophysics},
         year = "2017",
        month = "Jun",
       volume = {602},
          eid = {A80},
        pages = {A80},
     abstract = "{Context. Penumbral microjets (PMJs) are short-lived jets found in the
        penumbra of sunspots, first observed in wide-band Ca II H line
        observations as localized brightenings, and are thought to be
        caused by magnetic reconnection. Earlier work on PMJs has
        focused on smaller samples of by-eye selected events and case
        studies. <BR /> Aims: It is our goal to present an automated
        study of a large sample of PMJs to place the basic statistics of
        PMJs on a sure footing and to study the PMJ Ca II 8542 {\r{A}}
        spectral profile in detail. <BR /> Methods: High spatial
        resolution and spectrally well-sampled observations in the Ca II
        8542 {\r{A}} line obtained from the Swedish 1-m Solar Telescope
        (SST) were reduced by a principle component analysis and
        subsequently used in the automated detection of PMJs using the
        simple machine learning algorithm k-nearest neighbour. PMJ
        detections were verified with co-temporal Ca II H line
        observations. <BR /> Results: We find a total of 453 tracked PMJ
        events, 4253 PMJs detections tallied over all timeframes, and a
        detection rate of 21 events per timestep. From these, an average
        length, width and lifetime of 640 km, 210 km and 90 s are
        obtained. The average PMJ Ca II 8542 {\r{A}} line profile is
        characterized by enhanced inner wings, often in the form of one
        or two distinct peaks, and a brighter line core as compared to
        the quiet-Sun average. Average blue and red peak positions are
        determined at - 10.4 km s$^{-1}$ and + 10.2 km s$^{-1}$ offsets
        from the Ca II 8542 {\r{A}} line core. We find several clusters
        of PMJ hot-spots within the sunspot penumbra, in which PMJ
        events occur in the same general area repeatedly over time. <BR
        /> Conclusions: Our results indicate smaller average PMJs sizes
        and longer lifetimes compared to previously published values,
        but with statistics still in the same orders of magnitude. The
        investigation and analysis of the PMJ line profiles strengthens
        the proposed heating of PMJs to transition region temperatures.
        The presented statistics on PMJs form a solid basis for future
        investigations and numerical modelling of PMJs.}",
          doi = {10.1051/0004-6361/201630312},
archivePrefix = {arXiv},
       eprint = {1702.06078},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017A&A...602A..80D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017PhRvD..95j4059M,
       author = {{Mukund}, N. and {Abraham}, S. and {Kandhasamy}, S. and {Mitra}, S. and
         {Philip}, N.~S.},
        title = "{Transient classification in LIGO data using difference boosting neural network}",
      journal = {\prd},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, General Relativity and Quantum Cosmology},
         year = "2017",
        month = "May",
       volume = {95},
       number = {10},
          eid = {104059},
        pages = {104059},
     abstract = "{Detection and classification of transients in data from gravitational
        wave detectors are crucial for efficient searches for true
        astrophysical events and identification of noise sources. We
        present a hybrid method for classification of short duration
        transients seen in gravitational wave data using both supervised
        and unsupervised machine learning techniques. To train the
        classifiers, we use the relative wavelet energy and the
        corresponding entropy obtained by applying one-dimensional
        wavelet decomposition on the data. The prediction accuracy of
        the trained classifier on nine simulated classes of
        gravitational wave transients and also LIGO's sixth science run
        hardware injections are reported. Targeted searches for a couple
        of known classes of nonastrophysical signals in the first
        observational run of Advanced LIGO data are also presented. The
        ability to accurately identify transient classes using minimal
        training samples makes the proposed method a useful tool for
        LIGO detector characterization as well as searches for short
        duration gravitational wave signals.}",
          doi = {10.1103/PhysRevD.95.104059},
archivePrefix = {arXiv},
       eprint = {1609.07259},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017PhRvD..95j4059M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2017PhDT........10F,
       author = {{Ford}, John M.},
        title = "{Pulsar Search Using Supervised Machine Learning}",
     keywords = {Computer science;Astronomy;Artificial intelligence},
       school = {Nova Southeastern University},
         year = "2017",
        month = "May",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017PhDT........10F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.467L.110S,
       author = {{Schawinski}, Kevin and {Zhang}, Ce and {Zhang}, Hantian and
         {Fowler}, Lucas and {Santhanam}, Gokula Krishnan},
        title = "{Generative adversarial networks recover features in astrophysical images of galaxies beyond the deconvolution limit}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: image processing, galaxies: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2017",
        month = "May",
       volume = {467},
       number = {1},
        pages = {L110-L114},
     abstract = "{Observations of astrophysical objects such as galaxies are limited by
        various sources of random and systematic noise from the sky
        background, the optical system of the telescope and the detector
        used to record the data. Conventional deconvolution techniques
        are limited in their ability to recover features in imaging data
        by the Shannon-Nyquist sampling theorem. Here, we train a
        generative adversarial network (GAN) on a sample of 4550 images
        of nearby galaxies at 0.01 \&lt; z \&lt; 0.02 from the Sloan
        Digital Sky Survey and conduct 10{\texttimes} cross-validation
        to evaluate the results. We present a method using a GAN trained
        on galaxy images that can recover features from artificially
        degraded images with worse seeing and higher noise than the
        original with a performance that far exceeds simple
        deconvolution. The ability to better recover detailed features
        such as galaxy morphology from low signal to noise and low
        angular resolution imaging data significantly increases our
        ability to study existing data sets of astrophysical objects as
        well as future observations with observatories such as the Large
        Synoptic Sky Telescope (LSST) and the Hubble and James Webb
        space telescopes.}",
          doi = {10.1093/mnrasl/slx008},
archivePrefix = {arXiv},
       eprint = {1702.00403},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.467L.110S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.467.3576M,
       author = {{Morrison}, C.~B. and {Hildebrandt}, H. and {Schmidt}, S.~J. and
         {Baldry}, I.~K. and {Bilicki}, M. and {Choi}, A. and {Erben}, T. and
         {Schneider}, P.},
        title = "{the-wizz: clustering redshift estimation for everyone}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, galaxies: distances and redshifts, large-scale structure of Universe, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "May",
       volume = {467},
       number = {3},
        pages = {3576-3589},
     abstract = "{We present the-wizz, an open source and user-friendly software for
        estimating the redshift distributions of photometric galaxies
        with unknown redshifts by spatially cross-correlating them
        against a reference sample with known redshifts. The main
        benefit of the-wizz is in separating the angular pair finding
        and correlation estimation from the computation of the output
        clustering redshifts allowing anyone to create a clustering
        redshift for their sample without the intervention of an
        'expert'. It allows the end user of a given survey to select any
        subsample of photometric galaxies with unknown redshifts, match
        this sample's catalogue indices into a value-added data file and
        produce a clustering redshift estimation for this sample in a
        fraction of the time it would take to run all the angular
        correlations needed to produce a clustering redshift. We show
        results with this software using photometric data from the Kilo-
        Degree Survey (KiDS) and spectroscopic redshifts from the Galaxy
        and Mass Assembly survey and the Sloan Digital Sky Survey. The
        results we present for KiDS are consistent with the redshift
        distributions used in a recent cosmic shear analysis from the
        survey. We also present results using a hybrid machine learning-
        clustering redshift analysis that enables the estimation of
        clustering redshifts for individual galaxies. the-wizz can be
        downloaded at http://github.com/morriscb/The-wiZZ/.}",
          doi = {10.1093/mnras/stx342},
archivePrefix = {arXiv},
       eprint = {1609.09085},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.467.3576M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017AJ....153..204S,
       author = {{Sesar}, Branimir and {Hernitschek}, Nina and {Mitrovi{\'c}}, Sandra and
         {Ivezi{\'c}}, {\v{Z}}eljko and {Rix}, Hans-Walter and
         {Cohen}, Judith G. and {Bernard}, Edouard J. and {Grebel}, Eva K. and
         {Martin}, Nicolas F. and {Schlafly}, Edward F. and
         {Burgett}, William S. and {Draper}, Peter W. and {Flewelling}, Heather and
         {Kaiser}, Nick and {Kudritzki}, Rolf P. and {Magnier}, Eugene A. and
         {Metcalfe}, Nigel and {Tonry}, John L. and {Waters}, Christopher},
        title = "{Machine-learned Identification of RR Lyrae Stars from Sparse, Multi-band Data: The PS1 Sample}",
      journal = {\aj},
     keywords = {catalogs, Galaxy: halo, methods: data analysis, methods: statistical, stars: variables: RR Lyrae, surveys, Astrophysics - Astrophysics of Galaxies},
         year = "2017",
        month = "May",
       volume = {153},
       number = {5},
          eid = {204},
        pages = {204},
     abstract = "{RR Lyrae stars may be the best practical tracers of Galactic halo
        (sub-)structure and kinematics. The PanSTARRS1 (PS1)
        3{\ensuremath{\pi}} survey offers multi-band, multi-epoch,
        precise photometry across much of the sky, but a robust
        identification of RR Lyrae stars in this data set poses a
        challenge, given PS1's sparse, asynchronous multi-band light
        curves ({\ensuremath{\lesssim}} 12 epochs in each of five bands,
        taken over a 4.5 year period). We present a novel template
        fitting technique that uses well-defined and physically
        motivated multi-band light curves of RR Lyrae stars, and
        demonstrate that we get accurate period estimates, precise to 2
        s in \&gt; 80 \% of cases. We augment these light-curve fits
        with other features from photometric time-series and provide
        them to progressively more detailed machine-learned
        classification models. From these models, we are able to select
        the widest (three-fourths of the sky) and deepest (reaching 120
        kpc) sample of RR Lyrae stars to date. The PS1 sample of ̃45,000
        RRab stars is pure (90\%) and complete (80\% at 80 kpc) at high
        galactic latitudes. It also provides distances that are precise
        to 3\%, measured with newly derived period-luminosity relations
        for optical/near-infrared PS1 bands. With the addition of proper
        motions from Gaia and radial velocity measurements from multi-
        object spectroscopic surveys, we expect the PS1 sample of RR
        Lyrae stars to become the premier source for studying the
        structure, kinematics, and the gravitational potential of the
        Galactic halo. The techniques presented in this study should
        translate well to other sparse, multi-band data sets, such as
        those produced by the Dark Energy Survey and the upcoming Large
        Synoptic Survey Telescope Galactic plane sub-survey.}",
          doi = {10.3847/1538-3881/aa661b},
archivePrefix = {arXiv},
       eprint = {1611.08596},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017AJ....153..204S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017A&A...601A.137M,
       author = {{Meingast}, Stefan and {Lombardi}, Marco and {Alves}, Jo{\~a}o},
        title = "{Estimating extinction using unsupervised machine learning}",
      journal = {\aap},
     keywords = {dust, extinction, methods: data analysis, methods: statistical, techniques: miscellaneous, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Astrophysics - Solar and Stellar Astrophysics},
         year = "2017",
        month = "May",
       volume = {601},
          eid = {A137},
        pages = {A137},
     abstract = "{Dust extinction is the most robust tracer of the gas distribution in the
        interstellar medium, but measuring extinction is limited by the
        systematic uncertainties involved in estimating the intrinsic
        colors to background stars. In this paper we present a new
        technique, Pnicer, that estimates intrinsic colors and
        extinction for individual stars using unsupervised machine
        learning algorithms. This new method aims to be free from any
        priors with respect to the column density and intrinsic color
        distribution. It is applicable to any combination of parameters
        and works in arbitrary numbers of dimensions. Furthermore, it is
        not restricted to color space. Extinction toward single sources
        is determined by fitting Gaussian mixture models along the
        extinction vector to (extinction-free) control field
        observations. In this way it becomes possible to describe the
        extinction for observed sources with probability densities,
        rather than a single value. Pnicer effectively eliminates known
        biases found in similar methods and outperforms them in cases of
        deep observational data where the number of background galaxies
        is significant, or when a large number of parameters is used to
        break degeneracies in the intrinsic color distributions. This
        new method remains computationally competitive, making it
        possible to correctly de-redden millions of sources within a
        matter of seconds. With the ever-increasing number of large-
        scale high-sensitivity imaging surveys, Pnicer offers a fast and
        reliable way to efficiently calculate extinction for arbitrary
        parameter combinations without prior information on source
        characteristics. The Pnicer software package also offers access
        to the well-established Nicer technique in a simple unified
        interface and is capable of building extinction maps including
        the Nicest correction for cloud substructure. Pnicer is offered
        to the community as an open-source software solution and is
        entirely written in Python.}",
          doi = {10.1051/0004-6361/201630032},
archivePrefix = {arXiv},
       eprint = {1702.08456},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017A&A...601A.137M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017PhRvE..95d3305C,
       author = {{Chen}, Nicholas F.~Y. and {Kasim}, Muhammad Firmansyah and
         {Ceurvorst}, Luke and {Ratan}, Naren and {Sadler}, James and
         {Levy}, Matthew C. and {Trines}, Raoul and {Bingham}, Robert and
         {Norreys}, Peter},
        title = "{Machine learning applied to proton radiography of high-energy-density plasmas}",
      journal = {\pre},
         year = "2017",
        month = "Apr",
       volume = {95},
       number = {4},
          eid = {043305},
        pages = {043305},
     abstract = "{Proton radiography is a technique extensively used to resolve magnetic
        field structures in high-energy-density plasmas, revealing a
        whole variety of interesting phenomena such as magnetic
        reconnection and collisionless shocks found in astrophysical
        systems. Existing methods of analyzing proton radiographs give
        mostly qualitative results or specific quantitative parameters,
        such as magnetic field strength, and recent work showed that the
        line-integrated transverse magnetic field can be reconstructed
        in specific regimes where many simplifying assumptions were
        needed. Using artificial neural networks, we demonstrate for the
        first time 3D reconstruction of magnetic fields in the nonlinear
        regime, an improvement over existing methods, which reconstruct
        only in 2D and in the linear regime. A proof of concept is
        presented here, with mean reconstruction errors of less than 5\%
        even after introducing noise. We demonstrate that over the long
        term, this approach is more computationally efficient compared
        to other techniques. We also highlight the need for proton
        tomography because (i) certain field structures cannot be
        reconstructed from a single radiograph and (ii) errors can be
        further reduced when reconstruction is performed on radiographs
        generated by proton beams fired in different directions.}",
          doi = {10.1103/PhysRevE.95.043305},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017PhRvE..95d3305C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2017PhDT.......107D,
       author = {{Dobrycheva}, D.~V.},
        title = "{Morphological content and color indices bimodality of a new galaxy sample at the redshifts z \&lt; 0.1}",
     keywords = {galaxies, morphology, color indices, bimodality, Voronoi tessellation method, Holmberg effect},
       school = {Main Astronomical Observatory, NAS of Ukraine},
         year = "2017",
        month = "Apr",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017PhDT.......107D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.466.2364H,
       author = {{Huppenkothen}, Daniela and {Heil}, Lucy M. and {Hogg}, David W. and
         {Mueller}, Andreas},
        title = "{Using machine learning to explore the long-term evolution of GRS 1915+105}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, X-rays: binaries, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2017",
        month = "Apr",
       volume = {466},
       number = {2},
        pages = {2364-2377},
     abstract = "{Among the population of known Galactic black hole X-ray binaries, GRS
        1915+105 stands out in multiple ways. It has been in continuous
        outburst since 1992, and has shown a wide range of different
        states that can be distinguished by their timing and spectral
        properties. These states, also observed in IGR J17091-3624, have
        in the past been linked to accretion dynamics. Here, we present
        the first comprehensive study into the long-term evolution of
        GRS 1915+105, using the entire data set observed with Rossi
        X-ray Timing Explorer over its 16-yr lifetime. We develop a set
        of descriptive features allowing for automatic separation of
        states, and show that supervised machine learning in the form of
        logistic regression and random forests can be used to
        efficiently classify the entire data set. For the first time, we
        explore the duty cycle and time evolution of states over the
        entire 16-yr time span, and find that the temporal distribution
        of states has likely changed over the span of the observations.
        We connect the machine classification with physical
        interpretations of the phenomenology in terms of chaotic and
        stochastic processes.}",
          doi = {10.1093/mnras/stw3190},
archivePrefix = {arXiv},
       eprint = {1611.01332},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.466.2364H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.466.2039C,
       author = {{Cavuoti}, S. and {Tortora}, C. and {Brescia}, M. and {Longo}, G. and
         {Radovich}, M. and {Napolitano}, N.~R. and {Amaro}, V. and
         {Vellucci}, C. and {La Barbera}, F. and {Getman}, F. and {Grado}, A.},
        title = "{A cooperative approach among methods for photometric redshifts estimation: an application to KiDS data}",
      journal = {\mnras},
     keywords = {methods: data analysis, catalogues, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Apr",
       volume = {466},
       number = {2},
        pages = {2039-2053},
     abstract = "{Photometric redshifts (photo-z) are fundamental in galaxy surveys to
        address different topics, from gravitational lensing and dark
        matter distribution to galaxy evolution. The Kilo Degree Survey
        (KiDS), I.e. the European Southern Observatory (ESO) public
        survey on the VLT Survey Telescope (VST), provides the
        unprecedented opportunity to exploit a large galaxy data set
        with an exceptional image quality and depth in the optical
        wavebands. Using a KiDS subset of about 25000 galaxies with
        measured spectroscopic redshifts, we have derived photo-z using
        (I) three different empirical methods based on supervised
        machine learning; (II) the Bayesian photometric redshift model
        (or BPZ); and (III) a classical spectral energy distribution
        (SED) template fitting procedure (LE PHARE). We confirm that, in
        the regions of the photometric parameter space properly sampled
        by the spectroscopic templates, machine learning methods provide
        better redshift estimates, with a lower scatter and a smaller
        fraction of outliers. SED fitting techniques, however, provide
        useful information on the galaxy spectral type, which can be
        effectively used to constrain systematic errors and to better
        characterize potential catastrophic outliers. Such
        classification is then used to specialize the training of
        regression machine learning models, by demonstrating that a
        hybrid approach, involving SED fitting and machine learning in a
        single collaborative framework, can be effectively used to
        improve the accuracy of photo-z estimates.}",
          doi = {10.1093/mnras/stw3208},
archivePrefix = {arXiv},
       eprint = {1612.02173},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.466.2039C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ChA&A..41..282Q,
       author = {{Qin}, Hao-ran and {Lin}, Ji-ming and {Wang}, Jun-yi},
        title = "{Stacked Denoising Autoencoders Applied to Star/Galaxy Classification}",
      journal = {\caa},
     keywords = {methods: data analysis, galaxies: fundamental parameters, stars: fundamental parameters, cosmology: observations},
         year = "2017",
        month = "Apr",
       volume = {41},
       number = {2},
        pages = {282-292},
     abstract = "{In recent years, the deep learning algorithm, with the characteristics
        of strong adaptability, high accuracy, and structural
        complexity, has become more and more popular, but it has not yet
        been used in astronomy. In order to solve the problem that the
        star/galaxy classification accuracy is high for the bright
        source set, but low for the faint source set of the Sloan
        Digital Sky Survey (SDSS) data, we introduced the new deep
        learning algorithm, namely the SDA (stacked denoising
        autoencoder) neural network and the dropout fine-tuning
        technique, which can greatly improve the robustness and
        antinoise performance. We randomly selected respectively the
        bright source sets and faint source sets from the SDSS DR12 and
        DR7 data with spectroscopic measurements, and made preprocessing
        on them. Then, we randomly selected respectively the training
        sets and testing sets without replacement from the bright source
        sets and faint source sets. At last, using these training sets
        we made the training to obtain the SDA models of the bright
        sources and faint sources in the SDSS DR7 and DR12,
        respectively. We compared the test result of the SDA model on
        the DR12 testing set with the test results of the Library for
        Support Vector Machines (LibSVM), J48 decision tree, Logistic
        Model Tree (LMT), Support Vector Machine (SVM), Logistic
        Regression, and Decision Stump algorithm, and compared the test
        result of the SDA model on the DR7 testing set with the test
        results of six kinds of decision trees. The experiments show
        that the SDA has a better classification accuracy than other
        machine learning algorithms for the faint source sets of DR7 and
        DR12. Especially, when the completeness function is used as the
        evaluation index, compared with the decision tree algorithms,
        the correctness rate of SDA has improved about 15\% for the
        faint source set of SDSS-DR7.}",
          doi = {10.1016/j.chinastron.2017.04.009},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ChA&A..41..282Q},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...839..116A,
       author = {{Angelou}, George C. and {Bellinger}, Earl P. and {Hekker}, Saskia and
         {Basu}, Sarbani},
        title = "{On the Statistical Properties of the Lower Main Sequence}",
      journal = {\apj},
     keywords = {methods: statistical, stars: abundances, stars: fundamental parameters, stars: low-mass, stars: oscillations, stars: solar-type, Astrophysics - Solar and Stellar Astrophysics},
         year = "2017",
        month = "Apr",
       volume = {839},
       number = {2},
          eid = {116},
        pages = {116},
     abstract = "{Astronomy is in an era where all-sky surveys are mapping the Galaxy. The
        plethora of photometric, spectroscopic, asteroseismic, and
        astrometric data allows us to characterize the comprising stars
        in detail. Here we quantify to what extent precise stellar
        observations reveal information about the properties of a star,
        including properties that are unobserved, or even unobservable.
        We analyze the diagnostic potential of classical and
        asteroseismic observations for inferring stellar parameters such
        as age, mass, and radius from evolutionary tracks of solar-like
        oscillators on the lower main sequence. We perform rank
        correlation tests in order to determine the capacity of each
        observable quantity to probe structural components of stars and
        infer their evolutionary histories. We also analyze the
        principal components of classic and asteroseismic observables to
        highlight the degree of redundancy present in the measured
        quantities and demonstrate the extent to which information of
        the model parameters can be extracted. We perform multiple
        regression using combinations of observable quantities in a grid
        of evolutionary simulations and appraise the predictive utility
        of each combination in determining the properties of stars. We
        identify the combinations that are useful and provide limits to
        where each type of observable quantity can reveal information
        about a star. We investigate the accuracy with which targets in
        the upcoming TESS and PLATO missions can be characterized. We
        demonstrate that the combination of observations from GAIA and
        PLATO will allow us to tightly constrain stellar masses, ages,
        and radii with machine learning for the purposes of Galactic and
        planetary studies.}",
          doi = {10.3847/1538-4357/aa6a54},
archivePrefix = {arXiv},
       eprint = {1703.10165},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...839..116A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017A&A...600A.113J,
       author = {{Jones}, E. and {Singal}, J.},
        title = "{Analysis of a custom support vector machine for photometric redshift estimation and the inclusion of galaxy shape information}",
      journal = {\aap},
     keywords = {techniques: photometric, galaxies: statistics, methods: miscellaneous, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2017",
        month = "Apr",
       volume = {600},
          eid = {A113},
        pages = {A113},
     abstract = "{<BR /> Aims: We present a custom support vector machine classification
        package for photometric redshift estimation, including
        comparisons with other methods. We also explore the efficacy of
        including galaxy shape information in redshift estimation.
        Support vector machines, a type of machine learning, utilize
        optimization theory and supervised learning algorithms to
        construct predictive models based on the information content of
        data in a way that can treat different input features
        symmetrically, which can be a useful estimator of the
        information contained in additional features beyond photometry,
        such as those describing the morphology of galaxies. <BR />
        Methods: The custom support vector machine package we have
        developed is designated SPIDERz and made available to the
        community. As test data for evaluating performance and
        comparison with other methods, we apply SPIDERz to four distinct
        data sets: 1) the publicly available portion of the PHAT-1
        catalog based on the GOODS-N field with spectroscopic redshifts
        in the range z \&lt; 3.6, 2) 14 365 galaxies from the COSMOS
        bright survey with photometric band magnitudes, morphology, and
        spectroscopic redshifts inside z \&lt; 1.4, 3) 3048 galaxies
        from the overlap of COSMOS photometry and morphology with 3D-HST
        spectroscopy extending to z \&lt; 3.9, and 4) 2612 galaxies with
        five-band photometric magnitudes and morphology from the All-
        wavelength Extended Groth Strip International Survey and z \&lt;
        1.57. <BR /> Results: We find that SPIDERz achieves results
        competitive with other empirical packages on the PHAT-1 data,
        and performs quite well in estimating redshifts with the COSMOS
        and AEGIS data, including in the cases of a large redshift range
        (0 \&lt; z \&lt; 3.9). We also determine from analyses with both
        the COSMOS and AEGIS data that the inclusion of morphological
        information does not have a statistically significant benefit
        for photometric redshift estimation with the techniques employed
        here.}",
          doi = {10.1051/0004-6361/201629558},
archivePrefix = {arXiv},
       eprint = {1607.00044},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017A&A...600A.113J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017SSRv..206..575W,
       author = {{Wiltberger}, M. and {Rigler}, E.~J. and {Merkin}, V. and {Lyon}, J.~G.},
        title = "{Structure of High Latitude Currents in Magnetosphere-Ionosphere Models}",
      journal = {\ssr},
     keywords = {Magnetosphere, Ionosphere, Simulation, Modeling},
         year = "2017",
        month = "Mar",
       volume = {206},
       number = {1-4},
        pages = {575-598},
     abstract = "{Using three resolutions of the Lyon-Fedder-Mobarry global magnetosphere-
        ionosphere model (LFM) and the Weimer 2005 empirical model we
        examine the structure of the high latitude field-aligned current
        patterns. Each resolution was run for the entire Whole
        Heliosphere Interval which contained two high speed solar wind
        streams and modest interplanetary magnetic field strengths.
        Average states of the field-aligned current (FAC) patterns for 8
        interplanetary magnetic field clock angle directions are
        computed using data from these runs. Generally speaking the
        patterns obtained agree well with results obtained from the
        Weimer 2005 computing using the solar wind and IMF conditions
        that correspond to each bin. As the simulation resolution
        increases the currents become more intense and narrow. A machine
        learning analysis of the FAC patterns shows that the ratio of
        Region 1 (R1) to Region 2 (R2) currents decreases as the
        simulation resolution increases. This brings the simulation
        results into better agreement with observational predictions and
        the Weimer 2005 model results. The increase in R2 current
        strengths also results in the cross polar cap potential (CPCP)
        pattern being concentrated in higher latitudes. Current-voltage
        relationships between the R1 and CPCP are quite similar at the
        higher resolution indicating the simulation is converging on a
        common solution. We conclude that LFM simulations are capable of
        reproducing the statistical features of FAC patterns.}",
          doi = {10.1007/s11214-016-0271-2},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017SSRv..206..575W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017PrA....35..304X,
       author = {{Xu}, Yu-yun and {Li}, Di and {Liu}, Zhi-jie and {Wang}, Chen and
         {Wang}, Pei and {Zhang}, Lei and {Pan}, Zhi-che},
        title = "{Application of Artificial Intelligence in the Selection of Pulsar Candidate}",
      journal = {Progress in Astronomy},
     keywords = {Artificial Intelligence, pulsar, candidate selection},
         year = "2017",
        month = "Mar",
       volume = {35},
        pages = {304-315},
     abstract = "{Pulsar searching, aiming at discovering pulsars and transient sources
        such as Rotation RAdio Transients and Fast Radio Bursts(FRB), is
        the first step of sciences based on pulsars and transient
        sources. As more pulsars are discovered in pulsar searches, some
        special kinds of pulsars, including those having very short spin
        periods and very eccentric orbits, will also be illuminated.
        These special pulsars provide extreme physical conditions for us
        to examine our current understands, which is imperative to the
        research of the dense matter state equation, interplanetary
        navigation, the interstellar medium, and the measurement of
        gravitational waves. Nowadays, one typical pulsar search/survey
        in radio frequencies (e.g. 1.4 GHz) will bring us millions of
        candidates to confirm. To check the quality of so many
        candidates, manual operations are too slow and inefficient.
        Machine learning and artificial intelligence have greatly
        developed in recent years, helping us identify high quality
        pulsar candidates from pulsar search results. This paper will
        discuss the use of aritificial intelligence in pulsar candidates
        checking, statistics and analysis in pulsar candidates
        identification, and the efficiency expectations of using similar
        methods in identifying candidates from Five-hundred-meter
        Aperture Spherical radio Telescope.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017PrA....35..304X},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017PASP..129c4402W,
       author = {{Waszczak}, Adam and {Prince}, Thomas A. and {Laher}, Russ and
         {Masci}, Frank and {Bue}, Brian and {Rebbapragada}, Umaa and
         {Barlow}, Tom and {Surace}, Jason and {Helou}, George and
         {Kulkarni}, Shrinivas},
        title = "{Small Near-Earth Asteroids in the Palomar Transient Factory Survey: A Real-Time Streak-detection System}",
      journal = {\pasp},
     keywords = {Astrophysics - Earth and Planetary Astrophysics},
         year = "2017",
        month = "Mar",
       volume = {129},
       number = {973},
        pages = {034402},
     abstract = "{Near-Earth asteroids (NEAs) in the 1-100 meter size range are estimated
        to be ̃1,000 times more numerous than the ̃15,000 currently
        cataloged NEAs, most of which are in the 0.5-10 kilometer size
        range. Impacts from 10-100 meter size NEAs are not statistically
        life-threatening, but may cause significant regional damage,
        while 1-10 meter size NEAs with low velocities relative to Earth
        are compelling targets for space missions. We describe the
        implementation and initial results of a real-time NEA-discovery
        system specialized for the detection of small, high angular rate
        (visually streaked) NEAs in Palomar Transient Factory (PTF)
        images. PTF is a 1.2-m aperture, 7.3 deg$^{2}$ field of view
        (FOV) optical survey designed primarily for the discovery of
        extragalactic transients (e.g., supernovae) in 60-second
        exposures reaching ̃20.5 visual magnitude. Our real-time NEA
        discovery pipeline uses a machine-learned classifier to filter a
        large number of false-positive streak detections, permitting a
        human scanner to efficiently and remotely identify real asteroid
        streaks during the night. Upon recognition of a streaked NEA
        detection (typically within an hour of the discovery exposure),
        the scanner triggers follow-up with the same telescope and posts
        the observations to the Minor Planet Center for worldwide
        confirmation. We describe our 11 initial confirmed discoveries,
        all small NEAs that passed 0.3-15 lunar distances from Earth.
        Lastly, we derive useful scaling laws for comparing streaked-
        NEA-detection capabilities of different surveys as a function of
        their hardware and survey-pattern characteristics. This work
        most directly informs estimates of the streak-detection
        capabilities of the Zwicky Transient Facility (ZTF, planned to
        succeed PTF in 2017), which will apply PTF{\textquoteright}s
        current resolution and sensitivity over a 47-deg$^{2}$ FOV.}",
          doi = {10.1088/1538-3873/129/973/034402},
archivePrefix = {arXiv},
       eprint = {1609.08018},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017PASP..129c4402W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.465.4325O,
       author = {{Ostrovski}, Fernanda and {McMahon}, Richard G. and
         {Connolly}, Andrew J. and {Lemon}, Cameron A. and {Auger}, Matthew W. and
         {Banerji}, Manda and {Hung}, Johnathan M. and {Koposov}, Sergey E. and
         {Lidman}, Christopher E. and {Reed}, Sophie L. and {Allam}, Sahar and
         {Benoit-L{\'e}vy}, Aur{\'e}lien and {Bertin}, Emmanuel and
         {Brooks}, David and {Buckley-Geer}, Elizabeth and
         {Carnero Rosell}, Aurelio and {Carrasco Kind}, Matias and
         {Carretero}, Jorge and {Cunha}, Carlos E. and {da Costa}, Luiz N. and
         {Desai}, Shantanu and {Diehl}, H. Thomas and {Dietrich}, J{\"o}rg P. and
         {Evrard}, August E. and {Finley}, David A. and {Flaugher}, Brenna and
         {Fosalba}, Pablo and {Frieman}, Josh and {Gerdes}, David W. and
         {Goldstein}, Daniel A. and {Gruen}, Daniel and {Gruendl}, Robert A. and
         {Gutierrez}, Gaston and {Honscheid}, Klaus and {James}, David J. and
         {Kuehn}, Kyler and {Kuropatkin}, Nikolay and {Lima}, Marcos and
         {Lin}, Huan and {Maia}, Marcio A.~G. and {Marshall}, Jennifer L. and
         {Martini}, Paul and {Melchior}, Peter and {Miquel}, Ramon and {Ogand
        o}, Ricardo and {Plazas Malag{\'o}n}, Andr{\'e}s and {Reil}, Kevin and
         {Romer}, Kathy and {Sanchez}, Eusebio and {Santiago}, Basilio and
         {Scarpine}, Vic and {Sevilla-Noarbe}, Ignacio and
         {Soares-Santos}, Marcelle and {Sobreira}, Flavia and {Suchyta}, Eric and
         {Tarle}, Gregory and {Thomas}, Daniel and {Tucker}, Douglas L. and
         {Walker}, Alistair R.},
        title = "{VDES J2325-5229 a z = 2.7 gravitationally lensed quasar discovered using morphology-independent supervised machine learning}",
      journal = {\mnras},
     keywords = {gravitational lensing: strong, methods: observational, methods: statistical, quasars: general, Astrophysics - Astrophysics of Galaxies},
         year = "2017",
        month = "Mar",
       volume = {465},
       number = {4},
        pages = {4325-4334},
     abstract = "{We present the discovery and preliminary characterization of a
        gravitationally lensed quasar with a source redshift z$_{s}$ =
        2.74 and image separation of 2.9 arcsec lensed by a foreground
        z$_{l}$ = 0.40 elliptical galaxy. Since optical observations of
        gravitationally lensed quasars show the lens system as a
        superposition of multiple point sources and a foreground lensing
        galaxy, we have developed a morphology-independent multi-
        wavelength approach to the photometric selection of lensed
        quasar candidates based on Gaussian Mixture Models (GMM)
        supervised machine learning. Using this technique and gi
        multicolour photometric observations from the Dark Energy Survey
        (DES), near-IR JK photometry from the VISTA Hemisphere Survey
        (VHS) and WISE mid-IR photometry, we have identified a candidate
        system with two catalogue components with I$_{AB}$ = 18.61 and
        I$_{AB}$ = 20.44 comprising an elliptical galaxy and two blue
        point sources. Spectroscopic follow-up with NTT and the use of
        an archival AAT spectrum show that the point sources can be
        identified as a lensed quasar with an emission line redshift of
        z = 2.739 {\ensuremath{\pm}} 0.003 and a foreground early-type
        galaxy with z = 0.400 {\ensuremath{\pm}} 0.002. We model the
        system as a single isothermal ellipsoid and find the Einstein
        radius {\ensuremath{\theta}}$_{E}$ ̃ 1.47 arcsec, enclosed mass
        M$_{enc}$ ̃ 4 {\texttimes} {}10$^{11}$ M$_{☉}$ and a time delay
        of ̃52 d. The relatively wide separation, month scale time delay
        duration and high redshift make this an ideal system for
        constraining the expansion rate beyond a redshift of 1.}",
          doi = {10.1093/mnras/stw2958},
archivePrefix = {arXiv},
       eprint = {1607.01391},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.465.4325O},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.465.4311W,
       author = {{Wang}, Ke and {Guo}, Ping and {Luo}, A. -Li},
        title = "{A new automated spectral feature extraction method and its application in spectral classification and defective spectra recovery}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: numerical, methods: statistical, techniques: spectroscopic},
         year = "2017",
        month = "Mar",
       volume = {465},
       number = {4},
        pages = {4311-4324},
     abstract = "{Spectral feature extraction is a crucial procedure in automated spectral
        analysis. This procedure starts from the spectral data and
        produces informative and non-redundant features, facilitating
        the subsequent automated processing and analysis with machine-
        learning and data-mining techniques. In this paper, we present a
        new automated feature extraction method for astronomical
        spectra, with application in spectral classification and
        defective spectra recovery. The basic idea of our approach is to
        train a deep neural network to extract features of spectra with
        different levels of abstraction in different layers. The deep
        neural network is trained with a fast layer-wise learning
        algorithm in an analytical way without any iterative
        optimization procedure. We evaluate the performance of the
        proposed scheme on real-world spectral data. The results
        demonstrate that our method is superior regarding its
        comprehensive performance, and the computational cost is
        significantly lower than that for other methods. The proposed
        method can be regarded as a new valid alternative general-
        purpose feature extraction method for various tasks in spectral
        data analysis.}",
          doi = {10.1093/mnras/stw2894},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.465.4311W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.465.2634A,
       author = {{Armstrong}, D.~J. and {Pollacco}, D. and {Santerne}, A.},
        title = "{Transit shapes and self-organizing maps as a tool for ranking planetary candidates: application to Kepler and K2}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: miscellaneous, methods: statistical, planets and satellites: detection, planets and satellites: general, binaries: eclipsing, Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Mar",
       volume = {465},
       number = {3},
        pages = {2634-2642},
     abstract = "{A crucial step in planet hunting surveys is to select the best
        candidates for follow-up observations, given limited telescope
        resources. This is often performed by human 'eyeballing', a time
        consuming and statistically awkward process. Here, we present a
        new, fast machine learning technique to separate true planet
        signals from astrophysical false positives. We use self-
        organizing maps (SOMs) to study the transit shapes of Kepler and
        K2 known and candidate planets. We find that SOMs are capable of
        distinguishing known planets from known false positives with a
        success rate of 87.0 per cent, using the transit shape alone.
        Furthermore, they do not require any candidate to be
        dispositioned prior to use, meaning that they can be used early
        in a mission's lifetime. A method for classifying candidates
        using a SOM is developed, and applied to previously unclassified
        members of the Kepler Objects of Interest (KOI) list as well as
        candidates from the K2 mission. The method is extremely fast,
        taking minutes to run the entire KOI list on a typical laptop.
        We make PYTHON code for performing classifications publicly
        available, using either new SOMs or those created in this work.
        The SOM technique represents a novel method for ranking
        planetary candidate lists, and can be used both alone or as part
        of a larger autovetting code.}",
          doi = {10.1093/mnras/stw2881},
archivePrefix = {arXiv},
       eprint = {1611.01968},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.465.2634A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017GeoRL..44.2662D,
       author = {{DeVries}, Phoebe M.~R. and {Thompson}, T. Ben and {Meade}, Brendan J.},
        title = "{Enabling large-scale viscoelastic calculations via neural network acceleration}",
      journal = {\grl},
     keywords = {viscoelastic earthquake cycle models, artificial neural networks, Physics - Geophysics},
         year = "2017",
        month = "Mar",
       volume = {44},
       number = {6},
        pages = {2662-2669},
     abstract = "{One of the most significant challenges involved in efforts to understand
        the effects of repeated earthquake cycle activity is the
        computational costs of large-scale viscoelastic earthquake cycle
        models. Computationally intensive viscoelastic codes must be
        evaluated at thousands of times and locations, and as a result,
        studies tend to adopt a few fixed rheological structures and
        model geometries and examine the predicted time-dependent
        deformation over short (\&lt;10 years) time periods at a given
        depth after a large earthquake. Training a deep neural network
        to learn a computationally efficient representation of
        viscoelastic solutions, at any time, location, and for a large
        range of rheological structures, allows these calculations to be
        done quickly and reliably, with high spatial and temporal
        resolutions. We demonstrate that this machine learning approach
        accelerates viscoelastic calculations by more than 50,000\%.
        This magnitude of acceleration will enable the modeling of
        geometrically complex faults over thousands of earthquake cycles
        across wider ranges of model parameters and at larger spatial
        and temporal scales than have been previously possible.}",
          doi = {10.1002/2017GL072716},
archivePrefix = {arXiv},
       eprint = {1701.08884},
 primaryClass = {physics.geo-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017GeoRL..44.2662D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017CQGra..34f4003Z,
       author = {{Zevin}, M. and {Coughlin}, S. and {Bahaadini}, S. and {Besler}, E. and
         {Rohani}, N. and {Allen}, S. and {Cabero}, M. and {Crowston}, K. and
         {Katsaggelos}, A.~K. and {Larson}, S.~L. and {Lee}, T.~K. and
         {Lintott}, C. and {Littenberg}, T.~B. and {Lundgren}, A. and
         {{\O}sterlund}, C. and {Smith}, J.~R. and {Trouille}, L. and
         {Kalogera}, V.},
        title = "{Gravity Spy: integrating advanced LIGO detector characterization, machine learning, and citizen science}",
      journal = {Classical and Quantum Gravity},
     keywords = {General Relativity and Quantum Cosmology, Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Instrumentation and Methods for Astrophysics, Physics - Instrumentation and Detectors},
         year = "2017",
        month = "Mar",
       volume = {34},
       number = {6},
          eid = {064003},
        pages = {064003},
     abstract = "{With the first direct detection of gravitational waves, the advanced
        laser interferometer gravitational-wave observatory (LIGO) has
        initiated a new field of astronomy by providing an alternative
        means of sensing the universe. The extreme sensitivity required
        to make such detections is achieved through exquisite isolation
        of all sensitive components of LIGO from non-gravitational-wave
        disturbances. Nonetheless, LIGO is still susceptible to a
        variety of instrumental and environmental sources of noise that
        contaminate the data. Of particular concern are noise features
        known as glitches, which are transient and non-Gaussian in their
        nature, and occur at a high enough rate so that accidental
        coincidence between the two LIGO detectors is non-negligible.
        Glitches come in a wide range of time-frequency-amplitude
        morphologies, with new morphologies appearing as the detector
        evolves. Since they can obscure or mimic true gravitational-wave
        signals, a robust characterization of glitches is paramount in
        the effort to achieve the gravitational-wave detection rates
        that are predicted by the design sensitivity of LIGO. This
        proves a daunting task for members of the LIGO Scientific
        Collaboration alone due to the sheer amount of data. In this
        paper we describe an innovative project that combines
        crowdsourcing with machine learning to aid in the challenging
        task of categorizing all of the glitches recorded by the LIGO
        detectors. Through the Zooniverse platform, we engage and
        recruit volunteers from the public to categorize images of time-
        frequency representations of glitches into pre-identified
        morphological classes and to discover new classes that appear as
        the detectors evolve. In addition, machine learning algorithms
        are used to categorize images after being trained on human-
        classified examples of the morphological classes. Leveraging the
        strengths of both classification methods, we create a combined
        method with the aim of improving the efficiency and accuracy of
        each individual classifier. The resulting classification and
        characterization should help LIGO scientists to identify causes
        of glitches and subsequently eliminate them from the data or the
        detector entirely, thereby improving the rate and accuracy of
        gravitational-wave observations. We demonstrate these methods
        using a small subset of data from LIGO{\textquoteright}s first
        observing run.}",
          doi = {10.1088/1361-6382/aa5cea},
archivePrefix = {arXiv},
       eprint = {1611.04596},
 primaryClass = {gr-qc},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017CQGra..34f4003Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...838....5L,
       author = {{Leistedt}, Boris and {Hogg}, David W.},
        title = "{Data-driven, Interpretable Photometric Redshifts Trained on Heterogeneous and Unrepresentative Data}",
      journal = {\apj},
     keywords = {galaxies: distances and redshifts, large-scale structure of universe, 98.62.Py, 98.80.Es, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2017",
        month = "Mar",
       volume = {838},
       number = {1},
          eid = {5},
        pages = {5},
     abstract = "{We present a new method for inferring photometric redshifts in deep
        galaxy and quasar surveys, based on a data-driven model of
        latent spectral energy distributions (SEDs) and a physical model
        of photometric fluxes as a function of redshift. This
        conceptually novel approach combines the advantages of both
        machine learning methods and template fitting methods by
        building template SEDs directly from the spectroscopic training
        data. This is made computationally tractable with Gaussian
        processes operating in flux-redshift space, encoding the physics
        of redshifts and the projection of galaxy SEDs onto photometric
        bandpasses. This method alleviates the need to acquire
        representative training data or to construct detailed galaxy SED
        models; it requires only that the photometric bandpasses and
        calibrations be known or have parameterized unknowns. The
        training data can consist of a combination of spectroscopic and
        deep many-band photometric data with reliable redshifts, which
        do not need to entirely spatially overlap with the target survey
        of interest or even involve the same photometric bands. We
        showcase the method on the I-magnitude-selected,
        spectroscopically confirmed galaxies in the COSMOS field. The
        model is trained on the deepest bands (from SUBARU and HST) and
        photometric redshifts are derived using the shallower SDSS
        optical bands only. We demonstrate that we obtain accurate
        redshift point estimates and probability distributions despite
        the training and target sets having very different redshift
        distributions, noise properties, and even photometric bands. Our
        model can also be used to predict missing photometric fluxes or
        to simulate populations of galaxies with realistic fluxes and
        redshifts, for example.}",
          doi = {10.3847/1538-4357/aa6332},
archivePrefix = {arXiv},
       eprint = {1612.00847},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...838....5L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...837L..28C,
       author = {{Charnock}, Tom and {Moss}, Adam},
        title = "{Deep Recurrent Neural Networks for Supernovae Classification}",
      journal = {\apj},
     keywords = {methods: data analysis, supernovae: general, techniques: miscellaneous, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability},
         year = "2017",
        month = "Mar",
       volume = {837},
       number = {2},
          eid = {L28},
        pages = {L28},
     abstract = "{We apply deep recurrent neural networks, which are capable of learning
        complex sequential information, to classify supernovae (code
        available at https://github.com/adammoss/supernovae). The
        observational time and filter fluxes are used as inputs to the
        network, but since the inputs are agnostic, additional data such
        as host galaxy information can also be included. Using the
        Supernovae Photometric Classification Challenge (SPCC) data, we
        find that deep networks are capable of learning about light
        curves, however the performance of the network is highly
        sensitive to the amount of training data. For a training size of
        50\% of the representational SPCC data set (around {}10$^{4}$
        supernovae) we obtain a type-Ia versus non-type-Ia
        classification accuracy of 94.7\%, an area under the Receiver
        Operating Characteristic curve AUC of 0.986 and an SPCC figure-
        of-merit F $_{1}$ = 0.64. When using only the data for the
        early-epoch challenge defined by the SPCC, we achieve a
        classification accuracy of 93.1\%, AUC of 0.977, and F $_{1}$ =
        0.58, results almost as good as with the whole light curve. By
        employing bidirectional neural networks, we can acquire
        impressive classification results between supernovae types I, II
        and III at an accuracy of 90.4\% and AUC of 0.974. We also apply
        a pre-trained model to obtain classification probabilities as a
        function of time and show that it can give early indications of
        supernovae type. Our method is competitive with existing
        algorithms and has applications for future large-scale
        photometric surveys.}",
          doi = {10.3847/2041-8213/aa603d},
archivePrefix = {arXiv},
       eprint = {1606.07442},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...837L..28C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017APh....89....1K,
       author = {{Krause}, Maria and {Pueschel}, Elisa and {Maier}, Gernot},
        title = "{Improved {\ensuremath{\gamma}}/hadron separation for the detection of faint {\ensuremath{\gamma}}-ray sources using boosted decision trees}",
      journal = {Astroparticle Physics},
     keywords = {Multivariate analysis, {\ensuremath{\gamma}}-ray astronomy, {\ensuremath{\gamma}}/hadron discrimination, Cherenkov technique, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Mar",
       volume = {89},
        pages = {1-9},
     abstract = "{Imaging atmospheric Cherenkov telescopes record an enormous number of
        cosmic-ray background events. Suppressing these background
        events while retaining {\ensuremath{\gamma}}-rays is key to
        achieving good sensitivity to faint {\ensuremath{\gamma}}-ray
        sources. The differentiation between signal and background
        events can be accomplished using machine learning algorithms,
        which are already used in various fields of physics.
        Multivariate analyses combine several variables into a single
        variable that indicates the degree to which an event is
        {\ensuremath{\gamma}}-ray-like or cosmic-ray-like. In this paper
        we will focus on the use of ``boosted decision trees'' for
        {\ensuremath{\gamma}}/hadron separation. We apply the method to
        data from the Very Energetic Radiation Imaging Telescope Array
        System (VERITAS), and demonstrate an improved sensitivity
        compared to the VERITAS standard analysis.}",
          doi = {10.1016/j.astropartphys.2017.01.004},
archivePrefix = {arXiv},
       eprint = {1701.06928},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017APh....89....1K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.465.1959C,
       author = {{Cavuoti}, S. and {Amaro}, V. and {Brescia}, M. and {Vellucci}, C. and
         {Tortora}, C. and {Longo}, G.},
        title = "{METAPHOR: a machine-learning-based method for the probability density estimation of photometric redshifts}",
      journal = {\mnras},
     keywords = {techniques: photometric, galaxies: distances and redshifts, galaxies: photometry, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Feb",
       volume = {465},
       number = {2},
        pages = {1959-1973},
     abstract = "{A variety of fundamental astrophysical science topics require the
        determination of very accurate photometric redshifts (photo-z).
        A wide plethora of methods have been developed, based either on
        template models fitting or on empirical explorations of the
        photometric parameter space. Machine-learning-based techniques
        are not explicitly dependent on the physical priors and able to
        produce accurate photo-z estimations within the photometric
        ranges derived from the spectroscopic training set. These
        estimates, however, are not easy to characterize in terms of a
        photo-z probability density function (PDF), due to the fact that
        the analytical relation mapping the photometric parameters on to
        the redshift space is virtually unknown. We present METAPHOR
        (Machine-learning Estimation Tool for Accurate PHOtometric
        Redshifts), a method designed to provide a reliable PDF of the
        error distribution for empirical techniques. The method is
        implemented as a modular workflow, whose internal engine for
        photo-z estimation makes use of the MLPQNA neural network (Multi
        Layer Perceptron with Quasi Newton learning rule), with the
        possibility to easily replace the specific machine-learning
        model chosen to predict photo-z. We present a summary of results
        on SDSS-DR9 galaxy data, used also to perform a direct
        comparison with PDFs obtained by the LE PHARE spectral energy
        distribution template fitting. We show that METAPHOR is capable
        to estimate the precision and reliability of photometric
        redshifts obtained with three different self-adaptive
        techniques, I.e. MLPQNA, Random Forest and the standard
        K-Nearest Neighbors models.}",
          doi = {10.1093/mnras/stw2930},
archivePrefix = {arXiv},
       eprint = {1611.02162},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.465.1959C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.465.1144U,
       author = {{Ucci}, G. and {Ferrara}, A. and {Gallerani}, S. and {Pallottini}, A.},
        title = "{Inferring physical properties of galaxies from their emission-line spectra}",
      journal = {\mnras},
     keywords = {methods: data analysis, ISM: general, ISM: H II regions, ISM: lines and bands, galaxies: ISM, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2017",
        month = "Feb",
       volume = {465},
       number = {1},
        pages = {1144-1156},
     abstract = "{We present a new approach based on Supervised Machine Learning
        algorithms to infer key physical properties of galaxies
        (density, metallicity, column density and ionization parameter)
        from their emission-line spectra. We introduce a numerical code
        (called GAME, GAlaxy Machine learning for Emission lines)
        implementing this method and test it extensively. GAME delivers
        excellent predictive performances, especially for estimates of
        metallicity and column densities. We compare GAME with the most
        widely used diagnostics (e.g. R$_{23}$, [N II]
        {\ensuremath{\lambda}}6584/H{\ensuremath{\alpha}} indicators)
        showing that it provides much better accuracy and wider
        applicability range. GAME is particularly suitable for use in
        combination with Integral Field Unit spectroscopy, both for
        rest-frame optical/UV nebular lines and far-infrared/sub-
        millimeter lines arising from photodissociation regions.
        Finally, GAME can also be applied to the analysis of synthetic
        galaxy maps built from numerical simulations.}",
          doi = {10.1093/mnras/stw2836},
archivePrefix = {arXiv},
       eprint = {1611.00768},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.465.1144U},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.464.4463K,
       author = {{Kim}, Edward J. and {Brunner}, Robert J.},
        title = "{Star-galaxy classification using deep convolutional neural networks}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, techniques: image processing, surveys, stars: statistics, galaxies: statistics, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies, Computer Science - Computer Vision and Pattern Recognition},
         year = "2017",
        month = "Feb",
       volume = {464},
       number = {4},
        pages = {4463-4475},
     abstract = "{Most existing star-galaxy classifiers use the reduced summary
        information from catalogues, requiring careful feature
        extraction and selection. The latest advances in machine
        learning that use deep convolutional neural networks (ConvNets)
        allow a machine to automatically learn the features directly
        from the data, minimizing the need for input from human experts.
        We present a star-galaxy classification framework that uses deep
        ConvNets directly on the reduced, calibrated pixel values. Using
        data from the Sloan Digital Sky Survey and the Canada-France-
        Hawaii Telescope Lensing Survey, we demonstrate that ConvNets
        are able to produce accurate and well-calibrated probabilistic
        classifications that are competitive with conventional machine
        learning techniques. Future advances in deep learning may bring
        more success with current and forthcoming photometric surveys,
        such as the Dark Energy Survey and the Large Synoptic Survey
        Telescope, because deep neural networks require very little,
        manual feature engineering.}",
          doi = {10.1093/mnras/stw2672},
archivePrefix = {arXiv},
       eprint = {1608.04369},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.464.4463K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017IJRS...38.2623L,
       author = {{Longmore}, S.~N. and {Collins}, R.~P. and {Pfeifer}, S. and
         {Fox}, S.~E. and {Mulero-Pazmany}, M. and {Bezombes}, F. and
         {Goodwind}, A. and {de Juan Ovelar}, M. and {Knapen}, J.~H. and
         {Wich}, S.~A.},
        title = "{Adapting astronomical source detection software to help detect animals in thermal images obtained by unmanned aerial systems}",
      journal = {International Journal of Remote Sensing},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Feb",
       volume = {38},
       number = {8-10},
        pages = {2623-2638},
     abstract = "{In this paper we describe an unmanned aerial system equipped with a
        thermal-infrared camera and software pipeline that we have
        developed to monitor animal populations for conservation
        purposes. Taking a multi-disciplinary approach to tackle this
        problem, we use freely available astronomical source detection
        software and the associated expertise of astronomers, to
        efficiently and reliably detect humans and animals in aerial
        thermal-infrared footage. Combining this astronomical detection
        software with existing machine learning algorithms into a
        single, automated, end-to-end pipeline, we test the software
        using aerial video footage taken in a controlled, field-like
        environment. We demonstrate that the pipeline works reliably and
        describe how it can be used to estimate the completeness of
        different observational datasets to objects of a given type as a
        function of height, observing conditions etc. - a crucial step
        in converting video footage to scientifically useful information
        such as the spatial distribution and density of different animal
        species. Finally, having demonstrated the potential utility of
        the system, we describe the steps we are taking to adapt the
        system for work in the field, in particular systematic
        monitoring of endangered species at National Parks around the
        world.}",
          doi = {10.1080/01431161.2017.1280639},
archivePrefix = {arXiv},
       eprint = {1701.01611},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017IJRS...38.2623L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017CQGra..34d4004C,
       author = {{Coughlin}, Michael and {Earle}, Paul and {Harms}, Jan and
         {Biscans}, Sebastien and {Buchanan}, Christopher and {Coughlin}, Eric and
         {Donovan}, Fred and {Fee}, Jeremy and {Gabbard}, Hunter and
         {Guy}, Michelle and {Mukund}, Nikhil and {Perry}, Matthew},
        title = "{Limiting the effects of earthquakes on gravitational-wave interferometers}",
      journal = {Classical and Quantum Gravity},
     keywords = {General Relativity and Quantum Cosmology, Astrophysics - Instrumentation and Methods for Astrophysics, Physics - Instrumentation and Detectors},
         year = "2017",
        month = "Feb",
       volume = {34},
       number = {4},
          eid = {044004},
        pages = {044004},
     abstract = "{Ground-based gravitational wave interferometers such as the Laser
        Interferometer Gravitational-wave Observatory (LIGO) are
        susceptible to ground shaking from high-magnitude teleseismic
        events, which can interrupt their operation in science mode and
        significantly reduce their duty cycle. It can take several hours
        for a detector to stabilize enough to return to its nominal
        state for scientific observations. The down time can be reduced
        if advance warning of impending shaking is received and the
        impact is suppressed in the isolation system with the goal of
        maintaining stable operation even at the expense of increased
        instrumental noise. Here, we describe an early warning system
        for modern gravitational-wave observatories. The system relies
        on near real-time earthquake alerts provided by the U.S.
        Geological Survey (USGS) and the National Oceanic and
        Atmospheric Administration (NOAA). Preliminary low latency
        hypocenter and magnitude information is generally available in 5
        to 20{\,}min of a significant earthquake depending on its
        magnitude and location. The alerts are used to estimate arrival
        times and ground velocities at the gravitational-wave detectors.
        In general, 90\% of the predictions for ground-motion amplitude
        are within a factor of 5 of measured values. The error in both
        arrival time and ground-motion prediction introduced by using
        preliminary, rather than final, hypocenter and magnitude
        information is minimal. By using a machine learning algorithm,
        we develop a prediction model that calculates the probability
        that a given earthquake will prevent a detector from taking
        data. Our initial results indicate that by using detector
        control configuration changes, we could prevent interruption of
        operation from 40 to 100 earthquake events in a 6-month time-
        period.}",
          doi = {10.1088/1361-6382/aa5a60},
archivePrefix = {arXiv},
       eprint = {1611.09812},
 primaryClass = {gr-qc},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017CQGra..34d4004C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017CQGra..34c4002P,
       author = {{Powell}, Jade and {Torres-Forn{\'e}}, Alejandro and {Lynch}, Ryan and
         {Trifir{\`o}}, Daniele and {Cuoco}, Elena and {Cavagli{\`a}}, Marco and
         {Heng}, Ik Siong and {Font}, Jos{\'e} A.},
        title = "{Classification methods for noise transients in advanced gravitational-wave detectors II: performance tests on Advanced LIGO data}",
      journal = {Classical and Quantum Gravity},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Feb",
       volume = {34},
       number = {3},
          eid = {034002},
        pages = {034002},
     abstract = "{The data taken by the advanced LIGO and Virgo gravitational-wave
        detectors contains short duration noise transients that limit
        the significance of astrophysical detections and reduce the duty
        cycle of the instruments. As the advanced detectors are reaching
        sensitivity levels that allow for multiple detections of
        astrophysical gravitational-wave sources it is crucial to
        achieve a fast and accurate characterization of non-
        astrophysical transient noise shortly after it occurs in the
        detectors. Previously we presented three methods for the
        classification of transient noise sources. They are Principal
        Component Analysis for Transients (PCAT), Principal Component
        LALInference Burst (PC-LIB) and Wavelet Detection Filter with
        Machine Learning (WDF-ML). In this study we carry out the first
        performance tests of these algorithms on gravitational-wave data
        from the Advanced LIGO detectors. We use the data taken between
        the 3rd of June 2015 and the 14th of June 2015 during the 7th
        engineering run (ER7), and outline the improvements made to
        increase the performance and lower the latency of the algorithms
        on real data. This work provides an important test for
        understanding the performance of these methods on real, non
        stationary data in preparation for the second advanced
        gravitational-wave detector observation run, planned for later
        this year. We show that all methods can classify transients in
        non stationary data with a high level of accuracy and show the
        benefits of using multiple classifiers.}",
          doi = {10.1088/1361-6382/34/3/034002},
archivePrefix = {arXiv},
       eprint = {1609.06262},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017CQGra..34c4002P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...836...56K,
       author = {{Kessler}, R. and {Scolnic}, D.},
        title = "{Correcting Type Ia Supernova Distances for Selection Biases and Contamination in Photometrically Identified Samples}",
      journal = {\apj},
     keywords = {cosmological parameters, supernovae: general, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2017",
        month = "Feb",
       volume = {836},
       number = {1},
          eid = {56},
        pages = {56},
     abstract = "{We present a new technique to create a bin-averaged Hubble diagram (HD)
        from photometrically identified SN Ia data. The resulting HD is
        corrected for selection biases and contamination from core-
        collapse (CC) SNe, and can be used to infer cosmological
        parameters. This method, called {\textquotedblleft}BEAMS with
        Bias Corrections{\textquotedblright} (BBC), includes two fitting
        stages. The first BBC fitting stage uses a posterior
        distribution that includes multiple SN likelihoods, a Monte
        Carlo simulation to bias-correct the fitted SALT-II parameters,
        and CC probabilities determined from a machine-learning
        technique. The BBC fit determines (1) a bin-averaged HD (average
        distance versus redshift), and (2) the nuisance parameters
        {\ensuremath{\alpha}} and {\ensuremath{\beta}}, which multiply
        the stretch and color (respectively) to standardize the SN
        brightness. In the second stage, the bin-averaged HD is fit to a
        cosmological model where priors can be imposed. We perform high-
        precision tests of the BBC method by simulating large (150,000
        event) data samples corresponding to the Dark Energy Survey
        Supernova Program. Our tests include three models of intrinsic
        scatter, each with two different CC rates. In the BBC fit, the
        SALT-II nuisance parameters {\ensuremath{\alpha}} and
        {\ensuremath{\beta}} are recovered to within 1\% of their true
        values. In the cosmology fit, we determine the dark energy
        equation of state parameter w using a fixed value of
        \{\{\{{\ensuremath{\Omega}} \}\}\}$_{\{\{M}$\}\} as a prior:
        averaging over all six tests based on 6 {\texttimes} 150,000 =
        900,000 SNe, there is a small w-bias of 0.006+/- 0.002. Finally,
        the BBC fitting code is publicly available in the SNANA package.}",
          doi = {10.3847/1538-4357/836/1/56},
archivePrefix = {arXiv},
       eprint = {1610.04677},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...836...56K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...835..255V,
       author = {{Vieira}, Daniel and {Krems}, Roman V.},
        title = "{Rate Constants for Fine-structure Excitations in O-H Collisions with Error Bars Obtained by Machine Learning}",
      journal = {\apj},
     keywords = {ISM: atoms, Astrophysics - Astrophysics of Galaxies, Physics - Chemical Physics},
         year = "2017",
        month = "Feb",
       volume = {835},
       number = {2},
          eid = {255},
        pages = {255},
     abstract = "{We present an approach using a combination of coupled channel scattering
        calculations with a machine-learning technique based on Gaussian
        Process regression to determine the sensitivity of the rate
        constants for non-adiabatic transitions in inelastic atomic
        collisions to variations of the underlying adiabatic interaction
        potentials. Using this approach, we improve the previous
        computations of the rate constants for the fine-structure
        transitions in collisions of O(\{\}$^{3}$\{P\}$_{j}$) with
        atomic H. We compute the error bars of the rate constants
        corresponding to 20\% variations of the ab initio potentials and
        show that this method can be used to determine which of the
        individual adiabatic potentials are more or less important for
        the outcome of different fine-structure changing collisions.}",
          doi = {10.3847/1538-4357/835/2/255},
archivePrefix = {arXiv},
       eprint = {1701.01897},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...835..255V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...835..156N,
       author = {{Nishizuka}, N. and {Sugiura}, K. and {Kubo}, Y. and {Den}, M. and
         {Watari}, S. and {Ishii}, M.},
        title = "{Solar Flare Prediction Model with Three Machine-learning Algorithms using Ultraviolet Brightening and Vector Magnetograms}",
      journal = {\apj},
     keywords = {magnetic fields, methods: statistical, Sun: activity, Sun: chromosphere, Sun: flares, Sun: X-rays, gamma rays, Astrophysics - Solar and Stellar Astrophysics},
         year = "2017",
        month = "Feb",
       volume = {835},
       number = {2},
          eid = {156},
        pages = {156},
     abstract = "{We developed a flare prediction model using machine learning, which is
        optimized to predict the maximum class of flares occurring in
        the following 24 hr. Machine learning is used to devise
        algorithms that can learn from and make decisions on a huge
        amount of data. We used solar observation data during the period
        2010-2015, such as vector magnetograms, ultraviolet (UV)
        emission, and soft X-ray emission taken by the Solar Dynamics
        Observatory and the Geostationary Operational Environmental
        Satellite. We detected active regions (ARs) from the full-disk
        magnetogram, from which ̃60 features were extracted with their
        time differentials, including magnetic neutral lines, the
        current helicity, the UV brightening, and the flare history.
        After standardizing the feature database, we fully shuffled and
        randomly separated it into two for training and testing. To
        investigate which algorithm is best for flare prediction, we
        compared three machine-learning algorithms: the support vector
        machine, k-nearest neighbors (k-NN), and extremely randomized
        trees. The prediction score, the true skill statistic, was
        higher than 0.9 with a fully shuffled data set, which is higher
        than that for human forecasts. It was found that k-NN has the
        highest performance among the three algorithms. The ranking of
        the feature importance showed that previous flare activity is
        most effective, followed by the length of magnetic neutral
        lines, the unsigned magnetic flux, the area of UV brightening,
        and the time differentials of features over 24 hr, all of which
        are strongly correlated with the flux emergence dynamics in an
        AR.}",
          doi = {10.3847/1538-4357/835/2/156},
archivePrefix = {arXiv},
       eprint = {1611.01791},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...835..156N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017AJ....153...73M,
       author = {{Miller}, A.~A. and {Kulkarni}, M.~K. and {Cao}, Y. and {Laher}, R.~R. and
         {Masci}, F.~J. and {Surace}, J.~A.},
        title = "{Preparing for Advanced LIGO: A Star-Galaxy Separation Catalog for the Palomar Transient Factory}",
      journal = {\aj},
     keywords = {catalogs, galaxies: statistics, methods: data analysis, methods: statistical, stars: statistics, surveys, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Feb",
       volume = {153},
       number = {2},
          eid = {73},
        pages = {73},
     abstract = "{The search for fast optical transients, such as the expected
        electromagnetic counterparts to binary neutron star mergers, is
        riddled with false positives (FPs) ranging from asteroids to
        stellar flares. While moving objects are readily rejected via
        image pairs separated by ̃1 hr, stellar flares represent a
        challenging foreground, significantly outnumbering rapidly
        evolving explosions. Identifying stellar sources close to and
        fainter than the transient detection limit can eliminate these
        FPs. Here, we present a method to reliably identify stars in
        deep co-adds of Palomar Transient Factory (PTF) imaging. Our
        machine-learning methodology utilizes the random forest (RF)
        algorithm, which is trained using \&gt; 3{\texttimes}
        \{10\}$^{6}$ sources with Sloan Digital Sky Survey (SDSS)
        spectra. When evaluated on an independent test set, the PTF RF
        model outperforms the SExtractor star classifier by ̃4\%. For
        faint sources (r$^{\textbackslashprime}$
        {\ensuremath{\geq}}slant 21 mag), which dominate the field
        population, the PTF RF model produces a ̃19\% improvement over
        SExtractor. To avoid false negatives in the PTF transient-
        candidate stream, we adopt a conservative stellar classification
        threshold, corresponding to a galaxy misclassification rate of
        0.005. Ultimately, ̃1.70{\texttimes} \{10\}$^{8}$ objects are
        included in our PTF point-source catalog, of which only
        ̃{}10$^{6}$ are expected to be galaxies. We demonstrate that the
        PTF RF catalog reveals transients that otherwise would have been
        missed. To leverage its superior image quality, we additionally
        create an SDSS point-source catalog, which is also tuned to have
        a galaxy misclassification rate of 0.005. These catalogs have
        been incorporated into the PTF real-time pipelines to
        automatically reject stellar sources as non-extragalactic
        transients.}",
          doi = {10.3847/1538-3881/153/2/73},
archivePrefix = {arXiv},
       eprint = {1703.07356},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017AJ....153...73M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017AdSpR..59..996G,
       author = {{Gupta}, Dileep Kumar and {Prasad}, Rajendra and {Kumar}, Pradeep and
         {Vishwakarma}, Ajeet Kumar},
        title = "{Soil moisture retrieval using ground based bistatic scatterometer data at X-band}",
      journal = {Advances in Space Research},
     keywords = {Soil moisture, Microwave remote sensing, BPANN, RBFANN, GRANN, Regression analysis},
         year = "2017",
        month = "Feb",
       volume = {59},
       number = {4},
        pages = {996-1007},
     abstract = "{Several hydrological phenomenon and applications need high quality soil
        moisture information of the top Earth surface. The advent of
        technologies like bistatic scatterometer can retrieve soil
        moisture information with high accuracy and hence used in
        present study. The radar data is acquired by specially designed
        ground based bistatic scatterometer system in the specular
        direction of 20-70{\textdegree} incidence angles at steps of
        5{\textdegree} for HH and VV polarizations. This study provides
        first time comprehensive evaluation of different machine
        learning algorithms for the retrieval of soil moisture using the
        X-band bistatic scatterometer measurements. The comparison of
        different artificial neural network (ANN) models such as back
        propagation artificial neural network (BPANN), radial basis
        function artificial neural network (RBFANN), generalized
        regression artificial neural network (GRANN) along with linear
        regression model (LRM) are used to estimate the soil moisture.
        The performance indices such as \%Bias, Root Mean Squared Error
        (RMSE) and Nash-Sutcliffe Efficiency (NSE) are used to evaluate
        the performances of the machine learning techniques. Among
        different models employed in this study, the BPANN is found to
        have marginally higher performance in case of HH polarization
        while RBFANN is found suitable with VV polarization followed by
        GRANN and LRM. The results obtained are of considerable
        scientific and practical value to the wider scientific community
        for the number of practical applications and research studies in
        which radar datasets are used.}",
          doi = {10.1016/j.asr.2016.11.032},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017AdSpR..59..996G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2017PhDT.......293S,
       author = {{Silburt}, Ari},
        title = "{Statistics, Formation and Stability of Exoplanetary Systems}",
     keywords = {Astrophysics;Astronomy},
       school = {University of Toronto (Canada)},
         year = "2017",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017PhDT.......293S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2017PhDT.......140P,
       author = {{Prakash}, Abhishek},
        title = "{Probing the Large-Scale Structure of the Universe Through Luminous Red Galaxies}",
     keywords = {Physics;Astronomy;Astrophysics},
       school = {University of Pittsburgh},
         year = "2017",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017PhDT.......140P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017PASP..129a4002M,
       author = {{Masci}, Frank J. and {Laher}, Russ R. and {Rebbapragada}, Umaa D. and
         {Doran}, Gary B. and {Miller}, Adam A. and {Bellm}, Eric and
         {Kasliwal}, Mansi and {Ofek}, Eran O. and {Surace}, Jason and
         {Shupe}, David L. and {Grillmair}, Carl J. and {Jackson}, Ed and
         {Barlow}, Tom and {Yan}, Lin and {Cao}, Yi and {Cenko}, S. Bradley and
         {Storrie-Lombardi}, Lisa J. and {Helou}, George and
         {Prince}, Thomas A. and {Kulkarni}, Shrinivas R.},
        title = "{The IPAC Image Subtraction and Discovery Pipeline for the Intermediate Palomar Transient Factory}",
      journal = {\pasp},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Jan",
       volume = {129},
       number = {971},
        pages = {014002},
     abstract = "{We describe the near real-time transient-source discovery engine for the
        intermediate Palomar Transient Factory (iPTF), currently in
        operations at the Infrared Processing and Analysis Center
        (IPAC), Caltech. We coin this system the IPAC/iPTF Discovery
        Engine (or IDE). We review the algorithms used for PSF-matching,
        image subtraction, detection, photometry, and machine-learned
        (ML) vetting of extracted transient candidates. We also review
        the performance of our ML classifier. For a limiting signal-to-
        noise ratio of 4 in relatively unconfused regions, bogus
        candidates from processing artifacts and imperfect image
        subtractions outnumber real transients by ≃10:1. This can be
        considerably higher for image data with inaccurate astrometric
        and/or PSF-matching solutions. Despite this occasionally high
        contamination rate, the ML classifier is able to identify real
        transients with an efficiency (or completeness) of ≃97\% for a
        maximum tolerable false-positive rate of 1\% when classifying
        raw candidates. All subtraction-image metrics, source features,
        ML probability-based real-bogus scores, contextual metadata from
        other surveys, and possible associations with known Solar System
        objects are stored in a relational database for retrieval by the
        various science working groups. We review our efforts in
        mitigating false-positives and our experience in optimizing the
        overall system in response to the multitude of science projects
        underway with iPTF.}",
          doi = {10.1088/1538-3873/129/971/014002},
archivePrefix = {arXiv},
       eprint = {1608.01733},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017PASP..129a4002M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017OAP....30..178K,
       author = {{Khramtsov}, V. and {Akhmetov}, V.},
        title = "{Supervised Automatic Identification of Extragalactic Sources in the WISExSUPERCOSMOS Catalogue}",
      journal = {Odessa Astronomical Publications},
     keywords = {Catalogues, statistics, extragalactic objects, Methods, machine learning, data analysis},
         year = "2017",
        month = "Jan",
       volume = {30},
        pages = {178},
     abstract = "{We present new catalogue of ̃8,500,000 extragalactic objects as a result
        of automatic classification of WISE and SuperCOSMOS (SCOS)
        cross-identification product. The main goal is to create a set
        of candidates in extragalactic objects due to colour
        (photometric) features through machine learning techniques.
        Extragalactic sources were separated from stars in high-
        dimensional colour space using Support Vector Machine (SVM)
        classifier. Construction of catalogue of the extragalactic
        objects is based on the four important procedures: 1. Cross-
        identification of the WISExSCOS catalogues. 2. Training set
        creation (Gaia DR1 and 2MASX/XSC data). 3. Feature engineering
        and colour-space constructing for further learning and
        classification. 4. Fine-tuning of SVM and separation and
        classification processes. In result we got high-accuracy (̃98\%)
        algorithm for extragalactic source identification in built
        colour space. Product of algorithm realization is presented as
        photometric catalogue of the extragalactic objects and can be
        used for further astronomical investigations.}",
          doi = {10.18524/1810-4215.2017.30.114465},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017OAP....30..178K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017MNRAS.464.2577S,
       author = {{Stensbo-Smidt}, Kristoffer and {Gieseke}, Fabian and {Igel}, Christian and
         {Zirm}, Andrew and {Steenstrup Pedersen}, Kim},
        title = "{Sacrificing information for the greater good: how to select photometric bands for optimal accuracy}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, techniques: photometric, galaxies: distances and redshifts, galaxies: star formation, galaxies: statistics, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Machine Learning},
         year = "2017",
        month = "Jan",
       volume = {464},
       number = {3},
        pages = {2577-2596},
     abstract = "{Large-scale surveys make huge amounts of photometric data available.
        Because of the sheer amount of objects, spectral data cannot be
        obtained for all of them. Therefore, it is important to devise
        techniques for reliably estimating physical properties of
        objects from photometric information alone. These estimates are
        needed to automatically identify interesting objects worth a
        follow-up investigation as well as to produce the required data
        for a statistical analysis of the space covered by a survey. We
        argue that machine learning techniques are suitable to compute
        these estimates accurately and efficiently. This study promotes
        a feature selection algorithm, which selects the most
        informative magnitudes and colours for a given task of
        estimating physical quantities from photometric data alone.
        Using k-nearest neighbours regression, a well-known non-
        parametric machine learning method, we show that using the found
        features significantly increases the accuracy of the estimations
        compared to using standard features and standard methods. We
        illustrate the usefulness of the approach by estimating specific
        star formation rates (sSFRs) and redshifts (photo-z's) using
        only the broad-band photometry from the Sloan Digital Sky Survey
        (SDSS). For estimating sSFRs, we demonstrate that our method
        produces better estimates than traditional spectral energy
        distribution fitting. For estimating photo-z's, we show that our
        method produces more accurate photo-z's than the method employed
        by SDSS. The study highlights the general importance of
        performing proper model selection to improve the results of
        machine learning systems and how feature selection can provide
        insights into the predictive relevance of particular input
        features.}",
          doi = {10.1093/mnras/stw2476},
archivePrefix = {arXiv},
       eprint = {1511.05424},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017MNRAS.464.2577S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ICRC...35..826F,
       author = {{Feng}, Q. and {Jarvis}, J. and {VERITAS Collaboration}},
        title = "{A citizen-science approach to muon events in imaging atmospheric Cherenkov telescope data: the Muon Hunter}",
      journal = {International Cosmic Ray Conference},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2017",
        month = "Jan",
       volume = {301},
          eid = {826},
        pages = {826},
     abstract = "{Event classification is a common task in gamma-ray astrophysics. It can
        be treated with rapidly-advancing machine learning algorithms,
        which have the potential to outperform traditional analysis
        methods. However, a major challenge for machine learning models
        is extracting reliably labelled training examples from real
        data. Citizen science offers a promising approach to tackle this
        challenge. We present ``Muon Hunter'', a citizen science project
        hosted on the Zooniverse platform, where VERITAS data are
        classified multiple times by individual users in order to select
        and parameterize muon events, a product from cosmic ray induced
        showers. We use this dataset to train and validate a
        convolutional neural-network model to identify muon events for
        use in monitoring and calibration. The results of this work and
        our experience of using the Zooniverse are presented.}",
archivePrefix = {arXiv},
       eprint = {1708.06393},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ICRC...35..826F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ICRC...35..764G,
       author = {{Garrappa}, S. and {Gargano}, F. and {Mazziottai}, M.~N. and
         {Fusco}, P. and {Loparco}, F. and {Dampe Collaboration}},
        title = "{A Machine Learning classifier for photon selection with the DAMPE detector}",
      journal = {International Cosmic Ray Conference},
         year = "2017",
        month = "Jan",
       volume = {301},
          eid = {764},
        pages = {764},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ICRC...35..764G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ICRC...35..626E,
       author = {{Einecke}, S. and {Elsaesser}, D. and {Rhode}, W. and {Morik}, K.},
        title = "{Towards Refined Population Studies: High-Confidence Blazar Candidates and their Multiwavelength Counterparts using Machine Learning}",
      journal = {International Cosmic Ray Conference},
         year = "2017",
        month = "Jan",
       volume = {301},
          eid = {626},
        pages = {626},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ICRC...35..626E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ICRC...35..600L,
       author = {{Lefaucheur}, J. and {Boisson}, C. and {Goldoni}, P. and {Pita}, S.},
        title = "{Looking for infrared counterparts of Fermi/LAT blazar candidates}",
      journal = {International Cosmic Ray Conference},
     keywords = {Astrophysics - High Energy Astrophysical Phenomena},
         year = "2017",
        month = "Jan",
       volume = {301},
          eid = {600},
        pages = {600},
     abstract = "{The Fermi/LAT telescope is an efficient blazar-detector in the MeV/GeV
        range. More than 1100 (900) blazars detected above 100 MeV (10
        GeV) are clearly associated to BL Lacertae or Flat Spectrum
        Radio Quasar objects in the Fermi/LAT 3FGL catalogue. This
        number could significantly increase if multi-wavelength
        counterparts could be identified for the 573 3FGL blazars with
        unknown type, or even for the 1010 3FGL unassociated sources
        which are thought to be dominated by blazars, at least at high
        galactic latitude. Unfortunately, the size of the Fermi/LAT
        error box makes multi-wavelength follow-ups difficult. We
        propose a method to associate ``blazar-like'' infrared
        counterparts, having coordinates with a precision of a few
        arcseconds, to Fermi/LAT blazars and unassociated sources. To
        reach this goal, we built machine-learning classifiers based on
        the statistical differences of magnitude measurements obtained
        by the WISE satellite, between a sample of well-identified
        infrared blazars and samples of other types of infrared sources
        located in regions of the sky where no known blazar is present.
        We provide a list of potential infrared counterparts for 3FGL
        blazar candidates, along with the associated number of expected
        false positives. This study contributes to increase the number
        of well-identified extragalactic blazars and also provides
        promising blazar targets for the Cherenkov Telescope Array.}",
archivePrefix = {arXiv},
       eprint = {1709.10168},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ICRC...35..600L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017GeoRL..44..113P,
       author = {{Provost}, F. and {Hibert}, C. and {Malet}, J. -P.},
        title = "{Automatic classification of endogenous landslide seismicity using the Random Forest supervised classifier}",
      journal = {\grl},
     keywords = {landslide seismology, classification, random forest, machine learning},
         year = "2017",
        month = "Jan",
       volume = {44},
       number = {1},
        pages = {113-120},
     abstract = "{The deformation of slow-moving landslides developed in clays induces
        endogenous seismicity of mostly low-magnitude events
        (M$_{L}$\&lt;1). Long seismic records and complete catalogs are
        needed to identify the type of seismic sources and understand
        their mechanisms. Manual classification of long records is time-
        consuming and may be highly subjective. We propose an automatic
        classification method based on the computation of 71 seismic
        attributes and the use of a supervised classifier. No attribute
        was selected a priori in order to create a generic multi-class
        classification method applicable to many landslide contexts. The
        method can be applied directly on the results of a simple
        detector. We developed the approach on the seismic network of
        eight sensors of the Super-Sauze clay-rich landslide (South
        French Alps) for the detection of four types of seismic sources.
        The automatic algorithm retrieves 93\% of sensitivity in
        comparison to a manually interpreted catalog considered as
        reference.}",
          doi = {10.1002/2016GL070709},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017GeoRL..44..113P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017EM&P..119...47T,
       author = {{Tar}, P.~D. and {Bugiolacchi}, R. and {Thacker}, N.~A. and
         {Gilmour}, J.~D.},
        title = "{Estimating False Positive Contamination in Crater Annotations from Citizen Science Data}",
      journal = {Earth Moon and Planets},
         year = "2017",
        month = "Jan",
       volume = {119},
       number = {2-3},
        pages = {47-63},
     abstract = "{Web-based citizen science often involves the classification of image
        features by large numbers of minimally trained volunteers, such
        as the identification of lunar impact craters under the Moon Zoo
        project. Whilst such approaches facilitate the analysis of large
        image data sets, the inexperience of users and ambiguity in
        image content can lead to contamination from false positive
        identifications. We give an approach, using Linear Poisson
        Models and image template matching, that can quantify levels of
        false positive contamination in citizen science Moon Zoo crater
        annotations. Linear Poisson Models are a form of machine
        learning which supports predictive error modelling and goodness-
        of-fits, unlike most alternative machine learning methods. The
        proposed supervised learning system can reduce the variability
        in crater counts whilst providing predictive error assessments
        of estimated quantities of remaining true verses false
        annotations. In an area of research influenced by human
        subjectivity, the proposed method provides a level of
        objectivity through the utilisation of image evidence, guided by
        candidate crater identifications.}",
          doi = {10.1007/s11038-016-9499-9},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017EM&P..119...47T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...834..155M,
       author = {{Moss}, V.~A. and {Lockman}, F.~J. and {McClure-Griffiths}, N.~M.},
        title = "{Tracing Dense and Diffuse Neutral Hydrogen in the Halo of the Milky Way}",
      journal = {\apj},
     keywords = {catalogs, Galaxy: halo, ISM: clouds, radio lines: general, surveys, Astrophysics - Astrophysics of Galaxies},
         year = "2017",
        month = "Jan",
       volume = {834},
       number = {2},
          eid = {155},
        pages = {155},
     abstract = "{We have combined observations of Galactic high-velocity H I from two
        surveys: a very sensitive survey from the Green Bank 140 ft
        Telescope with limited sky coverage, and the less sensitive but
        complete Galactic All Sky Survey from the 64 m Parkes Radio
        Telescope. The two surveys preferentially detect different forms
        of neutral gas due to their sensitivity. We adopt a machine
        learning approach to divide our data into two populations that
        separate across a range in column density: (1) a narrow line-
        width population typical of the majority of bright high velocity
        cloud components, and (2) a fainter, broad line-width population
        that aligns well with that of the population found in the Green
        Bank survey. We refer to these populations as dense and diffuse
        gas, respectively, and find that diffuse gas is typically
        located at the edges and in the tails of high velocity clouds,
        surrounding dense components in the core. A fit to the average
        spectrum of each type of gas in the Galactic All Sky Survey data
        reveals the dense population to have a typical line width of ̃20
        km s$^{-1}$ and brightness temperature of ̃0.3 K, while the
        diffuse population has a typical line width of ̃30 km s$^{-1}$
        and a brightness temperature of ̃0.2 K. Our results confirm that
        most surveys of high velocity gas in the Milky Way halo are
        missing the majority of the ubiquitous diffuse gas, and that
        this gas is likely to contribute at least as much mass as the
        dense gas.}",
          doi = {10.3847/1538-4357/834/2/155},
archivePrefix = {arXiv},
       eprint = {1611.08329},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...834..155M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017ApJ...834...11R,
       author = {{Raboonik}, Abbas and {Safari}, Hossein and {Alipour}, Nasibe and
         {Wheatland}, Michael S.},
        title = "{Prediction of Solar Flares Using Unique Signatures of Magnetic Field Images}",
      journal = {\apj},
     keywords = {sunspots, Sun: activity, Sun: flares, Sun: magnetic fields, Astrophysics - Solar and Stellar Astrophysics},
         year = "2017",
        month = "Jan",
       volume = {834},
       number = {1},
          eid = {11},
        pages = {11},
     abstract = "{Prediction of solar flares is an important task in solar physics. The
        occurrence of solar flares is highly dependent on the structure
        and topology of solar magnetic fields. A new method for
        predicting large (M- and X-class) flares is presented, which
        uses machine learning methods applied to the Zernike moments
        (ZM) of magnetograms observed by the Helioseismic and Magnetic
        Imager on board the Solar Dynamics Observatory for a period of
        six years from 2010 June 2 to 2016 August 1. Magnetic field
        images consisting of the radial component of the magnetic field
        are converted to finite sets of ZMs and fed to the support
        vector machine classifier. ZMs have the capability to elicit
        unique features from any 2D image, which may allow more accurate
        classification. The results indicate whether an arbitrary active
        region has the potential to produce at least one large flare. We
        show that the majority of large flares can be predicted within
        48 hr before their occurrence, with only 10 false negatives out
        of 385 flaring active region magnetograms and 21 false positives
        out of 179 non-flaring active region magnetograms. Our method
        may provide a useful tool for the prediction of solar flares,
        which can be employed alongside other forecasting methods.}",
          doi = {10.3847/1538-4357/834/1/11},
archivePrefix = {arXiv},
       eprint = {1610.03222},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017ApJ...834...11R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017A&A...597A.135B,
       author = {{Bom}, C.~R. and {Makler}, M. and {Albuquerque}, M.~P. and {Brand
        t}, C.~H.},
        title = "{A neural network gravitational arc finder based on the Mediatrix filamentation method}",
      journal = {\aap},
     keywords = {gravitational lensing: strong, techniques: image processing, methods: numerical, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2017",
        month = "Jan",
       volume = {597},
          eid = {A135},
        pages = {A135},
     abstract = "{Context. Automated arc detection methods are needed to scan the ongoing
        and next-generation wide-field imaging surveys, which are
        expected to contain thousands of strong lensing systems. Arc
        finders are also required for a quantitative comparison between
        predictions and observations of arc abundance. Several
        algorithms have been proposed to this end, but machine learning
        methods have remained as a relatively unexplored step in the arc
        finding process. <BR /> Aims: In this work we introduce a new
        arc finder based on pattern recognition, which uses a set of
        morphological measurements that are derived from the Mediatrix
        filamentation method as entries to an artificial neural network
        (ANN). We show a full example of the application of the arc
        finder, first training and validating the ANN on simulated arcs
        and then applying the code on four Hubble Space Telescope (HST)
        images of strong lensing systems. <BR /> Methods: The simulated
        arcs use simple prescriptions for the lens and the source, while
        mimicking HST observational conditions. We also consider a
        sample of objects from HST images with no arcs in the training
        of the ANN classification. We use the training and validation
        process to determine a suitable set of ANN configurations,
        including the combination of inputs from the Mediatrix method,
        so as to maximize the completeness while keeping the false
        positives low. <BR /> Results: In the simulations the method was
        able to achieve a completeness of about 90\% with respect to the
        arcs that are input into the ANN after a preselection. However,
        this completeness drops to 70\% on the HST images. The false
        detections are on the order of 3\% of the objects detected in
        these images. <BR /> Conclusions: The combination of Mediatrix
        measurements with an ANN is a promising tool for the pattern-
        recognition phase of arc finding. More realistic simulations and
        a larger set of real systems are needed for a better training
        and assessment of the efficiency of the method.}",
          doi = {10.1051/0004-6361/201629159},
archivePrefix = {arXiv},
       eprint = {1607.04644},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017A&A...597A.135B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016PhRvD..94l4040T,
       author = {{Torres-Forn{\'e}}, Alejandro and {Marquina}, Antonio and
         {Font}, Jos{\'e} A. and {Ib{\'a}{\~n}ez}, Jos{\'e} M.},
        title = "{Denoising of gravitational wave signals via dictionary learning algorithms}",
      journal = {\prd},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, General Relativity and Quantum Cosmology},
         year = "2016",
        month = "Dec",
       volume = {94},
       number = {12},
          eid = {124040},
        pages = {124040},
     abstract = "{Gravitational wave astronomy has become a reality after the historical
        detections accomplished during the first observing run of the
        two advanced LIGO detectors. In the following years, the number
        of detections is expected to increase significantly with the
        full commissioning of the advanced LIGO, advanced Virgo and
        KAGRA detectors. The development of sophisticated data analysis
        techniques to improve the opportunities of detection for low
        signal-to-noise-ratio events is, hence, a most crucial effort.
        In this paper, we present one such technique, dictionary-
        learning algorithms, which have been extensively developed in
        the last few years and successfully applied mostly in the
        context of image processing. However, to the best of our
        knowledge, such algorithms have not yet been employed to denoise
        gravitational wave signals. By building dictionaries from
        numerical relativity templates of both binary black holes
        mergers and bursts of rotational core collapse, we show how
        machine-learning algorithms based on dictionaries can also be
        successfully applied for gravitational wave denoising. We use a
        subset of signals from both catalogs, embedded in nonwhite
        Gaussian noise, to assess our techniques with a large sample of
        tests and to find the best model parameters. The application of
        our method to the actual signal GW150914 shows promising
        results. Dictionary-learning algorithms could be a complementary
        addition to the gravitational wave data analysis toolkit. They
        may be used to extract signals from noise and to infer physical
        parameters if the data are in good enough agreement with the
        morphology of the dictionary atoms.}",
          doi = {10.1103/PhysRevD.94.124040},
archivePrefix = {arXiv},
       eprint = {1612.01305},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016PhRvD..94l4040T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016PASJ...68..104M,
       author = {{Morii}, Mikio and {Ikeda}, Shiro and {Tominaga}, Nozomu and
         {Tanaka}, Masaomi and {Morokuma}, Tomoki and {Ishiguro}, Katsuhiko and
         {Yamato}, Junji and {Ueda}, Naonori and {Suzuki}, Naotaka and
         {Yasuda}, Naoki and {Yoshida}, Naoki},
        title = "{Machine-learning selection of optical transients in the Subaru/Hyper Suprime-Cam survey}",
      journal = {\pasj},
     keywords = {methods: data analysis, stars: supernovae: general, techniques: miscellaneous, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Dec",
       volume = {68},
       number = {6},
          eid = {104},
        pages = {104},
     abstract = "{We present an application of machine-learning (ML) techniques to source
        selection in the optical transient survey data with the Hyper
        Suprime-Cam (HSC) on the Subaru telescope. Our goal is to select
        real transient events accurately and in a timely manner out of a
        large number of false candidates, obtained by the standard
        difference-imaging method. We have developed the transient
        selector, which is based on majority voting of the three ML
        machines of AUC Boosting, Random Forest, and Deep Neural
        Networks. We applied it to our observing runs of Subaru-HSC in
        2015 May and August, and proved it to be efficient in selecting
        optical transients. The false positive rate was 1.0\% at the
        true positive rate of 90\% in the magnitude range of 22.0-25.0
        mag for the May data. For the August run, we successfully
        detected and reported 10 supernovae candidates within the same
        day as the observation. From these runs, we learned the
        following lessons: (1) training using artificial objects is
        effective in filtering false candidates out, especially for
        faint objects, and (2) a combination of ML by majority voting is
        advantageous.}",
          doi = {10.1093/pasj/psw096},
archivePrefix = {arXiv},
       eprint = {1609.03249},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016PASJ...68..104M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016P&SS..134...64O,
       author = {{Ogohara}, Kazunori and {Munetomo}, Takafumi and {Hatanaka}, Yuji and
         {Okumura}, Susumu},
        title = "{Automated detection of Martian water ice clouds using Support Vector Machine and simple feature vectors}",
      journal = {\planss},
     keywords = {Mars, Water ice cloud, Automated detection, Mars Global Surveyor, Machine learning},
         year = "2016",
        month = "Dec",
       volume = {134},
        pages = {64-70},
     abstract = "{We present a method for evaluating the presence of Martian water ice
        clouds using difference images and cross-correlation
        distributions calculated from blue band images of the Valles
        Marineris obtained by the Mars Orbiter Camera onboard the Mars
        Global Surveyor (MGS/MOC). We derived one subtracted image and
        one cross-correlation distribution from two reflectance images.
        The difference between the maximum and the average, variance,
        kurtosis, and skewness of the subtracted image were calculated.
        Those of the cross-correlation distribution were also
        calculated. These eight statistics were used as feature vectors
        for training Support Vector Machine because they were the
        simplest of features that was expected to be closely associated
        with the physical properties of water ice clouds. The
        generalization ability was tested using 10-fold cross-
        validation. F-measure and accuracy tended to be approximately
        0.8 if the maximum in the normalized reflectance and the
        difference of the maximum and the average in the cross-
        correlation were selected as features. This result can be
        physically explained because the blue band as well as the red
        band is sensitive to water ice clouds. A simple and low-
        dimensional feature vector enables us to understand the detected
        water ice clouds physically and presents the lower bound of the
        score that classifiers trained using more sophisticated feature
        vectors have to achieve.}",
          doi = {10.1016/j.pss.2016.10.009},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016P&SS..134...64O},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016JCAP...12..008M,
       author = {{M{\"o}ller}, A. and {Ruhlmann-Kleider}, V. and {Leloup}, C. and
         {Neveu}, J. and {Palanque-Delabrouille}, N. and {Rich}, J. and
         {Carlberg}, R. and {Lidman}, C. and {Pritchet}, C.},
        title = "{Photometric classification of type Ia supernovae in the SuperNova Legacy Survey with supervised learning}",
      journal = {Journal of Cosmology and Astro-Particle Physics},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2016",
        month = "Dec",
       volume = {2016},
       number = {12},
          eid = {008},
        pages = {008},
     abstract = "{In the era of large astronomical surveys, photometric classification of
        supernovae (SNe) has become an important research field due to
        limited spectroscopic resources for candidate follow-up and
        classification. In this work, we present a method to
        photometrically classify type Ia supernovae based on machine
        learning with redshifts that are derived from the SN light-
        curves. This method is implemented on real data from the SNLS
        deferred pipeline, a purely photometric pipeline that identifies
        SNe Ia at high-redshifts (0.2 \&lt; z \&lt; 1.1). Our method
        consists of two stages: feature extraction (obtaining the SN
        redshift from photometry and estimating light-curve shape
        parameters) and machine learning classification. We study the
        performance of different algorithms such as Random Forest and
        Boosted Decision Trees. We evaluate the performance using SN
        simulations and real data from the first 3 years of the
        Supernova Legacy Survey (SNLS), which contains large
        spectroscopically and photometrically classified type Ia
        samples. Using the Area Under the Curve (AUC) metric, where
        perfect classification is given by 1, we find that our best-
        performing classifier (Extreme Gradient Boosting Decision Tree)
        has an AUC of 0.98.We show that it is possible to obtain a large
        photometrically selected type Ia SN sample with an estimated
        contamination of less than 5\%. When applied to data from the
        first three years of SNLS, we obtain 529 events. We investigate
        the differences between classifying simulated SNe, and real SN
        survey data. In particular, we find that applying a thorough set
        of selection cuts to the SN sample is essential for good
        classification. This work demonstrates for the first time the
        feasibility of machine learning classification in a high-z SN
        survey with application to real SN data.}",
          doi = {10.1088/1475-7516/2016/12/008},
archivePrefix = {arXiv},
       eprint = {1608.05423},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016JCAP...12..008M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016GeoRL..4312234A,
       author = {{Ashkezari}, Mohammad D. and {Hill}, Christopher N. and
         {Follett}, Christopher N. and {Forget}, Ga{\"e}l. and
         {Follows}, Michael J.},
        title = "{Oceanic eddy detection and lifetime forecast using machine learning methods}",
      journal = {\grl},
     keywords = {ocean, eddy, machine learning, eddy lifetime, remote sensing},
         year = "2016",
        month = "Dec",
       volume = {43},
       number = {23},
        pages = {12,234-12,241},
     abstract = "{We report a novel altimetry-based machine learning approach for eddy
        identification and characterization. The machine learning models
        use daily maps of geostrophic velocity anomalies and are trained
        according to the phase angle between the zonal and meridional
        components at each grid point. The trained models are then used
        to identify the corresponding eddy phase patterns and to predict
        the lifetime of a detected eddy structure. The performance of
        the proposed method is examined at two dynamically different
        regions to demonstrate its robust behavior and region
        independency.}",
          doi = {10.1002/2016GL071269},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016GeoRL..4312234A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016GeoRL..4312131G,
       author = {{Gulbrandsen}, Mats Lundh and {Minsley}, Burke J. and
         {Ball}, Lyndsay B. and {Hansen}, Thomas Mejer},
        title = "{Semiautomatic mapping of permafrost in the Yukon Flats, Alaska}",
      journal = {\grl},
     keywords = {permafrost mapping, airborne electromagnetic data, machine learning},
         year = "2016",
        month = "Dec",
       volume = {43},
       number = {23},
        pages = {12,131-12,137},
     abstract = "{Thawing of permafrost due to global warming can have major impacts on
        hydrogeological processes, climate feedback, arctic ecology, and
        local environments. To understand these effects and processes,
        it is crucial to know the distribution of permafrost. In this
        study we exploit the fact that airborne electromagnetic (AEM)
        data are sensitive to the distribution of permafrost and
        demonstrate how the distribution of permafrost in the Yukon
        Flats, Alaska, is mapped in an efficient (semiautomatic) way,
        using a combination of supervised and unsupervised (machine)
        learning algorithms, i.e., Smart Interpretation and K-means
        clustering. Clustering is used to sort unfrozen and frozen
        regions, and Smart Interpretation is used to predict the depth
        of permafrost based on expert interpretations. This workflow
        allows, for the first time, a quantitative and objective
        approach to efficiently map permafrost based on large amounts of
        AEM data.}",
          doi = {10.1002/2016GL071334},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016GeoRL..4312131G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJ...833...26L,
       author = {{Lawlor}, David and {Budav{\'a}ri}, Tam{\'a}s and {Mahoney}, Michael W.},
        title = "{Mapping the Similarities of Spectra: Global and Locally-biased Approaches to SDSS Galaxies}",
      journal = {\apj},
     keywords = {catalogs, galaxies: general, methods: data analysis, methods: statistical, surveys, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Data Structures and Algorithms, Statistics - Machine Learning},
         year = "2016",
        month = "Dec",
       volume = {833},
       number = {1},
          eid = {26},
        pages = {26},
     abstract = "{We present a novel approach to studying the diversity of galaxies. It is
        based on a novel spectral graph technique, that of locally-
        biased semi-supervised eigenvectors. Our method introduces new
        coordinates that summarize an entire spectrum, similar to but
        going well beyond the widely used Principal Component Analysis
        (PCA). Unlike PCA, however, this technique does not assume that
        the Euclidean distance between galaxy spectra is a good global
        measure of similarity. Instead, we relax that condition to only
        the most similar spectra, and we show that doing so yields more
        reliable results for many astronomical questions of interest.
        The global variant of our approach can identify very finely
        numerous astronomical phenomena of interest. The locally-biased
        variants of our basic approach enable us to explore subtle
        trends around a set of chosen objects. The power of the method
        is demonstrated in the Sloan Digital Sky Survey Main Galaxy
        Sample, by illustrating that the derived spectral coordinates
        carry an unprecedented amount of information.}",
          doi = {10.3847/0004-637X/833/1/26},
archivePrefix = {arXiv},
       eprint = {1609.03932},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJ...833...26L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJ...832L..22T,
       author = {{Tamayo}, Daniel and {Silburt}, Ari and {Valencia}, Diana and
         {Menou}, Kristen and {Ali-Dib}, Mohamad and {Petrovich}, Cristobal and
         {Huang}, Chelsea X. and {Rein}, Hanno and {van Laerhoven}, Christa and
         {Paradise}, Adiv and {Obertas}, Alysa and {Murray}, Norman},
        title = "{A Machine Learns to Predict the Stability of Tightly Packed Planetary Systems}",
      journal = {\apj},
     keywords = {celestial mechanics, chaos, planets and satellites: dynamical evolution and stability, Astrophysics - Earth and Planetary Astrophysics},
         year = "2016",
        month = "Dec",
       volume = {832},
       number = {2},
          eid = {L22},
        pages = {L22},
     abstract = "{The requirement that planetary systems be dynamically stable is often
        used to vet new discoveries or set limits on unconstrained
        masses or orbital elements. This is typically carried out via
        computationally expensive N-body simulations. We show that
        characterizing the complicated and multi-dimensional stability
        boundary of tightly packed systems is amenable to machine-
        learning methods. We find that training an XGBoost machine-
        learning algorithm on physically motivated features yields an
        accurate classifier of stability in packed systems. On the
        stability timescale investigated ({}10$^{7}$ orbits), it is
        three orders of magnitude faster than direct N-body simulations.
        Optimized machine-learning classifiers for dynamical stability
        may thus prove useful across the discipline, e.g., to
        characterize the exoplanet sample discovered by the upcoming
        Transiting Exoplanet Survey Satellite. This proof of concept
        motivates investing computational resources to train algorithms
        capable of predicting stability over longer timescales and over
        broader regions of phase space.}",
          doi = {10.3847/2041-8205/832/2/L22},
archivePrefix = {arXiv},
       eprint = {1610.05359},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJ...832L..22T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016AJ....152..154G,
       author = {{Gupta}, Ravi R. and {Kuhlmann}, Steve and {Kovacs}, Eve and
         {Spinka}, Harold and {Kessler}, Richard and {Goldstein}, Daniel A. and
         {Liotine}, Camille and {Pomian}, Katarzyna and {D'Andrea}, Chris B. and
         {Sullivan}, Mark and {Carretero}, Jorge and {Castander}, Francisco J. and
         {Nichol}, Robert C. and {Finley}, David A. and {Fischer}, John A. and
         {Foley}, Ryan J. and {Kim}, Alex G. and {Papadopoulos}, Andreas and
         {Sako}, Masao and {Scolnic}, Daniel M. and {Smith}, Mathew and
         {Tucker}, Brad E. and {Uddin}, Syed and {Wolf}, Rachel C. and
         {Yuan}, Fang and {Abbott}, Tim M.~C. and {Abdalla}, Filipe B. and
         {Benoit-L{\'e}vy}, Aur{\'e}lien and {Bertin}, Emmanuel and
         {Brooks}, David and {Carnero Rosell}, Aurelio and
         {Carrasco Kind}, Matias and {Cunha}, Carlos E. and {da Costa}, Luiz N. and
         {Desai}, Shantanu and {Doel}, Peter and {Eifler}, Tim F. and
         {Evrard}, August E. and {Flaugher}, Brenna and {Fosalba}, Pablo and
         {Gazta{\~n}aga}, Enrique and {Gruen}, Daniel and {Gruendl}, Robert and
         {James}, David J. and {Kuehn}, Kyler and {Kuropatkin}, Nikolay and
         {Maia}, Marcio A.~G. and {Marshall}, Jennifer L. and {Miquel}, Ramon and
         {Plazas}, Andr{\'e}s A. and {Romer}, A. Kathy and
         {S{\'a}nchez}, Eusebio and {Schubnell}, Michael and
         {Sevilla-Noarbe}, Ignacio and {Sobreira}, Fl{\'a}via and
         {Suchyta}, Eric and {Swanson}, Molly E.~C. and {Tarle}, Gregory and
         {Walker}, Alistair R. and {Wester}, William},
        title = "{Host Galaxy Identification for Supernova Surveys}",
      journal = {\aj},
     keywords = {catalogs, galaxies: general, supernovae: general, surveys, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2016",
        month = "Dec",
       volume = {152},
       number = {6},
          eid = {154},
        pages = {154},
     abstract = "{Host galaxy identification is a crucial step for modern supernova (SN)
        surveys such as the Dark Energy Survey and the Large Synoptic
        Survey Telescope, which will discover SNe by the thousands.
        Spectroscopic resources are limited, and so in the absence of
        real-time SN spectra these surveys must rely on host galaxy
        spectra to obtain accurate redshifts for the Hubble diagram and
        to improve photometric classification of SNe. In addition, SN
        luminosities are known to correlate with host-galaxy properties.
        Therefore, reliable identification of host galaxies is essential
        for cosmology and SN science. We simulate SN events and their
        locations within their host galaxies to develop and test methods
        for matching SNe to their hosts. We use both real and simulated
        galaxy catalog data from the Advanced Camera for Surveys General
        Catalog and MICECATv2.0, respectively. We also incorporate
        {\textquotedblleft}hostless{\textquotedblright} SNe residing in
        undetected faint hosts into our analysis, with an assumed
        hostless rate of 5\%. Our fully automated algorithm is run on
        catalog data and matches SNe to their hosts with 91\% accuracy.
        We find that including a machine learning component, run after
        the initial matching algorithm, improves the accuracy (purity)
        of the matching to 97\% with a 2\% cost in efficiency (true
        positive rate). Although the exact results are dependent on the
        details of the survey and the galaxy catalogs used, the method
        of identifying host galaxies we outline here can be applied to
        any transient survey.}",
          doi = {10.3847/0004-6256/152/6/154},
archivePrefix = {arXiv},
       eprint = {1604.06138},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016AJ....152..154G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016AcA....66..421P,
       author = {{Pawlak}, M. and {Soszy{\'n}ski}, I. and {Udalski}, A. and
         {Szyma{\'n}ski}, M.~K. and {Wyrzykowski}, {\L}. and {Ulaczyk}, K. and
         {Poleski}, R. and {Pietrukowicz}, P. and {Koz{\l}owski}, S. and
         {Skowron}, D.~M. and {Skowron}, J. and {Mr{\'o}z}, P. and
         {Hamanowicz}, A.},
        title = "{The OGLE Collection of Variable Stars. Eclipsing Binaries in the Magellanic System}",
      journal = {\actaa},
     keywords = {binaries: eclipsing, Stars: variables: general, Magellanic Clouds, Astrophysics - Solar and Stellar Astrophysics},
         year = "2016",
        month = "Dec",
       volume = {66},
       number = {4},
        pages = {421-432},
     abstract = "{We present the collection of eclipsing binaries in the Large and Small
        Magellanic Clouds, based on the OGLE survey. It contains 48 605
        systems, 40 204 belonging to the LMC and 8401 to the SMC. Out of
        the total number of presented here binaries, 16 374 are the new
        discoveries. We present the time-series photometry obtained for
        the selected objects during the fourth phase of the OGLE
        project. The catalog has been created using a two step machine
        learning procedure based on the Random Forest algorithm.}",
archivePrefix = {arXiv},
       eprint = {1612.06394},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016AcA....66..421P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016PASP..128k4502C,
       author = {{Cao}, Yi and {Nugent}, Peter E. and {Kasliwal}, Mansi M.},
        title = "{Intermediate Palomar Transient Factory: Realtime Image Subtraction Pipeline}",
      journal = {\pasp},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Nov",
       volume = {128},
       number = {969},
        pages = {114502},
     abstract = "{A fast-turnaround pipeline for realtime data reduction plays an
        essential role in discovering and permitting follow-up
        observations to young supernovae and fast-evolving transients in
        modern time-domain surveys. In this paper, we present the
        realtime image subtraction pipeline in the intermediate Palomar
        Transient Factory. By using high-performance computing,
        efficient databases, and machine-learning algorithms, this
        pipeline manages to reliably deliver transient candidates within
        10 minutes of images being taken. Our experience in using high-
        performance computing resources to process big data in astronomy
        serves as a trailblazer to dealing with data from large-scale
        time-domain facilities in the near future.}",
          doi = {10.1088/1538-3873/128/969/114502},
archivePrefix = {arXiv},
       eprint = {1608.01006},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016PASP..128k4502C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.462.4197C,
       author = {{Curran}, S.~J. and {Duchesne}, S.~W. and {Divoli}, A. and
         {Allison}, J.~R.},
        title = "{A comparative study of intervening and associated H I 21-cm absorption profiles in redshifted galaxies}",
      journal = {\mnras},
     keywords = {methods: data analysis, galaxies: active, galaxies: high redshift, galaxies: ISM, quasars: absorption lines, radio lines: galaxies, Astrophysics - Astrophysics of Galaxies},
         year = "2016",
        month = "Nov",
       volume = {462},
       number = {4},
        pages = {4197-4207},
     abstract = "{The star-forming reservoir in the distant Universe can be detected
        through H I 21-cm absorption arising from either cool gas
        associated with a radio source or from within a galaxy
        intervening the sightline to the continuum source. In order to
        test whether the nature of the absorber can be predicted from
        the profile shape, we have compiled and analysed all of the
        known redshifted (z {\ensuremath{\geq}} 0.1) H I 21-cm
        absorption profiles. Although between individual spectra there
        is too much variation to assign a typical spectral profile, we
        confirm that associated absorption profiles are, on average,
        wider than their intervening counterparts. It is widely
        hypothesized that this is due to high-velocity nuclear gas
        feeding the central engine, absent in the more quiescent
        intervening absorbers. Modelling the column density distribution
        of the mean associated and intervening spectra, we confirm that
        the additional low optical depth, wide dispersion component,
        typical of associated absorbers, arises from gas within the
        inner parsec. With regard to the potential of predicting the
        absorber type in the absence of optical spectroscopy, we have
        implemented machine learning techniques to the 55 associated and
        43 intervening spectra, with each of the tested models giving a
        {\ensuremath{\gtrsim}} 80 per cent accuracy in the prediction of
        the absorber type. Given the impracticability of follow-up
        optical spectroscopy of the large number of 21-cm detections
        expected from the next generation of large radio telescopes,
        this could provide a powerful new technique with which to
        determine the nature of the absorbing galaxy.}",
          doi = {10.1093/mnras/stw1938},
archivePrefix = {arXiv},
       eprint = {1608.01055},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.462.4197C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJS..227....6V,
       author = {{Vogt}, N. and {Contreras-Quijada}, A. and {Fuentes-Morales}, I. and
         {Vogt-Geisse}, S. and {Arcos}, C. and {Abarca}, C. and
         {Agurto-Gangas}, C. and {Caviedes}, M. and {DaSilva}, H. and
         {Flores}, J. and {Gotta}, V. and {Pe{\~n}aloza}, F. and {Rojas}, K. and
         {Villase{\~n}or}, J.~I.},
        title = "{Determination of Pulsation Periods and Other Parameters of 2875 Stars Classified as MIRA in the All Sky Automated Survey (ASAS)}",
      journal = {\apjs},
     keywords = {methods: data analysis, stars: late-type, stars: variable: general, Astrophysics - Solar and Stellar Astrophysics},
         year = "2016",
        month = "Nov",
       volume = {227},
       number = {1},
          eid = {6},
        pages = {6},
     abstract = "{We have developed an interactive PYTHON code and derived crucial
        ephemeris data of 99.4\% of all stars classified as
        {\textquotedblleft}Mira{\textquotedblright} in the ASAS
        database, referring to pulsation periods, mean maximum
        magnitudes, and whenever possible, the amplitudes among others.
        We present a statistical comparison between our results and
        those given by the International Variable Star Index (VSX) of
        the American Association of Variable Star Observers, as well as
        those determined with the machine learning automatic procedure
        of Richards et al. Our periods are in good agreement with those
        of the VSX in more than 95\% of the stars. However, when
        comparing our periods with those of Richards et al., the
        coincidence rate is only 76\% and most of the remaining cases
        refer to aliases. We conclude that automatic codes still require
        more refinements in order to provide reliable period values.
        Period distributions of the target stars show three local maxima
        around 215, 275, and 330 days, apparently of universal validity;
        their relative strength seems to depend on galactic longitude.
        Our visual amplitude distribution turns out to be bimodal,
        however, 1/3 of the targets have rather small amplitudes (A
        \&lt; 2.5$^{m}$) and could refer to semiregular variables (SR).
        We estimate that about 20\% of our targets belong to the SR
        class. We also provide a list of 63 candidates for period
        variations and a sample of 35 multiperiodic stars that seem to
        confirm the universal validity of typical sequences in the
        double period and in the Petersen diagrams.}",
          doi = {10.3847/0067-0049/227/1/6},
archivePrefix = {arXiv},
       eprint = {1609.05246},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJS..227....6V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJ...831..135N,
       author = {{Ntampaka}, M. and {Trac}, H. and {Sutherland}, D.~J. and
         {Fromenteau}, S. and {P{\'o}czos}, B. and {Schneider}, J.},
        title = "{Dynamical Mass Measurements of Contaminated Galaxy Clusters Using Machine Learning}",
      journal = {\apj},
     keywords = {cosmology: theory, dark matter, galaxies: clusters: general, galaxies: kinematics and dynamics, gravitation, large-scale structure of universe, methods: statistical, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2016",
        month = "Nov",
       volume = {831},
       number = {2},
          eid = {135},
        pages = {135},
     abstract = "{We study dynamical mass measurements of galaxy clusters contaminated by
        interlopers and show that a modern machine learning algorithm
        can predict masses by better than a factor of two compared to a
        standard scaling relation approach. We create two mock catalogs
        from Multidark{\textquoteright}s publicly available N-body MDPL1
        simulation, one with perfect galaxy cluster membership
        information and the other where a simple cylindrical cut around
        the cluster center allows interlopers to contaminate the
        clusters. In the standard approach, we use a power-law scaling
        relation to infer cluster mass from galaxy line-of-sight (LOS)
        velocity dispersion. Assuming perfect membership knowledge, this
        unrealistic case produces a wide fractional mass error
        distribution, with a width of \{\{{\ensuremath{\Delta}}
        \}\}{\ensuremath{\varepsilon}} {\ensuremath{\approx}} 0.87.
        Interlopers introduce additional scatter, significantly widening
        the error distribution further (\{\{{\ensuremath{\Delta}}
        \}\}{\ensuremath{\varepsilon}} {\ensuremath{\approx}} 2.13). We
        employ the support distribution machine (SDM) class of
        algorithms to learn from distributions of data to predict single
        values. Applied to distributions of galaxy observables such as
        LOS velocity and projected distance from the cluster center, SDM
        yields better than a factor-of-two improvement
        (\{\{{\ensuremath{\Delta}} \}\}{\ensuremath{\varepsilon}}
        {\ensuremath{\approx}} 0.67) for the contaminated case.
        Remarkably, SDM applied to contaminated clusters is better able
        to recover masses than even the scaling relation approach
        applied to uncontaminated clusters. We show that the SDM method
        more accurately reproduces the cluster mass function, making it
        a valuable tool for employing cluster observations to evaluate
        cosmological models.}",
          doi = {10.3847/0004-637X/831/2/135},
archivePrefix = {arXiv},
       eprint = {1509.05409},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJ...831..135N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016A&A...596A..39K,
       author = {{Krakowski}, T. and {Ma{\l}ek}, K. and {Bilicki}, M. and {Pollo}, A. and
         {Kurcz}, A. and {Krupa}, M.},
        title = "{Machine-learning identification of galaxies in the WISE {\texttimes} SuperCOSMOS all-sky catalogue}",
      journal = {\aap},
     keywords = {methods: data analysis, methods: numerical, astronomical databases: miscellaneous, galaxies: statistics, large-scale structure of Universe, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Nov",
       volume = {596},
          eid = {A39},
        pages = {A39},
     abstract = "{Context. The two currently largest all-sky photometric datasets, WISE
        and SuperCOSMOS, have been recently cross-matched to construct a
        novel photometric redshift catalogue on 70\% of the sky.
        Galaxies were separated from stars and quasars through colour
        cuts, which may leave imperfections because different source
        types may overlap in colour space. <BR /> Aims: The aim of the
        present work is to identify galaxies in the WISE {\texttimes}
        SuperCOSMOS catalogue through an alternative approach of machine
        learning. This allows us to define more complex separations in
        the multi-colour space than is possible with simple colour cuts,
        and should provide a more reliable source classification. <BR />
        Methods: For the automatised classification we used the support
        vector machines (SVM) learning algorithm and employed SDSS
        spectroscopic sources that we cross-matched with WISE
        {\texttimes} SuperCOSMOS to construct the training and
        verification set. We performed a number of tests to examine the
        behaviour of the classifier (completeness, purity, and accuracy)
        as a function of source apparent magnitude and Galactic
        latitude. We then applied the classifier to the full-sky data
        and analysed the resulting catalogue of candidate galaxies. We
        also compared the resulting dataset with the one obtained
        through colour cuts. <BR /> Results: The tests indicate very
        high accuracy, completeness, and purity (\&gt;95\%) of the
        classifier at the bright end; this deteriorates for the faintest
        sources, but still retains acceptable levels of 85\%. No
        significant variation in the classification quality with
        Galactic latitude is observed. When we applied the classifier to
        all-sky WISE {\texttimes} SuperCOSMOS data, we found 15 million
        galaxies after masking problematic areas. The resulting sample
        is purer than the one produced by applying colour cuts, at the
        price of a lower completeness across the sky. <BR />
        Conclusions: The automatic classification is a successful
        alternative approach to colour cuts for defining a reliable
        galaxy sample. The identifications we obtained are included in
        the public release of the WISE {\texttimes} SuperCOSMOS galaxy
        catalogue. The public release of the WISE {\texttimes}
        SuperCOSMOS galaxy catalogue is available from <A href=``http://
        ssa.roe.ac.uk/WISExSCOS''>http://ssa.roe.ac.uk/WISExSCOS</A>}",
          doi = {10.1051/0004-6361/201629165},
archivePrefix = {arXiv},
       eprint = {1607.01188},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016A&A...596A..39K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016A&A...595A..82E,
       author = {{Elorrieta}, Felipe and {Eyheramendy}, Susana and
         {Jord{\'a}n}, Andr{\'e}s and {D{\'e}k{\'a}ny}, Istv{\'a}n and
         {Catelan}, M{\'a}rcio and {Angeloni}, Rodolfo and
         {Alonso-Garc{\'\i}a}, Javier and {Contreras-Ramos}, Rodrigo and
         {Gran}, Felipe and {Hajdu}, Gergely and {Espinoza}, N{\'e}stor and
         {Saito}, Roberto K. and {Minniti}, Dante},
        title = "{A machine learned classifier for RR Lyrae in the VVV survey}",
      journal = {\aap},
     keywords = {stars: variables: RR Lyrae, methods: data analysis, methods: statistical, techniques: photometric, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Nov",
       volume = {595},
          eid = {A82},
        pages = {A82},
     abstract = "{Variable stars of RR Lyrae type are a prime tool with which to obtain
        distances to old stellar populations in the Milky Way. One of
        the main aims of the Vista Variables in the Via Lactea (VVV)
        near-infrared survey is to use them to map the structure of the
        Galactic Bulge. Owing to the large number of expected sources,
        this requires an automated mechanism for selecting RR Lyrae, and
        particularly those of the more easily recognized type ab (I.e.,
        fundamental-mode pulsators), from the {}10$^{6}$-{}10$^{7}$
        variables expected in the VVV survey area. In this work we
        describe a supervised machine-learned classifier constructed for
        assigning a score to a K$_{s}$-band VVV light curve that
        indicates its likelihood of being ab-type RR Lyrae. We describe
        the key steps in the construction of the classifier, which were
        the choice of features, training set, selection of aperture, and
        family of classifiers. We find that the AdaBoost family of
        classifiers give consistently the best performance for our
        problem, and obtain a classifier based on the AdaBoost algorithm
        that achieves a harmonic mean between false positives and false
        negatives of {\ensuremath{\approx}}7\% for typical VVV light-
        curve sets. This performance is estimated using cross-validation
        and through the comparison to two independent datasets that were
        classified by human experts.}",
          doi = {10.1051/0004-6361/201628700},
archivePrefix = {arXiv},
       eprint = {1610.05707},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016A&A...595A..82E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016PASP..128j4502S,
       author = {{Sadeh}, I. and {Abdalla}, F.~B. and {Lahav}, O.},
        title = "{ANNz2: Photometric Redshift and Probability Distribution Function Estimation using Machine Learning}",
      journal = {\pasp},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2016",
        month = "Oct",
       volume = {128},
       number = {968},
        pages = {104502},
     abstract = "{We present ANNz2, a new implementation of the public software for
        photometric redshift (photo-z) estimation of Collister \&amp;
        Lahav, which now includes generation of full probability
        distribution functions (PDFs). ANNz2 utilizes multiple machine
        learning methods, such as artificial neural networks and boosted
        decision/regression trees. The objective of the algorithm is to
        optimize the performance of the photo-z estimation, to properly
        derive the associated uncertainties, and to produce both single-
        value solutions and PDFs. In addition, estimators are made
        available, which mitigate possible problems of non-
        representative or incomplete spectroscopic training samples.
        ANNz2 has already been used as part of the first weak lensing
        analysis of the Dark Energy Survey, and is included in the
        experiment's first public data release. Here we illustrate the
        functionality of the code using data from the tenth data release
        of the Sloan Digital Sky Survey and the Baryon Oscillation
        Spectroscopic Survey. The code is available for download at <A h
        ref=``http://github.com/IftachSadeh/ANNZ''>http://github.com/Ift
        achSadeh/ANNZ</A>.}",
          doi = {10.1088/1538-3873/128/968/104502},
archivePrefix = {arXiv},
       eprint = {1507.00490},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016PASP..128j4502S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.462..726A,
       author = {{Almosallam}, Ibrahim A. and {Jarvis}, Matt J. and {Roberts}, Stephen J.},
        title = "{GPZ: non-stationary sparse Gaussian processes for heteroscedastic uncertainty estimation in photometric redshifts}",
      journal = {\mnras},
     keywords = {methods: data analysis, galaxies: distances and redshifts, Astrophysics - Instrumentation and Methods for Astrophysics, I.2.6},
         year = "2016",
        month = "Oct",
       volume = {462},
       number = {1},
        pages = {726-739},
     abstract = "{The next generation of cosmology experiments will be required to use
        photometric redshifts rather than spectroscopic redshifts.
        Obtaining accurate and well-characterized photometric redshift
        distributions is therefore critical for Euclid, the Large
        Synoptic Survey Telescope and the Square Kilometre Array.
        However, determining accurate variance predictions alongside
        single point estimates is crucial, as they can be used to
        optimize the sample of galaxies for the specific experiment
        (e.g. weak lensing, baryon acoustic oscillations, supernovae),
        trading off between completeness and reliability in the galaxy
        sample. The various sources of uncertainty in measurements of
        the photometry and redshifts put a lower bound on the accuracy
        that any model can hope to achieve. The intrinsic uncertainty
        associated with estimates is often non-uniform and input-
        dependent, commonly known in statistics as heteroscedastic
        noise. However, existing approaches are susceptible to outliers
        and do not take into account variance induced by non-uniform
        data density and in most cases require manual tuning of many
        parameters. In this paper, we present a Bayesian machine
        learning approach that jointly optimizes the model with respect
        to both the predictive mean and variance we refer to as Gaussian
        processes for photometric redshifts (GPZ). The predictive
        variance of the model takes into account both the variance due
        to data density and photometric noise. Using the Sloan Digital
        Sky Survey (SDSS) DR12 data, we show that our approach
        substantially outperforms other machine learning methods for
        photo-z estimation and their associated variance, such as TPZ
        and ANNZ2. We provide a MATLAB and PYTHON implementations that
        are available to download at https://github.com/OxfordML/GPz.}",
          doi = {10.1093/mnras/stw1618},
archivePrefix = {arXiv},
       eprint = {1604.03593},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.462..726A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.461.4151C,
       author = {{Carroll}, P.~A. and {Line}, J. and {Morales}, M.~F. and {Barry}, N. and
         {Beardsley}, A.~P. and {Hazelton}, B.~J. and {Jacobs}, D.~C. and
         {Pober}, J.~C. and {Sullivan}, I.~S. and {Webster}, R.~L. and
         {Bernardi}, G. and {Bowman}, J.~D. and {Briggs}, F. and
         {Cappallo}, R.~J. and {Corey}, B.~E. and {de Oliveira-Costa}, A. and
         {Dillon}, J.~S. and {Emrich}, D. and {Ewall-Wice}, A. and {Feng}, L. and
         {Gaensler}, B.~M. and {Goeke}, R. and {Greenhill}, L.~J. and
         {Hewitt}, J.~N. and {Hurley-Walker}, N. and {Johnston-Hollitt}, M. and
         {Kaplan}, D.~L. and {Kasper}, J.~C. and {Kim}, HS. and
         {Kratzenberg}, E. and {Lenc}, E. and {Loeb}, A. and {Lonsdale}, C.~J. and
         {Lynch}, M.~J. and {McKinley}, B. and {McWhirter}, S.~R. and
         {Mitchell}, D.~A. and {Morgan}, E. and {Neben}, A.~R. and {Oberoi}, D. and
         {Offringa}, A.~R. and {Ord}, S.~M. and {Paul}, S. and {Pindor}, B. and
         {Prabu}, T. and {Procopio}, P. and {Riding}, J. and {Rogers}, A.~E.~E. and
         {Roshi}, A. and {Shankar}, N. Udaya and {Sethi}, S.~K. and
         {Srivani}, K.~S. and {Subrahmanyan}, R. and {Tegmark}, M. and
         {Thyagarajan}, Nithyanandan and {Tingay}, S.~J. and {Trott}, C.~M. and
         {Waterson}, M. and {Wayth}, R.~B. and {Whitney}, A.~R. and
         {Williams}, A. and {Williams}, C.~L. and {Wu}, C. and
         {Wyithe}, J.~S.~B.},
        title = "{A high reliability survey of discrete Epoch of Reionization foreground sources in the MWA EoR0 field}",
      journal = {\mnras},
     keywords = {catalogues, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2016",
        month = "Oct",
       volume = {461},
       number = {4},
        pages = {4151-4175},
     abstract = "{Detection of the epoch of reionization H I signal requires a precise
        understanding of the intervening galaxies and AGN, both for
        instrumental calibration and foreground removal. We present a
        catalogue of 7394 extragalactic sources at 182 MHz detected in
        the RA = 0 field of the Murchison Widefield Array Epoch of
        Reionization observation programme. Motivated by unprecedented
        requirements for precision and reliability we develop new
        methods for source finding and selection. We apply machine
        learning methods to self-consistently classify the relative
        reliability of 9490 source candidates. A subset of 7466 are
        selected based on reliability class and signal-to-noise ratio
        criteria. These are statistically cross-matched to four other
        radio surveys using both position and flux density information.
        We find 7369 sources to have confident matches, including 90
        partially resolved sources that split into a total of 192 sub-
        components. An additional 25 unmatched sources are included as
        new radio detections. The catalogue sources have a median
        spectral index of -0.85. Spectral flattening is seen towards
        lower frequencies with a median of -0.71 predicted at 182 MHz.
        The astrometric error is 7 arcsec compared to a 2.3 arcmin beam
        FWHM. The resulting catalogue covers ̃1400 deg$^{2}$ and is
        complete to approximately 80 mJy within half beam power. This
        provides the most reliable discrete source sky model available
        to date in the MWA EoR0 field for precision foreground
        subtraction.}",
          doi = {10.1093/mnras/stw1599},
archivePrefix = {arXiv},
       eprint = {1607.03861},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.461.4151C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJ...830...31B,
       author = {{Bellinger}, Earl P. and {Angelou}, George C. and {Hekker}, Saskia and
         {Basu}, Sarbani and {Ball}, Warrick H. and {Guggenberger}, Elisabeth},
        title = "{Fundamental Parameters of Main-Sequence Stars in an Instant with Machine Learning}",
      journal = {\apj},
     keywords = {methods: statistical, stars: abundances, stars: fundamental parameters, stars: low-mass, stars: oscillations, stars: solar-type, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Artificial Intelligence},
         year = "2016",
        month = "Oct",
       volume = {830},
       number = {1},
          eid = {31},
        pages = {31},
     abstract = "{Owing to the remarkable photometric precision of space observatories
        like Kepler, stellar and planetary systems beyond our own are
        now being characterized en masse for the first time. These
        characterizations are pivotal for endeavors such as searching
        for Earth-like planets and solar twins, understanding the
        mechanisms that govern stellar evolution, and tracing the
        dynamics of our Galaxy. The volume of data that is becoming
        available, however, brings with it the need to process this
        information accurately and rapidly. While existing methods can
        constrain fundamental stellar parameters such as ages, masses,
        and radii from these observations, they require substantial
        computational effort to do so. We develop a method based on
        machine learning for rapidly estimating fundamental parameters
        of main-sequence solar-like stars from classical and
        asteroseismic observations. We first demonstrate this method on
        a hare-and-hound exercise and then apply it to the Sun, 16 Cyg A
        and B, and 34 planet-hosting candidates that have been observed
        by the Kepler spacecraft. We find that our estimates and their
        associated uncertainties are comparable to the results of other
        methods, but with the additional benefit of being able to
        explore many more stellar parameters while using much less
        computation time. We furthermore use this method to present
        evidence for an empirical diffusion-mass relation. Our method is
        open source and freely available for the community to use.$^{6}$}",
          doi = {10.3847/0004-637X/830/1/31},
archivePrefix = {arXiv},
       eprint = {1607.02137},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJ...830...31B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016A&C....17..129B,
       author = {{Bora}, K. and {Saha}, S. and {Agrawal}, S. and {Safonova}, M. and
         {Routh}, S. and {Narasimhamurthy}, A.},
        title = "{CD-HPF: New habitability score via data analytic modeling}",
      journal = {Astronomy and Computing},
     keywords = {Habitability score, Cobb-Douglas production function, Exoplanets, Machine learning, CDHS, Optimization, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Earth and Planetary Astrophysics},
         year = "2016",
        month = "Oct",
       volume = {17},
        pages = {129-143},
     abstract = "{The search for life on the planets outside the Solar System can be
        broadly classified into the following: looking for Earth-like
        conditions or the planets similar to the Earth (Earth
        similarity), and looking for the possibility of life in a form
        known or unknown to us (habitability). The two frequently used
        indices, Earth Similarity Index (ESI) and Planetary Habitability
        Index (PHI), describe heuristic methods to score habitability in
        the efforts to categorize different exoplanets (or exomoons).
        ESI, in particular, considers Earth as the reference frame for
        habitability, and is a quick screening tool to categorize and
        measure physical similarity of any planetary body with the
        Earth. The PHI assesses the potential habitability of any given
        planet, and is based on the essential requirements of known
        life: presence of a stable and protected substrate, energy,
        appropriate chemistry and a liquid medium. We propose here a
        different metric, a Cobb-Douglas Habitability Score (CDHS),
        based on Cobb-Douglas habitability production function (CD-HPF),
        which computes the habitability score by using measured and
        estimated planetary input parameters. As an initial set, we used
        radius, density, escape velocity and surface temperature of a
        planet. The values of the input parameters are normalized to the
        Earth Units (EU). The proposed metric, with exponents accounting
        for metric elasticity, is endowed with analytical properties
        that ensure global optima, and scales up to accommodate finitely
        many input parameters. The model is elastic, and, as we
        discovered, the standard PHI turns out to be a special case of
        the CDHS. Computed CDHS scores are fed to K-NN (K-Nearest
        Neighbor) classification algorithm with probabilistic herding
        that facilitates the assignment of exoplanets to appropriate
        classes via supervised feature learning methods, producing
        granular clusters of habitability. The proposed work describes a
        decision-theoretical model using the power of convex
        optimization and algorithmic machine learning.}",
          doi = {10.1016/j.ascom.2016.08.001},
archivePrefix = {arXiv},
       eprint = {1604.01722},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016A&C....17..129B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.461.2879V,
       author = {{Vernstrom}, T. and {Scott}, Douglas and {Wall}, J.~V. and
         {Condon}, J.~J. and {Cotton}, W.~D. and {Perley}, R.~A.},
        title = "{Deep 3-GHz observations of the Lockman Hole North with the Very Large Array - I. Source extraction and uncertainty analysis}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, cosmology: observations, radio continuum: galaxies, Astrophysics - Astrophysics of Galaxies},
         year = "2016",
        month = "Sep",
       volume = {461},
       number = {3},
        pages = {2879-2895},
     abstract = "{This is the first of two papers describing the observations and
        cataloguing of deep 3-GHz observations of the Lockman Hole North
        using the Karl G. Jansky Very Large Array. The aim of this paper
        is to investigate, through the use of simulated images, the
        uncertainties and accuracy of source-finding routines, as well
        as to quantify systematic effects due to resolution, such as
        source confusion and source size. While these effects are not
        new, this work is intended as a particular case study that can
        be scaled and translated to other surveys. We use the
        simulations to derive uncertainties in the fitted parameters, as
        well as bias corrections for the actual catalogue (presented in
        Paper II). We compare two different source-finding routines,
        OBIT and AEGEAN, and two different effective resolutions, 8 and
        2.75 arcsec. We find that the two routines perform comparably
        well, with OBIT being slightly better at de-blending sources,
        but slightly worse at fitting resolved sources. We show that
        30-70 per cent of sources are missed or fit inaccurately once
        the source size becomes larger than the beam, possibly
        explaining source count errors in high-resolution surveys. We
        also investigate the effect of blending, finding that any
        sources with separations smaller than the beam size are fit as
        single sources. We show that the use of machine-learning
        techniques can correctly identify blended sources up to 90 per
        cent of the time, and prior-driven fitting can lead to a 70 per
        cent improvement in the number of de-blended sources.}",
          doi = {10.1093/mnras/stw1530},
archivePrefix = {arXiv},
       eprint = {1603.03084},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.461.2879V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.461.2044S,
       author = {{Sasdelli}, M. and {Ishida}, E.~E.~O. and {Vilalta}, R. and
         {Aguena}, M. and {Busti}, V.~C. and {Camacho}, H. and
         {Trindade}, A.~M.~M. and {Gieseke}, F. and {de Souza}, R.~S. and
         {Fantaye}, Y.~T. and {Mazzali}, P.~A.},
        title = "{Exploring the spectroscopic diversity of Type Ia supernovae with DRACULA: a machine learning approach}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: miscellaneous, methods: statistical, supernovae: general, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Sep",
       volume = {461},
       number = {2},
        pages = {2044-2059},
     abstract = "{The existence of multiple subclasses of Type Ia supernovae (SNe Ia) has
        been the subject of great debate in the last decade. One major
        challenge inevitably met when trying to infer the existence of
        one or more subclasses is the time consuming, and subjective,
        process of subclass definition. In this work, we show how
        machine learning tools facilitate identification of subtypes of
        SNe Ia through the establishment of a hierarchical group
        structure in the continuous space of spectral diversity formed
        by these objects. Using deep learning, we were capable of
        performing such identification in a four-dimensional feature
        space (+1 for time evolution), while the standard principal
        component analysis barely achieves similar results using 15
        principal components. This is evidence that the progenitor
        system and the explosion mechanism can be described by a small
        number of initial physical parameters. As a proof of concept, we
        show that our results are in close agreement with a previously
        suggested classification scheme and that our proposed method can
        grasp the main spectral features behind the definition of such
        subtypes. This allows the confirmation of the velocity of lines
        as a first-order effect in the determination of SN Ia subtypes,
        followed by 91bg-like events. Given the expected data deluge in
        the forthcoming years, our proposed approach is essential to
        allow a quick and statistically coherent identification of SNe
        Ia subtypes (and outliers). All tools used in this work were
        made publicly available in the PYTHON package Dimensionality
        Reduction And Clustering for Unsupervised Learning in Astronomy
        (DRACULA) and can be found within COINtoolbox
        (https://github.com/COINtoolbox/DRACULA).}",
          doi = {10.1093/mnras/stw1228},
archivePrefix = {arXiv},
       eprint = {1512.06810},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.461.2044S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.461.1431R,
       author = {{Rozo}, E. and {Rykoff}, E.~S. and {Abate}, A. and {Bonnett}, C. and
         {Crocce}, M. and {Davis}, C. and {Hoyle}, B. and {Leistedt}, B. and
         {Peiris}, H.~V. and {Wechsler}, R.~H. and {Abbott}, T. and
         {Abdalla}, F.~B. and {Banerji}, M. and {Bauer}, A.~H. and
         {Benoit-L{\'e}vy}, A. and {Bernstein}, G.~M. and {Bertin}, E. and
         {Brooks}, D. and {Buckley-Geer}, E. and {Burke}, D.~L. and
         {Capozzi}, D. and {Rosell}, A. Carnero and {Carollo}, D. and
         {Kind}, M. Carrasco and {Carretero}, J. and {Castander}, F.~J. and
         {Childress}, M.~J. and {Cunha}, C.~E. and {D'Andrea}, C.~B. and
         {Davis}, T. and {DePoy}, D.~L. and {Desai}, S. and {Diehl}, H.~T. and
         {Dietrich}, J.~P. and {Doel}, P. and {Eifler}, T.~F. and
         {Evrard}, A.~E. and {Neto}, A. Fausti and {Flaugher}, B. and
         {Fosalba}, P. and {Frieman}, J. and {Gaztanaga}, E. and
         {Gerdes}, D.~W. and {Glazebrook}, K. and {Gruen}, D. and
         {Gruendl}, R.~A. and {Honscheid}, K. and {James}, D.~J. and
         {Jarvis}, M. and {Kim}, A.~G. and {Kuehn}, K. and {Kuropatkin}, N. and
         {Lahav}, O. and {Lidman}, C. and {Lima}, M. and {Maia}, M.~A.~G. and
         {March}, M. and {Martini}, P. and {Melchior}, P. and {Miller}, C.~J. and
         {Miquel}, R. and {Mohr}, J.~J. and {Nichol}, R.~C. and {Nord}, B. and
         {O'Neill}, C.~R. and {Ogando}, R. and {Plazas}, A.~A. and
         {Romer}, A.~K. and {Roodman}, A. and {Sako}, M. and {Sanchez}, E. and
         {Santiago}, B. and {Schubnell}, M. and {Sevilla-Noarbe}, I. and
         {Smith}, R.~C. and {Soares-Santos}, M. and {Sobreira}, F. and
         {Suchyta}, E. and {Swanson}, M.~E.~C. and {Thaler}, J. and
         {Thomas}, D. and {Uddin}, S. and {Vikram}, V. and {Walker}, A.~R. and
         {Wester}, W. and {Zhang}, Y. and {da Costa}, L.~N.},
        title = "{redMaGiC: selecting luminous red galaxies from the DES Science Verification data}",
      journal = {\mnras},
     keywords = {methods: statistical, techniques: photometric, galaxies: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2016",
        month = "Sep",
       volume = {461},
       number = {2},
        pages = {1431-1450},
     abstract = "{We introduce redMaGiC, an automated algorithm for selecting luminous red
        galaxies (LRGs). The algorithm was specifically developed to
        minimize photometric redshift uncertainties in photometric
        large-scale structure studies. redMaGiC achieves this by self-
        training the colour cuts necessary to produce a luminosity-
        thresholded LRG sample of constant comoving density. We
        demonstrate that redMaGiC photo-zs are very nearly as accurate
        as the best machine learning-based methods, yet they require
        minimal spectroscopic training, do not suffer from extrapolation
        biases, and are very nearly Gaussian. We apply our algorithm to
        Dark Energy Survey (DES) Science Verification (SV) data to
        produce a redMaGiC catalogue sampling the redshift range z
        {\ensuremath{\in}} [0.2, 0.8]. Our fiducial sample has a
        comoving space density of 10$^{-3}$ (h$^{-1}$ Mpc)$^{-3}$, and a
        median photo-z bias (z$_{spec}$ - z$_{photo}$) and scatter
        ({\ensuremath{\sigma}}$_{z}$/(1 + z)) of 0.005 and 0.017,
        respectively. The corresponding 5{\ensuremath{\sigma}} outlier
        fraction is 1.4 per cent. We also test our algorithm with Sloan
        Digital Sky Survey Data Release 8 and Stripe 82 data, and
        discuss how spectroscopic training can be used to control
        photo-z biases at the 0.1 per cent level.}",
          doi = {10.1093/mnras/stw1281},
archivePrefix = {arXiv},
       eprint = {1507.05460},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.461.1431R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016AsBio..16..661C,
       author = {{Cabrol}, Nathalie A.},
        title = "{Alien Mindscapes{\textemdash}A Perspective on the Search for Extraterrestrial Intelligence}",
      journal = {Astrobiology},
     keywords = {SETI, Astrobiology, Coevolution of Earth and life, Planetary habitability and biosignatures., News \&amp; Views},
         year = "2016",
        month = "Sep",
       volume = {16},
       number = {9},
        pages = {661-676},
     abstract = "{Advances in planetary and space sciences, astrobiology, and life and
        cognitive sciences, combined with developments in communication
        theory, bioneural computing, machine learning, and big data
        analysis, create new opportunities to explore the probabilistic
        nature of alien life. Brought together in a multidisciplinary
        approach, they have the potential to support an integrated and
        expanded Search for Extraterrestrial Intelligence (SETI$^{1}$),
        a search that includes looking for life as we do not know it.
        This approach will augment the odds of detecting a signal by
        broadening our understanding of the evolutionary and systemic
        components in the search for extraterrestrial intelligence
        (ETI), provide more targets for radio and optical SETI, and
        identify new ways of decoding and coding messages using
        universal markers.}",
          doi = {10.1089/ast.2016.1536},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016AsBio..16..661C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016PhRvD..94d2005B,
       author = {{Bonnett}, C. and {Troxel}, M.~A. and {Hartley}, W. and {Amara}, A. and
         {Leistedt}, B. and {Becker}, M.~R. and {Bernstein}, G.~M. and
         {Bridle}, S.~L. and {Bruderer}, C. and {Busha}, M.~T. and
         {Carrasco Kind}, M. and {Childress}, M.~J. and {Castander}, F.~J. and
         {Chang}, C. and {Crocce}, M. and {Davis}, T.~M. and {Eifler}, T.~F. and
         {Frieman}, J. and {Gangkofner}, C. and {Gaztanaga}, E. and
         {Glazebrook}, K. and {Gruen}, D. and {Kacprzak}, T. and {King}, A. and
         {Kwan}, J. and {Lahav}, O. and {Lewis}, G. and {Lidman}, C. and
         {Lin}, H. and {MacCrann}, N. and {Miquel}, R. and {O'Neill}, C.~R. and
         {Palmese}, A. and {Peiris}, H.~V. and {Refregier}, A. and {Rozo}, E. and
         {Rykoff}, E.~S. and {Sadeh}, I. and {S{\'a}nchez}, C. and
         {Sheldon}, E. and {Uddin}, S. and {Wechsler}, R.~H. and {Zuntz}, J. and
         {Abbott}, T. and {Abdalla}, F.~B. and {Allam}, S. and {Armstrong}, R. and
         {Banerji}, M. and {Bauer}, A.~H. and {Benoit-L{\'e}vy}, A. and
         {Bertin}, E. and {Brooks}, D. and {Buckley-Geer}, E. and
         {Burke}, D.~L. and {Capozzi}, D. and {Carnero Rosell}, A. and
         {Carretero}, J. and {Cunha}, C.~E. and {D'Andrea}, C.~B. and
         {da Costa}, L.~N. and {DePoy}, D.~L. and {Desai}, S. and
         {Diehl}, H.~T. and {Dietrich}, J.~P. and {Doel}, P. and
         {Fausti Neto}, A. and {Fernandez}, E. and {Flaugher}, B. and
         {Fosalba}, P. and {Gerdes}, D.~W. and {Gruendl}, R.~A. and
         {Honscheid}, K. and {Jain}, B. and {James}, D.~J. and {Jarvis}, M. and
         {Kim}, A.~G. and {Kuehn}, K. and {Kuropatkin}, N. and {Li}, T.~S. and
         {Lima}, M. and {Maia}, M.~A.~G. and {March}, M. and {Marshall}, J.~L. and
         {Martini}, P. and {Melchior}, P. and {Miller}, C.~J. and {Neilsen}, E. and
         {Nichol}, R.~C. and {Nord}, B. and {Ogando}, R. and {Plazas}, A.~A. and
         {Reil}, K. and {Romer}, A.~K. and {Roodman}, A. and {Sako}, M. and
         {Sanchez}, E. and {Santiago}, B. and {Smith}, R.~C. and
         {Soares-Santos}, M. and {Sobreira}, F. and {Suchyta}, E. and
         {Swanson}, M.~E.~C. and {Tarle}, G. and {Thaler}, J. and {Thomas}, D. and
         {Vikram}, V. and {Walker}, A.~R. and {Dark Energy Survey Collaboration}},
        title = "{Redshift distributions of galaxies in the Dark Energy Survey Science Verification shear catalogue and implications for weak lensing}",
      journal = {\prd},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2016",
        month = "Aug",
       volume = {94},
       number = {4},
          eid = {042005},
        pages = {042005},
     abstract = "{We present photometric redshift estimates for galaxies used in the weak
        lensing analysis of the Dark Energy Survey Science Verification
        (DES SV) data. Four model- or machine learning-based photometric
        redshift methods{\textemdash}annz2, bpz calibrated against BCC-
        Ufig simulations, skynet, and tpz{\textemdash}are analyzed. For
        training, calibration, and testing of these methods, we
        construct a catalogue of spectroscopically confirmed galaxies
        matched against DES SV data. The performance of the methods is
        evaluated against the matched spectroscopic catalogue, focusing
        on metrics relevant for weak lensing analyses, with additional
        validation against COSMOS photo-z 's. From the galaxies in the
        DES SV shear catalogue, which have mean redshift 0.72
        {\ensuremath{\pm}}0.01 over the range 0.3 \&lt;z \&lt;1.3 , we
        construct three tomographic bins with means of z =\{0.45 ,0.67
        ,1.00 \} . These bins each have systematic uncertainties
        {\ensuremath{\delta}} z {\ensuremath{\lesssim}}0.05 in the mean
        of the fiducial skynet photo-z n (z ) . We propagate the errors
        in the redshift distributions through to their impact on
        cosmological parameters estimated with cosmic shear, and find
        that they cause shifts in the value of
        {\ensuremath{\sigma}}$_{8}$ of approximately 3\%. This shift is
        within the one sigma statistical errors on
        {\ensuremath{\sigma}}$_{8}$ for the DES SV shear catalogue. We
        further study the potential impact of systematic differences on
        the critical surface density, {\ensuremath{\Sigma}}$_{crit}$ ,
        finding levels of bias safely less than the statistical power of
        DES SV data. We recommend a final Gaussian prior for the photo-z
        bias in the mean of n (z ) of width 0.05 for each of the three
        tomographic bins, and show that this is a sufficient bias model
        for the corresponding cosmology analysis.}",
          doi = {10.1103/PhysRevD.94.042005},
archivePrefix = {arXiv},
       eprint = {1507.05909},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016PhRvD..94d2005B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016PASP..128h4503W,
       author = {{Wagstaff}, Kiri L. and {Tang}, Benyang and {Thompson}, David R. and
         {Khudikyan}, Shakeh and {Wyngaard}, Jane and {Deller}, Adam T. and
         {Palaniswamy}, Divya and {Tingay}, Steven J. and {Wayth}, Randall B.},
        title = "{A Machine Learning Classifier for Fast Radio Burst Detection at the VLBA}",
      journal = {\pasp},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Aug",
       volume = {128},
       number = {966},
        pages = {084503},
     abstract = "{Time domain radio astronomy observing campaigns frequently generate
        large volumes of data. Our goal is to develop automated methods
        that can identify events of interest buried within the larger
        data stream. The V-FASTR fast transient system was designed to
        detect rare fast radio bursts within data collected by the Very
        Long Baseline Array. The resulting event candidates constitute a
        significant burden in terms of subsequent human reviewing time.
        We have trained and deployed a machine learning classifier that
        marks each candidate detection as a pulse from a known pulsar,
        an artifact due to radio frequency interference, or a potential
        new discovery. The classifier maintains high reliability by
        restricting its predictions to those with at least 90\%
        confidence. We have also implemented several efficiency and
        usability improvements to the V-FASTR web-based candidate review
        system. Overall, we found that time spent reviewing decreased
        and the fraction of interesting candidates increased. The
        classifier now classifies (and therefore filters) 80\%-90\% of
        the candidates, with an accuracy greater than 98\%, leaving only
        the 10\%-20\% most promising candidates to be reviewed by
        humans.}",
          doi = {10.1088/1538-3873/128/966/084503},
archivePrefix = {arXiv},
       eprint = {1606.08605},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016PASP..128h4503W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.460.3152Z,
       author = {{Zitlau}, Roman and {Hoyle}, Ben and {Paech}, Kerstin and
         {Weller}, Jochen and {Rau}, Markus Michael and {Seitz}, Stella},
        title = "{Stacking for machine learning redshifts applied to SDSS galaxies}",
      journal = {\mnras},
     keywords = {catalogues, surveys, galaxies: distances and redshifts, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Machine Learning},
         year = "2016",
        month = "Aug",
       volume = {460},
       number = {3},
        pages = {3152-3162},
     abstract = "{We present an analysis of a general machine learning technique called
        `stacking' for the estimation of photometric redshifts. Stacking
        techniques can feed the photometric redshift estimate, as output
        by a base algorithm, back into the same algorithm as an
        additional input feature in a subsequent learning round. We show
        how all tested base algorithms benefit from at least one
        additional stacking round (or layer). To demonstrate the benefit
        of stacking, we apply the method to both unsupervised machine
        learning techniques based on self-organizing maps (SOMs), and
        supervised machine learning methods based on decision trees. We
        explore a range of stacking architectures, such as the number of
        layers and the number of base learners per layer. Finally we
        explore the effectiveness of stacking even when using a
        successful algorithm such as AdaBoost. We observe a significant
        improvement of between 1.9 per cent and 21 per cent on all
        computed metrics when stacking is applied to weak learners (such
        as SOMs and decision trees). When applied to strong learning
        algorithms (such as AdaBoost) the ratio of improvement shrinks,
        but still remains positive and is between 0.4 per cent and 2.5
        per cent for the explored metrics and comes at almost no
        additional computational cost.}",
          doi = {10.1093/mnras/stw1454},
archivePrefix = {arXiv},
       eprint = {1602.06294},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.460.3152Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016JASTP.146..110Z,
       author = {{Zou}, Ling and {Wang}, Lunche and {Lin}, Aiwen and {Zhu}, Hongji and
         {Peng}, Yuling and {Zhao}, Zhenzhen},
        title = "{Estimation of global solar radiation using an artificial neural network based on an interpolation technique in southeast China}",
      journal = {Journal of Atmospheric and Solar-Terrestrial Physics},
     keywords = {Artificial neural network, Spatial interpolation, Empirical models, Global solar radiation},
         year = "2016",
        month = "Aug",
       volume = {146},
        pages = {110-122},
     abstract = "{Solar radiation plays important roles in energy application, vegetation
        growth and climate change. Empirical relations and machine-
        learning methods have been widely used to estimate global solar
        radiation (GSR) in recent years. An artificial neural network
        (ANN) based on spatial interpolation is developed to estimate
        GSR in southeast China. The improved Bristow-Campbell (IBC)
        model and the improved {\r{A}}ngstr{\"o}m-Prescott (IA-P) model
        are compared with the ANN model to explore the best model in
        solar radiation modeling. Daily meteorological parameters, such
        as sunshine duration hours, mean temperature, maximum
        temperature, minimum temperature, relative humidity,
        precipitation, air pressure, water vapor pressure, and wind
        speed, along with station-measured GSR and a daily surface GSR
        dataset over China obtained from the Data Assimilation and
        Modeling Center for Tibetan Multi-spheres (DAM), are used to
        predict GSR and to validate the models in this work. The ANN
        model with the network of 9-17-1 provides better accuracy than
        the two improved empirical models in GSR estimation. The root-
        mean-square error (RMSE), mean bias error (MBE), and
        determination coefficient (R$^{2}$) are 2.65 MJ m$^{-2}$, -0.94
        MJ m$^{-2}$, and 0.68 in the IA-P model; 2.19 MJ m$^{-2}$, 1.11
        MJ m$^{-2}$, and 0.83 in the IBC model; 1.34 MJ m$^{-2}$, -0.11
        MJ m$^{-2}$, and 0.91 in the ANN model, respectively. The
        regional monthly mean GSR in the measured dataset, DAM dataset,
        and ANN model is analyzed. The RMSE (RMSE \%) is 1.07 MJ
        m$^{-2}$ (8.91\%) and the MBE (MBE \%) is -0.62 MJ m$^{-2}$
        (-5.21\%) between the measured and ANN-estimated GSR. The
        statistical errors of RMSE (RMSE \%) are 0.91 MJ m$^{-2}$
        (7.28\%) and those of MBE (MBE \%) are -0.15 MJ m$^{-2}$
        (-1.20\%) between DAM and ANN-modeled GSR. The correlation
        coefficients and R$^{2}$ are larger than 0.95. The regional mean
        GSR is 12.58 MJ m$^{-2}$. The lowest GSR is observed in the
        northwest area, and it increases from northwest to southeast.
        The annual mean GSR decreases by 0.02 MJ m$^{-2}$ decade$^{-1}$
        over the entire southeast China. The GSR in 52 stations
        experiences a decreasing trend, and 21\% of the stations are
        significant at the 95\% level.}",
          doi = {10.1016/j.jastp.2016.05.013},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016JASTP.146..110Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016Galax...4...14E,
       author = {{Einecke}, Sabrina},
        title = "{Search for High-Confidence Blazar Candidates and Their MWL Counterparts in the Fermi-LAT Catalog Using Machine Learning}",
      journal = {Galaxies},
         year = "2016",
        month = "Aug",
       volume = {4},
       number = {3},
        pages = {14},
          doi = {10.3390/galaxies4030014},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016Galax...4...14E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJS..225...31L,
       author = {{Lochner}, Michelle and {McEwen}, Jason D. and {Peiris}, Hiranya V. and
         {Lahav}, Ofer and {Winter}, Max K.},
        title = "{Photometric Supernova Classification with Machine Learning}",
      journal = {\apjs},
     keywords = {cosmology: observations, methods: data analysis, supernovae: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2016",
        month = "Aug",
       volume = {225},
       number = {2},
          eid = {31},
        pages = {31},
     abstract = "{Automated photometric supernova classification has become an active area
        of research in recent years in light of current and upcoming
        imaging surveys such as the Dark Energy Survey (DES) and the
        Large Synoptic Survey Telescope, given that spectroscopic
        confirmation of type for all supernovae discovered will be
        impossible. Here, we develop a multi-faceted classification
        pipeline, combining existing and new approaches. Our pipeline
        consists of two stages: extracting descriptive features from the
        light curves and classification using a machine learning
        algorithm. Our feature extraction methods vary from model-
        dependent techniques, namely SALT2 fits, to more independent
        techniques that fit parametric models to curves, to a completely
        model-independent wavelet approach. We cover a range of
        representative machine learning algorithms, including naive
        Bayes, k-nearest neighbors, support vector machines, artificial
        neural networks, and boosted decision trees (BDTs). We test the
        pipeline on simulated multi-band DES light curves from the
        Supernova Photometric Classification Challenge. Using the
        commonly used area under the curve (AUC) of the Receiver
        Operating Characteristic as a metric, we find that the SALT2
        fits and the wavelet approach, with the BDTs algorithm, each
        achieve an AUC of 0.98, where 1 represents perfect
        classification. We find that a representative training set is
        essential for good classification, whatever the feature set or
        algorithm, with implications for spectroscopic follow-up.
        Importantly, we find that by using either the SALT2 or the
        wavelet feature sets with a BDT algorithm, accurate
        classification is possible purely from light curve data, without
        the need for any redshift information.}",
          doi = {10.3847/0067-0049/225/2/31},
archivePrefix = {arXiv},
       eprint = {1603.00882},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJS..225...31L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJ...827....5J,
       author = {{Jensen}, Hannes and {Zackrisson}, Erik and {Pelckmans}, Kristiaan and
         {Binggeli}, Christian and {Ausmees}, Kristiina and {Lundholm}, Ulrika},
        title = "{A Machine-learning Approach to Measuring the Escape of Ionizing Radiation from Galaxies in the Reionization Epoch}",
      journal = {\apj},
     keywords = {dark ages, reionization, first stars, galaxies: high-redshift, methods: statistical, Astrophysics - Astrophysics of Galaxies},
         year = "2016",
        month = "Aug",
       volume = {827},
       number = {1},
          eid = {5},
        pages = {5},
     abstract = "{Recent observations of galaxies at z{\ensuremath{\gtrsim}} 7, along with
        the low value of the electron scattering optical depth measured
        by the Planck mission, make galaxies plausible as dominant
        sources of ionizing photons during the epoch of reionization.
        However, scenarios of galaxy-driven reionization hinge on the
        assumption that the average escape fraction of ionizing photons
        is significantly higher for galaxies in the reionization epoch
        than in the local universe. The NIRSpec instrument on the James
        Webb Space Telescope (JWST) will enable spectroscopic
        observations of large samples of reionization-epoch galaxies.
        While the leakage of ionizing photons will not be directly
        measurable from these spectra, the leakage is predicted to have
        an indirect effect on the spectral slope and the strength of
        nebular emission lines in the rest-frame ultraviolet and
        optical. Here, we apply a machine learning technique known as
        lasso regression on mock JWST/NIRSpec observations of simulated
        z = 7 galaxies in order to obtain a model that can predict the
        escape fraction from JWST/NIRSpec data. Barring systematic
        biases in the simulated spectra, our method is able to retrieve
        the escape fraction with a mean absolute error of
        \{\{{\ensuremath{\Delta}}
        \}\}\{f\}$_{\{esc}$\}{\ensuremath{\approx}} 0.12 for spectra
        with signal-to-noise ratio {\ensuremath{\approx}} 5 at a rest-
        frame wavelength of 1500 {\r{A}} for our fiducial simulation.
        This prediction accuracy represents a significant improvement
        over previous similar approaches.}",
          doi = {10.3847/0004-637X/827/1/5},
archivePrefix = {arXiv},
       eprint = {1603.09610},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJ...827....5J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.459.3721B,
       author = {{Bass}, G. and {Borne}, K.},
        title = "{Supervised ensemble classification of Kepler variable stars}",
      journal = {\mnras},
     keywords = {methods: data analysis, stars: variables: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2016",
        month = "Jul",
       volume = {459},
       number = {4},
        pages = {3721-3737},
     abstract = "{Variable star analysis and classification is an important task in the
        understanding of stellar features and processes. While
        historically classifications have been done manually by highly
        skilled experts, the recent and rapid expansion in the quantity
        and quality of data has demanded new techniques, most notably
        automatic classification through supervised machine learning. We
        present an expansion of existing work on the field by analysing
        variable stars in the Kepler field using an ensemble approach,
        combining multiple characterization and classification
        techniques to produce improved classification rates.
        Classifications for each of the roughly 150 000 stars observed
        by Kepler are produced separating the stars into one of 14
        variable star classes.}",
          doi = {10.1093/mnras/stw810},
archivePrefix = {arXiv},
       eprint = {1604.01355},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.459.3721B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJ...825...69M,
       author = {{Mirabal}, N. and {Charles}, E. and {Ferrara}, E.~C. and
         {Gonthier}, P.~L. and {Harding}, A.~K. and {S{\'a}nchez-Conde}, M.~A. and
         {Thompson}, D.~J.},
        title = "{3FGL Demographics Outside the Galactic Plane using Supervised Machine Learning: Pulsar and Dark Matter Subhalo Interpretations}",
      journal = {\apj},
     keywords = {dark matter, gamma rays: general, pulsars: general, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2016",
        month = "Jul",
       volume = {825},
       number = {1},
          eid = {69},
        pages = {69},
     abstract = "{Nearly one-third of the sources listed in the Third Fermi Large Area
        Telescope (LAT) catalog (3FGL) remain unassociated. It is
        possible that predicted and even unanticipated gamma-ray source
        classes are present in these data waiting to be discovered.
        Taking advantage of the excellent spectral capabilities achieved
        by the Fermi LAT, we use machine-learning classifiers (Random
        Forest and XGBoost) to pinpoint potentially novel source classes
        in the unassociated 3FGL sample outside the Galactic plane. Here
        we report a total of 34 high-confidence Galactic candidates at |
        b| {\ensuremath{\geq}}slant 5\^\textbackslashcirc . The
        currently favored standard astrophysical interpretations for
        these objects are pulsars or low-luminosity globular clusters
        hosting millisecond pulsars (MSPs). Yet these objects could also
        be interpreted as dark matter annihilation taking place in
        ultra-faint dwarf galaxies or dark matter subhalos.
        Unfortunately, Fermi LAT spectra are not sufficient to break
        degeneracies between the different scenarios. Careful visual
        inspection of archival optical images reveals no obvious
        evidence for low-luminosity globular clusters or ultra-faint
        dwarf galaxies inside the 95\% error ellipses. If these are
        pulsars, this would bring the total number of MSPs at | b|
        {\ensuremath{\geq}}slant 5\^\textbackslashcirc to 106, down to
        an energy flux {\ensuremath{\approx}}4.0 {\texttimes} 10$^{-12}$
        erg cm$^{-2}$ s$^{-1}$ between 100 MeV and 100 GeV. We find this
        number to be in excellent agreement with predictions from a new
        population synthesis of MSPs that predicts 100-126 high-latitude
        3FGL MSPs depending on the choice of high-energy emission model.
        If, however, these are dark matter substructures, we can place
        upper limits on the number of Galactic subhalos surviving today
        and on dark matter annihilation cross sections. These limits are
        beginning to approach the canonical thermal relic cross section
        for dark matter particle masses below ̃100 GeV in the bottom
        quark (b\textbackslashbar\{b\}) annihilation channel.}",
          doi = {10.3847/0004-637X/825/1/69},
archivePrefix = {arXiv},
       eprint = {1605.00711},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJ...825...69M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016AcASn..57..389G,
       author = {{Gao}, W. and {Li}, X.~R.},
        title = "{Application of Multi-task Sparse Group Lasso Feature Extraction and Support Vector Machine Regression in the Stellar Atmospheric Parametrization}",
      journal = {Acta Astronomica Sinica},
     keywords = {stars: fundamental parameters, methods: data analysis, methods: statistical, methods: miscellaneous},
         year = "2016",
        month = "Jul",
       volume = {57},
       number = {4},
        pages = {389-401},
     abstract = "{The multi-task learning puts the multiple tasks together to analyse and
        calculate for discovering the correlation between them, which
        can improve the accuracy of analysis results. This kind of
        methods have been widely studied in machine learning, pattern
        recognition, computer vision, and other related fields. This
        paper investigates the application of multi-task learning in
        estimating the effective temperature (T\_\{eff\}), surface
        gravity (lg g), and chemical abundance ([Fe/H]). Firstly, the
        spectral characteristics of the three atmospheric physical
        parameters are extracted by using the multi-task Sparse Group
        Lasso algorithm, and then the support vector machine is used to
        estimate the atmospheric physical parameters. The proposed
        scheme is evaluated on both Sloan stellar spectra and
        theoretical spectra computed from Kurucz's New Opacity
        Distribution Function (NEWODF) model. The mean absolute errors
        (MAEs) on the Sloan spectra are: 0.0064 for lg (T\_\{eff\}/K),
        0.1622 for lg (g/(cm\textbackslashcdot s\^\{-2\})), and 0.1221
        dex for [Fe/H]; The MAEs on synthetic spectra are 0.0006 for lg
        (T\_\{eff\}/K), 0.0098 for lg (g/(cm\textbackslashcdot
        s\^\{-2\})), and 0.0082 dex for [Fe/H]. Experimental results
        show that the proposed scheme is excellent for atmospheric
        parameter estimation.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016AcASn..57..389G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016AcASn..57..379P,
       author = {{Pan}, R.~Y. and {Li}, X.~R.},
        title = "{Stellar Atmospheric Parameterization Based on Deep Learning}",
      journal = {Acta Astronomica Sinica},
     keywords = {stars: fundamental parameters, stars: atmospheres, stars: abundances, methods: data analysis, methods: statistical},
         year = "2016",
        month = "Jul",
       volume = {57},
       number = {4},
        pages = {379-388},
     abstract = "{Deep learning is a typical learning method widely studied in machine
        learning, pattern recognition, and artificial intelligence. This
        work investigates the stellar atmospheric parameterization
        problem by constructing a deep neural network with five layers.
        The proposed scheme is evaluated on both real spectra from Sloan
        Digital Sky Survey (SDSS) and the theoretic spectra computed
        with Kurucz's New Opacity Distribution Function (NEWODF) model.
        On the SDSS spectra, the mean absolute errors (MAEs) are 79.95
        for the effective temperature (T\_\{eff\}/K), 0.0058 for lg
        (T\_\{eff\}/K), 0.1706 for surface gravity (lg
        (g/(cm\textbackslashcdot s\^\{-2\}))), and 0.1294 dex for
        metallicity ([Fe/H]), respectively; On the theoretic spectra,
        the MAEs are 15.34 for T\_\{eff\}/K, 0.0011 for lg
        (T\_\{eff\}/K), 0.0214 for lg (g/(cm\textbackslashcdot
        s\^\{-2\})), and 0.0121 dex for [Fe/H], respectively.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016AcASn..57..379P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016A&C....16..174M,
       author = {{Mahmoud}, E. and {Takey}, A. and {Shoukry}, A.},
        title = "{Spectral clustering for optical confirmation and redshift estimation of X-ray selected galaxy cluster candidates in the SDSS Stripe 82}",
      journal = {Astronomy and Computing},
     keywords = {X-rays: galaxies: clusters, Catalogs, Surveys, Techniques: photometric, Methods: machine learning - spectral clustering, Astrophysics - Astrophysics of Galaxies, Computer Science - Other Computer Science},
         year = "2016",
        month = "Jul",
       volume = {16},
        pages = {174-184},
     abstract = "{We develop a galaxy cluster finding algorithm based on spectral
        clustering technique to identify optical counterparts and
        estimate optical redshifts for X-ray selected cluster
        candidates. As an application, we run our algorithm on a sample
        of X-ray cluster candidates selected from the third XMM-Newton
        serendipitous source catalog (3XMM-DR5) that are located in the
        Stripe 82 of the Sloan Digital Sky Survey (SDSS). Our method
        works on galaxies described in the color-magnitude feature
        space. We begin by examining 45 galaxy clusters with published
        spectroscopic redshifts in the range of 0.10-0.8 with a median
        of 0.36. As a result, we are able to identify their optical
        counterparts and estimate their photometric redshifts, which
        have a typical accuracy of 0.025 and agree with the published
        ones. Then, we investigate another 40 X-ray cluster candidates
        (from the same cluster survey) with no redshift information in
        the literature and found that 12 candidates are considered as
        galaxy clusters in the redshift range from 0.29 to 0.76 with a
        median of 0.57. These systems are newly discovered clusters in
        X-rays and optical data. Among them 7 clusters have
        spectroscopic redshifts for at least one member galaxy.}",
          doi = {10.1016/j.ascom.2016.07.001},
archivePrefix = {arXiv},
       eprint = {1607.04193},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016A&C....16..174M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016A&C....16...34H,
       author = {{Hoyle}, B.},
        title = "{Measuring photometric redshifts using galaxy images and Deep Neural Networks}",
      journal = {Astronomy and Computing},
     keywords = {Astronomy, Machine learning, Cosmology, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies, Physics - Data Analysis, Statistics and Probability},
         year = "2016",
        month = "Jul",
       volume = {16},
        pages = {34-40},
     abstract = "{We propose a new method to estimate the photometric redshift of galaxies
        by using the full galaxy image in each measured band. This
        method draws from the latest techniques and advances in machine
        learning, in particular Deep Neural Networks. We pass the entire
        multi-band galaxy image into the machine learning architecture
        to obtain a redshift estimate that is competitive, in terms of
        the measured point prediction metrics, with the best existing
        standard machine learning techniques. The standard techniques
        estimate redshifts using post-processed features, such as
        magnitudes and colours, which are extracted from the galaxy
        images and are deemed to be salient by the user. This new method
        removes the user from the photometric redshift estimation
        pipeline. However we do note that Deep Neural Networks require
        many orders of magnitude more computing resources than standard
        machine learning architectures, and as such are only tractable
        for making predictions on datasets of size
        {\ensuremath{\leq}}50k before implementing parallelisation
        techniques.}",
          doi = {10.1016/j.ascom.2016.03.006},
archivePrefix = {arXiv},
       eprint = {1504.07255},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016A&C....16...34H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016A&A...592A..25K,
       author = {{Kurcz}, A. and {Bilicki}, M. and {Solarz}, A. and {Krupa}, M. and
         {Pollo}, A. and {Ma{\l}ek}, K.},
        title = "{Towards automatic classification of all WISE sources}",
      journal = {\aap},
     keywords = {methods: data analysis, methods: statistical, astronomical databases: miscellaneous, catalogs, infrared: general, surveys, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Jul",
       volume = {592},
          eid = {A25},
        pages = {A25},
     abstract = "{Context. The Wide-field Infrared Survey Explorer (WISE) has detected
        hundreds of millions of sources over the entire sky. Classifying
        them reliably is, however, a challenging task owing to
        degeneracies in WISE multicolour space and low levels of
        detection in its two longest-wavelength bandpasses. Simple
        colour cuts are often not sufficient; for satisfactory levels of
        completeness and purity, more sophisticated classification
        methods are needed. <BR /> Aims: Here we aim to obtain
        comprehensive and reliable star, galaxy, and quasar catalogues
        based on automatic source classification in full-sky WISE data.
        This means that the final classification will employ only
        parameters available from WISE itself, in particular those which
        are reliably measured for the majority of sources. <BR />
        Methods: For the automatic classification we applied a
        supervised machine learning algorithm, support vector machines
        (SVM). It requires a training sample with relevant classes
        already identified, and we chose to use the SDSS spectroscopic
        dataset (DR10) for that purpose. We tested the performance of
        two kernels used by the classifier, and determined the minimum
        number of sources in the training set required to achieve stable
        classification, as well as the minimum dimension of the
        parameter space. We also tested SVM classification accuracy as a
        function of extinction and apparent magnitude. Thus, the
        calibrated classifier was finally applied to all-sky WISE data,
        flux-limited to 16 mag (Vega) in the 3.4 {\ensuremath{\mu}}m
        channel. <BR /> Results: By calibrating on the test data drawn
        from SDSS, we first established that a polynomial kernel is
        preferred over a radial one for this particular dataset. Next,
        using three classification parameters (W1 magnitude, W1-W2
        colour, and a differential aperture magnitude) we obtained very
        good classification efficiency in all the tests. At the bright
        end, the completeness for stars and galaxies reaches
        \raisebox{-0.5ex}\textasciitilde95\%, deteriorating to
        \raisebox{-0.5ex}\textasciitilde80\% at W1 = 16 mag, while for
        quasars it stays at a level of
        \raisebox{-0.5ex}\textasciitilde95\% independently of magnitude.
        Similar numbers are obtained for purity. Application of the
        classifier to full-sky WISE data and appropriate a posteriori
        cleaning allowed us to obtain catalogues of star and galaxy
        candidates that appear reliable. However, the sources flagged by
        the classifier as ``quasars'' are in fact dominated by dusty
        galaxies; they also exhibit contamination from sources located
        mainly at low ecliptic latitudes, consistent with solar system
        objects.}",
          doi = {10.1051/0004-6361/201628142},
archivePrefix = {arXiv},
       eprint = {1604.04229},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016A&A...592A..25K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.459.1659T,
       author = {{Tammour}, A. and {Gallagher}, S.~C. and {Daley}, M. and
         {Richards}, G.~T.},
        title = "{Insights into quasar UV spectra using unsupervised clustering analysis}",
      journal = {\mnras},
     keywords = {quasars: absorption lines, quasars: emission lines, Astrophysics - Astrophysics of Galaxies},
         year = "2016",
        month = "Jun",
       volume = {459},
       number = {2},
        pages = {1659-1681},
     abstract = "{Machine learning techniques can provide powerful tools to detect
        patterns in multidimensional parameter space. We use K-means - a
        simple yet powerful unsupervised clustering algorithm which
        picks out structure in unlabelled data - to study a sample of
        quasar UV spectra from the Quasar Catalog of the 10th Data
        Release of the Sloan Digital Sky Survey (SDSS-DR10) of Paris et
        al. Detecting patterns in large data sets helps us gain insights
        into the physical conditions and processes giving rise to the
        observed properties of quasars. We use K-means to find clusters
        in the parameter space of the equivalent width (EW), the blue-
        and red-half-width at half-maximum (HWHM) of the Mg II 2800
        {\r{A}} line, the C IV 1549 {\r{A}} line, and the C III] 1908
        {\r{A}} blend in samples of broad absorption line (BAL) and non-
        BAL quasars at redshift 1.6-2.1. Using this method, we
        successfully recover correlations well-known in the UV regime
        such as the anti-correlation between the EW and blueshift of the
        C IV emission line and the shape of the ionizing spectra energy
        distribution (SED) probed by the strength of He II and the Si
        III]/C III] ratio. We find this to be particularly evident when
        the properties of C III] are used to find the clusters, while
        those of Mg II proved to be less strongly correlated with the
        properties of the other lines in the spectra such as the width
        of C IV or the Si III]/C III] ratio. We conclude that
        unsupervised clustering methods (such as K-means) are powerful
        methods for finding `natural' binning boundaries in
        multidimensional data sets and discuss caveats and future work.}",
          doi = {10.1093/mnras/stw586},
archivePrefix = {arXiv},
       eprint = {1603.03318},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.459.1659T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.459.1519D,
       author = {{Devine}, Thomas Ryan and {Goseva-Popstojanova}, Katerina and
         {McLaughlin}, Maura},
        title = "{Detection of dispersed radio pulses: a machine learning approach to candidate identification and classification}",
      journal = {\mnras},
     keywords = {methods: data analysis, pulsars: general, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Jun",
       volume = {459},
       number = {2},
        pages = {1519-1532},
     abstract = "{Searching for extraterrestrial, transient signals in astronomical data
        sets is an active area of current research. However, machine
        learning techniques are lacking in the literature concerning
        single-pulse detection. This paper presents a new, two-stage
        approach for identifying and classifying dispersed pulse groups
        (DPGs) in single-pulse search output. The first stage identified
        DPGs and extracted features to characterize them using a new
        peak identification algorithm which tracks sloping tendencies
        around local maxima in plots of signal-to-noise ratio versus
        dispersion measure. The second stage used supervised machine
        learning to classify DPGs. We created four benchmark data sets:
        one unbalanced and three balanced versions using three different
        imbalance treatments. We empirically evaluated 48 classifiers by
        training and testing binary and multiclass versions of six
        machine learning algorithms on each of the four benchmark
        versions. While each classifier had advantages and
        disadvantages, all classifiers with imbalance treatments had
        higher recall values than those with unbalanced data, regardless
        of the machine learning algorithm used. Based on the
        benchmarking results, we selected a subset of classifiers to
        classify the full, unlabelled data set of over 1.5 million DPGs
        identified in 42 405 observations made by the Green Bank
        Telescope. Overall, the classifiers using a multiclass ensemble
        tree learner in combination with two oversampling imbalance
        treatments were the most efficient; they identified additional
        known pulsars not in the benchmark data set and provided six
        potential discoveries, with significantly less false positives
        than the other classifiers.}",
          doi = {10.1093/mnras/stw655},
archivePrefix = {arXiv},
       eprint = {1603.09461},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.459.1519D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.459.1104L,
       author = {{Lyon}, R.~J. and {Stappers}, B.~W. and {Cooper}, S. and
         {Brooke}, J.~M. and {Knowles}, J.~D.},
        title = "{Fifty Years of Pulsar Candidate Selection: From simple filters to a new principled real-time classification approach}",
      journal = {\mnras},
     keywords = {pulsars: general, methods: statistical, methods: data analysis, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2016",
        month = "Jun",
       volume = {459},
        pages = {1104},
     abstract = "{Improving survey specifications are causing an exponential rise in
        pulsar candidate numbers and data volumes. We study the
        candidate filters used to mitigate these problems during the
        past fifty years. We find that some existing methods such as
        applying constraints on the total number of candidates collected
        per observation, may have detrimental effects on the success of
        pulsar searches. Those methods immune to such effects are found
        to be ill-equipped to deal with the problems associated with
        increasing data volumes and candidate numbers, motivating the
        development of new approaches. We therefore present a new method
        designed for on-line operation. It selects promising candidates
        using a purpose-built tree-based machine learning classifier,
        the Gaussian Hellinger Very Fast Decision Tree (GH-VFDT), and a
        new set of features for describing candidates. The features have
        been chosen so as to I) maximise the separation between
        candidates arising from noise and those of probable
        astrophysical origin, and II) be as survey-independent as
        possible. Using these features our new approach can process
        millions of candidates in seconds (̃1 million every 15 seconds),
        with high levels of pulsar recall (90\%+). This technique is
        therefore applicable to the large volumes of data expected to be
        produced by the Square Kilometre Array (SKA). Use of this
        approach has assisted in the discovery of 20 new pulsars in data
        obtained during the LOFAR Tied-Array All-Sky Survey (LOTAAS).}",
          doi = {10.1093/mnras/stw656},
archivePrefix = {arXiv},
       eprint = {1603.05166},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.459.1104L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.458.4498H,
       author = {{Hoyle}, Ben and {Paech}, Kerstin and {Rau}, Markus Michael and
         {Seitz}, Stella and {Weller}, Jochen},
        title = "{Tuning target selection algorithms to improve galaxy redshift estimates}",
      journal = {\mnras},
     keywords = {catalogues, surveys, galaxies: distances and redshifts, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2016",
        month = "Jun",
       volume = {458},
       number = {4},
        pages = {4498-4511},
     abstract = "{We showcase machine learning (ML) inspired target selection algorithms
        to determine which of all potential targets should be selected
        first for spectroscopic follow-up. Efficient target selection
        can improve the ML redshift uncertainties as calculated on an
        independent sample, while requiring less targets to be observed.
        We compare seven different ML targeting algorithms with the
        Sloan Digital Sky Survey (SDSS) target order, and with a random
        targeting algorithm. The ML inspired algorithms are constructed
        iteratively by estimating which of the remaining target galaxies
        will be most difficult for the ML methods to accurately estimate
        redshifts using the previously observed data. This is performed
        by predicting the expected redshift error and redshift offset
        (or bias) of all of the remaining target galaxies. We find that
        the predicted values of bias and error are accurate to better
        than 10-30 per cent of the true values, even with only limited
        training sample sizes. We construct a hypothetical follow-up
        survey and find that some of the ML targeting algorithms are
        able to obtain the same redshift predictive power with 2-3 times
        less observing time, as compared to that of the SDSS, or random,
        target selection algorithms. The reduction in the required
        follow-up resources could allow for a change to the follow-up
        strategy, for example by obtaining deeper spectroscopy, which
        could improve ML redshift estimates for deeper test data.}",
          doi = {10.1093/mnras/stw563},
archivePrefix = {arXiv},
       eprint = {1508.06280},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.458.4498H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.458.3821U,
       author = {{Ukwatta}, T.~N. and {Wo{\'z}niak}, P.~R. and {Gehrels}, N.},
        title = "{Machine-z: rapid machine-learned redshift indicator for Swift gamma-ray bursts}",
      journal = {\mnras},
     keywords = {gamma-ray burst: general, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2016",
        month = "Jun",
       volume = {458},
       number = {4},
        pages = {3821-3829},
     abstract = "{Studies of high-redshift gamma-ray bursts (GRBs) provide important
        information about the early Universe such as the rates of
        stellar collapsars and mergers, the metallicity content,
        constraints on the re-ionization period, and probes of the
        Hubble expansion. Rapid selection of high-z candidates from GRB
        samples reported in real time by dedicated space missions such
        as Swift is the key to identifying the most distant bursts
        before the optical afterglow becomes too dim to warrant a good
        spectrum. Here, we introduce `machine-z', a redshift prediction
        algorithm and a `high-z' classifier for Swift GRBs based on
        machine learning. Our method relies exclusively on canonical
        data commonly available within the first few hours after the GRB
        trigger. Using a sample of 284 bursts with measured redshifts,
        we trained a randomized ensemble of decision trees (random
        forest) to perform both regression and classification. Cross-
        validated performance studies show that the correlation
        coefficient between machine-z predictions and the true redshift
        is nearly 0.6. At the same time, our high-z classifier can
        achieve 80 per cent recall of true high-redshift bursts, while
        incurring a false positive rate of 20 per cent. With 40 per cent
        false positive rate the classifier can achieve ̃100 per cent
        recall. The most reliable selection of high-redshift GRBs is
        obtained by combining predictions from both the high-z
        classifier and the machine-z regressor.}",
          doi = {10.1093/mnras/stw559},
archivePrefix = {arXiv},
       eprint = {1512.07671},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.458.3821U},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJS..224...18P,
       author = {{Proctor}, D.~D.},
        title = "{A Selection of Giant Radio Sources from NVSS}",
      journal = {\apjs},
     keywords = {astronomical databases: miscellaneous, catalogs, galaxies: general, Astrophysics - Astrophysics of Galaxies, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
         year = "2016",
        month = "Jun",
       volume = {224},
       number = {2},
          eid = {18},
        pages = {18},
     abstract = "{Results of the application of pattern-recognition techniques to the
        problem of identifying giant radio sources (GRSs) from the data
        in the NVSS catalog are presented, and issues affecting the
        process are explored. Decision-tree pattern-recognition software
        was applied to training-set source pairs developed from known
        NVSS large-angular-size radio galaxies. The full training set
        consisted of 51,195 source pairs, 48 of which were known GRSs
        for which each lobe was primarily represented by a single
        catalog component. The source pairs had a maximum separation of
        20$^{\textbackslashprime}$ and a minimum component area of 1.87
        square arcmin at the 1.4 mJy level. The importance of comparing
        the resulting probability distributions of the training and
        application sets for cases of unknown class ratio is
        demonstrated. The probability of correctly ranking a randomly
        selected (GRS, non-GRS) pair from the best of the tested
        classifiers was determined to be 97.8 {\ensuremath{\pm}} 1.5\%.
        The best classifiers were applied to the over 870,000 candidate
        pairs from the entire catalog. Images of higher-ranked sources
        were visually screened, and a table of over 1600 candidates,
        including morphological annotation, is presented. These systems
        include doubles and triples, wide-angle tail and narrow-angle
        tail, S- or Z-shaped systems, and core-jets and resolved cores.
        While some resolved-lobe systems are recovered with this
        technique, generally it is expected that such systems would
        require a different approach.}",
          doi = {10.3847/0067-0049/224/2/18},
archivePrefix = {arXiv},
       eprint = {1603.06895},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJS..224...18P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJ...824...16J,
       author = {{J{\'o}hannesson}, G. and {Ruiz de Austri}, R. and {Vincent}, A.~C. and
         {Moskalenko}, I.~V. and {Orlando}, E. and {Porter}, T.~A. and
         {Strong}, A.~W. and {Trotta}, R. and {Feroz}, F. and {Graff}, P. and
         {Hobson}, M.~P.},
        title = "{Bayesian Analysis of Cosmic Ray Propagation: Evidence against Homogeneous Diffusion}",
      journal = {\apj},
     keywords = {astroparticle physics, cosmic rays, diffusion, Galaxy: general, ISM: general, methods: statistical, Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Astrophysics of Galaxies},
         year = "2016",
        month = "Jun",
       volume = {824},
       number = {1},
          eid = {16},
        pages = {16},
     abstract = "{We present the results of the most complete scan of the parameter space
        for cosmic ray (CR) injection and propagation. We perform a
        Bayesian search of the main GALPROP parameters, using the
        MultiNest nested sampling algorithm, augmented by the BAMBI
        neural network machine-learning package. This is the first study
        to separate out low-mass isotopes (p, \textbackslashbar\{p\},
        and He) from the usual light elements (Be, B, C, N, and O). We
        find that the propagation parameters that best-fit
        p,\textbackslashbar\{p\}, and He data are significantly
        different from those that fit light elements, including the B/C
        and $^{10}$Be/$^{9}$Be secondary-to-primary ratios normally used
        to calibrate propagation parameters. This suggests that each set
        of species is probing a very different interstellar medium, and
        that the standard approach of calibrating propagation parameters
        using B/C can lead to incorrect results. We present posterior
        distributions and best-fit parameters for propagation of both
        sets of nuclei, as well as for the injection abundances of
        elements from H to Si. The input GALDEF files with these new
        parameters will be included in an upcoming public GALPROP
        update.}",
          doi = {10.3847/0004-637X/824/1/16},
archivePrefix = {arXiv},
       eprint = {1602.02243},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJ...824...16J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016A&A...591A..54K,
       author = {{Kuntzer}, T. and {Tewes}, M. and {Courbin}, F.},
        title = "{Stellar classification from single-band imaging using machine learning}",
      journal = {\aap},
     keywords = {methods: data analysis, methods: statistical, techniques: photometric, stars: fundamental parameters, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Jun",
       volume = {591},
          eid = {A54},
        pages = {A54},
     abstract = "{Information on the spectral types of stars is of great interest in view
        of the exploitation of space-based imaging surveys. In this
        article, we investigate the classification of stars into
        spectral types using only the shape of their diffraction pattern
        in a single broad-band image. We propose a supervised machine
        learning approach to this endeavour, based on principal
        component analysis (PCA) for dimensionality reduction, followed
        by artificial neural networks (ANNs) estimating the spectral
        type. Our analysis is performed with image simulations mimicking
        the Hubble Space Telescope (HST) Advanced Camera for Surveys
        (ACS) in the F606W and F814W bands, as well as the Euclid VIS
        imager. We first demonstrate this classification in a simple
        context, assuming perfect knowledge of the point spread function
        (PSF) model and the possibility of accurately generating mock
        training data for the machine learning. We then analyse its
        performance in a fully data-driven situation, in which the
        training would be performed with a limited subset of bright
        stars from a survey, and an unknown PSF with spatial variations
        across the detector. We use simulations of main-sequence stars
        with flat distributions in spectral type and in signal-to-noise
        ratio, and classify these stars into 13 spectral subclasses,
        from O5 to M5. Under these conditions, the algorithm achieves a
        high success rate both for Euclid and HST images, with typical
        errors of half a spectral class. Although more detailed
        simulations would be needed to assess the performance of the
        algorithm on a specific survey, this shows that stellar
        classification from single-band images is well possible.}",
          doi = {10.1051/0004-6361/201628660},
archivePrefix = {arXiv},
       eprint = {1605.03201},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016A&A...591A..54K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.458..226D,
       author = {{de Los Rios}, Mart{\'\i}n and {Dom{\'\i}nguez R.}, Mariano J. and
         {Paz}, Dante and {Merch{\'a}n}, Manuel},
        title = "{The MeSsI (merging systems identification) algorithm and catalogue.}",
      journal = {\mnras},
     keywords = {galaxies: clusters: general galaxies: kinematics and dynamics dark matter, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2016",
        month = "May",
       volume = {458},
       number = {1},
        pages = {226-232},
     abstract = "{Merging galaxy systems provide observational evidence of the existence
        of dark matter and constraints on its properties. Therefore,
        statistically uniform samples of merging systems would be a
        powerful tool for several studies. In this paper, we present a
        new methodology for the identification of merging systems and
        the results of its application to galaxy redshift surveys. We
        use as a starting point a mock catalogue of galaxy systems,
        identified using friends-of-friends algorithms, that have
        experienced a major merger, as indicated by its merger tree. By
        applying machine learning techniques in this training sample,
        and using several features computed from the observable
        properties of galaxy members, it is possible to select galaxy
        groups that have a high probability of having experienced a
        major merger. Next, we apply a mixture of Gaussian techniques on
        galaxy members in order to reconstruct the properties of the
        haloes involved in such mergers. This methodology provides a
        highly reliable sample of merging systems with low contamination
        and precisely recovered properties. We apply our techniques to
        samples of galaxy systems obtained from the Sloan Digital Sky
        Survey Data Release 7, the Wide-Field Nearby Galaxy-Cluster
        Survey (WINGS) and the Hectospec Cluster Survey (HeCS). Our
        results recover previously known merging systems and provide
        several new candidates. We present their measured properties and
        discuss future analysis on current and forthcoming samples.}",
          doi = {10.1093/mnras/stw215},
archivePrefix = {arXiv},
       eprint = {1509.02524},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.458..226D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016AcASn..57..344Q,
       author = {{Qin}, H.~R. and {Lin}, J.~M. and {Wang}, J.~Y.},
        title = "{Stacked Denoising Autoencoders Applied to Star/Galaxy Classification}",
      journal = {Acta Astronomica Sinica},
     keywords = {methods: data analysis, techniques: photometric, galaxies: fundamental parameters, stars: fundamental parameters, cosmology: observations},
         year = "2016",
        month = "May",
       volume = {57},
       number = {3},
        pages = {344-352},
     abstract = "{In recent years, the deep learning has been becoming more and more
        popular because it is well-adapted, and has a high accuracy and
        complex structure, but it has not been used in astronomy. In
        order to resolve the question that the classification accuracy
        of star/galaxy is high on the bright set, but low on the faint
        set of the Sloan Digital Sky Survey (SDSS), we introduce the new
        deep learning SDA (stacked denoising autoencoders) and dropout
        technology, which can greatly improve robustness and anti-noise
        performance. We randomly selected the bright source set and
        faint source set from DR12 and DR7 with spectroscopic
        measurements, and preprocessed them. Afterwards, we randomly
        selected the training set and testing set without replacement
        from the bright set and faint set. At last, we used the obtained
        training set to train the SDA model of SDSS-DR7 and SDSS-DR12.
        We compared the testing result with the results of Library for
        Support Vector Machines (LibSVM), J48, Logistic Model Trees
        (LMT), Support Vector Machine (SVM), Logistic Regression, and
        Decision Stump algorithm on the SDSS-DR12 testing set, and the
        results of six kinds of decision trees on the SDSS-DR7 testing
        set. The simulation shows that SDA has a better classification
        accuracy than other machine learning algorithms. When we use
        completeness function as the test parameter, the test accuracy
        rate is improved by about 15\% on the faint set of SDSS-DR7.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016AcASn..57..344Q},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016A&A...589A..95P,
       author = {{Pasquato}, Mario and {Chung}, Chul},
        title = "{Merged or monolithic? Using machine-learning to reconstruct the dynamical history of simulated star clusters}",
      journal = {\aap},
     keywords = {globular clusters: general, Galaxy: evolution, methods: statistical, methods: numerical, Astrophysics - Astrophysics of Galaxies},
         year = "2016",
        month = "May",
       volume = {589},
          eid = {A95},
        pages = {A95},
     abstract = "{Context. Machine-learning (ML) solves problems by learning patterns from
        data with limited or no human guidance. In astronomy, ML is
        mainly applied to large observational datasets, e.g. for
        morphological galaxy classification. <BR /> Aims: We apply ML to
        gravitational N-body simulations of star clusters that are
        either formed by merging two progenitors or evolved in
        isolation, planning to later identify globular clusters (GCs)
        that may have a history of merging from observational data. <BR
        /> Methods: We create mock-observations from simulated GCs, from
        which we measure a set of parameters (also called features in
        the machine-learning field). After carrying out dimensionality
        reduction on the feature space, the resulting datapoints are fed
        in to various classification algorithms. Using repeated random
        subsampling validation, we check whether the groups identified
        by the algorithms correspond to the underlying physical
        distinction between mergers and monolithically evolved
        simulations. <BR /> Results: The three algorithms we considered
        (C5.0 trees, k-nearest neighbour, and support-vector machines)
        all achieve a test misclassification rate of about 10\% without
        parameter tuning, with support-vector machines slightly
        outperforming the others. The first principal component of
        feature space correlates with cluster concentration. If we
        exclude it from the regression, the performance of the
        algorithms is only slightly reduced.}",
          doi = {10.1051/0004-6361/201425181},
archivePrefix = {arXiv},
       eprint = {1602.00993},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016A&A...589A..95P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016A&A...589A..54G,
       author = {{Gomez Gonzalez}, C.~A. and {Absil}, O. and {Absil}, P. -A. and
         {Van Droogenbroeck}, M. and {Mawet}, D. and {Surdej}, J.},
        title = "{Low-rank plus sparse decomposition for exoplanet detection in direct-imaging ADI sequences. The LLSG algorithm}",
      journal = {\aap},
     keywords = {methods: data analysis, techniques: high angular resolution, techniques: image processing, planetary systems, planets and satellites: detection, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Earth and Planetary Astrophysics},
         year = "2016",
        month = "May",
       volume = {589},
          eid = {A54},
        pages = {A54},
     abstract = "{Context. Data processing constitutes a critical component of high-
        contrast exoplanet imaging. Its role is almost as important as
        the choice of a coronagraph or a wavefront control system, and
        it is intertwined with the chosen observing strategy. Among the
        data processing techniques for angular differential imaging
        (ADI), the most recent is the family of principal component
        analysis (PCA) based algorithms. It is a widely used statistical
        tool developed during the first half of the past century. PCA
        serves, in this case, as a subspace projection technique for
        constructing a reference point spread function (PSF) that can be
        subtracted from the science data for boosting the detectability
        of potential companions present in the data. Unfortunately, when
        building this reference PSF from the science data itself, PCA
        comes with certain limitations such as the sensitivity of the
        lower dimensional orthogonal subspace to non-Gaussian noise. <BR
        /> Aims: Inspired by recent advances in machine learning
        algorithms such as robust PCA, we aim to propose a localized
        subspace projection technique that surpasses current PCA-based
        post-processing algorithms in terms of the detectability of
        companions at near real-time speed, a quality that will be
        useful for future direct imaging surveys. <BR /> Methods: We
        used randomized low-rank approximation methods recently proposed
        in the machine learning literature, coupled with entry-wise
        thresholding to decompose an ADI image sequence locally into
        low-rank, sparse, and Gaussian noise components (LLSG). This
        local three-term decomposition separates the starlight and the
        associated speckle noise from the planetary signal, which mostly
        remains in the sparse term. We tested the performance of our new
        algorithm on a long ADI sequence obtained on
        {\ensuremath{\beta}} Pictoris with VLT/NACO. <BR /> Results:
        Compared to a standard PCA approach, LLSG decomposition reaches
        a higher signal-to-noise ratio and has an overall better
        performance in the receiver operating characteristic space. This
        three-term decomposition brings a detectability boost compared
        to the full-frame standard PCA approach, especially in the small
        inner working angle region where complex speckle noise prevents
        PCA from discerning true companions from noise.}",
          doi = {10.1051/0004-6361/201527387},
archivePrefix = {arXiv},
       eprint = {1602.08381},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016A&A...589A..54G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.457.3119D,
       author = {{D'Isanto}, A. and {Cavuoti}, S. and {Brescia}, M. and {Donalek}, C. and
         {Longo}, G. and {Riccio}, G. and {Djorgovski}, S.~G.},
        title = "{An analysis of feature relevance in the classification of astronomical transients with machine learning methods}",
      journal = {\mnras},
     keywords = {methods: data analysis, novae, cataclysmic variables, supernovae: general, stars: variables: general, stars: variables: RR Lyrae, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Apr",
       volume = {457},
       number = {3},
        pages = {3119-3132},
     abstract = "{The exploitation of present and future synoptic (multiband and multi-
        epoch) surveys requires an extensive use of automatic methods
        for data processing and data interpretation. In this work, using
        data extracted from the Catalina Real Time Transient Survey
        (CRTS), we investigate the classification performance of some
        well tested methods: Random Forest, MultiLayer Perceptron with
        Quasi Newton Algorithm and K-Nearest Neighbours, paying special
        attention to the feature selection phase. In order to do so,
        several classification experiments were performed. Namely:
        identification of cataclysmic variables, separation between
        galactic and extragalactic objects and identification of
        supernovae.}",
          doi = {10.1093/mnras/stw157},
archivePrefix = {arXiv},
       eprint = {1601.03931},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.457.3119D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.457.2845T,
       author = {{Tortora}, C. and {La Barbera}, F. and {Napolitano}, N.~R. and
         {Roy}, N. and {Radovich}, M. and {Cavuoti}, S. and {Brescia}, M. and
         {Longo}, G. and {Getman}, F. and {Capaccioli}, M. and {Grado}, A. and
         {Kuijken}, K.~H. and {de Jong}, J.~T.~A. and {McFarland}, J.~P. and
         {Puddu}, E.},
        title = "{Towards a census of supercompact massive galaxies in the Kilo Degree Survey}",
      journal = {\mnras},
     keywords = {galaxies: elliptical and lenticular, cD, galaxies: evolution, galaxies: general, galaxies: structure, Astrophysics - Astrophysics of Galaxies},
         year = "2016",
        month = "Apr",
       volume = {457},
       number = {3},
        pages = {2845-2854},
     abstract = "{The abundance of compact, massive, early-type galaxies (ETGs) provides
        important constraints to galaxy formation scenarios. Thanks to
        the area covered, depth, excellent spatial resolution and
        seeing, the ESO Public optical Kilo Degree Survey (KiDS),
        carried out with the VLT Survey Telescope, offers a unique
        opportunity to conduct a complete census of the most compact
        galaxies in the Universe. This paper presents a first census of
        such systems from the first 156 deg$^{2}$ of KiDS. Our analysis
        relies on g-, r- and I-band effective radii (R$_{e}$), derived
        by fitting galaxy images with point spread function
        (PSF)-convolved S{\'e}rsic models, high-quality photometric
        redshifts, z$_{phot}$, estimated from machine learning
        techniques, and stellar masses, M$_{{\ensuremath{\star}}}$,
        calculated from KiDS aperture photometry. After massiveness
        (\{M\_\{{\ensuremath{\star}}\}\}{\ensuremath{\gtrsim}} 8
        {\texttimes} 10\^\{10\} M\_\{☉\}) and compactness
        (\{R\_e\}{\ensuremath{\lesssim}} 1.5 kpc in g, r and I bands)
        criteria are applied, a visual inspection of the candidates plus
        near-infrared photometry from VIKING-DR1 are used to refine our
        sample. The final catalogue, to be spectroscopically confirmed,
        consists of 92 systems in the redshift range z ̃ 0.2-0.7. This
        sample, which we expect to increase by a factor of 10 over the
        total survey area, represents the first attempt to select
        massive supercompact ETGs
        (\&lt;monospace\&gt;MSCGs\&lt;/monospace\&gt;) in KiDS. We
        investigate the impact of redshift systematics in the selection,
        finding that this seems to be a major source of contamination in
        our sample. A preliminary analysis shows that
        \&lt;monospace\&gt;MSCGs\&lt;/monospace\&gt; exhibit negative
        internal colour gradients, consistent with a passive evolution
        of these systems. We find that the number density of
        \&lt;monospace\&gt;MSCGs\&lt;/monospace\&gt; is only mildly
        consistent with predictions from simulations at z \&gt; 0.2,
        while no such system is found at z \&lt; 0.2.}",
          doi = {10.1093/mnras/stw184},
archivePrefix = {arXiv},
       eprint = {1507.00731},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.457.2845T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.457.1162K,
       author = {{Kamdar}, Harshil M. and {Turk}, Matthew J. and {Brunner}, Robert J.},
        title = "{Machine learning and cosmological simulations - II. Hydrodynamical simulations}",
      journal = {\mnras},
     keywords = {galaxies: evolution, galaxies: formation, galaxies: haloes, cosmology: theory, large-scale structure of Universe, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2016",
        month = "Apr",
       volume = {457},
       number = {2},
        pages = {1162-1179},
     abstract = "{We extend a machine learning (ML) framework presented previously to
        model galaxy formation and evolution in a hierarchical universe
        using N-body + hydrodynamical simulations. In this work, we show
        that ML is a promising technique to study galaxy formation in
        the backdrop of a hydrodynamical simulation. We use the
        Illustris simulation to train and test various sophisticated ML
        algorithms. By using only essential dark matter halo physical
        properties and no merger history, our model predicts the gas
        mass, stellar mass, black hole mass, star formation rate, g - r
        colour, and stellar metallicity fairly robustly. Our results
        provide a unique and powerful phenomenological framework to
        explore the galaxy-halo connection that is built upon a solid
        hydrodynamical simulation. The promising reproduction of the
        listed galaxy properties demonstrably place ML as a promising
        and a significantly more computationally efficient tool to study
        small-scale structure formation. We find that ML mimics a full-
        blown hydrodynamical simulation surprisingly well in a
        computation time of mere minutes. The population of galaxies
        simulated by ML, while not numerically identical to Illustris,
        is statistically robust and physically consistent with Illustris
        galaxies and follows the same fundamental observational
        constraints. ML offers an intriguing and promising technique to
        create quick mock galaxy catalogues in the future.}",
          doi = {10.1093/mnras/stv2981},
archivePrefix = {arXiv},
       eprint = {1510.07659},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.457.1162K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJ...821..127B,
       author = {{Bobra}, M.~G. and {Ilonidis}, S.},
        title = "{Predicting Coronal Mass Ejections Using Machine Learning Methods}",
      journal = {\apj},
     keywords = {Sun: activity, Sun: flares, Astrophysics - Solar and Stellar Astrophysics},
         year = "2016",
        month = "Apr",
       volume = {821},
       number = {2},
          eid = {127},
        pages = {127},
     abstract = "{Of all the activity observed on the Sun, two of the most energetic
        events are flares and coronal mass ejections (CMEs). Usually,
        solar active regions that produce large flares will also produce
        a CME, but this is not always true. Despite advances in
        numerical modeling, it is still unclear which circumstances will
        produce a CME. Therefore, it is worthwhile to empirically
        determine which features distinguish flares associated with CMEs
        from flares that are not. At this time, no extensive study has
        used physically meaningful features of active regions to
        distinguish between these two populations. As such, we attempt
        to do so by using features derived from (1) photospheric vector
        magnetic field data taken by the Solar Dynamics
        Observatory{\textquoteright}s Helioseismic and Magnetic Imager
        instrument and (2) X-ray flux data from the Geostationary
        Operational Environmental Satellite{\textquoteright}s X-ray Flux
        instrument. We build a catalog of active regions that either
        produced both a flare and a CME (the positive class) or simply a
        flare (the negative class). We then use machine-learning
        algorithms to (1) determine which features distinguish these two
        populations, and (2) forecast whether an active region that
        produces an M- or X-class flare will also produce a CME. We
        compute the True Skill Statistic, a forecast verification
        metric, and find that it is a relatively high value of
        {\ensuremath{\sim}}0.8 {\ensuremath{\pm}} 0.2. We conclude that
        a combination of six parameters, which are all intensive in
        nature, will capture most of the relevant information contained
        in the photospheric magnetic field.}",
          doi = {10.3847/0004-637X/821/2/127},
archivePrefix = {arXiv},
       eprint = {1603.03775},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJ...821..127B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJ...821...86H,
       author = {{Heinis}, S. and {Kumar}, S. and {Gezari}, S. and {Burgett}, W.~S. and
         {Chambers}, K.~C. and {Draper}, P.~W. and {Flewelling}, H. and
         {Kaiser}, N. and {Magnier}, E.~A. and {Metcalfe}, N. and {Waters}, C.},
        title = "{Of Genes and Machines: Application of a Combination of Machine Learning Tools to Astronomy Data Sets}",
      journal = {\apj},
     keywords = {methods: numerical, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Apr",
       volume = {821},
       number = {2},
          eid = {86},
        pages = {86},
     abstract = "{We apply a combination of genetic algorithm (GA) and support vector
        machine (SVM) machine learning algorithms to solve two important
        problems faced by the astronomical community: star-galaxy
        separation and photometric redshift estimation of galaxies in
        survey catalogs. We use the GA to select the relevant features
        in the first step, followed by optimization of SVM parameters in
        the second step to obtain an optimal set of parameters to
        classify or regress, in the process of which we avoid
        overfitting. We apply our method to star-galaxy separation in
        Pan-STARRS1 data. We show that our method correctly classifies
        98\% of objects down to \{I\}$_{\{\{P1}$\}\}=24.5, with a
        completeness (or true positive rate) of 99\% for galaxies and
        88\% for stars. By combining colors with morphology, our star-
        galaxy separation method yields better results than the new
        SExtractor classifier spread\_model, in particular at the faint
        end (\{I\}$_{\{\{P1}$\}\}\textbackslashgt 22). We also use our
        method to derive photometric redshifts for galaxies in the
        COSMOS bright multiwavelength data set down to an error in (1+z)
        of {\ensuremath{\sigma}} =0.013, which compares well with
        estimates from spectral energy distribution fitting on the same
        data ({\ensuremath{\sigma}} =0.007) while making a significantly
        smaller number of assumptions.}",
          doi = {10.3847/0004-637X/821/2/86},
archivePrefix = {arXiv},
       eprint = {1603.00967},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJ...821...86H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016AJ....151...98M,
       author = {{Mommert}, M. and {Trilling}, D.~E. and {Borth}, D. and {Jedicke}, R. and
         {Butler}, N. and {Reyes-Ruiz}, M. and {Pichardo}, B. and
         {Petersen}, E. and {Axelrod}, T. and {Moskovitz}, N.},
        title = "{First Results from the Rapid-response Spectrophotometric Characterization of Near-Earth Objects using UKIRT}",
      journal = {\aj},
     keywords = {asteroids: individual: near-Earth objects, minor planets, surveys, Astrophysics - Earth and Planetary Astrophysics},
         year = "2016",
        month = "Apr",
       volume = {151},
       number = {4},
          eid = {98},
        pages = {98},
     abstract = "{Using the Wide Field Camera for the United Kingdom Infrared Telescope
        (UKIRT), we measure the near-infrared colors of near-Earth
        objects (NEOs) in order to put constraints on their taxonomic
        classifications. The rapid-response character of our
        observations allows us to observe NEOs when they are close to
        the Earth and bright. Here we present near-infrared color
        measurements of 86 NEOs, most of which were observed within a
        few days of their discovery, allowing us to characterize NEOs
        with diameters of only a few meters. Using machine-learning
        methods, we compare our measurements to existing asteroid
        spectral data and provide probabilistic taxonomic
        classifications for our targets. Our observations allow us to
        distinguish between S-complex, C/X-complex, D-type, and V-type
        asteroids. Our results suggest that the fraction of S-complex
        asteroids in the whole NEO population is lower than the fraction
        of ordinary chondrites in the meteorite fall statistics. Future
        data obtained with UKIRT will be used to investigate the
        significance of this discrepancy.}",
          doi = {10.3847/0004-6256/151/4/98},
archivePrefix = {arXiv},
       eprint = {1602.06000},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016AJ....151...98M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.457..362B,
       author = {{Beck}, R{\'o}bert and {Dobos}, L{\'a}szl{\'o} and {Yip}, Ching-Wa and
         {Szalay}, Alexander S. and {Csabai}, Istv{\'a}n},
        title = "{Quantifying correlations between galaxy emission lines and stellar continua}",
      journal = {\mnras},
     keywords = {methods: data analysis, galaxies: active, galaxies: starburst, galaxies: stellar content, Astrophysics - Astrophysics of Galaxies},
         year = "2016",
        month = "Mar",
       volume = {457},
       number = {1},
        pages = {362-374},
     abstract = "{We analyse the correlations between continuum properties and emission
        line equivalent widths of star-forming and active galaxies from
        the Sloan Digital Sky Survey. Since upcoming large sky surveys
        will make broad-band observations only, including strong
        emission lines into theoretical modelling of spectra will be
        essential to estimate physical properties of photometric
        galaxies. We show that emission line equivalent widths can be
        fairly well reconstructed from the stellar continuum using local
        multiple linear regression in the continuum principal component
        analysis (PCA) space. Line reconstruction is good for star-
        forming galaxies and reasonable for galaxies with active nuclei.
        We propose a practical method to combine stellar population
        synthesis models with empirical modelling of emission lines. The
        technique will help generate more accurate model spectra and
        mock catalogues of galaxies to fit observations of the new
        surveys. More accurate modelling of emission lines is also
        expected to improve template-based photometric redshift
        estimation methods. We also show that, by combining PCA
        coefficients from the pure continuum and the emission lines,
        automatic distinction between hosts of weak active galactic
        nuclei (AGNs) and quiescent star-forming galaxies can be made.
        The classification method is based on a training set consisting
        of high-confidence starburst galaxies and AGNs, and allows for
        the similar separation of active and star-forming galaxies as
        the empirical curve found by Kauffmann et al. We demonstrate the
        use of three important machine learning algorithms in the paper:
        k-nearest neighbour finding, k-means clustering and support
        vector machines.}",
          doi = {10.1093/mnras/stv2986},
archivePrefix = {arXiv},
       eprint = {1601.02417},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.457..362B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016JSWSC...6A..16S,
       author = {{Shahamatnia}, Ehsan and {Dorotovi{\v{c}}}, Ivan and {Fonseca}, Jose M. and
         {Ribeiro}, Rita A.},
        title = "{An evolutionary computation based algorithm for calculating solar differential rotation by automatic tracking of coronal bright points}",
      journal = {Journal of Space Weather and Space Climate},
     keywords = {Sun, Helioinformatics, Corona, Solar image processing, Machine learning},
         year = "2016",
        month = "Mar",
       volume = {6},
          eid = {A16},
        pages = {A16},
     abstract = "{Developing specialized software tools is essential to support studies of
        solar activity evolution. With new space missions such as Solar
        Dynamics Observatory (SDO), solar images are being produced in
        unprecedented volumes. To capitalize on that huge data
        availability, the scientific community needs a new generation of
        software tools for automatic and efficient data processing. In
        this paper a prototype of a modular framework for solar feature
        detection, characterization, and tracking is presented. To
        develop an efficient system capable of automatic solar feature
        tracking and measuring, a hybrid approach combining specialized
        image processing, evolutionary optimization, and soft computing
        algorithms is being followed. The specialized hybrid algorithm
        for tracking solar features allows automatic feature tracking
        while gathering characterization details about the tracked
        features. The hybrid algorithm takes advantages of the snake
        model, a specialized image processing algorithm widely used in
        applications such as boundary delineation, image segmentation,
        and object tracking. Further, it exploits the flexibility and
        efficiency of Particle Swarm Optimization (PSO), a stochastic
        population based optimization algorithm. PSO has been used
        successfully in a wide range of applications including
        combinatorial optimization, control, clustering, robotics,
        scheduling, and image processing and video analysis
        applications. The proposed tool, denoted PSO-Snake model, was
        already successfully tested in other works for tracking sunspots
        and coronal bright points. In this work, we discuss the
        application of the PSO-Snake algorithm for calculating the
        sidereal rotational angular velocity of the solar corona. To
        validate the results we compare them with published manual
        results performed by an expert.}",
          doi = {10.1051/swsc/2016010},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016JSWSC...6A..16S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016JGRA..121.2423B,
       author = {{Bortnik}, J. and {Li}, W. and {Thorne}, R.~M. and {Angelopoulos}, V.},
        title = "{A unified approach to inner magnetospheric state prediction}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {machine learning, plasmasphere, space weather, prediction, neural networks, THEMIS},
         year = "2016",
        month = "Mar",
       volume = {121},
       number = {3},
        pages = {2423-2430},
     abstract = "{This brief technique paper presents a method of reconstructing the
        global, time-varying distribution of some physical quantity Q
        that has been sparsely sampled at various locations within the
        magnetosphere and at different times. The quantity Q can be
        essentially any measurement taken on the satellite including a
        variety of waves (chorus, hiss, magnetosonic, and ion
        cyclotron), electrons of various energies ranging from cold to
        relativistic, and ions of various species and energies. As an
        illustrative example, we chose Q to be the electron number
        density (inferred from spacecraft potential) measured by three
        Time History of Events and Macroscale Interactions during
        Substorms (THEMIS) probes between 2008 and 2014 and use the
        SYM-H index, taken at a 5 min cadence for the 5 h preceding each
        observed data point as the main regressor, although the
        predictor can also be any suitable geomagnetic index or solar
        wind parameter. Results show that the equatorial electron number
        density can be accurately reconstructed throughout the whole of
        the inner magnetosphere as a function of space and time, even
        capturing the dynamics of elementary plasmaspheric plume
        formation and corotation, suggesting that the dynamics of
        various other physical quantities could be similarly captured.
        For our main model, we use a simple, fully connected feedforward
        neural network with two hidden layers having sigmoidal
        activation functions and an output layer with a linear
        activation function to perform the reconstruction. The training
        is performed using the Levenberg-Marquardt algorithm and gives
        typical RMS errors of \raisebox{-0.5ex}\textasciitilde1.7 and
        regression of \&gt;0.93, which is considered excellent. We also
        present a discussion on the different applications and future
        extensions of the present model, for modeling various physical
        quantities.}",
          doi = {10.1002/2015JA021733},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016JGRA..121.2423B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJ...820....8S,
       author = {{Saz Parkinson}, P.~M. and {Xu}, H. and {Yu}, P.~L.~H. and
         {Salvetti}, D. and {Marelli}, M. and {Falcone}, A.~D.},
        title = "{Classification and Ranking of Fermi LAT Gamma-ray Sources from the 3FGL Catalog using Machine Learning Techniques}",
      journal = {\apj},
     keywords = {gamma rays: stars, methods: statistical, pulsars: general, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2016",
        month = "Mar",
       volume = {820},
       number = {1},
          eid = {8},
        pages = {8},
     abstract = "{We apply a number of statistical and machine learning techniques to
        classify and rank gamma-ray sources from the Third Fermi Large
        Area Telescope Source Catalog (3FGL), according to their
        likelihood of falling into the two major classes of gamma-ray
        emitters: pulsars (PSR) or active galactic nuclei (AGNs). Using
        1904 3FGL sources that have been identified/associated with AGNs
        (1738) and PSR (166), we train (using 70\% of our sample) and
        test (using 30\%) our algorithms and find that the best overall
        accuracy (\&gt;96\%) is obtained with the Random Forest (RF)
        technique, while using a logistic regression (LR) algorithm
        results in only marginally lower accuracy. We apply the same
        techniques on a subsample of 142 known gamma-ray pulsars to
        classify them into two major subcategories: young (YNG) and
        millisecond pulsars (MSP). Once more, the RF algorithm has the
        best overall accuracy ({\ensuremath{\sim}}90\%), while a boosted
        LR analysis comes a close second. We apply our two best models
        (RF and LR) to the entire 3FGL catalog, providing predictions on
        the likely nature of unassociated sources, including the likely
        type of pulsar (YNG or MSP). We also use our predictions to shed
        light on the possible nature of some gamma-ray sources with
        known associations (e.g., binaries, supernova remnants/pulsar
        wind nebulae). Finally, we provide a list of plausible X-ray
        counterparts for some pulsar candidates, obtained using Swift,
        Chandra, and XMM. The results of our study will be of interest
        both for in-depth follow-up searches (e.g., pulsar) at various
        wavelengths and for broader population studies.}",
          doi = {10.3847/0004-637X/820/1/8},
archivePrefix = {arXiv},
       eprint = {1602.00385},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJ...820....8S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJ...819...18P,
       author = {{Pichara}, Karim and {Protopapas}, Pavlos and {Le{\'o}n}, Daniel},
        title = "{Meta-classification for Variable Stars}",
      journal = {\apj},
     keywords = {methods: data analysis, stars: statistics, stars: variables: general, surveys, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Mar",
       volume = {819},
       number = {1},
          eid = {18},
        pages = {18},
     abstract = "{The need for the development of automatic tools to explore astronomical
        databases has been recognized since the inception of CCDs and
        modern computers. Astronomers already have developed solutions
        to tackle several science problems, such as automatic
        classification of stellar objects, outlier detection, and
        globular clusters identification, among others. New scientific
        problems emerge, and it is critical to be able to reuse the
        models learned before, without rebuilding everything from the
        beginning when the sciencientific problem changes. In this
        paper, we propose a new meta-model that automatically integrates
        existing classification models of variable stars. The proposed
        meta-model incorporates existing models that are trained in a
        different context, answering different questions and using
        different representations of data. A conventional mixture of
        expert algorithms in machine learning literature cannot be used
        since each expert (model) uses different inputs. We also
        consider the computational complexity of the model by using the
        most expensive models only when it is necessary. We test our
        model with EROS-2 and MACHO data sets, and we show that we solve
        most of the classification challenges only by training a meta-
        model to learn how to integrate the previous experts.}",
          doi = {10.3847/0004-637X/819/1/18},
archivePrefix = {arXiv},
       eprint = {1601.03013},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJ...819...18P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016A&A...587A..18K,
       author = {{Kim}, Dae-Won and {Bailer-Jones}, Coryn A.~L.},
        title = "{A package for the automated classification of periodic variable stars}",
      journal = {\aap},
     keywords = {methods: data analysis, methods: statistical, stars: variables: general, techniques: miscellaneous, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2016",
        month = "Mar",
       volume = {587},
          eid = {A18},
        pages = {A18},
     abstract = "{We present a machine learning package for the classification of periodic
        variable stars. Our package is intended to be general: it can
        classify any single band optical light curve comprising at least
        a few tens of observations covering durations from weeks to
        years with arbitrary time sampling. We use light curves of
        periodic variable stars taken from OGLE and EROS-2 to train the
        model. To make our classifier relatively survey-independent, it
        is trained on 16 features extracted from the light curves (e.g.,
        period, skewness, Fourier amplitude ratio). The model classifies
        light curves into one of seven superclasses -
        {\ensuremath{\delta}} Scuti, RR Lyrae, Cepheid, Type II Cepheid,
        eclipsing binary, long-period variable, non-variable - as well
        as subclasses of these, such as ab, c, d, and e types for RR
        Lyraes. When trained to give only superclasses, our model
        achieves 0.98 for both recall and precision as measured on an
        independent validation dataset (on a scale of 0 to 1). When
        trained to give subclasses, it achieves 0.81 for both recall and
        precision. The majority of misclassifications of the subclass
        model is caused by confusion within a superclass rather than
        between superclasses. To assess classification performance of
        the subclass model, we applied it to the MACHO, LINEAR, and ASAS
        periodic variables, which gave recall/precision of 0.92/0.98,
        0.89/0.96, and 0.84/0.88, respectively. We also applied the
        subclass model to Hipparcos periodic variable stars of many
        other variability types that do not exist in our training set,
        in order to examine how much those types degrade the
        classification performance of our target classes. In addition,
        we investigate how the performance varies with the number of
        data points and duration of observations. We find that recall
        and precision do not vary significantly if there are more than
        80 data points and the duration is more than a few weeks. The
        classifier software of the subclass model is available (in
        Python) from the GitHub repository (<A
        href=``http://goo.gl/xmFO6Q''>http://https://goo.gl/xmFO6Q</A>).}",
          doi = {10.1051/0004-6361/201527188},
archivePrefix = {arXiv},
       eprint = {1512.01611},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016A&A...587A..18K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.456.2260A,
       author = {{Armstrong}, D.~J. and {Kirk}, J. and {Lam}, K.~W.~F. and
         {McCormac}, J. and {Osborn}, H.~P. and {Spake}, J. and {Walker}, S. and
         {Brown}, D.~J.~A. and {Kristiansen}, M.~H. and {Pollacco}, D. and
         {West}, R. and {Wheatley}, P.~J.},
        title = "{K2 variable catalogue - II. Machine learning classification of variable stars and eclipsing binaries in K2 fields 0-4}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: photometric, catalogues, binaries: eclipsing, stars: variables: general, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Feb",
       volume = {456},
       number = {2},
        pages = {2260-2272},
     abstract = "{We are entering an era of unprecedented quantities of data from current
        and planned survey telescopes. To maximize the potential of such
        surveys, automated data analysis techniques are required. Here
        we implement a new methodology for variable star classification,
        through the combination of Kohonen Self-Organizing Maps (SOMs,
        an unsupervised machine learning algorithm) and the more common
        Random Forest (RF) supervised machine learning technique. We
        apply this method to data from the K2 mission fields 0-4,
        finding 154 ab-type RR Lyraes (10 newly discovered), 377
        {\ensuremath{\delta}} Scuti pulsators, 133 {\ensuremath{\gamma}}
        Doradus pulsators, 183 detached eclipsing binaries, 290
        semidetached or contact eclipsing binaries and 9399 other
        periodic (mostly spot-modulated) sources, once class
        significance cuts are taken into account. We present light-curve
        features for all K2 stellar targets, including their three
        strongest detected frequencies, which can be used to study
        stellar rotation periods where the observed variability arises
        from spot modulation. The resulting catalogue of variable stars,
        classes, and associated data features are made available online.
        We publish our SOM code in PYTHON as part of the open source
        PYMVPA package, which in combination with already available RF
        modules can be easily used to recreate the method.}",
          doi = {10.1093/mnras/stv2836},
archivePrefix = {arXiv},
       eprint = {1512.01246},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.456.2260A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.456.2183D,
       author = {{Davies}, G.~R. and {Silva Aguirre}, V. and {Bedding}, T.~R. and {Hand
        berg}, R. and {Lund}, M.~N. and {Chaplin}, W.~J. and {Huber}, D. and
         {White}, T.~R. and {Benomar}, O. and {Hekker}, S. and {Basu}, S. and
         {Campante}, T.~L. and {Christensen-Dalsgaard}, J. and {Elsworth}, Y. and
         {Karoff}, C. and {Kjeldsen}, H. and {Lundkvist}, M.~S. and
         {Metcalfe}, T.~S. and {Stello}, D.},
        title = "{Oscillation frequencies for 35 Kepler solar-type planet-hosting stars using Bayesian techniques and machine learning}",
      journal = {\mnras},
     keywords = {asteroseismology, planets and satellites: fundamental parameters, stars: evolution, stars: fundamental parameters, stars: oscillations, planetary systems, Astrophysics - Solar and Stellar Astrophysics},
         year = "2016",
        month = "Feb",
       volume = {456},
       number = {2},
        pages = {2183-2195},
     abstract = "{Kepler has revolutionized our understanding of both exoplanets and their
        host stars. Asteroseismology is a valuable tool in the
        characterization of stars and Kepler is an excellent observing
        facility to perform asteroseismology. Here we select a sample of
        35 Kepler solar-type stars which host transiting exoplanets (or
        planet candidates) with detected solar-like oscillations. Using
        available Kepler short cadence data up to Quarter 16 we create
        power spectra optimized for asteroseismology of solar-type
        stars. We identify modes of oscillation and estimate mode
        frequencies by `peak bagging' using a Bayesian Markov Chain
        Monte Carlo framework. In addition, we expand the methodology of
        quality assurance using a Bayesian unsupervised machine learning
        approach. We report the measured frequencies of the modes of
        oscillation for all 35 stars and frequency ratios commonly used
        in detailed asteroseismic modelling. Due to the high
        correlations associated with frequency ratios we report the
        covariance matrix of all frequencies measured and frequency
        ratios calculated. These frequencies, frequency ratios, and
        covariance matrices can be used to obtain tight constraint on
        the fundamental parameters of these planet-hosting stars.}",
          doi = {10.1093/mnras/stv2593},
archivePrefix = {arXiv},
       eprint = {1511.02105},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.456.2183D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.456.1618A,
       author = {{Acquaviva}, Viviana},
        title = "{How to measure metallicity from five-band photometry with supervised machine learning algorithms}",
      journal = {\mnras},
     keywords = {methods: statistical, galaxies: photometry, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2016",
        month = "Feb",
       volume = {456},
       number = {2},
        pages = {1618-1626},
     abstract = "{We demonstrate that it is possible to measure metallicity from the SDSS
        five-band photometry to better than 0.1 dex using supervised
        machine learning algorithms. Using spectroscopic estimates of
        metallicity as ground truth, we build, optimize and train
        several estimators to predict metallicity. We use the observed
        photometry, as well as derived quantities such as stellar mass
        and photometric redshift, as features, and we build two sample
        data sets at median redshifts of 0.103 and 0.218 and median
        r-band magnitude of 17.5 and 18.3, respectively. We find that
        ensemble methods, such as random forests of trees and extremely
        randomized trees and support vector machines all perform
        comparably well and can measure metallicity with a Root Mean
        Square Error (RMSE) of 0.081 and 0.090 for the two data sets
        when all objects are included. The fraction of outliers (objects
        for which |Z$_{true}$ - Z$_{pred}$| \&gt; 0.2 dex) is 2.2 and
        3.9 per cent, respectively and the RMSE decreases to 0.068 and
        0.069 if those objects are excluded. Because of the ability of
        these algorithms to capture complex relationships between data
        and target, our technique performs better than previously
        proposed methods that sought to fit metallicity using an
        analytic fitting formula, and has 3{\texttimes} more
        constraining power than SED fitting-based methods. Additionally,
        this method is extremely forgiving of contamination in the
        training set, and can be used with very satisfactory results for
        sample sizes of a few hundred objects. We distribute all the
        routines to reproduce our results and apply them to other data
        sets.}",
          doi = {10.1093/mnras/stv2703},
archivePrefix = {arXiv},
       eprint = {1510.08076},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.456.1618A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.455.4301C,
       author = {{Crocce}, M. and {Carretero}, J. and {Bauer}, A.~H. and {Ross}, A.~J. and
         {Sevilla-Noarbe}, I. and {Giannantonio}, T. and {Sobreira}, F. and
         {Sanchez}, J. and {Gaztanaga}, E. and {Carrasco Kind}, M. and
         {S{\'a}nchez}, C. and {Bonnett}, C. and {Benoit-L{\'e}vy}, A. and
         {Brunner}, R.~J. and {Carnero Rosell}, A. and {Cawthon}, R. and
         {Fosalba}, P. and {Hartley}, W. and {Kim}, E.~J. and {Leistedt}, B. and
         {Miquel}, R. and {Peiris}, H.~V. and {Percival}, W.~J. and
         {Rosenfeld}, R. and {Rykoff}, E.~S. and {S{\'a}nchez}, E. and
         {Abbott}, T. and {Abdalla}, F.~B. and {Allam}, S. and {Banerji}, M. and
         {Bernstein}, G.~M. and {Bertin}, E. and {Brooks}, D. and
         {Buckley-Geer}, E. and {Burke}, D.~L. and {Capozzi}, D. and {Castand
        er}, F.~J. and {Cunha}, C.~E. and {D'Andrea}, C.~B. and
         {da Costa}, L.~N. and {Desai}, S. and {Diehl}, H.~T. and
         {Eifler}, T.~F. and {Evrard}, A.~E. and {Fausti Neto}, A. and {Fernand
        ez}, E. and {Finley}, D.~A. and {Flaugher}, B. and {Frieman}, J. and
         {Gerdes}, D.~W. and {Gruen}, D. and {Gruendl}, R.~A. and
         {Gutierrez}, G. and {Honscheid}, K. and {James}, D.~J. and {Kuehn}, K. and
         {Kuropatkin}, N. and {Lahav}, O. and {Li}, T.~S. and {Lima}, M. and
         {Maia}, M.~A.~G. and {March}, M. and {Marshall}, J.~L. and
         {Martini}, P. and {Melchior}, P. and {Miller}, C.~J. and {Neilsen}, E. and
         {Nichol}, R.~C. and {Nord}, B. and {Ogando}, R. and {Plazas}, A.~A. and
         {Romer}, A.~K. and {Sako}, M. and {Santiago}, B. and {Schubnell}, M. and
         {Smith}, R.~C. and {Soares-Santos}, M. and {Suchyta}, E. and
         {Swanson}, M.~E.~C. and {Tarle}, G. and {Thaler}, J. and {Thomas}, D. and
         {Vikram}, V. and {Walker}, A.~R. and {Wechsler}, R.~H. and
         {Weller}, J. and {Zuntz}, J. and {DES Collaboration}},
        title = "{Galaxy clustering, photometric redshifts and diagnosis of systematics in the DES Science Verification data}",
      journal = {\mnras},
     keywords = {surveys, cosmology: observations, large-scale structure of Universe, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2016",
        month = "Feb",
       volume = {455},
       number = {4},
        pages = {4301-4324},
     abstract = "{We study the clustering of galaxies detected at I \&lt; 22.5 in the
        Science Verification observations of the Dark Energy Survey
        (DES). Two-point correlation functions are measured using 2.3
        {\texttimes} {}10$^{6}$ galaxies over a contiguous 116 deg$^{2}$
        region in five bins of photometric redshift width
        {\ensuremath{\Delta}}z = 0.2 in the range 0.2 \&lt; z \&lt; 1.2.
        The impact of photometric redshift errors is assessed by
        comparing results using a template-based photo-z algorithm (BPZ)
        to a machine-learning algorithm (TPZ). A companion paper
        presents maps of several observational variables (e.g. seeing,
        sky brightness) which could modulate the galaxy density. Here we
        characterize and mitigate systematic errors on the measured
        clustering which arise from these observational variables, in
        addition to others such as Galactic dust and stellar
        contamination. After correcting for systematic effects, we
        measure galaxy bias over a broad range of linear scales relative
        to mass clustering predicted from the Planck
        {\ensuremath{\Lambda}} cold dark matter model, finding agreement
        with the Canada-France-Hawaii Telescope Legacy Survey (CFHTLS)
        measurements with {\ensuremath{\chi}}$^{2}$ of 4.0 (8.7) with 5
        degrees of freedom for the TPZ (BPZ) redshifts. We test a
        `linear bias' model, in which the galaxy clustering is a fixed
        multiple of the predicted non-linear dark matter clustering. The
        precision of the data allows us to determine that the linear
        bias model describes the observed galaxy clustering to 2.5 per
        cent accuracy down to scales at least 4-10 times smaller than
        those on which linear theory is expected to be sufficient.}",
          doi = {10.1093/mnras/stv2590},
archivePrefix = {arXiv},
       eprint = {1507.05360},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.455.4301C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016ApJ...818...55G,
       author = {{Graff}, Philip B. and {Lien}, Amy Y. and {Baker}, John G. and
         {Sakamoto}, Takanori},
        title = "{Modeling the Swift BAT Trigger Algorithm with Machine Learning}",
      journal = {\apj},
     keywords = {gamma-ray burst: general, gamma-rays: general, methods: data analysis},
         year = "2016",
        month = "Feb",
       volume = {818},
       number = {1},
          eid = {55},
        pages = {55},
     abstract = "{To draw inferences about gamma-ray burst (GRB) source populations based
        on Swift observations, it is essential to understand the
        detection efficiency of the Swift burst alert telescope (BAT).
        This study considers the problem of modeling the Swift/BAT
        triggering algorithm for long GRBs, a computationally expensive
        procedure, and models it using machine learning algorithms. A
        large sample of simulated GRBs from Lien et al. is used to train
        various models: random forests, boosted decision trees (with
        AdaBoost), support vector machines, and artificial neural
        networks. The best models have accuracies of
        {\ensuremath{\gtrsim}}97\% ({\ensuremath{\lesssim}}3\% error),
        which is a significant improvement on a cut in GRB flux, which
        has an accuracy of 89.6\% (10.4\% error). These models are then
        used to measure the detection efficiency of Swift as a function
        of redshift z, which is used to perform Bayesian parameter
        estimation on the GRB rate distribution. We find a local GRB
        rate density of \{n\}$_{0}${\ensuremath{\sim}}
        \{0.48\}$_{-0.23}$$^{+0.41}$ \{\{\{Gpc\}\}\}$^{-3}$
        \{\{\{yr\}\}\}$^{-1}$ with power-law indices of
        \{n\}$_{1}${\ensuremath{\sim}} \{1.7\}$_{-0.5}$$^{+0.6}$ and
        \{n\}$_{2}${\ensuremath{\sim}} -\{5.9\}$_{-0.1}$$^{+5.7}$ for
        GRBs above and below a break point of
        \{z\}$_{1}${\ensuremath{\sim}} \{6.8\}$_{-3.2}$$^{+2.8}$. This
        methodology is able to improve upon earlier studies by more
        accurately modeling Swift detection and using this for fully
        Bayesian model fitting.}",
          doi = {10.3847/0004-637X/818/1/55},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ApJ...818...55G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016PhRvE..93a2906M,
       author = {{Martiniani}, Stefano and {Schrenk}, K. Julian and
         {Stevenson}, Jacob D. and {Wales}, David J. and {Frenkel}, Daan},
        title = "{Turning intractable counting into sampling: Computing the configurational entropy of three-dimensional jammed packings}",
      journal = {\pre},
     keywords = {Condensed Matter - Statistical Mechanics, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Soft Condensed Matter, Physics - Computational Physics},
         year = "2016",
        month = "Jan",
       volume = {93},
       number = {1},
          eid = {012906},
        pages = {012906},
     abstract = "{We present a numerical calculation of the total number of disordered
        jammed configurations {\ensuremath{\Omega}} of N repulsive,
        three-dimensional spheres in a fixed volume V . To make these
        calculations tractable, we increase the computational efficiency
        of the approach of Xu et al. [Phys. Rev. Lett. 106, 245502
        (2011), 10.1103/PhysRevLett.106.245502] and Asenjo et al. [Phys.
        Rev. Lett. 112, 098002 (2014), 10.1103/PhysRevLett.112.098002]
        and we extend the method to allow computation of the
        configurational entropy as a function of pressure. The approach
        that we use computes the configurational entropy by sampling the
        absolute volume of basins of attraction of the stable packings
        in the potential energy landscape. We find a surprisingly strong
        correlation between the pressure of a configuration and the
        volume of its basin of attraction in the potential energy
        landscape. This relation is well described by a power law. Our
        methodology to compute the number of minima in the potential
        energy landscape should be applicable to a wide range of other
        enumeration problems in statistical physics, string theory,
        cosmology, and machine learning that aim to find the
        distribution of the extrema of a scalar cost function that
        depends on many degrees of freedom.}",
          doi = {10.1103/PhysRevE.93.012906},
archivePrefix = {arXiv},
       eprint = {1509.03964},
 primaryClass = {cond-mat.stat-mech},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016PhRvE..93a2906M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2016PhDT.......137B,
       author = {{Boada}, Steven Alvaro},
        title = "{Measuring the scatter in the cluster optical richness-mass relation with machine learning}",
     keywords = {Astronomy;Astrophysics},
       school = {Texas A\&amp;M University},
         year = "2016",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016PhDT.......137B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2016PhDT........76V,
       author = {{Vedachalam}, Arun},
        title = "{Surprise Discovery in Scientific Databases: A Framework for Data Intensive Science Utilizing the Power of Citizen Science}",
     keywords = {Statistics;Computer science;Astronomy},
       school = {George Mason University},
         year = "2016",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016PhDT........76V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2016PhDT........17K,
       author = {{Kapadia}, Shasvath Jagat},
        title = "{Topics in the Detection of Gravitational Waves from Compact Binary Inspirals}",
     keywords = {Physics;Astrophysics},
       school = {University of Arkansas},
         year = "2016",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016PhDT........17K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016OAP....29..109V,
       author = {{Vavilova}, I.~B.},
        title = "{Astroinformatics as a New Research Field. UkrVO Astroinformation Resources: Tasks and Prospective}",
      journal = {Odessa Astronomical Publications},
     keywords = {Astroinformatics; methods; Star catalogues; Galaxies; High-energy astrophysics},
         year = "2016",
        month = "Jan",
       volume = {29},
        pages = {109},
     abstract = "{The data-oriented astronomy has allowed classifying the Astroinformatics
        as a new academic research field, which covers various multi-
        disciplinary applications of the e-Astronomy. Among them are the
        data modeling, data mining, metadata standards development, data
        access, digital astronomical databases, image archives and
        visualization, machine learning, statistics and other
        computational methods and software for work with astronomical
        survey and catalogues with their teta- topeta-scale
        astroinformation resource. In this review we describe briefly
        the astroinformatics applications and software/services
        performed for different astronomical tasks in frame of the
        VIrtual Roentgen and Gamma Observatory (VIRGO) and Ukrainian
        VirtualObservatory (UkrVO). Among them there are projects based
        on the archival space-born data of X-ray and gamma space
        observatories and on the Joint Digitized Archive (JDA) database
        of astroplate network collections. The UkrVO JDA DR1 deals with
        the star catalogues (FON, Polar zone, open clusters, GRB star
        fields) as well as the UkrVO JDA DR2 deals with the Solar System
        bodies (giant and small planets, satellites, astronomical
        heritage images).}",
          doi = {10.18524/1810-4215.2016.29.85269},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016OAP....29..109V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016OAP....29...37D,
       author = {{Dobrycheva}, D.~V. and {Vavilova}, I.~B.},
        title = "{No the Holmberg Effect for Galaxy Pairs Selected from the SDSS DR9 at z {\ensuremath{\leq}} 0.06}",
      journal = {Odessa Astronomical Publications},
     keywords = {galaxies: color indices},
         year = "2016",
        month = "Jan",
       volume = {29},
        pages = {37-41},
     abstract = "{We studied the Holmberg effect in galaxy pairs selected from the SDSS
        DR9, where 60561 galaxies were limited by redshift 0.02 \&lt; z
        \&lt; 0.06 and absolute magnitude: M$_{r}$ {\ensuremath{\leq}}
        20.7$^{m}$ for central galaxies (N=18578) and M$_{r}$ \&gt;
        21.5$^{m}$ for neighbor galaxies (N=41983). We have made a
        morphological classification for each galaxy using both the
        visual inspection and machine learning methods. We considered
        four morphological types of galaxy pairs (E, early, and L, late,
        types) for testing the Holmberg effect: E- E, E-L, L-E, L-L
        (first companion of pairs is a central galaxy and second one is
        a faint satellite galaxy). We concluded about the absence of the
        Holmberg effect: R$_{g-i}$ = 0.3 for L-E pairs at 0.04 \&lt; z
        {\ensuremath{\leq}} 0.06 and R$_{g-i}$ = 0.2 for E-E and E-L
        pairs at 0.02 {\ensuremath{\leq}} z {\ensuremath{\leq}} 0.04.
        Summarizing, a correlation of color indices in pairs for the
        samples of galaxies composed with the half of large sky surveys
        likely SDSS was not confirmed or confirmed partially. The
        Holmberg effect is rather connected with morphological types of
        galaxies than with their color indices. Taking into account a
        scenario of the secular evolution, the presence of at least one
        elliptical galaxy in pair may be indicator of previous mergers
        in the earlier epoch. So, figuring manifestations of the
        Holmberg effect in its original interpretation no longer seems
        such urgent.}",
          doi = {10.18524/1810-4215.2016.29.85268},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016OAP....29...37D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.455.2387A,
       author = {{Almosallam}, Ibrahim A. and {Lindsay}, Sam N. and {Jarvis}, Matt J. and
         {Roberts}, Stephen J.},
        title = "{A sparse Gaussian process framework for photometric redshift estimation}",
      journal = {\mnras},
     keywords = {methods: data analysis, galaxies: distances and redshifts, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Computer Science - Computer Vision and Pattern Recognition},
         year = "2016",
        month = "Jan",
       volume = {455},
       number = {3},
        pages = {2387-2401},
     abstract = "{Accurate photometric redshifts are a lynchpin for many future
        experiments to pin down the cosmological model and for studies
        of galaxy evolution. In this study, a novel sparse regression
        framework for photometric redshift estimation is presented.
        Synthetic data set simulating the Euclid survey and real data
        from SDSS DR12 are used to train and test the proposed models.
        We show that approaches which include careful data preparation
        and model design offer a significant improvement in comparison
        with several competing machine learning algorithms. Standard
        implementations of most regression algorithms use the
        minimization of the sum of squared errors as the objective
        function. For redshift inference, this induces a bias in the
        posterior mean of the output distribution, which can be
        problematic. In this paper, we directly minimize the target
        metric {\ensuremath{\Delta}}z = (z$_{s}$ - z$_{p}$)/(1 +
        z$_{s}$) and address the bias problem via a distribution-based
        weighting scheme, incorporated as part of the optimization
        objective. The results are compared with other machine learning
        algorithms in the field such as artificial neural networks
        (ANN), Gaussian processes (GPs) and sparse GPs. The proposed
        framework reaches a mean absolute {\ensuremath{\Delta}}z =
        0.0026(1 + z$_{s}$), over the redshift range of 0
        {\ensuremath{\leq}} z$_{s}$ {\ensuremath{\leq}} 2 on the
        simulated data, and {\ensuremath{\Delta}}z = 0.0178(1 + z$_{s}$)
        over the entire redshift range on the SDSS DR12 survey,
        outperforming the standard ANNz used in the literature. We also
        investigate how the relative size of the training sample affects
        the photometric redshift accuracy. We find that a training
        sample of \&gt;30 per cent of total sample size, provides little
        additional constraint on the photometric redshifts, and note
        that our GP formalism strongly outperforms ANNz in the sparse
        data regime for the simulated data set.}",
          doi = {10.1093/mnras/stv2425},
archivePrefix = {arXiv},
       eprint = {1505.05489},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.455.2387A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.455..642K,
       author = {{Kamdar}, Harshil M. and {Turk}, Matthew J. and {Brunner}, Robert J.},
        title = "{Machine learning and cosmological simulations - I. Semi-analytical models}",
      journal = {\mnras},
     keywords = {galaxies: evolution, galaxies: formation, galaxies: haloes, cosmology: theory, large-scale structure of Universe, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2016",
        month = "Jan",
       volume = {455},
       number = {1},
        pages = {642-658},
     abstract = "{We present a new exploratory framework to model galaxy formation and
        evolution in a hierarchical Universe by using machine learning
        (ML). Our motivations are two-fold: (1) presenting a new,
        promising technique to study galaxy formation, and (2)
        quantitatively analysing the extent of the influence of dark
        matter halo properties on galaxies in the backdrop of semi-
        analytical models (SAMs). We use the influential Millennium
        Simulation and the corresponding Munich SAM to train and test
        various sophisticated ML algorithms (k-Nearest Neighbors,
        decision trees, random forests, and extremely randomized trees).
        By using only essential dark matter halo physical properties for
        haloes of M \&gt; {}10$^{12}$ M$_{☉}$ and a partial merger tree,
        our model predicts the hot gas mass, cold gas mass, bulge mass,
        total stellar mass, black hole mass and cooling radius at z = 0
        for each central galaxy in a dark matter halo for the Millennium
        run. Our results provide a unique and powerful phenomenological
        framework to explore the galaxy-halo connection that is built
        upon SAMs and demonstrably place ML as a promising and a
        computationally efficient tool to study small-scale structure
        formation.}",
          doi = {10.1093/mnras/stv2310},
archivePrefix = {arXiv},
       eprint = {1510.06402},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.455..642K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016MNRAS.455..626M,
       author = {{Mislis}, D. and {Bachelet}, E. and {Alsubai}, K.~A. and
         {Bramich}, D.~M. and {Parley}, N.},
        title = "{SIDRA: a blind algorithm for signal detection in photometric surveys}",
      journal = {\mnras},
     keywords = {techniques: photometric, planets and satellites: detection, planets and satellites: fundamental parameters, planetary systems, Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Jan",
       volume = {455},
       number = {1},
        pages = {626-633},
     abstract = "{We present the Signal Detection using Random-Forest Algorithm (SIDRA).
        SIDRA is a detection and classification algorithm based on the
        Machine Learning technique (Random Forest). The goal of this
        paper is to show the power of SIDRA for quick and accurate
        signal detection and classification. We first diagnose the power
        of the method with simulated light curves and try it on a subset
        of the Kepler space mission catalogue. We use five classes of
        simulated light curves (CONSTANT, TRANSIT, VARIABLE, MLENS and
        EB for constant light curves, transiting exoplanet, variable,
        microlensing events and eclipsing binaries, respectively) to
        analyse the power of the method. The algorithm uses four
        features in order to classify the light curves. The training
        sample contains 5000 light curves (1000 from each class) and 50
        000 random light curves for testing. The total SIDRA success
        ratio is {\ensuremath{\geq}}90 per cent. Furthermore, the
        success ratio reaches 95-100 per cent for the CONSTANT,
        VARIABLE, EB and MLENS classes and 92 per cent for the TRANSIT
        class with a decision probability of 60 per cent. Because the
        TRANSIT class is the one which fails the most, we run a
        simultaneous fit using SIDRA and a Box Least Square (BLS)-based
        algorithm for searching for transiting exoplanets. As a result,
        our algorithm detects 7.5 per cent more planets than a classic
        BLS algorithm, with better results for lower signal-to-noise
        light curves. SIDRA succeeds to catch 98 per cent of the planet
        candidates in the Kepler sample and fails for 7 per cent of the
        false alarms subset. SIDRA promises to be useful for developing
        a detection algorithm and/or classifier for large photometric
        surveys such as TESS and PLATO exoplanet future space missions.}",
          doi = {10.1093/mnras/stv2333},
archivePrefix = {arXiv},
       eprint = {1511.03456},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016MNRAS.455..626M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016GeoRL..43..359S,
       author = {{Shen}, Mao-Lin and {Keenlyside}, Noel and {Selten}, Frank and
         {Wiegerinck}, Wim and {Duane}, Gregory S.},
        title = "{Dynamically combining climate models to ``supermodel'' the tropical Pacific}",
      journal = {\grl},
     keywords = {supermodeling, interactive ensemble, synchronization, Nelder-Mead Method},
         year = "2016",
        month = "Jan",
       volume = {43},
       number = {1},
        pages = {359-366},
     abstract = "{We construct an interactive ensemble of two different climate models to
        improve simulation of key aspects of tropical Pacific climate.
        Our so-called supermodel is based on two atmospheric general
        circulation models (AGCMs) coupled to a single ocean GCM, which
        is driven by a weighted average of the air-sea fluxes. Optimal
        weights are determined using a machine learning algorithm to
        minimize sea surface temperature errors over the tropical
        Pacific. This coupling strategy synchronizes atmospheric
        variability in the two AGCMs over the equatorial Pacific, where
        it improves the representation of ocean-atmosphere interaction
        and the climate state. In particular, the common double
        Intertropical Convergence Zone error is suppressed, and the
        positive Bjerknes feedback improves substantially to match
        observations well, and the negative heat flux feedback is also
        much improved. This study supports the concept of supermodeling
        as a promising multimodel ensemble strategy to improve weather
        and climate predictions.}",
          doi = {10.1002/2015GL066562},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016GeoRL..43..359S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INPROCEEDINGS{2016ASSP...42..123T,
       author = {{Tortora}, C. and {Napolitano}, N.~R. and {La Barbera}, F. and
         {Roy}, N. and {Radovich}, M. and {Getman}, F. and {Brescia}, M. and
         {Cavuoti}, S. and {Capaccioli}, M. and {Longo}, G.},
        title = "{Galaxy Evolution Within the Kilo-Degree Survey}",
     keywords = {Physics, Astrophysics - Astrophysics of Galaxies},
    booktitle = {The Universe of Digital Sky Surveys},
         year = "2016",
       editor = {{Napolitano}, Nicola R. and {Longo}, Giuseppe and {Marconi}, Marcella and
         {Paolillo}, Maurizio and {Iodice}, Enrichetta},
       volume = {42},
        month = "Jan",
        pages = {123},
     abstract = "{The ESO Public Kilo-Degree Survey (KiDS) is an optical wide-field
        imaging survey carried out with the VLT Survey Telescope and the
        OmegaCAM camera. KiDS will scan 1,500 deg$^{2}$ in four optical
        filters (u, g, r, i). Designed to be a weak lensing survey, it
        is ideal for galaxy evolution studies, thanks to the high
        spatial resolution of VST, the excellent seeing and the
        photometric depth. The surface photometry has provided with
        structural parameters (e.g. size and S{\'e}rsic index), aperture
        and total magnitudes have been used to obtain photometric
        redshifts from Machine Learning methods and stellar
        masses/luminositites from stellar population synthesis. Our
        project aimed at investigating the evolution of the colour and
        structural properties of galaxies with mass and environment up
        to redshift z ̃ 0.5 and more, to put constraints on galaxy
        evolution processes, as galaxy mergers.}",
          doi = {10.1007/978-3-319-19330-4_19},
archivePrefix = {arXiv},
       eprint = {1507.00736},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ASSP...42..123T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INPROCEEDINGS{2016ASSP...42...91E,
       author = {{Elliott}, J. and {de Souza}, R.~S. and {Krone-Martins}, A. and
         {Cameron}, E. and {Ishida}, E.~E.~O. and {Hilbe}, J.},
        title = "{Using Gamma Regression for Photometric Redshifts of Survey Galaxies}",
     keywords = {Physics, Astrophysics - Instrumentation and Methods for Astrophysics},
    booktitle = {The Universe of Digital Sky Surveys},
         year = "2016",
       editor = {{Napolitano}, Nicola R. and {Longo}, Giuseppe and {Marconi}, Marcella and
         {Paolillo}, Maurizio and {Iodice}, Enrichetta},
       volume = {42},
        month = "Jan",
        pages = {91},
     abstract = "{Machine learning techniques offer a plethora of opportunities in
        tackling big data within the astronomical community. We present
        the set of Generalized Linear Models as a fast alternative for
        determining photometric redshifts of galaxies, a set of tools
        not commonly applied within astronomy, despite being widely used
        in other professions. With this technique, we achieve
        catastrophic outlier rates of the order of ̃ 1\%, that can be
        achieved in a matter of seconds on large datasets of size ̃
        1,000,000. To make these techniques easily accessible to the
        astronomical community, we developed a set of libraries and
        tools that are publicly available.}",
          doi = {10.1007/978-3-319-19330-4_13},
archivePrefix = {arXiv},
       eprint = {1507.01293},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016ASSP...42...91E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016AcA....66...15K,
       author = {{Klencki}, J. and {Wyrzykowski}, {\L}. and {Kostrzewa-Rutkowska}, Z. and
         {Udalski}, A.},
        title = "{Robust Filtering of Artifacts in Difference Imaging for Rapid Transients Detection}",
      journal = {\actaa},
     keywords = {Methods: data analysis, Methods: statistical, Surveys, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2016",
        month = "Jan",
       volume = {66},
       number = {1},
        pages = {15-29},
     abstract = "{Real-time analysis and classification of observational data collected
        within synoptic sky surveys is a huge challenge due to constant
        growth of data volumes. Machine learning techniques are often
        applied in order to perform this task automatically. The current
        bottleneck of transients detection in most surveys is the
        process of filtering numerous artifacts from candidate
        detection. We present a new method for automated artifact
        filtering based on hierarchical unsupervised classifier
        employing Self-Organizing Maps (SOMs). The system accepts 97\%
        of real transients and removes 97.5\% of artifacts when tested
        on the OGLE-IV Transient Detection System. The improvement of
        the artifacts filtering allows for single-frame-based detection
        of transients within OGLE-IV, which now alerts on transient
        discoveries in less than 15 minutes from the image acquisition.}",
archivePrefix = {arXiv},
       eprint = {1601.06320},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016AcA....66...15K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.454.2026D,
       author = {{du Buisson}, L. and {Sivanandam}, N. and {Bassett}, Bruce A. and
         {Smith}, M.},
        title = "{Machine learning classification of SDSS transient survey images}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: observational, methods: statistical, techniques: image processing, techniques: photometric, surveys, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Computer Vision and Pattern Recognition},
         year = "2015",
        month = "Dec",
       volume = {454},
       number = {2},
        pages = {2026-2038},
     abstract = "{We show that multiple machine learning algorithms can match human
        performance in classifying transient imaging data from the Sloan
        Digital Sky Survey (SDSS) supernova survey into real objects and
        artefacts. This is a first step in any transient science
        pipeline and is currently still done by humans, but future
        surveys such as the Large Synoptic Survey Telescope (LSST) will
        necessitate fully machine-enabled solutions. Using features
        trained from eigenimage analysis (principal component analysis,
        PCA) of single-epoch g, r and i difference images, we can reach
        a completeness (recall) of 96 per cent, while only incorrectly
        classifying at most 18 per cent of artefacts as real objects,
        corresponding to a precision (purity) of 84 per cent. In
        general, random forests performed best, followed by the
        k-nearest neighbour and the SkyNet artificial neural net
        algorithms, compared to other methods such as naive Bayes and
        kernel support vector machine. Our results show that PCA-based
        machine learning can match human success levels and can
        naturally be extended by including multiple epochs of data,
        transient colours and host galaxy information which should allow
        for significant further improvements, especially at low signal-
        to-noise.}",
          doi = {10.1093/mnras/stv2041},
archivePrefix = {arXiv},
       eprint = {1407.4118},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015MNRAS.454.2026D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015GeoRL..4210640M,
       author = {{Martin}, Kylara M. and {Wood}, Warren T. and {Becker}, Joseph J.},
        title = "{A global prediction of seafloor sediment porosity using machine learning}",
      journal = {\grl},
     keywords = {seafloor porosity, machine learning, random forests, marine sediment properties, interpolation techniques},
         year = "2015",
        month = "Dec",
       volume = {42},
       number = {24},
        pages = {10,640-10,646},
     abstract = "{Porosity (void ratio) is a critical parameter in models of acoustic
        propagation, bearing strength, and many other seafloor
        phenomena. However, like many seafloor phenomena, direct
        measurements are expensive and sparse. We show here how porosity
        everywhere at the seafloor can be estimated using a machine
        learning technique (specifically, Random Forests). Such
        techniques use sparsely acquired direct samples and dense grids
        of other parameters to produce a statistically optimal estimate
        where direct measurements are lacking. Our porosity estimate is
        both qualitatively more consistent with geologic principles than
        the results produced by interpolation and quantitatively more
        accurate than results produced by interpolation or regression
        methods. We present here a seafloor porosity estimate on a 5 arc
        min, pixel registered grid, produced using widely available,
        densely sampled grids of other seafloor properties. These
        techniques represent the only practical means of estimating
        seafloor properties in inaccessible regions of the seafloor
        (e.g., the Arctic).}",
          doi = {10.1002/2015GL065279},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015GeoRL..4210640M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015CQGra..32x5002K,
       author = {{Kim}, Kyungmin and {Harry}, Ian W. and {Hodge}, Kari A. and
         {Kim}, Young-Min and {Lee}, Chang-Hwan and {Lee}, Hyun Kyu and
         {Oh}, John J. and {Oh}, Sang Hoon and {Son}, Edwin J.},
        title = "{Application of artificial neural network to search for gravitational-wave signals associated with short gamma-ray bursts}",
      journal = {Classical and Quantum Gravity},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - High Energy Astrophysical Phenomena, General Relativity and Quantum Cosmology},
         year = "2015",
        month = "Dec",
       volume = {32},
       number = {24},
          eid = {245002},
        pages = {245002},
     abstract = "{We apply a machine learning algorithm, the artificial neural network, to
        the search for gravitational-wave signals associated with short
        gamma-ray bursts (GRBs). The multi-dimensional samples
        consisting of data corresponding to the statistical and physical
        quantities from the coherent search pipeline are fed into the
        artificial neural network to distinguish simulated
        gravitational-wave signals from background noise artifacts. Our
        result shows that the data classification efficiency at a fixed
        false alarm probability (FAP) is improved by the artificial
        neural network in comparison to the conventional detection
        statistic. Specifically, the distance at 50\% detection
        probability at a fixed false positive rate is increased about
        8\%-14\% for the considered waveform models. We also evaluate a
        few seconds of the gravitational-wave data segment using the
        trained networks and obtain the FAP. We suggest that the
        artificial neural network can be a complementary method to the
        conventional detection statistic for identifying gravitational-
        wave signals related to the short GRBs.}",
          doi = {10.1088/0264-9381/32/24/245002},
archivePrefix = {arXiv},
       eprint = {1410.6878},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015CQGra..32x5002K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015SpWea..13..778M,
       author = {{Muranushi}, Takayuki and {Shibayama}, Takuya and
         {Muranushi}, Yuko Hada and {Isobe}, Hiroaki and {Nemoto}, Shigeru and
         {Komazaki}, Kenji and {Shibata}, Kazunari},
        title = "{UFCORIN: A fully automated predictor of solar flares in GOES X-ray flux}",
      journal = {Space Weather},
     keywords = {space weather, flare forecast, machine learning, Astrophysics - Solar and Stellar Astrophysics},
         year = "2015",
        month = "Nov",
       volume = {13},
       number = {11},
        pages = {778-796},
     abstract = "{We have developed UFCORIN, a platform for studying and automating space
        weather prediction. Using our system we have tested 6160
        different combinations of Solar Dynamic Observatory/Helioseismic
        and Magnetic Imager data as input data, and simulated the
        prediction of GOES X-ray flux for 2 years (2011-2012) with 1 h
        cadence. We have found that direct comparison of the true skill
        statistic (TSS) from small cross-validation sets is ill posed
        and used the standard scores (z) of the TSS to compare the
        performance of the various prediction strategies. The z of a
        strategy is a stochastic variable of the stochastically chosen
        cross-validation data set, and the z for the three strategies
        best at predicting X-, {\ensuremath{\geq}}M-, and
        {\ensuremath{\geq}}C-class flares are better than the average z
        of the 6160 strategies by 2.3{\ensuremath{\sigma}},
        2.1{\ensuremath{\sigma}}, and 3.8{\ensuremath{\sigma}}
        confidence levels, respectively. The best three TSS values were
        0.75 {\ensuremath{\pm}} 0.07, 0.48 {\ensuremath{\pm}} 0.02, and
        0.56 {\ensuremath{\pm}} 0.04, respectively.}",
          doi = {10.1002/2015SW001257},
archivePrefix = {arXiv},
       eprint = {1507.08011},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015SpWea..13..778M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.454.1132T,
       author = {{Tarnopolski}, M.},
        title = "{Distinguishing short and long Fermi gamma-ray bursts}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, gamma-rays: general, Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Cosmology and Nongalactic Astrophysics, Statistics - Machine Learning},
         year = "2015",
        month = "Nov",
       volume = {454},
       number = {1},
        pages = {1132-1139},
     abstract = "{Two classes of gamma-ray bursts (GRBs), short and long, have been
        determined without any doubts, and are usually ascribed to
        different progenitors, yet these classes overlap for a variety
        of descriptive parameters. A subsample of 46 long and 22 short
        Fermi GRBs with estimated Hurst Exponents (HEs), complemented by
        minimum variability time-scales (MVTS) and durations (T$_{90}$)
        is used to perform a supervised machine learning and Monte Carlo
        simulation using a support vector machine algorithm. It is found
        that while T$_{90}$ itself performs very well in distinguishing
        short and long GRBs, the overall success ratio is higher when
        the training set is complemented by MVTS and HE. These results
        may allow us to introduce a new (non-linear) parameter that
        might provide less ambiguous classification of GRBs.}",
          doi = {10.1093/mnras/stv2061},
archivePrefix = {arXiv},
       eprint = {1507.04886},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015MNRAS.454.1132T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015CQGra..32u5012P,
       author = {{Powell}, Jade and {Trifir{\`o}}, Daniele and {Cuoco}, Elena and
         {Heng}, Ik Siong and {Cavagli{\`a}}, Marco},
        title = "{Classification methods for noise transients in advanced gravitational-wave detectors}",
      journal = {Classical and Quantum Gravity},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "Nov",
       volume = {32},
       number = {21},
          eid = {215012},
        pages = {215012},
     abstract = "{Noise of non-astrophysical origin will contaminate science data taken by
        the advanced laser interferometer gravitational-wave observatory
        and advanced Virgo gravitational-wave detectors. Prompt
        characterization of instrumental and environmental noise
        transients will be critical for improving the sensitivity of the
        advanced detectors in the upcoming science runs. During the
        science runs of the initial gravitational-wave detectors, noise
        transients were manually classified by visually examining the
        time-frequency scan of each event. Here, we present three new
        algorithms designed for the automatic classification of noise
        transients in advanced detectors. Two of these algorithms are
        based on principal component analysis. They are principal
        component analysis for transients and an adaptation of
        LALInference burst. The third algorithm is a combination of an
        event generator called wavelet detection filter and machine
        learning techniques for classification. We test these algorithms
        on simulated data sets, and we show their ability to
        automatically classify transients by frequency, signal to noise
        ratio and waveform morphology.}",
          doi = {10.1088/0264-9381/32/21/215012},
archivePrefix = {arXiv},
       eprint = {1505.01299},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015CQGra..32u5012P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJ...813...28F,
       author = {{Farrell}, Sean A. and {Murphy}, Tara and {Lo}, Kitty K.},
        title = "{Autoclassification of the Variable 3XMM Sources Using the Random Forest Machine Learning Algorithm}",
      journal = {\apj},
     keywords = {catalogs, methods: statistical, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2015",
        month = "Nov",
       volume = {813},
       number = {1},
          eid = {28},
        pages = {28},
     abstract = "{In the current era of large surveys and massive data sets,
        autoclassification of astrophysical sources using intelligent
        algorithms is becoming increasingly important. In this paper we
        present the catalog of variable sources in the Third XMM-Newton
        Serendipitous Source catalog (3XMM) autoclassified using the
        Random Forest machine learning algorithm. We used a sample of
        manually classified variable sources from the second data
        release of the XMM-Newton catalogs (2XMMi-DR2) to train the
        classifier, obtaining an accuracy of {\ensuremath{\sim}}92\%. We
        also evaluated the effectiveness of identifying spurious
        detections using a sample of spurious sources, achieving an
        accuracy of {\ensuremath{\sim}}95\%. Manual investigation of a
        random sample of classified sources confirmed these accuracy
        levels and showed that the Random Forest machine learning
        algorithm is highly effective at automatically classifying 3XMM
        sources. Here we present the catalog of classified 3XMM variable
        sources. We also present three previously unidentified unusual
        sources that were flagged as outlier sources by the algorithm: a
        new candidate supergiant fast X-ray transient, a 400 s X-ray
        pulsar, and an eclipsing 5 hr binary system coincident with a
        known Cepheid.}",
          doi = {10.1088/0004-637X/813/1/28},
archivePrefix = {arXiv},
       eprint = {1509.03714},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJ...813...28F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015A&C....13...86S,
       author = {{Schuh}, M.~A. and {Angryk}, R.~A. and {Martens}, P.~C.},
        title = "{Solar image parameter data from the SDO: Long-term curation and data mining}",
      journal = {Astronomy and Computing},
     keywords = {Solar images, Computer vision, Data analysis, Data mining, Big data},
         year = "2015",
        month = "Nov",
       volume = {13},
        pages = {86-98},
     abstract = "{The Solar Dynamics Observatory (SDO) mission captures thousands of
        images of the Sun per day, motivating the need for efficient and
        effective storage, representation, and search over a massive
        repository of data. This work investigates the general-purpose
        image parameter data produced by the SDO Feature Finding Team's
        trainable module, which operates at a fixed six minute cadence
        over all AIA channels. The data contains ten numerical measures
        computed for each image cell over a 64 {\texttimes} 64 grid for
        each image. We analyze all available data and metadata produced
        over the first three years and present comprehensive statistics
        and outliers while validating the cleanliness and usability of
        the data source for future research. We then utilize a database
        of automated solar event reports to create large-scale region-
        labeled datasets available to the public. We highlight the new-
        found potential for data-driven discovery by presenting several
        best-case labeling scenarios that establish a baseline for
        comparing machine learning classification and attribute (image
        parameter) evaluation results. Future work focuses on continued
        dataset curation and spatiotemporal data mining.}",
          doi = {10.1016/j.ascom.2015.10.004},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015A&C....13...86S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015SoPh..290.2693T,
       author = {{Teng}, Fei},
        title = "{Application of Kernel Based Machine Learning to the Inversion Problem of Photospheric Magnetic Fields}",
      journal = {\solphys},
     keywords = {Statistical machine learning, Support vector machine regression, Inversion of photospheric magnetic field, Solar spectropolarimeter},
         year = "2015",
        month = "Oct",
       volume = {290},
       number = {10},
        pages = {2693-2708},
     abstract = "{For the purpose of fast methods for handling huge amounts of data coming
        from future solar spectropolarimeters, the statistical machine-
        learning techniques based on Mercer's kernel were applied to the
        inversion of the photospheric magnetic fields from polarimetric
        data. In particular, the Regularized Neural Network and the
        Support Vector Machine were tested for the data from the
        Helioseismic and Magnetic Imager on the Solar Dynamics
        Observatory.}",
          doi = {10.1007/s11207-015-0781-1},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015SoPh..290.2693T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.453..507K,
       author = {{Kim}, Edward J. and {Brunner}, Robert J. and {Carrasco Kind}, Matias},
        title = "{A hybrid ensemble learning approach to star-galaxy classification}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, surveys, stars: statistics, galaxies: statistics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "Oct",
       volume = {453},
       number = {1},
        pages = {507-521},
     abstract = "{There exist a variety of star-galaxy classification techniques, each
        with their own strengths and weaknesses. In this paper, we
        present a novel meta-classification framework that combines and
        fully exploits different techniques to produce a more robust
        star-galaxy classification. To demonstrate this hybrid, ensemble
        approach, we combine a purely morphological classifier, a
        supervised machine learning method based on random forest, an
        unsupervised machine learning method based on self-organizing
        maps, and a hierarchical Bayesian template-fitting method. Using
        data from the CFHTLenS survey (Canada-France-Hawaii Telescope
        Lensing Survey), we consider different scenarios: when a high-
        quality training set is available with spectroscopic labels from
        DEEP2 (Deep Extragalactic Evolutionary Probe Phase 2 ), SDSS
        (Sloan Digital Sky Survey), VIPERS (VIMOS Public Extragalactic
        Redshift Survey), and VVDS (VIMOS VLT Deep Survey), and when the
        demographics of sources in a low-quality training set do not
        match the demographics of objects in the test data set. We
        demonstrate that our Bayesian combination technique improves the
        overall performance over any individual classification method in
        these scenarios. Thus, strategies that combine the predictions
        of different classifiers may prove to be optimal in currently
        ongoing and forthcoming photometric surveys, such as the Dark
        Energy Survey and the Large Synoptic Survey Telescope.}",
          doi = {10.1093/mnras/stv1608},
archivePrefix = {arXiv},
       eprint = {1505.02200},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015MNRAS.453..507K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.452.4183H,
       author = {{Hoyle}, Ben and {Rau}, Markus Michael and {Paech}, Kerstin and
         {Bonnett}, Christopher and {Seitz}, Stella and {Weller}, Jochen},
        title = "{Anomaly detection for machine learning redshifts applied to SDSS galaxies}",
      journal = {\mnras},
     keywords = {catalogues, surveys, galaxies: distances and redshifts, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2015",
        month = "Oct",
       volume = {452},
       number = {4},
        pages = {4183-4194},
     abstract = "{We present an analysis of anomaly detection for machine learning
        redshift estimation. Anomaly detection allows the removal of
        poor training examples, which can adversely influence redshift
        estimates. Anomalous training examples may be photometric
        galaxies with incorrect spectroscopic redshifts, or galaxies
        with one or more poorly measured photometric quantity. We select
        2.5 million `clean' SDSS DR12 galaxies with reliable
        spectroscopic redshifts, and 6730 `anomalous' galaxies with
        spectroscopic redshift measurements which are flagged as
        unreliable. We contaminate the clean base galaxy sample with
        galaxies with unreliable redshifts and attempt to recover the
        contaminating galaxies using the Elliptical Envelope technique.
        We then train four machine learning architectures for redshift
        analysis on both the contaminated sample and on the preprocessed
        `anomaly-removed' sample and measure redshift statistics on a
        clean validation sample generated without any preprocessing. We
        find an improvement on all measured statistics of up to 80 per
        cent when training on the anomaly removed sample as compared
        with training on the contaminated sample for each of the machine
        learning routines explored. We further describe a method to
        estimate the contamination fraction of a base data sample.}",
          doi = {10.1093/mnras/stv1551},
archivePrefix = {arXiv},
       eprint = {1503.08214},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015MNRAS.452.4183H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015JGRA..120.8440L,
       author = {{Li}, J.~D. and {Spasojevic}, M. and {Inan}, U.~S.},
        title = "{Predicting conditions for the reception of one-hop signals from the Siple transmitter experiment using the Kp index}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {Siple station, machine learning, ELF/VLF, wave-particle interactions},
         year = "2015",
        month = "Oct",
       volume = {120},
       number = {10},
        pages = {8440-8447},
     abstract = "{Wave injection experiments provide an opportunity to explore and
        quantify aspects of nonlinear wave-particle phenomena in a
        controlled manner. Waves are injected into space from ground-
        based ELF/VLF transmitters, and the modified waves are measured
        by radio receivers on the ground in the conjugate hemisphere.
        These experiments are expensive and challenging projects to
        build and to operate, and the transmitted waves are not always
        detected in the conjugate region. Even the powerful transmitter
        located at Siple Station, Antarctica in 1986, estimated to
        radiate over 1 kW, only reported a reception rate of ̃40\%,
        indicating that a significant number of transmissions served no
        observable scientific purpose and reflecting the difficulty in
        determining suitable conditions for transmission and reception.
        Leveraging modern machine-learning classification techniques, we
        apply two statistical techniques, a Bayes and a support vector
        machine classifier, to predict the occurrence of detectable one-
        hop transmissions from Siple data with accuracies on the order
        of 80\%-90\%. Applying these classifiers to our 1986 Siple data
        set, we detect 406 receptions of Siple transmissions which we
        analyze to generate more robust statistics on nonlinear growth
        rates, 3 dB/s-270 dB/s, and nonlinear total amplification, 3
        dB-41 dB.}",
          doi = {10.1002/2015JA021547},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015JGRA..120.8440L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015JASTP.133...25I,
       author = {{Islam}, Tanvir and {Srivastava}, Prashant K. and {Dai}, Qiang and
         {Gupta}, Manika and {Wan Jaafar}, Wan Zurina},
        title = "{Stratiform/convective rain delineation for TRMM microwave imager}",
      journal = {Journal of Atmospheric and Solar-Terrestrial Physics},
     keywords = {Stratiform/convective rain, Precipitation type classification, Passive microwave (PMW) sensor, Satellite measurements, Precipitation radar (PR), Brightness temperature, Machine learning algorithms},
         year = "2015",
        month = "Oct",
       volume = {133},
        pages = {25-35},
     abstract = "{This article investigates the potential for using machine learning
        algorithms to delineate stratiform/convective (S/C) rain regimes
        for passive microwave imager taking calibrated brightness
        temperatures as only spectral parameters. The algorithms have
        been implemented for the Tropical Rainfall Measuring Mission
        (TRMM) microwave imager (TMI), and calibrated as well as
        validated taking the Precipitation Radar (PR) S/C information as
        the target class variables. Two different algorithms are
        particularly explored for the delineation. The first one is
        metaheuristic adaptive boosting algorithm that includes the
        real, gentle, and modest versions of the AdaBoost. The second
        one is the classical linear discriminant analysis that includes
        the Fisher's and penalized versions of the linear discriminant
        analysis. Furthermore, prior to the development of the
        delineation algorithms, a feature selection analysis has been
        conducted for a total of 85 features, which contains the
        combinations of brightness temperatures from 10 GHz to 85 GHz
        and some derived indexes, such as scattering index, polarization
        corrected temperature, and polarization difference with the help
        of mutual information aided minimal redundancy maximal relevance
        criterion (mRMR). It has been found that the polarization
        corrected temperature at 85 GHz and the features derived from
        the ``addition'' operator associated with the 85 GHz channels
        have good statistical dependency to the S/C target class
        variables. Further, it has been shown how the mRMR feature
        selection technique helps to reduce the number of features
        without deteriorating the results when applying through the
        machine learning algorithms. The proposed scheme is able to
        delineate the S/C rain regimes with reasonable accuracy. Based
        on the statistical validation experience from the validation
        period, the Matthews correlation coefficients are in the range
        of 0.60-0.70. Since, the proposed method does not rely on any a
        priori information, this makes it very suitable for other
        microwave sensors having similar channels to the TMI. The method
        could possibly benefit the constellation sensors in the Global
        Precipitation Measurement (GPM) mission era.}",
          doi = {10.1016/j.jastp.2015.07.009},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015JASTP.133...25I},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015E&SS....2..417F,
       author = {{Fuchs}, Thomas J. and {Thompson}, David R. and {Bue}, Brian D. and
         {Castillo-Rogez}, Julie and {Chien}, Steve A. and {Gharibian}, Dero and
         {Wagstaff}, Kiri L.},
        title = "{Enhanced flyby science with onboard computer vision: Tracking and surface feature detection at small bodies}",
      journal = {Earth and Space Science},
     keywords = {small bodies, comets, asteroids, flyby, machine learning, computer vision},
         year = "2015",
        month = "Oct",
       volume = {2},
       number = {10},
        pages = {417-434},
     abstract = "{Spacecraft autonomy is crucial to increase the science return of optical
        remote sensing observations at distant primitive bodies. To
        date, most small bodies exploration has involved short timescale
        flybys that execute prescripted data collection sequences. Light
        time delay means that the spacecraft must operate completely
        autonomously without direct control from the ground, but in most
        cases the physical properties and morphologies of prospective
        targets are unknown before the flyby. Surface features of
        interest are highly localized, and successful observations must
        account for geometry and illumination constraints. Under these
        circumstances onboard computer vision can improve science yield
        by responding immediately to collected imagery. It can reacquire
        bad data or identify features of opportunity for additional
        targeted measurements. We present a comprehensive framework for
        onboard computer vision for flyby missions at small bodies. We
        introduce novel algorithms for target tracking, target
        segmentation, surface feature detection, and anomaly detection.
        The performance and generalization power are evaluated in detail
        using expert annotations on data sets from previous encounters
        with primitive bodies.}",
          doi = {10.1002/2014EA000042},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015E&SS....2..417F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJ...812...46T,
       author = {{Thompson}, Susan E. and {Mullally}, Fergal and {Coughlin}, Jeff and
         {Christiansen}, Jessie L. and {Henze}, Christopher E. and
         {Haas}, Michael R. and {Burke}, Christopher J.},
        title = "{A Machine Learning Technique to Identify Transit Shaped Signals}",
      journal = {\apj},
     keywords = {binaries: eclipsing, methods: data analysis, planetary systems, Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2015",
        month = "Oct",
       volume = {812},
       number = {1},
          eid = {46},
        pages = {46},
     abstract = "{We describe a new metric that uses machine learning to determine if a
        periodic signal found in a photometric time series appears to be
        shaped like the signature of a transiting exoplanet. This metric
        uses dimensionality reduction and k-nearest neighbors to
        determine whether a given signal is sufficiently similar to
        known transits in the same data set. This metric is being used
        by the Kepler Robovetter to determine which signals should be
        part of the Q1-Q17 DR24 catalog of planetary candidates. The
        Kepler Mission reports roughly 20,000 potential transiting
        signals with each run of its pipeline, yet only a few thousand
        appear to be sufficiently transit shaped to be part of the
        catalog. The other signals tend to be variable stars and
        instrumental noise. With this metric, we are able to remove more
        than 90\% of the non-transiting signals while retaining more
        than 99\% of the known planet candidates. When tested with
        injected transits, less than 1\% are lost. This metric will
        enable the Kepler mission and future missions looking for
        transiting planets to rapidly and consistently find the best
        planetary candidates for follow-up and cataloging.}",
          doi = {10.1088/0004-637X/812/1/46},
archivePrefix = {arXiv},
       eprint = {1509.00041},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJ...812...46T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.452.3100C,
       author = {{Cavuoti}, S. and {Brescia}, M. and {Tortora}, C. and {Longo}, G. and
         {Napolitano}, N.~R. and {Radovich}, M. and {La Barbera}, F. and
         {Capaccioli}, M. and {de Jong}, J.~T.~A. and {Getman}, F. and
         {Grado}, A. and {Paolillo}, M.},
        title = "{Machine-learning-based photometric redshifts for galaxies of the ESO Kilo-Degree Survey data release 2}",
      journal = {\mnras},
     keywords = {techniques: photometric, galaxies: distances and redshifts, galaxies: photometry, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2015",
        month = "Sep",
       volume = {452},
       number = {3},
        pages = {3100-3105},
     abstract = "{We have estimated photometric redshifts (z$_{phot}$) for more than 1.1
        million galaxies of the public European Southern Observatory
        (ESO) Kilo-Degree Survey (KiDS) data release 2. KiDS is an
        optical wide-field imaging survey carried out with the Very
        Large Telescope (VLT) Survey Telescope (VST) and the OmegaCAM
        camera, which aims to tackle open questions in cosmology and
        galaxy evolution, such as the origin of dark energy and the
        channel of galaxy mass growth. We present a catalogue of
        photometric redshifts obtained using the Multi-Layer Perceptron
        with Quasi-Newton Algorithm (MLPQNA) model, provided within the
        framework of the DAta Mining and Exploration Web Application
        REsource (DAMEWARE). These photometric redshifts are based on a
        spectroscopic knowledge base that was obtained by merging
        spectroscopic data sets from the Galaxy and Mass Assembly (GAMA)
        data release 2 and the Sloan Digital Sky Survey III (SDSS-III)
        data release 9. The overall 1{\ensuremath{\sigma}} uncertainty
        on {\ensuremath{\Delta}}z = (z$_{spec}$ - z$_{phot}$)/(1 +
        z$_{spec}$) is ̃0.03, with a very small average bias of ̃0.001,
        a normalized median absolute deviation of ̃0.02 and a fraction
        of catastrophic outliers (|{\ensuremath{\Delta}}z| \&gt; 0.15)
        of ̃0.4 per cent.}",
          doi = {10.1093/mnras/stv1496},
archivePrefix = {arXiv},
       eprint = {1507.00754},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015MNRAS.452.3100C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015JCAP...09..005A,
       author = {{Aslanyan}, Grigor and {Easther}, Richard and {Price}, Layne C.},
        title = "{Learn-as-you-go acceleration of cosmological parameter estimates}",
      journal = {Journal of Cosmology and Astro-Particle Physics},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2015",
        month = "Sep",
       volume = {2015},
       number = {9},
          eid = {005},
        pages = {005},
     abstract = "{Cosmological analyses can be accelerated by approximating slow
        calculations using a training set, which is either precomputed
        or generated dynamically. However, this approach is only safe if
        the approximations are well understood and controlled. This
        paper surveys issues associated with the use of machine-learning
        based emulation strategies for accelerating cosmological
        parameter estimation. We describe a learn-as-you-go algorithm
        that is implemented in the Cosmo++ code and (1) trains the
        emulator while simultaneously estimating posterior
        probabilities; (2) identifies unreliable estimates, computing
        the exact numerical likelihoods if necessary; and (3)
        progressively learns and updates the error model as the
        calculation progresses. We explicitly describe and model the
        emulation error and show how this can be propagated into the
        posterior probabilities. We apply these techniques to the Planck
        likelihood and the calculation of {\ensuremath{\Lambda}}CDM
        posterior probabilities. The computation is significantly
        accelerated without a pre-defined training set and uncertainties
        in the posterior probabilities are subdominant to statistical
        fluctuations. We have obtained a speedup factor of 6.5 for
        Metropolis-Hastings and 3.5 for nested sampling. Finally, we
        discuss the general requirements for a credible error model and
        show how to update them on-the-fly.}",
          doi = {10.1088/1475-7516/2015/09/005},
archivePrefix = {arXiv},
       eprint = {1506.01079},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015JCAP...09..005A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJ...811...30M,
       author = {{Miller}, A.~A.},
        title = "{The Synthetic-Oversampling Method: Using Photometric Colors to Discover Extremely Metal-poor Stars}",
      journal = {\apj},
     keywords = {methods: data analysis, methods: statistical, stars: general, stars: statistics, stars: fundamental parameters, surveys, Astrophysics - Solar and Stellar Astrophysics},
         year = "2015",
        month = "Sep",
       volume = {811},
       number = {1},
          eid = {30},
        pages = {30},
     abstract = "{Extremely metal-poor (EMP) stars ([Fe/H] {\ensuremath{\leq}} -3.0 dex)
        provide a unique window into understanding the first generation
        of stars and early chemical enrichment of the universe. EMP
        stars are exceptionally rare, however, and the relatively small
        number of confirmed discoveries limits our ability to exploit
        these near-field probes of the first ̃500 Myr after the Big
        Bang. Here, a new method to photometrically estimate [Fe/H] from
        only broadband photometric colors is presented. I show that the
        method, which utilizes machine-learning algorithms and a
        training set of ̃170,000 stars with spectroscopically measured
        [Fe/H], produces a typical scatter of ̃0.29 dex. This
        performance is similar to what is achievable via low-resolution
        spectroscopy, and outperforms other photometric techniques,
        while also being more general. I further show that a slight
        alteration to the model, wherein synthetic EMP stars are added
        to the training set, yields the robust identification of EMP
        candidates. In particular, this synthetic-oversampling method
        recovers ̃20\% of the EMP stars in the training set, at a
        precision of ̃0.05. Furthermore, ̃65\% of the false positives
        from the model are very metal-poor stars ([Fe/H]
        {\ensuremath{\leq}} -2.0 dex). The synthetic-oversampling method
        is biased toward the discovery of warm (̃F-type) stars, a
        consequence of the targeting bias from the Sloan Digital Sky
        Survey/Sloan Extension for Galactic Understanding survey. This
        EMP selection method represents a significant improvement over
        alternative broadband optical selection techniques. The models
        are applied to \&gt;12 million stars, with an expected yield of
        ̃600 new EMP stars, which promises to open new avenues for
        exploring the early universe.}",
          doi = {10.1088/0004-637X/811/1/30},
archivePrefix = {arXiv},
       eprint = {1505.01854},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJ...811...30M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015AJ....150...82G,
       author = {{Goldstein}, D.~A. and {D'Andrea}, C.~B. and {Fischer}, J.~A. and
         {Foley}, R.~J. and {Gupta}, R.~R. and {Kessler}, R. and {Kim}, A.~G. and
         {Nichol}, R.~C. and {Nugent}, P.~E. and {Papadopoulos}, A. and
         {Sako}, M. and {Smith}, M. and {Sullivan}, M. and {Thomas}, R.~C. and
         {Wester}, W. and {Wolf}, R.~C. and {Abdalla}, F.~B. and {Banerji}, M. and
         {Benoit-L{\'e}vy}, A. and {Bertin}, E. and {Brooks}, D. and
         {Carnero Rosell}, A. and {Castander}, F.~J. and {da Costa}, L.~N. and
         {Covarrubias}, R. and {DePoy}, D.~L. and {Desai}, S. and
         {Diehl}, H.~T. and {Doel}, P. and {Eifler}, T.~F. and
         {Fausti Neto}, A. and {Finley}, D.~A. and {Flaugher}, B. and
         {Fosalba}, P. and {Frieman}, J. and {Gerdes}, D. and {Gruen}, D. and
         {Gruendl}, R.~A. and {James}, D. and {Kuehn}, K. and {Kuropatkin}, N. and
         {Lahav}, O. and {Li}, T.~S. and {Maia}, M.~A.~G. and {Makler}, M. and
         {March}, M. and {Marshall}, J.~L. and {Martini}, P. and
         {Merritt}, K.~W. and {Miquel}, R. and {Nord}, B. and {Ogando}, R. and
         {Plazas}, A.~A. and {Romer}, A.~K. and {Roodman}, A. and {Sanchez}, E. and
         {Scarpine}, V. and {Schubnell}, M. and {Sevilla-Noarbe}, I. and
         {Smith}, R.~C. and {Soares-Santos}, M. and {Sobreira}, F. and
         {Suchyta}, E. and {Swanson}, M.~E.~C. and {Tarle}, G. and {Thaler}, J. and
         {Walker}, A.~R.},
        title = "{Automated Transient Identification in the Dark Energy Survey}",
      journal = {\aj},
     keywords = {methods: data analysis, methods: statistical, supernovae: general, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "Sep",
       volume = {150},
       number = {3},
          eid = {82},
        pages = {82},
     abstract = "{We describe an algorithm for identifying point-source transients and
        moving objects on reference-subtracted optical images containing
        artifacts of processing and instrumentation. The algorithm makes
        use of the supervised machine learning technique known as Random
        Forest. We present results from its use in the Dark Energy
        Survey Supernova program (DES-SN), where it was trained using a
        sample of 898,963 signal and background events generated by the
        transient detection pipeline. After reprocessing the data
        collected during the first DES-SN observing season (2013
        September through 2014 February) using the algorithm, the number
        of transient candidates eligible for human scanning decreased by
        a factor of 13.4, while only 1.0\% of the artificial Type Ia
        supernovae (SNe) injected into search images to monitor survey
        efficiency were lost, most of which were very faint events. Here
        we characterize the algorithm{\textquoteright}s performance in
        detail, and we discuss how it can inform pipeline design
        decisions for future time-domain imaging surveys, such as the
        Large Synoptic Survey Telescope and the Zwicky Transient
        Facility. An implementation of the algorithm and the training
        data used in this paper are available at at <A href=``http://por
        tal.nersc.gov/project/dessn/autoscan''>http://portal.nersc.gov/p
        roject/dessn/autoscan</A>.}",
          doi = {10.1088/0004-6256/150/3/82},
archivePrefix = {arXiv},
       eprint = {1504.02936},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015AJ....150...82G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015A&C....12...67K,
       author = {{Kremer}, J. and {Gieseke}, F. and {Steenstrup Pedersen}, K. and
         {Igel}, C.},
        title = "{Nearest neighbor density ratio estimation for large-scale applications in astronomy}",
      journal = {Astronomy and Computing},
     keywords = {Methods: data analysis, Methods: statistical, Galaxies: distances and redshifts, Sample selection bias, Nearest neighbors, Large-scale learning},
         year = "2015",
        month = "Sep",
       volume = {12},
        pages = {67-72},
     abstract = "{In astronomical applications of machine learning, the distribution of
        objects used for building a model is often different from the
        distribution of the objects the model is later applied to. This
        is known as sample selection bias, which is a major challenge
        for statistical inference as one can no longer assume that the
        labeled training data are representative. To address this issue,
        one can re-weight the labeled training patterns to match the
        distribution of unlabeled data that are available already in the
        training phase. There are many examples in practice where this
        strategy yielded good results, but estimating the weights
        reliably from a finite sample is challenging. We consider an
        efficient nearest neighbor density ratio estimator that can
        exploit large samples to increase the accuracy of the weight
        estimates. To solve the problem of choosing the right
        neighborhood size, we propose to use cross-validation on a model
        selection criterion that is unbiased under covariate shift. The
        resulting algorithm is our method of choice for density ratio
        estimation when the feature space dimensionality is small and
        sample sizes are large. The approach is simple and, because of
        the model selection, robust. We empirically find that it is on a
        par with established kernel-based methods on relatively small
        regression benchmark datasets. However, when applied to large-
        scale photometric redshift estimation, our approach outperforms
        the state-of-the-art.}",
          doi = {10.1016/j.ascom.2015.06.005},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015A&C....12...67K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015A&C....12...60S,
       author = {{Schutter}, A. and {Shamir}, L.},
        title = "{Galaxy morphology - An unsupervised machine learning approach}",
      journal = {Astronomy and Computing},
     keywords = {Galaxies, Structure-galaxies, Evolution-methods, Analytical-techniques, Image processing, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "Sep",
       volume = {12},
        pages = {60-66},
     abstract = "{Structural properties poses valuable information about the formation and
        evolution of galaxies, and are important for understanding the
        past, present, and future universe. Here we use unsupervised
        machine learning methodology to analyze a network of
        similarities between galaxy morphological types, and
        automatically deduce a morphological sequence of galaxies.
        Application of the method to the EFIGI catalog show that the
        morphological scheme produced by the algorithm is largely in
        agreement with the De Vaucouleurs system, demonstrating the
        ability of computer vision and machine learning methods to
        automatically profile galaxy morphological sequences. The
        unsupervised analysis method is based on comprehensive computer
        vision techniques that compute the visual similarities between
        the different morphological types. Rather than relying on human
        cognition, the proposed system deduces the similarities between
        sets of galaxy images in an automatic manner, and is therefore
        not limited by the number of galaxies being analyzed. The source
        code of the method is publicly available, and the protocol of
        the experiment is included in the paper so that the experiment
        can be replicated, and the method can be used to analyze user-
        defined datasets of galaxy images.}",
          doi = {10.1016/j.ascom.2015.05.002},
archivePrefix = {arXiv},
       eprint = {1505.04876},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015A&C....12...60S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ARA&A..53..247M,
       author = {{Marshall}, Philip J. and {Lintott}, Chris J. and {Fletcher}, Leigh N.},
        title = "{Ideas for Citizen Science in Astronomy}",
      journal = {\araa},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "Aug",
       volume = {53},
        pages = {247-278},
     abstract = "{We review the expanding, internet-enabled, and rapidly evolving field of
        citizen astronomy, focusing on research projects in stellar,
        extragalactic, and planetary science that have benefited from
        the participation of members of the public. These volunteers
        contribute in various ways: making and analyzing new
        observations, visually classifying features in images and light
        curves, exploring models constrained by astronomical data sets,
        and initiating new scientific enquiries. The most productive
        citizen astronomy projects involve close collaboration between
        the professionals and amateurs involved and occupy scientific
        niches not easily filled by great observatories or machine
        learning methods: Citizen astronomers are motivated by being of
        service to science, as well as by their interest in the subject.
        We expect participation and productivity in citizen astronomy to
        increase, as data sets get larger and citizen science platforms
        become more efficient. Opportunities include engaging citizens
        in ever-more advanced analyses and facilitating citizen-led
        enquiry through professional tools designed with citizens in
        mind.}",
          doi = {10.1146/annurev-astro-081913-035959},
archivePrefix = {arXiv},
       eprint = {1409.4291},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ARA&A..53..247M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJS..219...39R,
       author = {{Richards}, Gordon T. and {Myers}, Adam D. and {Peters}, Christina M. and
         {Krawczyk}, Coleman M. and {Chase}, Greg and {Ross}, Nicholas P. and
         {Fan}, Xiaohui and {Jiang}, Linhua and {Lacy}, Mark and
         {McGreer}, Ian D. and {Trump}, Jonathan R. and {Riegel}, Ryan N.},
        title = "{Bayesian High-redshift Quasar Classification from Optical and Mid-IR Photometry}",
      journal = {\apjs},
     keywords = {catalogs, infrared: galaxies, methods: statistical, quasars: general, Astrophysics - Astrophysics of Galaxies},
         year = "2015",
        month = "Aug",
       volume = {219},
       number = {2},
          eid = {39},
        pages = {39},
     abstract = "{We identify 885,503 type 1 quasar candidates to i{\ensuremath{\lesssim}}
        22 using the combination of optical and mid-IR photometry.
        Optical photometry is taken from the Sloan Digital Sky Survey-
        III: Baryon Oscillation Spectroscopic Survey (SDSS-III/BOSS),
        while mid-IR photometry comes from a combination of data from
        the Wide-field Infrared Survey Explorer (WISE)
        {\textquotedblleft}AllWISE{\textquotedblright} data release and
        several large-area Spitzer Space Telescope fields. Selection is
        based on a Bayesian kernel density algorithm with a training
        sample of 157,701 spectroscopically confirmed type 1 quasars
        with both optical and mid-IR data. Of the quasar candidates,
        733,713 lack spectroscopic confirmation (and 305,623 are objects
        that we have not previously classified as photometric quasar
        candidates). These candidates include 7874 objects targeted as
        high-probability potential quasars with 3.5\textbackslashlt
        z\textbackslashlt 5 (of which 6779 are new photometric
        candidates). Our algorithm is more complete to z\textbackslashgt
        3.5 than the traditional mid-IR selection
        {\textquotedblleft}wedges{\textquotedblright} and to
        2.2\textbackslashlt z\textbackslashlt 3.5 quasars than the SDSS-
        III/BOSS project. Number counts and luminosity function analysis
        suggest that the resulting catalog is relatively complete to
        known quasars and is identifying new high-z quasars at
        z\textbackslashgt 3. This catalog paves the way for luminosity-
        dependent clustering investigations of large numbers of faint,
        high-redshift quasars and for further machine-learning quasar
        selection using Spitzer and WISE data combined with other large-
        area optical imaging surveys.}",
          doi = {10.1088/0067-0049/219/2/39},
archivePrefix = {arXiv},
       eprint = {1507.07788},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJS..219...39R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJ...809...40G,
       author = {{Gopalan}, Giri and {Vrtilek}, Saeqa Dil and {Bornn}, Luke},
        title = "{Classifying X-Ray Binaries: A Probabilistic Approach}",
      journal = {\apj},
     keywords = {methods: data analysis, methods: statistical, pulsars: general, stars: black holes, stars: neutron, X-rays: binaries, Astrophysics - High Energy Astrophysical Phenomena, Statistics - Applications, Statistics - Machine Learning},
         year = "2015",
        month = "Aug",
       volume = {809},
       number = {1},
          eid = {40},
        pages = {40},
     abstract = "{In X-ray binary star systems consisting of a compact object that
        accretes material from an orbiting secondary star, there is no
        straightforward means to decide whether the compact object is a
        black hole or a neutron star. To assist in this process, we
        develop a Bayesian statistical model that makes use of the fact
        that X-ray binary systems appear to cluster based on their
        compact object type when viewed from a three-dimensional
        coordinate system derived from X-ray spectral data where the
        first coordinate is the ratio of counts in the mid- to low-
        energy band (color 1), the second coordinate is the ratio of
        counts in the high- to low-energy band (color 2), and the third
        coordinate is the sum of counts in all three bands. We use this
        model to estimate the probabilities of an X-ray binary system
        containing a black hole, non-pulsing neutron star, or pulsing
        neutron star. In particular, we utilize a latent variable model
        in which the latent variables follow a Gaussian process prior
        distribution, and hence we are able to induce the spatial
        correlation which we believe exists between systems of the same
        type. The utility of this approach is demonstrated by the
        accurate prediction of system types using Rossi X-ray Timing
        Explorer All Sky Monitor data, but it is not flawless. In
        particular, non-pulsing neutron systems containing
        {\textquotedblleft}bursters{\textquotedblright} that are close
        to the boundary demarcating systems containing black holes tend
        to be classified as black hole systems. As a byproduct of our
        analyses, we provide the astronomer with the public R code which
        can be used to predict the compact object type of XRBs given
        training data.}",
          doi = {10.1088/0004-637X/809/1/40},
archivePrefix = {arXiv},
       eprint = {1507.03538},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJ...809...40G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.451..332B,
       author = {{Baron}, Dalya and {Poznanski}, Dovi and {Watson}, Darach and
         {Yao}, Yushu and {Cox}, Nick L.~J. and {Prochaska}, J. Xavier},
        title = "{Using Machine Learning to classify the diffuse interstellar bands}",
      journal = {\mnras},
     keywords = {techniques: spectroscopic, surveys, dust, extinction, ISM: general, ISM: lines and bands, ISM: molecules, Astrophysics - Astrophysics of Galaxies},
         year = "2015",
        month = "Jul",
       volume = {451},
       number = {1},
        pages = {332-352},
     abstract = "{Using over a million and a half extragalactic spectra from the Sloan
        Digital Sky Survey we study the correlations of the diffuse
        interstellar bands (DIBs) in the Milky Way. We measure the
        correlation between DIB strength and dust extinction for 142
        DIBs using 24 stacked spectra in the reddening range E(B - V)
        \&lt; 0.2, many more lines than ever studied before. Most of the
        DIBs do not correlate with dust extinction. However, we find 10
        weak and barely studied DIBs with correlations that are higher
        than 0.7 with dust extinction and confirm the high correlation
        of additional five strong DIBs. Furthermore, we find a pair of
        DIBs, 5925.9 and 5927.5 {\r{A}}, which exhibits significant
        negative correlation with dust extinction, indicating that their
        carrier may be depleted on dust. We use Machine Learning
        algorithms to divide the DIBs to spectroscopic families based on
        250 stacked spectra. By removing the dust dependence, we study
        how DIBs follow their local environment. We thus obtain six
        groups of weak DIBs, four of which are tightly associated with
        C$_{2}$ or CN absorption lines.}",
          doi = {10.1093/mnras/stv977},
archivePrefix = {arXiv},
       eprint = {1501.04631},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015MNRAS.451..332B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.450.3893B,
       author = {{Brescia}, M. and {Cavuoti}, S. and {Longo}, G.},
        title = "{Automated physical classification in the SDSS DR10. A catalogue of candidate quasars}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: photometric, catalogues, galaxies: active, quasars: general, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2015",
        month = "Jul",
       volume = {450},
       number = {4},
        pages = {3893-3903},
     abstract = "{We discuss whether modern machine learning methods can be used to
        characterize the physical nature of the large number of objects
        sampled by the modern multiband digital surveys. In particular,
        we applied the MLPQNA (Multi Layer Perceptron with Quasi Newton
        Algorithm) method to the optical data of the Sloan Digital Sky
        Survey (SDSS) Data Release 10, investigating whether photometric
        data alone suffice to disentangle different classes of objects
        as they are defined in the SDSS spectroscopic classification. We
        discuss three groups of classification problems: (i) the
        simultaneous classification of galaxies, quasars and stars; (ii)
        the separation of stars from quasars; (iii) the separation of
        galaxies with normal spectral energy distribution from those
        with peculiar spectra, such as starburst or star-forming
        galaxies and AGN. While confirming the difficulty of
        disentangling AGN from normal galaxies on a photometric basis
        only, MLPQNA proved to be quite effective in the three-class
        separation. In disentangling quasars from stars and galaxies,
        our method achieved an overall efficiency of 91.31 per cent and
        a QSO class purity of ̃95 per cent. The resulting catalogue of
        candidate quasars/AGNs consists of ̃3.6 million objects, of
        which about half a million are also flagged as robust
        candidates, and will be made available on CDS VizieR facility.}",
          doi = {10.1093/mnras/stv854},
archivePrefix = {arXiv},
       eprint = {1504.03857},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015MNRAS.450.3893B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015JSWSC...5A..23R,
       author = {{Reiss}, Martin A. and {Hofmeister}, Stefan J. and {De Visscher}, Ruben and
         {Temmer}, Manuela and {Veronig}, Astrid M. and
         {Delouille}, V{\'e}ronique and {Mampaey}, Benjamin and
         {Ahammer}, Helmut},
        title = "{Improvements on coronal hole detection in SDO/AIA images using supervised classification}",
      journal = {Journal of Space Weather and Space Climate},
     keywords = {Solar wind, Coronal holes, Filament channels, Feature extraction, Supervised Classification, Textural features, Astrophysics - Solar and Stellar Astrophysics},
         year = "2015",
        month = "Jul",
       volume = {5},
          eid = {A23},
        pages = {A23},
     abstract = "{We demonstrate the use of machine learning algorithms in combination
        with segmentation techniques in order to distinguish coronal
        holes and filaments in SDO/AIA EUV images of the Sun. Based on
        two coronal hole detection techniques (intensity-based
        thresholding, SPoCA), we prepared datasets of manually labeled
        coronal hole and filament channel regions present on the Sun
        during the time range 2011-2013. By mapping the extracted
        regions from EUV observations onto HMI line-of-sight
        magnetograms we also include their magnetic characteristics. We
        computed shape measures from the segmented binary maps as well
        as first order and second order texture statistics from the
        segmented regions in the EUV images and magnetograms. These
        attributes were used for data mining investigations to identify
        the most performant rule to differentiate between coronal holes
        and filament channels. We applied several classifiers, namely
        Support Vector Machine (SVM), Linear Support Vector Machine,
        Decision Tree, and Random Forest, and found that all
        classification rules achieve good results in general, with
        linear SVM providing the best performances (with a true skill
        statistic of {\ensuremath{\approx}} 0.90). Additional
        information from magnetic field data systematically improves the
        performance across all four classifiers for the SPoCA detection.
        Since the calculation is inexpensive in computing time, this
        approach is well suited for applications on real-time data. This
        study demonstrates how a machine learning approach may help
        improve upon an unsupervised feature extraction method.}",
          doi = {10.1051/swsc/2015025},
archivePrefix = {arXiv},
       eprint = {1506.06623},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015JSWSC...5A..23R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015JSWSC...5A..19B,
       author = {{Byrne}, Jason P.},
        title = "{Investigating the kinematics of coronal mass ejections with the automated CORIMP catalog}",
      journal = {Journal of Space Weather and Space Climate},
     keywords = {Sun, Coronal mass ejection (CME), Space weather, Solar image processing, Machine learning, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Earth and Planetary Astrophysics, Physics - Data Analysis, Statistics and Probability, Physics - Space Physics},
         year = "2015",
        month = "Jul",
       volume = {5},
          eid = {A19},
        pages = {A19},
     abstract = "{Studying coronal mass ejections (CMEs) in coronagraph data can be
        challenging due to their diffuse structure and transient nature,
        compounded by the variations in their dynamics, morphology and
        frequency of occurrence. The large amounts of data available
        from missions like the Solar and Heliospheric Observatory (SOHO)
        make manual cataloging of CMEs tedious and prone to human error,
        and so a robust method of detection and analysis is required and
        often preferred. A new coronal image processing catalog called
        CORIMP has been developed in an effort to achieve this, through
        the implementation of a dynamic background separation technique
        and multiscale edge detection. These algorithms together isolate
        and characterise CME structure in the field-of-view of the Large
        Angle Spectrometric Coronagraph (LASCO) onboard SOHO. CORIMP
        also applies a Savitzky-Golay filter, along with quadratic and
        linear fits, to the height-time measurements for better
        revealing the true CME speed and acceleration profiles across
        the plane-of-sky. Here we present a sample of new results from
        the CORIMP CME catalog, and directly compare them with the other
        automated catalogs of Computer Aided CME Tracking (CACTus) and
        Solar Eruptive Events Detection System (SEEDS), as well as the
        manual CME catalog at the Coordinated Data Analysis Workshop
        (CDAW) Data Center and a previously published study of the
        sample events. We further investigate a form of unsupervised
        machine learning by using a k-means clustering algorithm to
        distinguish detections of multiple CMEs that occur close
        together in space and time. While challenges still exist, this
        investigation and comparison of results demonstrate the
        reliability and robustness of the CORIMP catalog, proving its
        effectiveness at detecting and tracking CMEs throughout the
        LASCO dataset.}",
          doi = {10.1051/swsc/2015020},
archivePrefix = {arXiv},
       eprint = {1506.04046},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015JSWSC...5A..19B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ChA&A..39..368L,
       author = {{Lei}, Yu and {Zhao}, Dan-Ning and {Gao}, Yu-Ping and {Cai}, Hong-Bing},
        title = "{Prediction of Length-of-day Variations Based on Gaussian Processes}",
      journal = {\caa},
     keywords = {astrometry, time, methods: data analysis},
         year = "2015",
        month = "Jul",
       volume = {39},
       number = {3},
        pages = {368-379},
     abstract = "{Due to the complicated time-varying characteristics of the length-of-day
        (LOD) variations, the accuracies of traditional linear models
        for the prediction of the LOD variations, such as the least
        squares extrapolation model, the time-series analysis model and
        so on, cannot satisfy the requirements for the real-time and
        high-precision applications. In this paper, a new machine
        learning algorithm - the Gaussian process (GP) model is employed
        to forecast the LOD variations. Its prediction accuracy is
        analyzed and compared with those of the back propagation neural
        networks (BPNN), general regression neural networks (GRNN), and
        the Earth Orientation Parameters Prediction Comparison Campaign
        (EOP PCC). The results demonstrate that the application of the
        GP model to the prediction of the LOD variations is efficient
        and feasible.}",
          doi = {10.1016/j.chinastron.2015.07.008},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ChA&A..39..368L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ChA&A..39..319C,
       author = {{Chang}, Li-Na and {Zhang}, Pei-Ai},
        title = "{Application of Multi-task Lasso Regression in the Parametrization of Stellar Spectra}",
      journal = {\caa},
     keywords = {stars: fundamental parameters, methods: data analysis, methods: statistical, methods: miscellaneous},
         year = "2015",
        month = "Jul",
       volume = {39},
       number = {3},
        pages = {319-329},
     abstract = "{The multi-task learning approaches have attracted the increasing
        attention in the fields of machine learning, computer vision,
        and artificial intelligence. By utilizing the correlations in
        tasks, learning multiple related tasks simultaneously is better
        than learning each task independently. An efficient multi-task
        Lasso (Least Absolute Shrinkage Selection and Operator)
        regression algorithm is proposed in this paper to estimate the
        physical parameters of stellar spectra. It not only can obtain
        the information about the common features of the different
        physical parameters, but also can preserve effectively their own
        peculiar features. Experiments were done based on the ELODIE
        synthetic spectral data simulated with the stellar atmospheric
        model, and on the SDSS data released by the American large-scale
        survey Sloan. The estimation precision of our model is better
        than those of the methods in the related literature, especially
        for the estimates of the gravitational acceleration (lg g) and
        the chemical abundance ([Fe/H]). In the experiments we changed
        the spectral resolution, and applied the noises with different
        signal-to-noise ratios (SNRs) to the spectral data, so as to
        illustrate the stability of the model. The results show that the
        model is influenced by both the resolution and the noise. But
        the influence of the noise is larger than that of the
        resolution. In general, the multi-task Lasso regression
        algorithm is easy to operate, it has a strong stability, and can
        also improve the overall prediction accuracy of the model.}",
          doi = {10.1016/j.chinastron.2015.07.004},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ChA&A..39..319C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015AstL...41..307M,
       author = {{Meshcheryakov}, A.~V. and {Glazkova}, V.~V. and {Gerasimov}, S.~V. and
         {Burenin}, R.~A. and {Khorunzhev}, G.~A.},
        title = "{High-accuracy redshift measurements for galaxy clusters at z \&lt; 0.45 based on SDSS-III photometry}",
      journal = {Astronomy Letters},
     keywords = {observational cosmology, galaxy clusters, photometric redshifts, machine learning, SDSS},
         year = "2015",
        month = "Jul",
       volume = {41},
       number = {7},
        pages = {307-316},
     abstract = "{A new method for measuring the redshifts of galaxy clusters based on
        photometric SDSSIII data is presented. Highly accurate photo-z
        measurements for red-sequence galaxies using machine learning
        techniques on a training sample of luminous red BOSS LOWZ
        galaxies allow the redshifts of clusters at z \&lt; 0.45 to be
        determinedwith an accuracy {\ensuremath{\sigma}} $_{norm}$ =
        0.011. The accuracy of the proposed method has been estimated on
        galaxy clusters from the 400d catalog.}",
          doi = {10.1134/S1063773715070038},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015AstL...41..307M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJ...807..182M,
       author = {{Meyers}, Joshua E. and {Burchat}, Patricia R.},
        title = "{Impact of Atmospheric Chromatic Effects on Weak Lensing Measurements}",
      journal = {\apj},
     keywords = {atmospheric effects, cosmology: observations, gravitational lensing: weak, techniques: image processing, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "Jul",
       volume = {807},
       number = {2},
          eid = {182},
        pages = {182},
     abstract = "{Current and future imaging surveys will measure cosmic shear with
        statistical precision that demands a deeper understanding of
        potential systematic biases in galaxy shape measurements than
        has been achieved to date. We use analytic and computational
        techniques to study the impact on shape measurements of two
        atmospheric chromatic effects for ground-based surveys such as
        the Dark Energy Survey and the Large Synoptic Survey Telescope
        (LSST): (1) atmospheric differential chromatic refraction and
        (2) wavelength dependence of seeing. We investigate the effects
        of using the point-spread function (PSF) measured with stars to
        determine the shapes of galaxies that have different spectral
        energy distributions than the stars. We find that both chromatic
        effects lead to significant biases in galaxy shape measurements
        for current and future surveys, if not corrected. Using
        simulated galaxy images, we find a form of chromatic
        {\textquotedblleft}model bias{\textquotedblright} that arises
        when fitting a galaxy image with a model that has been convolved
        with a stellar, instead of galactic, PSF. We show that both
        forms of atmospheric chromatic biases can be predicted (and
        corrected) with minimal model bias by applying an ordered set of
        perturbative PSF-level corrections based on machine-learning
        techniques applied to six-band photometry. Catalog-level
        corrections do not address the model bias. We conclude that
        achieving the ultimate precision for weak lensing from current
        and future ground-based imaging surveys requires a detailed
        understanding of the wavelength dependence of the PSF from the
        atmosphere, and from other sources such as optics and sensors.
        The source code for this analysis is available at <A href=``http
        s://github.com/DarkEnergyScienceCollaboration/chroma''>https://g
        ithub.com/DarkEnergyScienceCollaboration/chroma</A>.}",
          doi = {10.1088/0004-637X/807/2/182},
archivePrefix = {arXiv},
       eprint = {1409.6273},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJ...807..182M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJ...807..175A,
       author = {{Alipour}, N. and {Safari}, H.},
        title = "{Statistical Properties of Solar Coronal Bright Points}",
      journal = {\apj},
     keywords = {Sun: activity, Sun: corona, Sun: flares, Sun: magnetic fields, sunspots},
         year = "2015",
        month = "Jul",
       volume = {807},
       number = {2},
          eid = {175},
        pages = {175},
     abstract = "{Here, we aim to study the statistical properties (i.e., spatial,
        temporal, and magnetic structures) of extreme ultraviolet
        coronal bright points (CBPs) observed by SDO during a 4.4 yr
        period (2010 June 1 to 2014 October 31). We developed the
        automatic detection method for CBPs based on the machine-
        learning technique and Zernike image moments. The average number
        and the mean density of CBPs are estimated to be about 572 (per
        full disk image taken at 193 {\r{A}}) and 1.9{\texttimes}
        \{10\}$^{-4}$ Mm$^{-2}$, respectively. There is a negative
        correlation (-0.7) between the number of CBPs and the number of
        sunspots. The size and lifetime frequency distribution of CBPs
        show the lognormal and power-law (exponent equal to -1.6)
        behaviors, respectively. The relationship between the lifetime
        and size of CBPs is clearly treated by a power-law function with
        an exponent equal to 0.13. Around 1.3\% of the solar surface is
        covered by the bright cores of CBPs and 2.6\% of that is covered
        by their total area. About 52\% of CBPs have lifetimes of less
        than 20 minutes and the remaining 48\% have mean lifetimes of 6
        hr. More than 95\% of CBPs with lifetimes of less than 20 hr and
        nine CBPs with lifetimes of more than 72 hr are detected. The
        average number of the new CBPs emerging every 45 s in the whole
        of the Sun is about 27 {\ensuremath{\pm}} 3. The temporal self-
        affinity of the time series of CBPs that emerged, indexed by the
        Hurst exponent determined using both detrended fluctuation
        analysis and R/S analysis, is 0.78. This long-temporal
        correlation suggests that CBPs form a system of self-organized
        criticality.}",
          doi = {10.1088/0004-637X/807/2/175},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJ...807..175A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015AdSpR..56..273L,
       author = {{Li}, Weipeng and {Huang}, Hai and {Peng}, Fujun},
        title = "{Trajectory classification in circular restricted three-body problem using support vector machine}",
      journal = {Advances in Space Research},
     keywords = {Three-body problem, Transit orbit, Support vector machine, Machine learning, Gaussian kernel},
         year = "2015",
        month = "Jul",
       volume = {56},
       number = {2},
        pages = {273-280},
     abstract = "{In the circular restricted three-body problem (CR3BP), transit orbit is
        a class of orbit which can pass through the bottleneck region of
        the zero velocity curve and escapes from the vicinity of the
        primary or the secondary. This kind of orbit plays a very
        important role in the design of space exploration missions. A
        kind of low-energy interplanetary transfer, which is called
        Interplanetary Superhighway (IPS), can be realized by utilizing
        transit orbits. To use the transit orbit in actual mission
        design, a key issue is to find an algorithm which can separate
        the states corresponding to transit orbits from the states
        corresponding to other types of orbits rapidly. In fact, the
        distribution of transit orbit in the phase space has been
        investigated by numerical method, and a Fourier series
        approximation method has been introduced to describe the
        boundary of transit orbits. However, the Fourier series
        approximation method needs several hundred sets of Fourier
        series. The coefficients of these Fourier series are neither
        easy to be computed nor convenient to be stored, which makes the
        method can hardly be used in actual mission design. In this
        paper, the support vector machine (SVM) is used to classify the
        trajectories in the CR3BP. Using the Gaussian kernel, the
        6-dimensional states in the CR3BP are mapped into an infinite-
        dimensional space, and the bound of the transit orbits is
        described by a hyperplane. A training data generation method is
        introduced, which reduces the size of training data by
        generating the states near the hyperplane. The numerical results
        show that the proposed algorithm gives the good correct rate of
        classification, and its computing speed is much faster than that
        of the Fourier series approximation method.}",
          doi = {10.1016/j.asr.2015.04.017},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015AdSpR..56..273L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INPROCEEDINGS{2015salt.confE..57B,
       author = {{Bassett}, B.},
        title = "{Machine Learning Classification of Transients}",
    booktitle = {SALT Science Conference 2015 (SSC2015)},
         year = "2015",
        month = "Jun",
          eid = {57},
        pages = {57},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015salt.confE..57B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.450.1441D,
       author = {{Dieleman}, Sander and {Willett}, Kyle W. and {Dambre}, Joni},
        title = "{Rotation-invariant convolutional neural networks for galaxy morphology prediction}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: image processing, catalogues, galaxies: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = "2015",
        month = "Jun",
       volume = {450},
       number = {2},
        pages = {1441-1459},
     abstract = "{Measuring the morphological parameters of galaxies is a key requirement
        for studying their formation and evolution. Surveys such as the
        Sloan Digital Sky Survey have resulted in the availability of
        very large collections of images, which have permitted
        population-wide analyses of galaxy morphology. Morphological
        analysis has traditionally been carried out mostly via visual
        inspection by trained experts, which is time consuming and does
        not scale to large ({\ensuremath{\gtrsim}}{}10$^{4}$) numbers of
        images. Although attempts have been made to build automated
        classification systems, these have not been able to achieve the
        desired level of accuracy. The Galaxy Zoo project successfully
        applied a crowdsourcing strategy, inviting online users to
        classify images by answering a series of questions.
        Unfortunately, even this approach does not scale well enough to
        keep up with the increasing availability of galaxy images. We
        present a deep neural network model for galaxy morphology
        classification which exploits translational and rotational
        symmetry. It was developed in the context of the Galaxy
        Challenge, an international competition to build the best model
        for morphology classification based on annotated images from the
        Galaxy Zoo project. For images with high agreement among the
        Galaxy Zoo participants, our model is able to reproduce their
        consensus with near-perfect accuracy (\&gt;99 per cent) for most
        questions. Confident model predictions are highly accurate,
        which makes the model suitable for filtering large collections
        of images and forwarding challenging images to experts for
        manual annotation. This approach greatly reduces the experts'
        workload without affecting accuracy. The application of these
        algorithms to larger sets of training data will be critical for
        analysing results from future surveys such as the Large Synoptic
        Survey Telescope.}",
          doi = {10.1093/mnras/stv632},
archivePrefix = {arXiv},
       eprint = {1503.07077},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015MNRAS.450.1441D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.450..305H,
       author = {{Hoyle}, Ben and {Rau}, Markus Michael and {Bonnett}, Christopher and
         {Seitz}, Stella and {Weller}, Jochen},
        title = "{Data augmentation for machine learning redshifts applied to Sloan Digital Sky Survey galaxies}",
      journal = {\mnras},
     keywords = {catalogues, surveys, galaxies: distances and redshifts, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2015",
        month = "Jun",
       volume = {450},
       number = {1},
        pages = {305-316},
     abstract = "{We present analyses of data augmentation for machine learning redshift
        estimation. Data augmentation makes a training sample more
        closely resemble a test sample, if the two base samples differ,
        in order to improve measured statistics of the test sample. We
        perform two sets of analyses by selecting 800 000 (1.7 million)
        Sloan Digital Sky Survey Data Release 8 (Data Release 10)
        galaxies with spectroscopic redshifts. We construct a base
        training set by imposing an artificial r-band apparent magnitude
        cut to select only bright galaxies and then augment this base
        training set by using simulations and by applying the K-CORRECT
        package to artificially place training set galaxies at a higher
        redshift. We obtain redshift estimates for the remaining faint
        galaxy sample, which are not used during training. We find that
        data augmentation reduces the error on the recovered redshifts
        by 40 per cent in both sets of analyses, when compared to the
        difference in error between the ideal case and the non-augmented
        case. The outlier fraction is also reduced by at least 10 per
        cent and up to 80 per cent using data augmentation. We finally
        quantify how the recovered redshifts degrade as one probes to
        deeper magnitudes past the artificial magnitude limit of the
        bright training sample. We find that at all apparent magnitudes
        explored, the use of data augmentation with tree-based methods
        provide an estimate of the galaxy redshift with a low value of
        bias, although the error on the recovered redshifts increases as
        we probe to deeper magnitudes. These results have applications
        for surveys which have a spectroscopic training set which forms
        a biased sample of all photometric galaxies, for example if the
        spectroscopic detection magnitude limit is shallower than the
        photometric limit.}",
          doi = {10.1093/mnras/stv599},
archivePrefix = {arXiv},
       eprint = {1501.06759},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015MNRAS.450..305H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJ...806....6M,
       author = {{McCauliff}, Sean D. and {Jenkins}, Jon M. and {Catanzarite}, Joseph and
         {Burke}, Christopher J. and {Coughlin}, Jeffrey L. and
         {Twicken}, Joseph D. and {Tenenbaum}, Peter and {Seader}, Shawn and
         {Li}, Jie and {Cote}, Miles},
        title = "{Automatic Classification of Kepler Planetary Transit Candidates}",
      journal = {\apj},
     keywords = {astronomical databases: miscellaneous, binaries: eclipsing, catalogs, methods: statistical, planets and satellites: detection, techniques: photometric, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "Jun",
       volume = {806},
       number = {1},
          eid = {6},
        pages = {6},
     abstract = "{In the first three years of operation, the Kepler mission found 3697
        planet candidates (PCs) from a set of 18,406 transit-like
        features detected on more than 200,000 distinct stars. Vetting
        candidate signals manually by inspecting light curves and other
        diagnostic information is a labor intensive effort.
        Additionally, this classification methodology does not yield any
        information about the quality of PCs; all candidates are as
        credible as any other. The torrent of exoplanet discoveries will
        continue after Kepler, because a number of exoplanet surveys
        will have an even broader search area. This paper presents the
        application of machine-learning techniques to the classification
        of the exoplanet transit-like signals present in the Kepler
        light curve data. Transit-like detections are transformed into a
        uniform set of real-numbered attributes, the most important of
        which are described in this paper. Each of the known transit-
        like detections is assigned a class of PC; astrophysical false
        positive; or systematic, instrumental noise. We use a random
        forest algorithm to learn the mapping from attributes to classes
        on this training set. The random forest algorithm has been used
        previously to classify variable stars; this is the first time it
        has been used for exoplanet classification. We are able to
        achieve an overall error rate of 5.85\% and an error rate for
        classifying exoplanets candidates of 2.81\%.}",
          doi = {10.1088/0004-637X/806/1/6},
archivePrefix = {arXiv},
       eprint = {1408.1496},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJ...806....6M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015AdSpR..55.2657K,
       author = {{Kozoderov}, Vladimir and {Kondranin}, Timofei and {Dmitriev}, Egor and
         {Kamentsev}, Vladimir},
        title = "{Bayesian classifier applications of airborne hyperspectral imagery processing for forested areas}",
      journal = {Advances in Space Research},
     keywords = {Optical remote sensing, Pattern recognition, Forest classes of different species and ages},
         year = "2015",
        month = "Jun",
       volume = {55},
       number = {11},
        pages = {2657-2667},
     abstract = "{Pattern recognition problem is outlined in the context of textural and
        spectral analysis of remote sensing imagery processing. Main
        attention is paid to Bayesian classifier that can be used to
        realize the processing procedures based on parallel machine-
        learning algorithms and high-productive computers. We consider
        the maximum of the posterior probability principle and the
        formalism of Markov random fields for the neighborhood
        description of the pixels for the related classes of objects
        with the emphasis on forests of different species and ages. The
        energy category of the selected classes serves to account for
        the likelihood measure between the registered radiances and the
        theoretical distribution functions approximating remotely sensed
        data. Optimization procedures are undertaken to solve the
        pattern recognition problem of the texture description for the
        forest classes together with finding thin nuances of their
        spectral distribution in the feature space. As a result,
        possible redundancy of the channels for imaging spectrometer due
        to their correlations is removed. Difficulties are revealed due
        to different sampling data while separating pixels, which
        characterize the sunlit tops, shaded space and intermediate
        cases of the Sun illumination conditions on the hyperspectral
        images. Such separation of pixels for the forest classes is
        maintained to enhance the recognition accuracy, but learning
        ensembles of data need to be agreed for these categories of
        pixels. We present some results of the Bayesian classifier
        applicability for recognizing airborne hyperspectral images
        using the relevant improvements in separating such pixels for
        the forest classes on a test area of the 4 {\texttimes} 10 km
        size encompassed by 13 airborne tracks, each forming the images
        by 500 pixels across the track and from 10,000 to 14,000 pixels
        along the track. The spatial resolution of each image is near to
        1 m from the altitude near to 2 km above the ground level. The
        results of the hyperspectral imagery processing have shown that
        for the ensembles of data corresponding to the prevailing pine
        species and sunlit tree's tops the young forests (13-26 years
        old) and mature forests (106-136 years old) on the test area are
        recognized with high accuracy, but difficulties are essential
        for intermediate ages (36-96 years old) though some of these
        ages (for example, 47 and 76 years old) are also recognized with
        high accuracy. This is due to insufficient sampling of the
        pixels relating to the sunlit tops for the intermediate ages
        using the classifier employed. A conclusion is made that a
        fusion of the passive imaging spectrometer and the active lidar
        (laser scanner) on the same gyro-stabilized airborne platform is
        needed to overcome these difficulties.}",
          doi = {10.1016/j.asr.2015.02.015},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015AdSpR..55.2657K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015A&C....11...64S,
       author = {{Sevilla-Noarbe}, I. and {Etayo-Sotos}, P.},
        title = "{Effect of training characteristics on object classification: An application using Boosted Decision Trees}",
      journal = {Astronomy and Computing},
     keywords = {Techniques: photometric, Catalogs, Supervised learning by classification, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "Jun",
       volume = {11},
        pages = {64-72},
     abstract = "{We present an application of a particular machine-learning method
        (Boosted Decision Trees, BDTs using AdaBoost) to separate stars
        and galaxies in photometric images using their catalog
        characteristics. BDTs are a well established machine learning
        technique used for classification purposes. They have been
        widely used specially in the field of particle and astroparticle
        physics, and we use them here in an optical astronomy
        application. This algorithm is able to improve from simple
        thresholding cuts on standard separation variables that may be
        affected by local effects such as blending, badly calculated
        background levels or which do not include information in other
        bands. The improvements are shown using the Sloan Digital Sky
        Survey Data Release 9, with respect to the type photometric
        classifier. We obtain an improvement in the impurity of the
        galaxy sample of a factor 2-4 for this particular dataset,
        adjusting for the same efficiency of the selection. Another main
        goal of this study is to verify the effects that different input
        vectors and training sets have on the classification
        performance, the results being of wider use to other machine
        learning techniques.}",
          doi = {10.1016/j.ascom.2015.03.010},
archivePrefix = {arXiv},
       eprint = {1504.06776},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015A&C....11...64S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015SpWea..13..286W,
       author = {{Winter}, L.~M. and {Balasubramaniam}, K.},
        title = "{Using the maximum X-ray flux ratio and X-ray background to predict solar flare class}",
      journal = {Space Weather},
     keywords = {X-ray flares, Astrophysics - Solar and Stellar Astrophysics, Physics - Space Physics},
         year = "2015",
        month = "May",
       volume = {13},
       number = {5},
        pages = {286-297},
     abstract = "{We present the discovery of a relationship between the maximum ratio of
        the flare flux (namely, 0.5-4 {\r{A}} to the 1-8 {\r{A}} flux)
        and nonflare background (namely, the 1-8 {\r{A}} background
        flux), which clearly separates flares into classes by peak flux
        level. We established this relationship based on an analysis of
        the Geostationary Operational Environmental Satellites X-ray
        observations of ̃ 50,000 X, M, C, and flares derived from the
        NOAA/Space Weather Prediction Center flares catalog. Employing a
        combination of machine learning techniques (K-nearest neighbors
        and nearest centroid algorithms) we show a separation of the
        observed parameters for the different peak flaring energies.
        This analysis is validated by successfully predicting the flare
        classes for 100\% of the X-class flares, 76\% of the M-class
        flares, 80\% of the C-class flares, and 81\% of the B-class
        flares for solar cycle 24, based on the training of the
        parametric extracts for solar flares in cycles 22-23.}",
          doi = {10.1002/2015SW001170},
archivePrefix = {arXiv},
       eprint = {1504.00294},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015SpWea..13..286W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015PhRvD..91i2004S,
       author = {{Schneck}, K. and {Cabrera}, B. and {Cerde{\~n}o}, D.~G. and {Mand
        ic}, V. and {Rogers}, H.~E. and {Agnese}, R. and {Anderson}, A.~J. and
         {Asai}, M. and {Balakishiyeva}, D. and {Barker}, D. and
         {Basu Thakur}, R. and {Bauer}, D.~A. and {Billard}, J. and {Borgland
        }, A. and {Brandt}, D. and {Brink}, P.~L. and {Bunker}, R. and
         {Caldwell}, D.~O. and {Calkins}, R. and {Chagani}, H. and {Chen}, Y. and
         {Cooley}, J. and {Cornell}, B. and {Crewdson}, C.~H. and {Cushman}, P. and
         {Daal}, M. and {Di Stefano}, P.~C.~F. and {Doughty}, T. and
         {Esteban}, L. and {Fallows}, S. and {Figueroa-Feliciano}, E. and
         {Godfrey}, G.~L. and {Golwala}, S.~R. and {Hall}, J. and
         {Harris}, H.~R. and {Hofer}, T. and {Holmgren}, D. and {Hsu}, L. and
         {Huber}, M.~E. and {Jardin}, D.~M. and {Jastram}, A. and {Kamaev}, O. and
         {Kara}, B. and {Kelsey}, M.~H. and {Kennedy}, A. and {Leder}, A. and
         {Loer}, B. and {Lopez Asamar}, E. and {Lukens}, P. and {Mahapatra}, R. and
         {McCarthy}, K.~A. and {Mirabolfathi}, N. and {Moffatt}, R.~A. and
         {Morales Mendoza}, J.~D. and {Oser}, S.~M. and {Page}, K. and
         {Page}, W.~A. and {Partridge}, R. and {Pepin}, M. and {Phipps}, A. and
         {Prasad}, K. and {Pyle}, M. and {Qiu}, H. and {Rau}, W. and {Redl}, P. and
         {Reisetter}, A. and {Ricci}, Y. and {Roberts}, A. and {Saab}, T. and
         {Sadoulet}, B. and {Sander}, J. and {Schnee}, R.~W. and {Scorza}, S. and
         {Serfass}, B. and {Shank}, B. and {Speller}, D. and {Toback}, D. and
         {Upadhyayula}, S. and {Villano}, A.~N. and {Welliver}, B. and
         {Wilson}, J.~S. and {Wright}, D.~H. and {Yang}, X. and {Yellin}, S. and
         {Yen}, J.~J. and {Young}, B.~A. and {Zhang}, J. and
         {SuperCDMS Collaboration}},
        title = "{Dark matter effective field theory scattering in direct detection experiments}",
      journal = {\prd},
     keywords = {95.35.+d, 29.40.Wk, 95.30.Cq, Dark matter, Solid-state detectors, Elementary particle processes, Astrophysics - Cosmology and Nongalactic Astrophysics, High Energy Physics - Experiment, High Energy Physics - Phenomenology},
         year = "2015",
        month = "May",
       volume = {91},
       number = {9},
          eid = {092004},
        pages = {092004},
     abstract = "{We examine the consequences of the effective field theory (EFT) of dark
        matter-nucleon scattering for current and proposed direct
        detection experiments. Exclusion limits on EFT coupling
        constants computed using the optimum interval method are
        presented for SuperCDMS Soudan, CDMS II, and LUX, and the
        necessity of combining results from multiple experiments in
        order to determine dark matter parameters is discussed. We
        demonstrate that spectral differences between the standard dark
        matter model and a general EFT interaction can produce a bias
        when calculating exclusion limits and when developing signal
        models for likelihood and machine learning techniques. We also
        discuss the implications of the EFT for the next-generation (G2)
        direct detection experiments and point out regions of
        complementarity in the EFT parameter space.}",
          doi = {10.1103/PhysRevD.91.092004},
archivePrefix = {arXiv},
       eprint = {1503.03379},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015PhRvD..91i2004S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015NatPh..11..414R,
       author = {{Ried}, Katja and {Agnew}, Megan and {Vermeyden}, Lydia and
         {Janzing}, Dominik and {Spekkens}, Robert W. and {Resch}, Kevin J.},
        title = "{A quantum advantage for inferring causal structure}",
      journal = {Nature Physics},
     keywords = {Quantum Physics, Computer Science - Machine Learning, General Relativity and Quantum Cosmology, Statistics - Machine Learning},
         year = "2015",
        month = "May",
       volume = {11},
       number = {5},
        pages = {414-420},
     abstract = "{The problem of inferring causal relations from observed correlations is
        relevant to a wide variety of scientific disciplines. Yet given
        the correlations between just two classical variables, it is
        impossible to determine whether they arose from a causal
        influence of one on the other or a common cause influencing
        both. Only a randomized trial can settle the issue. Here we
        consider the problem of causal inference for quantum variables.
        We show that the analogue of a randomized trial, causal
        tomography, yields a complete solution. We also show that, in
        contrast to the classical case, one can sometimes infer the
        causal structure from observations alone. We implement a
        quantum-optical experiment wherein we control the causal
        relation between two optical modes, and two measurement schemes
        --with and without randomization--that extract this relation
        from the observed correlations. Our results show that
        entanglement and quantum coherence provide an advantage for
        causal inference.}",
          doi = {10.1038/nphys3266},
archivePrefix = {arXiv},
       eprint = {1406.5036},
 primaryClass = {quant-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015NatPh..11..414R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.449.1275H,
       author = {{Hoyle}, Ben and {Rau}, Markus Michael and {Zitlau}, Roman and
         {Seitz}, Stella and {Weller}, Jochen},
        title = "{Feature importance for machine learning redshifts applied to SDSS galaxies}",
      journal = {\mnras},
     keywords = {catalogues, surveys, galaxies: distances and redshifts, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2015",
        month = "May",
       volume = {449},
       number = {2},
        pages = {1275-1283},
     abstract = "{We present an analysis of importance feature selection applied to
        photometric redshift estimation using the machine learning
        architecture Decision Trees with the ensemble learning routine
        ADABOOST (hereafter RDF). We select a list of 85 easily measured
        (or derived) photometric quantities (or `features') and
        spectroscopic redshifts for almost two million galaxies from the
        Sloan Digital Sky Survey Data Release 10. After identifying
        which features have the most predictive power, we use standard
        artificial Neural Networks (aNNs) to show that the addition of
        these features, in combination with the standard magnitudes and
        colours, improves the machine learning redshift estimate by 18
        per cent and decreases the catastrophic outlier rate by 32 per
        cent. We further compare the redshift estimate using RDF with
        those from two different aNNs, and with photometric redshifts
        available from the Sloan Digital Sky Survey (SDSS). We find that
        the RDF requires orders of magnitude less computation time than
        the aNNs to obtain a machine learning redshift while reducing
        both the catastrophic outlier rate by up to 43 per cent, and the
        redshift error by up to 25 per cent. When compared to the SDSS
        photometric redshifts, the RDF machine learning redshifts both
        decreases the standard deviation of residuals scaled by 1/(1+z)
        by 36 per cent from 0.066 to 0.041, and decreases the fraction
        of catastrophic outliers by 57 per cent from 2.32 to 0.99 per
        cent.}",
          doi = {10.1093/mnras/stv373},
archivePrefix = {arXiv},
       eprint = {1410.4696},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015MNRAS.449.1275H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.449..451W,
       author = {{Wright}, D.~E. and {Smartt}, S.~J. and {Smith}, K.~W. and {Miller}, P. and
         {Kotak}, R. and {Rest}, A. and {Burgett}, W.~S. and {Chambers}, K.~C. and
         {Flewelling}, H. and {Hodapp}, K.~W. and {Huber}, M. and {Jedicke}, R. and
         {Kaiser}, N. and {Metcalfe}, N. and {Price}, P.~A. and {Tonry}, J.~L. and
         {Wainscoat}, R.~J. and {Waters}, C.},
        title = "{Machine learning for transient discovery in Pan-STARRS1 difference imaging}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, techniques: image processing, surveys, supernovae: general, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "May",
       volume = {449},
       number = {1},
        pages = {451-466},
     abstract = "{Efficient identification and follow-up of astronomical transients is
        hindered by the need for humans to manually select promising
        candidates from data streams that contain many false positives.
        These artefacts arise in the difference images that are produced
        by most major ground-based time-domain surveys with large format
        CCD cameras. This dependence on humans to reject bogus
        detections is unsustainable for next generation all-sky surveys
        and significant effort is now being invested to solve the
        problem computationally. In this paper, we explore a simple
        machine learning approach to real-bogus classification by
        constructing a training set from the image data of ̃32 000 real
        astrophysical transients and bogus detections from the Pan-
        STARRS1 Medium Deep Survey. We derive our feature representation
        from the pixel intensity values of a 20 {\texttimes} 20 pixel
        stamp around the centre of the candidates. This differs from
        previous work in that it works directly on the pixels rather
        than catalogued domain knowledge for feature design or
        selection. Three machine learning algorithms are trained
        (artificial neural networks, support vector machines and random
        forests) and their performances are tested on a held-out subset
        of 25 per cent of the training data. We find the best results
        from the random forest classifier and demonstrate that by
        accepting a false positive rate of 1 per cent, the classifier
        initially suggests a missed detection rate of around 10 per
        cent. However, we also find that a combination of bright star
        variability, nuclear transients and uncertainty in human
        labelling means that our best estimate of the missed detection
        rate is approximately 6 per cent.}",
          doi = {10.1093/mnras/stv292},
archivePrefix = {arXiv},
       eprint = {1501.05470},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015MNRAS.449..451W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015A&A...577A..47B,
       author = {{Blanco-Cuaresma}, S. and {Soubiran}, C. and {Heiter}, U. and
         {Asplund}, M. and {Carraro}, G. and {Costado}, M.~T. and
         {Feltzing}, S. and {Gonz{\'a}lez-Hern{\'a}ndez}, J.~I. and
         {Jim{\'e}nez-Esteban}, F. and {Korn}, A.~J. and {Marino}, A.~F. and
         {Montes}, D. and {San Roman}, I. and {Tabernero}, H.~M. and
         {Tautvai{\v{s}}ien{\.{e}}}, G.},
        title = "{Testing the chemical tagging technique with open clusters}",
      journal = {\aap},
     keywords = {stars: abundances, techniques: spectroscopic, Galaxy: abundances, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "May",
       volume = {577},
          eid = {A47},
        pages = {A47},
     abstract = "{Context. Stars are born together from giant molecular clouds and, if we
        assume that the priors were chemically homogeneous and well-
        mixed, we expect them to share the same chemical composition.
        Most of the stellar aggregates are disrupted while orbiting the
        Galaxy and most of the dynamic information is lost, thus the
        only possibility of reconstructing the stellar formation history
        is to analyze the chemical abundances that we observe today. <BR
        /> Aims: The chemical tagging technique aims to recover
        disrupted stellar clusters based merely on their chemical
        composition. We evaluate the viability of this technique to
        recover co-natal stars that are no longer gravitationally bound.
        <BR /> Methods: Open clusters are co-natal aggregates that have
        managed to survive together. We compiled stellar spectra from 31
        old and intermediate-age open clusters, homogeneously derived
        atmospheric parameters, and 17 abundance species, and applied
        machine learning algorithms to group the stars based on their
        chemical composition. This approach allows us to evaluate the
        viability and efficiency of the chemical tagging technique. <BR
        /> Results: We found that stars at different evolutionary stages
        have distinct chemical patterns that may be due to NLTE effects,
        atomic diffusion, mixing, and biases. When separating stars into
        dwarfs and giants, we observed that a few open clusters show
        distinct chemical signatures while the majority show a high
        degree of overlap. This limits the recovery of co-natal
        aggregates by applying the chemical tagging technique.
        Nevertheless, there is room for improvement if more elements are
        included and models are improved. Based on observations obtained
        at the Telescope Bernard Lyot (USR5026) operated by the
        Observatoire Midi-Pyr{\'e}n{\'e}es, Universit{\'e} de Toulouse
        (Paul Sabatier), Centre National de la Recherche Scientifique of
        France, and on public data obtained from the ESO Science Archive
        Facility under requests number 81252 and 81618.}",
          doi = {10.1051/0004-6361/201425232},
archivePrefix = {arXiv},
       eprint = {1503.02082},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015A&A...577A..47B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.448.1446A,
       author = {{Agnello}, Adriano and {Kelly}, Brandon C. and {Treu}, Tommaso and
         {Marshall}, Philip J.},
        title = "{Data mining for gravitationally lensed quasars}",
      journal = {\mnras},
     keywords = {gravitational lensing: strong, methods: statistical, techniques: image processing, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "Apr",
       volume = {448},
       number = {2},
        pages = {1446-1462},
     abstract = "{Gravitationally lensed quasars are brighter than their unlensed
        counterparts and produce images with distinctive morphological
        signatures. Past searches and target-selection algorithms, in
        particular the Sloan Quasar Lens Search (SQLS), have relied on
        basic morphological criteria, which were applied to samples of
        bright, spectroscopically confirmed quasars. The SQLS techniques
        are not sufficient for searching into new surveys (e.g. DES,
        PS1, LSST), because spectroscopic information is not readily
        available and the large data volume requires higher purity in
        target/candidate selection. We carry out a systematic
        exploration of machine-learning techniques and demonstrate that
        a two-step strategy can be highly effective. In the first step,
        we use catalogue-level information (griz+WISE magnitudes, second
        moments) to pre-select targets, using artificial neural
        networks. The accepted targets are then inspected with pixel-by-
        pixel pattern recognition algorithms (gradient-boosted trees),
        to form a final set of candidates. The results from this
        procedure can be used to further refine the simpler SQLS
        algorithms, with a twofold (or threefold) gain in purity and the
        same (or 80 per cent) completeness at target-selection stage, or
        a purity of 70 per cent and a completeness of 60 per cent after
        the candidate-selection step. Simpler photometric searches in
        griz+WISE based on colour cuts would provide samples with 7 per
        cent purity or less. Our technique is extremely fast, as a list
        of candidates can be obtained from a Stage III experiment (e.g.
        DES catalogue/data base) in a few CPU hours. The techniques are
        easily extendable to Stage IV experiments like LSST with the
        addition of time domain information.}",
          doi = {10.1093/mnras/stv037},
archivePrefix = {arXiv},
       eprint = {1410.4565},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015MNRAS.448.1446A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015MNRAS.448.1305K,
       author = {{Kov{\'a}cs}, Andr{\'a}s and {Szapudi}, Istv{\'a}n},
        title = "{Star-galaxy separation strategies for WISE-2MASS all-sky infrared galaxy catalogues}",
      journal = {\mnras},
     keywords = {catalogues, large-scale structure of Universe, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2015",
        month = "Apr",
       volume = {448},
       number = {2},
        pages = {1305-1313},
     abstract = "{We combine photometric information of the Wide-Field Infrared Survey
        Explorer (WISE) and Two Micron All Sky Survey (2MASS) all-sky
        infrared data bases, and demonstrate how to produce clean and
        complete galaxy catalogues for future analyses. Adding 2MASS
        colours to WISE photometry improves star-galaxy separation
        efficiency substantially at the expense of losing a small
        fraction of the galaxies. We find that 93 per cent of the WISE
        objects within W1 \&lt; 15.2 mag have a 2MASS match, and that a
        class of supervised machine learning algorithms, support vector
        machines (SVM), are efficient classifiers of objects in our
        multicolour data set. We constructed a training set from the
        Sloan Digital Sky Survey PhotoObj table with known star-galaxy
        separation, and determined redshift distribution of our sample
        from the Galaxy and Mass Assembly spectroscopic survey. Varying
        the combination of photometric parameters input into our
        algorithm we show that W1$_{WISE}$ - J$_{2MASS}$ is a simple and
        effective star-galaxy separator, capable of producing results
        comparable to the multidimensional SVM classification. We
        present a detailed description of our star-galaxy separation
        methods, and characterize the robustness of our tools in terms
        of contamination, completeness, and accuracy. We explore
        systematics of the full sky WISE-2MASS galaxy map, such as
        contamination from moon glow. We show that the homogeneity of
        the full sky galaxy map is improved by an additional J$_{2MASS}$
        \&lt; 16.5 mag flux limit. The all-sky galaxy catalogue we
        present in this paper covers 21 200 deg$^{2}$ with dusty regions
        masked out, and has an estimated stellar contamination of 1.2
        per cent and completeness of 70.1 per cent among 2.4 million
        galaxies with z$_{med}$ {\ensuremath{\approx}} 0.14. WISE-2MASS
        galaxy maps with well controlled stellar contamination will be
        useful for spatial statistical analyses, including cross-
        correlations with other cosmological random fields, such as the
        cosmic microwave background. The same techniques also yield a
        statistically controlled sample of stars as well.}",
          doi = {10.1093/mnras/stv063},
archivePrefix = {arXiv},
       eprint = {1401.0156},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015MNRAS.448.1305K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015GeoRL..42.2968E,
       author = {{Exbrayat}, Jean-Fran{\c{c}}ois and {Williams}, Mathew},
        title = "{Quantifying the net contribution of the historical Amazonian deforestation to climate change}",
      journal = {\grl},
     keywords = {deforestation, remote sensing, Amazon basin, aboveground biomass, random forest},
         year = "2015",
        month = "Apr",
       volume = {42},
       number = {8},
        pages = {2968-2976},
     abstract = "{Recent large-scale carbon (C) emissions from deforestation have been
        estimated by combining remotely-sensed land use change
        information with satellite-based aboveground biomass (AGB) data.
        However, these estimates are constrained to the satellite era
        while regions such as the Amazon basin have been heavily
        impacted by deforestation before this period. Assessing the net
        contribution of past tropical deforestation to the growth in
        atmospheric CO$_{2}$ is therefore challenging. We address this
        lack of data by constructing two maps of potential AGB with a
        machine learning algorithm trained on the relationship between
        AGB and climate and topography in intact forest landscapes of
        the Amazon basin. Reconstructions converge to a current deficit
        of 11.5-12\% in AGB or a net loss of
        \raisebox{-0.5ex}\textasciitilde7-8 Pg C of AGB in the Amazon
        basin compared to current estimates. This represents a net
        contribution of \raisebox{-0.5ex}\textasciitilde1.8 ppm of
        atmospheric CO$_{2}$ or 1.5\% of the historical growth.}",
          doi = {10.1002/2015GL063497},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015GeoRL..42.2968E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJ...803...50N,
       author = {{Ntampaka}, M. and {Trac}, H. and {Sutherland}, D.~J. and
         {Battaglia}, N. and {P{\'o}czos}, B. and {Schneider}, J.},
        title = "{A Machine Learning Approach for Dynamical Mass Measurements of Galaxy Clusters}",
      journal = {\apj},
     keywords = {cosmology: theory, dark matter, galaxies: clusters: general, galaxies: kinematics and dynamics, gravitation, large-scale structure of universe, methods: statistical, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2015",
        month = "Apr",
       volume = {803},
       number = {2},
          eid = {50},
        pages = {50},
     abstract = "{We present a modern machine learning (ML) approach for cluster dynamical
        mass measurements that is a factor-of-two improvement over using
        a conventional scaling relation. Different methods are tested
        against a mock cluster catalog constructed using halos with mass
        {\ensuremath{\geq}}slant \{\{10\}$^{14}$\} \{\{M\}$_{☉
        }$\}\{\{\textbackslashtext\{h\}\}$^{-1}$\} from
        Multidark{\textquoteright}s publicly available N-body MDPL halo
        catalog. In the conventional method, we use a standard
        M({\ensuremath{\sigma}}$_{v}$) power-law scaling relation to
        infer cluster mass, M, from line of sight (LOS) galaxy velocity
        dispersion, {\ensuremath{\sigma}}$_{v}$. The resulting
        fractional mass error distribution is broad, with width
        {\ensuremath{\Delta}}{\ensuremath{\in}} {\ensuremath{\approx}}
        0.87 (68\% scatter), and has extended high-error tails. The
        standard scaling relation can be simply enhanced by including
        higher-order moments of the LOS velocity distribution. Applying
        the kurtosis as a correction term to log
        (\{{\ensuremath{\sigma}} \}$_{v}$\}) reduces the width of the
        error distribution to \{{\ensuremath{\Delta}}{\ensuremath{\in}}
        {\ensuremath{\approx}} 0.74 (16\% improvement). ML can be used
        to take full advantage of all the information in the velocity
        distribution. We employ the Support Distribution Machines (SDMs)
        algorithm that learns from distributions of data to predict
        single values. SDMs trained and tested on the distribution of
        LOS velocities yield {\ensuremath{\Delta}}{\ensuremath{\in}}
        {\ensuremath{\approx}} 0.46 (47\% improvement). Furthermore, the
        problematic tails of the mass error distribution are effectively
        eliminated. Decreasing cluster mass errors will improve
        measurements of the growth of structure and lead to tighter
        constraints on cosmological parameters.}",
          doi = {10.1088/0004-637X/803/2/50},
archivePrefix = {arXiv},
       eprint = {1410.0686},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJ...803...50N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015AJ....149..138L,
       author = {{Lindner}, Robert R. and {Vera-Ciro}, Carlos and {Murray}, Claire E. and
         {Stanimirovi{\'c}}, Sne{\v{z}}ana and {Babler}, Brian and
         {Heiles}, Carl and {Hennebelle}, Patrick and {Goss}, W.~M. and
         {Dickey}, John},
        title = "{Autonomous Gaussian Decomposition}",
      journal = {\aj},
     keywords = {ISM: atoms, ISM: clouds, ISM: lines and bands, line: identification, methods: data analysis, techniques: spectroscopic, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2015",
        month = "Apr",
       volume = {149},
       number = {4},
          eid = {138},
        pages = {138},
     abstract = "{We present a new algorithm, named Autonomous Gaussian Decomposition
        (AGD), for automatically decomposing spectra into Gaussian
        components. AGD uses derivative spectroscopy and machine
        learning to provide optimized guesses for the number of Gaussian
        components in the data, and also their locations, widths, and
        amplitudes. We test AGD and find that it produces results
        comparable to human-derived solutions on 21 cm absorption
        spectra from the 21 cm SPectral line Observations of Neutral Gas
        with the EVLA (21-SPONGE) survey. We use AGD with Monte Carlo
        methods to derive the H i line completeness as a function of
        peak optical depth and velocity width for the 21-SPONGE data,
        and also show that the results of AGD are stable against varying
        observational noise intensity. The autonomy and computational
        efficiency of the method over traditional manual Gaussian fits
        allow for truly unbiased comparisons between observations and
        simulations, and for the ability to scale up and interpret the
        very large data volumes from the upcoming Square Kilometer Array
        and pathfinder telescopes.}",
          doi = {10.1088/0004-6256/149/4/138},
archivePrefix = {arXiv},
       eprint = {1409.2840},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015AJ....149..138L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015A&C....10...61E,
       author = {{Elliott}, J. and {de Souza}, R.~S. and {Krone-Martins}, A. and
         {Cameron}, E. and {Ishida}, E.~E.~O. and {Hilbe}, J. and
         {COIN Collaboration}},
        title = "{The overlooked potential of Generalized Linear Models in astronomy-II: Gamma regression and photometric redshifts}",
      journal = {Astronomy and Computing},
     keywords = {Techniques: photometric, Methods: statistical, Methods: analytical, Galaxies: distances and redshifts, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2015",
        month = "Apr",
       volume = {10},
        pages = {61-72},
     abstract = "{Machine learning techniques offer a precious tool box for use within
        astronomy to solve problems involving so-called big data. They
        provide a means to make accurate predictions about a particular
        system without prior knowledge of the underlying physical
        processes of the data. In this article, and the companion papers
        of this series, we present the set of Generalized Linear Models
        (GLMs) as a fast alternative method for tackling general
        astronomical problems, including the ones related to the machine
        learning paradigm. To demonstrate the applicability of GLMs to
        inherently positive and continuous physical observables, we
        explore their use in estimating the photometric redshifts of
        galaxies from their multi-wavelength photometry. Using the gamma
        family with a log link function we predict redshifts from the
        PHoto-z Accuracy Testing simulated catalogue and a subset of the
        Sloan Digital Sky Survey from Data Release 10. We obtain fits
        that result in catastrophic outlier rates as low as
        {\ensuremath{\sim}}1\% for simulated and {\ensuremath{\sim}}2\%
        for real data. Moreover, we can easily obtain such levels of
        precision within a matter of seconds on a normal desktop
        computer and with training sets that contain merely thousands of
        galaxies. Our software is made publicly available as a user-
        friendly package developed in Python, R and via an interactive
        web application. This software allows users to apply a set of
        GLMs to their own photometric catalogues and generates
        publication quality plots with minimum effort. By facilitating
        their ease of use to the astronomical community, this paper
        series aims to make GLMs widely known and to encourage their
        implementation in future large-scale projects, such as the Large
        Synoptic Survey Telescope.}",
          doi = {10.1016/j.ascom.2015.01.002},
archivePrefix = {arXiv},
       eprint = {1409.7699},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015A&C....10...61E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015A&A...576A.132K,
       author = {{K{\"u}gler}, S.~D. and {Polsterer}, K. and {Hoecker}, M.},
        title = "{Determining spectroscopic redshifts by using k nearest neighbor regression. I. Description of method and analysis}",
      journal = {\aap},
     keywords = {methods: data analysis, astronomical databases: miscellaneous, methods: statistical, galaxies: distances and redshifts, catalogs, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "Apr",
       volume = {576},
          eid = {A132},
        pages = {A132},
     abstract = "{Context. In astronomy, new approaches to process and analyze the
        exponentially increasing amount of data are inevitable. For
        spectra, such as in the Sloan Digital Sky Survey spectral
        database, usually templates of well-known classes are used for
        classification. In case the fitting of a template fails, wrong
        spectral properties (e.g. redshift) are derived. Validation of
        the derived properties is the key to understand the caveats of
        the template-based method. <BR /> Aims: In this paper we present
        a method for statistically computing the redshift z based on a
        similarity approach. This allows us to determine redshifts in
        spectra for emission and absorption features without using any
        predefined model. Additionally, we show how to determine the
        redshift based on single features. As a consequence we are, for
        example, able to filter objects that show multiple redshift
        components. <BR /> Methods: The redshift calculation is
        performed by comparing predefined regions in the spectra and
        individually applying a nearest neighbor regression model to
        each predefined emission and absorption region. <BR /> Results:
        The choice of the model parameters controls the quality and the
        completeness of the redshifts. For {\ensuremath{\approx}}90\% of
        the analyzed 16 000 spectra of our reference and test sample, a
        certain redshift can be computed that is comparable to the
        completeness of SDSS (96\%). The redshift calculation yields a
        precision for every individually tested feature that is
        comparable to the overall precision of the redshifts of SDSS.
        Using the new method to compute redshifts, we could also
        identify 14 spectra with a significant shift between emission
        and absorption or between emission and emission lines. The
        results already show the immense power of this simple machine-
        learning approach for investigating huge databases such as the
        SDSS.}",
          doi = {10.1051/0004-6361/201424801},
archivePrefix = {arXiv},
       eprint = {1409.8417},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015A&A...576A.132K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015PhRvD..91f2004B,
       author = {{Baker}, Paul T. and {Caudill}, Sarah and {Hodge}, Kari A. and
         {Talukder}, Dipongkar and {Capano}, Collin and {Cornish}, Neil J.},
        title = "{Multivariate classification with random forests for gravitational wave searches of black hole binary coalescence}",
      journal = {\prd},
     keywords = {04.80.Nn, 07.05.Kf, 07.05.Mh, Gravitational wave detectors and experiments, Data analysis: algorithms and implementation, data management, Neural networks fuzzy logic artificial intelligence, General Relativity and Quantum Cosmology},
         year = "2015",
        month = "Mar",
       volume = {91},
       number = {6},
          eid = {062004},
        pages = {062004},
     abstract = "{Searches for gravitational waves produced by coalescing black hole
        binaries with total masses {\ensuremath{\gtrsim}}25 M$_{☉}$ use
        matched filtering with templates of short duration. Non-Gaussian
        noise bursts in gravitational wave detector data can mimic short
        signals and limit the sensitivity of these searches. Previous
        searches have relied on empirically designed statistics
        incorporating signal-to-noise ratio and signal-based vetoes to
        separate gravitational wave candidates from noise candidates. We
        report on sensitivity improvements achieved using a multivariate
        candidate ranking statistic derived from a supervised machine
        learning algorithm. We apply the random forest of bagged
        decision trees technique to two separate searches in the high
        mass ({\ensuremath{\gtrsim}}25 M$_{☉}$ ) parameter space. For a
        search which is sensitive to gravitational waves from the
        inspiral, merger, and ringdown of binary black holes with total
        mass between 25 M$_{☉}$ and 100 M$_{☉}$ , we find sensitive
        volume improvements as high as
        70$_{{\ensuremath{\pm}}13}$\%-109$_{{\ensuremath{\pm}}11}$\%
        when compared to the previously used ranking statistic. For a
        ringdown-only search which is sensitive to gravitational waves
        from the resultant perturbed intermediate mass black hole with
        mass roughly between 10 M$_{☉}$ and 600 M$_{☉}$ , we find
        sensitive volume improvements as high as
        61$_{{\ensuremath{\pm}}4}$\%-241$_{{\ensuremath{\pm}}12}$\% when
        compared to the previously used ranking statistic. We also
        report how sensitivity improvements can differ depending on mass
        regime, mass ratio, and available data quality information.
        Finally, we describe the techniques used to tune and train the
        random forest classifier that can be generalized to its use in
        other searches for gravitational waves.}",
          doi = {10.1103/PhysRevD.91.062004},
archivePrefix = {arXiv},
       eprint = {1412.6479},
 primaryClass = {gr-qc},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015PhRvD..91f2004B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015IJACS...6...81K,
       author = {{Kowsari}, Kamram},
        title = "{Construction of FuzzyFind Dictionary using Golay Coding Transformation for Searching Applications}",
      journal = {International Journal of Adaptive Control and Signal Processing},
     keywords = {FuzzyFind Dictionary, Data Structure, Learning Algorithms, Pigeonhole Principle, Informational Retrieval, Approximate search, Big Data, Fuzzy search engine, Unsupervised learning, Golay Code Transformation Hash Table, Golay Code, Computer Science - Databases, Computer Science - Artificial Intelligence, Computer Science - Data Structures and Algorithms, Computer Science - Information Retrieval, Computer Science - Machine Learning},
         year = "2015",
        month = "Mar",
       volume = {6},
       number = {3},
        pages = {81-87},
     abstract = "{searching through a large volume of data is very critical for companies,
        scientists, and searching engines applications due to time
        complexity and memory complexity. In this paper, a new technique
        of generating FuzzyFind Dictionary for text mining was
        introduced. We simply mapped the 23 bits of the English alphabet
        into a FuzzyFind Dictionary or more than 23 bits by using more
        FuzzyFind Dictionary, and reflecting the presence or absence of
        particular letters. This representation preserves closeness of
        word distortions in terms of closeness of the created binary
        vectors within Hamming distance of 2 deviations. This paper
        talks about the Golay Coding Transformation Hash Table and how
        it can be used on a FuzzyFind Dictionary as a new technology for
        using in searching through big data. This method is introduced
        by linear time complexity for generating the dictionary and
        constant time complexity to access the data and update by new
        data sets, also updating for new data sets is linear time
        depends on new data points. This technique is based on searching
        only for letters of English that each segment has 23 bits, and
        also we have more than 23-bit and also it could work with more
        segments as reference table.}",
          doi = {10.14569/IJACSA.2015.060313},
archivePrefix = {arXiv},
       eprint = {1503.06483},
 primaryClass = {cs.DB},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015IJACS...6...81K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ExA....39...45C,
       author = {{Cavuoti}, S. and {Brescia}, M. and {De Stefano}, V. and {Longo}, G.},
        title = "{Photometric redshift estimation based on data mining with PhotoRApToR}",
      journal = {Experimental Astronomy},
     keywords = {Techniques: photometric, Galaxies: distances and redshifts, Galaxies: photometry, Cosmology: observations, Methods: data analysis, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "Mar",
       volume = {39},
       number = {1},
        pages = {45-71},
     abstract = "{Photometric redshifts (photo-z) are crucial to the scientific
        exploitation of modern panchromatic digital surveys. In this
        paper we present PhotoRApToR (Photometric Research Application
        To Redshift): a Java/C ++ based desktop application capable to
        solve non-linear regression and multi-variate classification
        problems, in particular specialized for photo-z estimation. It
        embeds a machine learning algorithm, namely a multi-layer neural
        network trained by the Quasi Newton learning rule, and special
        tools dedicated to pre- and post-processing data. PhotoRApToR
        has been successfully tested on several scientific cases. The
        application is available for free download from the DAME Program
        web site.}",
          doi = {10.1007/s10686-015-9443-4},
archivePrefix = {arXiv},
       eprint = {1501.06506},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ExA....39...45C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJ...800...36S,
       author = {{Sanders}, N.~E. and {Betancourt}, M. and {Soderberg}, A.~M.},
        title = "{Unsupervised Transient Light Curve Analysis via Hierarchical Bayesian Inference}",
      journal = {\apj},
     keywords = {methods: numerical, methods: statistical, supernovae: general, surveys, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2015",
        month = "Feb",
       volume = {800},
       number = {1},
          eid = {36},
        pages = {36},
     abstract = "{Historically, light curve studies of supernovae (SNe) and other
        transient classes have focused on individual objects with
        copious and high signal-to-noise observations. In the nascent
        era of wide field transient searches, objects with detailed
        observations are decreasing as a fraction of the overall known
        SN population, and this strategy sacrifices the majority of the
        information contained in the data about the underlying
        population of transients. A population level modeling approach,
        simultaneously fitting all available observations of objects in
        a transient sub-class of interest, fully mines the data to infer
        the properties of the population and avoids certain systematic
        biases. We present a novel hierarchical Bayesian statistical
        model for population level modeling of transient light curves,
        and discuss its implementation using an efficient Hamiltonian
        Monte Carlo technique. As a test case, we apply this model to
        the Type IIP SN sample from the Pan-STARRS1 Medium Deep Survey,
        consisting of 18,837 photometric observations of 76 SNe,
        corresponding to a joint posterior distribution with 9176
        parameters under our model. Our hierarchical model fits provide
        improved constraints on light curve parameters relevant to the
        physical properties of their progenitor stars relative to
        modeling individual light curves alone. Moreover, we directly
        evaluate the probability for occurrence rates of unseen light
        curve characteristics from the model hyperparameters, addressing
        observational biases in survey methodology. We view this
        modeling framework as an unsupervised machine learning technique
        with the ability to maximize scientific returns from data to be
        collected by future wide field transient searches like LSST.}",
          doi = {10.1088/0004-637X/800/1/36},
archivePrefix = {arXiv},
       eprint = {1404.3619},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJ...800...36S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJ...799..208S,
       author = {{Sanders}, N.~E. and {Soderberg}, A.~M. and {Gezari}, S. and
         {Betancourt}, M. and {Chornock}, R. and {Berger}, E. and
         {Foley}, R.~J. and {Challis}, P. and {Drout}, M. and {Kirshner}, R.~P. and
         {Lunnan}, R. and {Marion}, G.~H. and {Margutti}, R. and {McKinnon}, R. and
         {Milisavljevic}, D. and {Narayan}, G. and {Rest}, A. and {Kankare}, E. and
         {Mattila}, S. and {Smartt}, S.~J. and {Huber}, M.~E. and
         {Burgett}, W.~S. and {Draper}, P.~W. and {Hodapp}, K.~W. and
         {Kaiser}, N. and {Kudritzki}, R.~P. and {Magnier}, E.~A. and
         {Metcalfe}, N. and {Morgan}, J.~S. and {Price}, P.~A. and
         {Tonry}, J.~L. and {Wainscoat}, R.~J. and {Waters}, C.},
        title = "{Toward Characterization of the Type IIP Supernova Progenitor Population: A Statistical Sample of Light Curves from Pan-STARRS1}",
      journal = {\apj},
     keywords = {supernovae: general, surveys, Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Solar and Stellar Astrophysics},
         year = "2015",
        month = "Feb",
       volume = {799},
       number = {2},
          eid = {208},
        pages = {208},
     abstract = "{In recent years, wide-field sky surveys providing deep multiband imaging
        have presented a new path for indirectly characterizing the
        progenitor populations of core-collapse supernovae (SNe):
        systematic light-curve studies. We assemble a set of 76 grizy-
        band Type IIP SN light curves from Pan-STARRS1, obtained over a
        constant survey program of 4 yr and classified using both
        spectroscopy and machine-learning-based photometric techniques.
        We develop and apply a new Bayesian model for the full multiband
        evolution of each light curve in the sample. We find no evidence
        of a subpopulation of fast-declining explosions (historically
        referred to as ``Type IIL'' SNe). However, we identify a highly
        significant relation between the plateau phase decay rate and
        peak luminosity among our SNe IIP. These results argue in favor
        of a single parameter, likely determined by initial stellar
        mass, predominantly controlling the explosions of red
        supergiants. This relation could also be applied for SN
        cosmology, offering a standardizable candle good to an intrinsic
        scatter of \&lt;\raisebox{-0.5ex}\textasciitilde 0.2 mag. We
        compare each light curve to physical models from hydrodynamic
        simulations to estimate progenitor initial masses and other
        properties of the Pan-STARRS1 Type IIP SN sample. We show that
        correction of systematic discrepancies between modeled and
        observed SN IIP light-curve properties and an expanded grid of
        progenitor properties are needed to enable robust progenitor
        inferences from multiband light-curve samples of this kind. This
        work will serve as a pathfinder for photometric studies of core-
        collapse SNe to be conducted through future wide-field transient
        searches.}",
          doi = {10.1088/0004-637X/799/2/208},
archivePrefix = {arXiv},
       eprint = {1404.2004},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJ...799..208S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015PhRvE..91a3311D,
       author = {{Dorn}, Sebastian and {En{\ss}lin}, Torsten A. and {Greiner}, Maksim and
         {Selig}, Marco and {Boehm}, Vanessa},
        title = "{Signal inference with unknown response: Calibration-uncertainty renormalized estimator}",
      journal = {\pre},
     keywords = {02.70.-c, 02.50.-r, 05.10.-a, 07.05.Kf, Computational techniques, simulations, Probability theory stochastic processes and statistics, Computational methods in statistical physics and nonlinear dynamics, Data analysis: algorithms and implementation, data management, Physics - Data Analysis, Statistics and Probability, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Information Theory, Statistics - Machine Learning},
         year = "2015",
        month = "Jan",
       volume = {91},
       number = {1},
          eid = {013311},
        pages = {013311},
     abstract = "{The calibration of a measurement device is crucial for every scientific
        experiment, where a signal has to be inferred from data. We
        present CURE, the calibration-uncertainty renormalized
        estimator, to reconstruct a signal and simultaneously the
        instrument's calibration from the same data without knowing the
        exact calibration, but its covariance structure. The idea of the
        CURE method, developed in the framework of information field
        theory, is to start with an assumed calibration to successively
        include more and more portions of calibration uncertainty into
        the signal inference equations and to absorb the resulting
        corrections into renormalized signal (and calibration)
        solutions. Thereby, the signal inference and calibration problem
        turns into a problem of solving a single system of ordinary
        differential equations and can be identified with common
        resummation techniques used in field theories. We verify the
        CURE method by applying it to a simplistic toy example and
        compare it against existent self-calibration schemes, Wiener
        filter solutions, and Markov chain Monte Carlo sampling. We
        conclude that the method is able to keep up in accuracy with the
        best self-calibration methods and serves as a noniterative
        alternative to them.}",
          doi = {10.1103/PhysRevE.91.013311},
archivePrefix = {arXiv},
       eprint = {1410.6289},
 primaryClass = {physics.data-an},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015PhRvE..91a3311D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2015PhDT.......385M,
       author = {{Mitidis}, Andonis},
        title = "{Constraining the r-mode saturation amplitude from a hypothetical detection of r-mode gravitational waves from a newborn neutron star: Detection strategies and machine learning algorithms}",
     keywords = {Computer engineering;Astrophysics},
       school = {University of Florida},
         year = "2015",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015PhDT.......385M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2015PhDT.......337C,
       author = {{Carrasco Kind}, Matias},
        title = "{Probabilistic Photometric Redshifts In The Era Of Petascale Astronomy}",
     keywords = {Photometric redshift, machine learning, data mining, galaxy surveys, petascale astronomy},
       school = {University of Illinois at Urbana Champaign},
         year = "2015",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015PhDT.......337C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2015PhDT.......270B,
       author = {{Bass}, Gideon},
        title = "{Ensemble supervised and unsupervised learning with Kepler variable stars}",
     keywords = {Astrophysics},
       school = {George Mason University},
         year = "2015",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015PhDT.......270B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015JCAP...01..038H,
       author = {{Hajian}, Amir and {Alvarez}, Marcelo A. and {Bond}, J. Richard},
        title = "{Machine learning etudes in astrophysics: selection functions for mock cluster catalogs}",
      journal = {Journal of Cosmology and Astro-Particle Physics},
     keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2015",
        month = "Jan",
       volume = {2015},
       number = {1},
          eid = {038},
        pages = {038},
     abstract = "{Making mock simulated catalogs is an important component of
        astrophysical data analysis. Selection criteria for observed
        astronomical objects are often too complicated to be derived
        from first principles. However the existence of an observed
        group of objects is a well-suited problem for machine learning
        classification. In this paper we use one-class classifiers to
        learn the properties of an observed catalog of clusters of
        galaxies from ROSAT and to pick clusters from mock simulations
        that resemble the observed ROSAT catalog. We show how this
        method can be used to study the cross-correlations of thermal
        Sunya'ev-Zeldovich signals with number density maps of X-ray
        selected cluster catalogs. The method reduces the bias due to
        hand-tuning the selection function and is readily scalable to
        large catalogs with a high-dimensional space of astrophysical
        features.}",
          doi = {10.1088/1475-7516/2015/01/038},
archivePrefix = {arXiv},
       eprint = {1409.1576},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015JCAP...01..038H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015CS&D....8a4009S,
       author = {{SunPy Community} and {Mumford}, Stuart J. and {Christe}, Steven and
         {P{\'e}rez-Su{\'a}rez}, David and {Ireland}, Jack and
         {Shih}, Albert Y. and {Inglis}, Andrew R. and {Liedtke}, Simon and
         {Hewett}, Russell J. and {Mayer}, Florian and {Hughitt}, Keith and
         {Freij}, Nabil and {Meszaros}, Tomas and {Bennett}, Samuel M. and
         {Malocha}, Michael and {Evans}, John and {Agrawal}, Ankit and
         {Leonard}, Andrew J. and {Robitaille}, Thomas P. and
         {Mampaey}, Benjamin and {Campos-Rozo}, Jose Iv{\'a}n and
         {Kirk}, Michael S.},
        title = "{SunPy{\textemdash}Python for solar physics}",
      journal = {Computational Science and Discovery},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2015",
        month = "Jan",
       volume = {8},
       number = {1},
          eid = {014009},
        pages = {014009},
     abstract = "{This paper presents SunPy (version 0.5), a community-developed Python
        package for solar physics. Python, a free, cross-platform,
        general-purpose, high-level programming language, has seen
        widespread adoption among the scientific community, resulting in
        the availability of a large number of software packages, from
        numerical computation (NumPy, SciPy) and machine learning
        (scikit-learn) to visualization and plotting (matplotlib). SunPy
        is a data-analysis environment specializing in providing the
        software necessary to analyse solar and heliospheric data in
        Python. SunPy is open-source software (BSD licence) and has an
        open and transparent development workflow that anyone can
        contribute to. SunPy provides access to solar data through
        integration with the Virtual Solar Observatory (VSO), the
        Heliophysics Event Knowledgebase (HEK), and the HELiophysics
        Integrated Observatory (HELIO) webservices. It currently
        supports image data from major solar missions (e.g., SDO, SOHO,
        STEREO, and IRIS), time-series data from missions such as GOES,
        SDO/EVE, and PROBA2/LYRA, and radio spectra from e-Callisto and
        STEREO/SWAVES. We describe SunPy's functionality, provide
        examples of solar data analysis in SunPy, and show how Python-
        based solar data-analysis can leverage the many existing tools
        already available in Python. We discuss the future goals of the
        project and encourage interested users to become involved in the
        planning and development of SunPy.}",
          doi = {10.1088/1749-4699/8/1/014009},
archivePrefix = {arXiv},
       eprint = {1505.02563},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015CS&D....8a4009S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJS..216...12W,
       author = {{Wyrzykowski}, {\L}ukasz and {Rynkiewicz}, Alicja E. and {Skowron}, Jan and
         {Koz{\l}owski}, Szymon and {Udalski}, Andrzej and
         {Szyma{\'n}ski}, Micha{\l} K. and {Kubiak}, Marcin and
         {Soszy{\'n}ski}, Igor and {Pietrzy{\'n}ski}, Grzegorz and
         {Poleski}, Rados{\l}aw and {Pietrukowicz}, Pawe{\l} and
         {Pawlak}, Micha{\l}},
        title = "{OGLE-III Microlensing Events and the Structure of the Galactic Bulge}",
      journal = {\apjs},
     keywords = {catalogs, Galaxy: bulge, Galaxy: structure, gravitational lensing: micro, Astrophysics - Solar and Stellar Astrophysics},
         year = "2015",
        month = "Jan",
       volume = {216},
       number = {1},
          eid = {12},
        pages = {12},
     abstract = "{We present and study the largest and most comprehensive catalog of
        microlensing events ever constructed. The sample of standard
        microlensing events comprises 3718 unique events from 2001-2009
        with 1409 events that had not been detected before in real-time
        by the Early Warning System of the Optical Gravitational Lensing
        Experiment. The search pipeline uses machine learning algorithms
        to help find rare phenomena among 150 million objects and to
        derive the detection efficiency. Applications of the catalog can
        be numerous, from analyzing individual events to large
        statistical studies of the Galactic mass, kinematics
        distributions, and planetary abundances. We derive maps of the
        mean Einstein ring crossing time of events spanning 31 deg$^{2}$
        toward the Galactic center and compare the observed
        distributions with the most recent models. We find good
        agreement within the observed region and we see the signature of
        the tilt of the bar in the microlensing data. However, the
        asymmetry of the mean timescales seems to rise more steeply than
        predicted, indicating either a somewhat different orientation of
        the bar or a larger bar width. The map of events with sources in
        the Galactic bulge shows a dependence of the mean timescale on
        the Galactic latitude, signaling an increasing contribution from
        disk lenses closer to the plane relative to the height of the
        disk. Our data present a perfect set for comparing and enhancing
        new models of the central parts of the Milky Way and creating a
        three-dimensional picture of the Galaxy. Based on observations
        obtained with the 1.3 m Warsaw telescope at the Las Campanas
        Observatory of the Carnegie Institution for Science.}",
          doi = {10.1088/0067-0049/216/1/12},
archivePrefix = {arXiv},
       eprint = {1405.3134},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJS..216...12W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJ...798..135B,
       author = {{Bobra}, M.~G. and {Couvidat}, S.},
        title = "{Solar Flare Prediction Using SDO/HMI Vector Magnetic Field Data with a Machine-learning Algorithm}",
      journal = {\apj},
     keywords = {Sun: activity, Sun: flares, Astrophysics - Solar and Stellar Astrophysics},
         year = "2015",
        month = "Jan",
       volume = {798},
       number = {2},
          eid = {135},
        pages = {135},
     abstract = "{We attempt to forecast M- and X-class solar flares using a machine-
        learning algorithm, called support vector machine (SVM), and
        four years of data from the Solar Dynamics Observatory's
        Helioseismic and Magnetic Imager, the first instrument to
        continuously map the full-disk photospheric vector magnetic
        field from space. Most flare forecasting efforts described in
        the literature use either line-of-sight magnetograms or a
        relatively small number of ground-based vector magnetograms.
        This is the first time a large data set of vector magnetograms
        has been used to forecast solar flares. We build a catalog of
        flaring and non-flaring active regions sampled from a database
        of 2071 active regions, comprised of 1.5 million active region
        patches of vector magnetic field data, and characterize each
        active region by 25 parameters. We then train and test the
        machine-learning algorithm and we estimate its performances
        using forecast verification metrics with an emphasis on the true
        skill statistic (TSS). We obtain relatively high TSS scores and
        overall predictive abilities. We surmise that this is partly due
        to fine-tuning the SVM for this purpose and also to an
        advantageous set of features that can only be calculated from
        vector magnetic field data. We also apply a feature selection
        algorithm to determine which of our 25 features are useful for
        discriminating between flaring and non-flaring active regions
        and conclude that only a handful are needed for good predictive
        abilities.}",
          doi = {10.1088/0004-637X/798/2/135},
archivePrefix = {arXiv},
       eprint = {1411.1405},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJ...798..135B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015ApJ...798..122M,
       author = {{Miller}, A.~A. and {Bloom}, J.~S. and {Richards}, J.~W. and
         {Lee}, Y.~S. and {Starr}, D.~L. and {Butler}, N.~R. and {Tokarz}, S. and
         {Smith}, N. and {Eisner}, J.~A.},
        title = "{A Machine-learning Method to Infer Fundamental Stellar Parameters from Photometric Light Curves}",
      journal = {\apj},
     keywords = {methods: data analysis, methods: statistical, stars: general, stars: statistics, stars: variables: general, surveys, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2015",
        month = "Jan",
       volume = {798},
       number = {2},
          eid = {122},
        pages = {122},
     abstract = "{A fundamental challenge for wide-field imaging surveys is obtaining
        follow-up spectroscopic observations: there are \&gt;{}10$^{9}$
        photometrically cataloged sources, yet modern spectroscopic
        surveys are limited to
        \raisebox{-0.5ex}\textasciitildefew{\texttimes} {}10$^{6}$
        targets. As we approach the Large Synoptic Survey Telescope era,
        new algorithmic solutions are required to cope with the data
        deluge. Here we report the development of a machine-learning
        framework capable of inferring fundamental stellar parameters (T
        $_{eff}$, log g, and [Fe/H]) using photometric-brightness
        variations and color alone. A training set is constructed from a
        systematic spectroscopic survey of variables with Hectospec
        /Multi-Mirror Telescope. In sum, the training set includes
        \raisebox{-0.5ex}\textasciitilde9000 spectra, for which stellar
        parameters are measured using the SEGUE Stellar Parameters
        Pipeline (SSPP). We employed the random forest algorithm to
        perform a non-parametric regression that predicts T $_{eff}$,
        log g, and [Fe/H] from photometric time-domain observations. Our
        final optimized model produces a cross-validated rms error
        (RMSE) of 165 K, 0.39 dex, and 0.33 dex for T $_{eff}$, log g,
        and [Fe/H], respectively. Examining the subset of sources for
        which the SSPP measurements are most reliable, the RMSE reduces
        to 125 K, 0.37 dex, and 0.27 dex, respectively, comparable to
        what is achievable via low-resolution spectroscopy. For variable
        stars this represents a {\ensuremath{\approx}}12\%-20\%
        improvement in RMSE relative to models trained with single-epoch
        photometric colors. As an application of our method, we estimate
        stellar parameters for \raisebox{-0.5ex}\textasciitilde54,000
        known variables. We argue that this method may convert
        photometric time-domain surveys into pseudo-spectrographic
        engines, enabling the construction of extremely detailed maps of
        the Milky Way, its structure, and history.}",
          doi = {10.1088/0004-637X/798/2/122},
archivePrefix = {arXiv},
       eprint = {1411.1073},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015ApJ...798..122M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015AcASn..56...53L,
       author = {{Lei}, Y. and {Zhao}, D.~N. and {Gao}, Y.~P. and {Cai}, H.~B.},
        title = "{The Prediction of Length-of-day Variations Based on Gaussian Processes}",
      journal = {Acta Astronomica Sinica},
     keywords = {astrometry, time, methods: data analysis},
         year = "2015",
        month = "Jan",
       volume = {56},
       number = {1},
        pages = {53-62},
     abstract = "{Due to the complicated time-varying characteristics of the length-of-day
        (LOD) variations, the accuracies of traditional strategies for
        the prediction of the LOD variations such as the least squares
        extrapolation model, the time-series analysis model, and so on,
        have not met the requirements for real-time and high-precision
        applications. In this paper, a new machine learning algorithm
        --- the Gaussian process (GP) model is employed to forecast the
        LOD variations. Its prediction precisions are analyzed and
        compared with those of the back propagation neural networks
        (BPNN), general regression neural networks (GRNN) models, and
        the Earth Orientation Parameters Prediction Comparison Campaign
        (EOP PCC). The results demonstrate that the application of the
        GP model to the prediction of the LOD variations is efficient
        and feasible.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015AcASn..56...53L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2015AcASn..56...26C,
       author = {{Chang}, L.~N. and {Zhang}, P.~A.},
        title = "{Application of Multi-task Lasso Regression in the Stellar Parametrization}",
      journal = {Acta Astronomica Sinica},
     keywords = {stars: fundamental parameters, methods: data analysis, methods: statistical, methods: miscellaneous},
         year = "2015",
        month = "Jan",
       volume = {56},
       number = {1},
        pages = {26-34},
     abstract = "{The multi-task learning approaches have attracted the increasing
        attention in the fields of machine learning, computer vision,
        and artificial intelligence. By utilizing the correlations in
        tasks, learning multiple related tasks simultaneously is better
        than learning each task independently. An efficient multi-task
        Lasso (Least Absolute Shrinkage Selection and Operator)
        regression algorithm is proposed in this paper to estimate the
        physical parameters of stellar spectra. It not only makes
        different physical parameters share the common features, but
        also can effectively preserve their own peculiar features.
        Experiments were done based on the ELODIE data simulated with
        the stellar atmospheric simulation model, and on the SDSS data
        released by the American large survey Sloan. The precision of
        the model is better than those of the methods in the related
        literature, especially for the acceleration of gravity (lg g)
        and the chemical abundance ([Fe/H]). In the experiments, we
        changed the resolution of the spectrum, and applied the noises
        with different signal-to-noise ratio (SNR) to the spectrum, so
        as to illustrate the stability of the model. The results show
        that the model is influenced by both the resolution and the
        noise. But the influence of the noise is larger than that of the
        resolution. In general, the multi-task Lasso regression
        algorithm is easy to operate, has a strong stability, and also
        can improve the overall accuracy of the model.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015AcASn..56...26C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014AdSpR..54.2419D,
       author = {{Di}, Kaichang and {Li}, Wei and {Yue}, Zongyu and {Sun}, Yiwei and
         {Liu}, Yiliang},
        title = "{A machine learning approach to crater detection from topographic data}",
      journal = {Advances in Space Research},
     keywords = {Crater detection algorithm, Topographic data, Machine learning, AdaBoost, Martian surface},
         year = "2014",
        month = "Dec",
       volume = {54},
       number = {11},
        pages = {2419-2429},
     abstract = "{Craters are distinctive features on the surfaces of most terrestrial
        planets. Craters reveal the relative ages of surface units and
        provide information on surface geology. Extracting craters is
        one of the fundamental tasks in planetary research. Although
        many automated crater detection algorithms have been developed
        to exact craters from image or topographic data, most of them
        are applicable only in particular regions, and only a few can be
        widely used, especially in complex surface settings. In this
        study, we present a machine learning approach to crater
        detection from topographic data. This approach includes two
        steps: detecting square regions which contain one crater with
        the use of a boosting algorithm and delineating the rims of the
        crater in each square region by local terrain analysis and
        circular Hough transform. A new variant of Haar-like features
        (scaled Haar-like features) is proposed and combined with
        traditional Haar-like features and local binary pattern features
        to enhance the performance of the classifier. Experimental
        results with the use of Mars topographic data demonstrate that
        the developed approach can significantly decrease the false
        positive detection rate while maintaining a relatively high true
        positive detection rate even in challenging sites.}",
          doi = {10.1016/j.asr.2014.08.018},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014AdSpR..54.2419D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014RAA....14.1491S,
       author = {{Sharma}, Mradul and {Nayak}, Jitadeepa and {Krishna Koul}, Maharaj and
         {Bose}, Smarajit and {Mitra}, Abhas},
        title = "{Gamma/hadron segregation for a ground based imaging atmospheric Cherenkov telescope using machine learning methods: Random Forest leads}",
      journal = {Research in Astronomy and Astrophysics},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2014",
        month = "Nov",
       volume = {14},
       number = {11},
          eid = {1491-1503},
        pages = {1491-1503},
     abstract = "{A detailed case study of {\ensuremath{\gamma}}-hadron segregation for a
        ground based atmospheric Cherenkov telescope is presented. We
        have evaluated and compared various supervised machine learning
        methods such as the Random Forest method, Artificial Neural
        Network, Linear Discriminant method, Naive Bayes Classifiers,
        Support Vector Machines as well as the conventional dynamic
        supercut method by simulating triggering events with the Monte
        Carlo method and applied the results to a Cherenkov telescope.
        It is demonstrated that the Random Forest method is the most
        sensitive machine learning method for
        {\ensuremath{\gamma}}-hadron segregation.}",
          doi = {10.1088/1674-4527/14/11/012},
archivePrefix = {arXiv},
       eprint = {1410.5125},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014RAA....14.1491S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014PhRvE..90d3301E,
       author = {{En{\ss}lin}, Torsten A. and {Junklewitz}, Henrik and
         {Winderling}, Lars and {Greiner}, Maksim and {Selig}, Marco},
        title = "{Improving self-calibration}",
      journal = {\pre},
     keywords = {89.70.Eg, 11.10.-z, Computational complexity, Field theory, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Information Theory, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
         year = "2014",
        month = "Oct",
       volume = {90},
       number = {4},
          eid = {043301},
        pages = {043301},
     abstract = "{Response calibration is the process of inferring how much the measured
        data depend on the signal one is interested in. It is essential
        for any quantitative signal estimation on the basis of the data.
        Here, we investigate self-calibration methods for linear signal
        measurements and linear dependence of the response on the
        calibration parameters. The common practice is to augment an
        external calibration solution using a known reference signal
        with an internal calibration on the unknown measurement signal
        itself. Contemporary self-calibration schemes try to find a
        self-consistent solution for signal and calibration by
        exploiting redundancies in the measurements. This can be
        understood in terms of maximizing the joint probability of
        signal and calibration. However, the full uncertainty structure
        of this joint probability around its maximum is thereby not
        taken into account by these schemes. Therefore, better schemes,
        in sense of minimal square error, can be designed by accounting
        for asymmetries in the uncertainty of signal and calibration. We
        argue that at least a systematic correction of the common self-
        calibration scheme should be applied in many measurement
        situations in order to properly treat uncertainties of the
        signal on which one calibrates. Otherwise, the calibration
        solutions suffer from a systematic bias, which consequently
        distorts the signal reconstruction. Furthermore, we argue that
        nonparametric, signal-to-noise filtered calibration should
        provide more accurate reconstructions than the common bin
        averages and provide a new, improved self-calibration scheme. We
        illustrate our findings with a simplistic numerical example.}",
          doi = {10.1103/PhysRevE.90.043301},
archivePrefix = {arXiv},
       eprint = {1312.1349},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014PhRvE..90d3301E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014PASP..126..959K,
       author = {{Kuminski}, Evan and {George}, Joe and {Wallin}, John and {Shamir}, Lior},
        title = "{Combining Human and Machine Learning for Morphological Analysis of Galaxy Images}",
      journal = {\pasp},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
         year = "2014",
        month = "Oct",
       volume = {126},
       number = {944},
        pages = {959},
     abstract = "{The increasing importance of digital sky surveys collecting many
        millions of galaxy images has reinforced the need for robust
        methods that can perform morphological analysis of large galaxy
        image databases. Citizen science initiatives such as Galaxy Zoo
        showed that large datasets of galaxy images can be analyzed
        effectively by non-scientist volunteers, but since databases
        generated by robotic telescopes grow much faster than the
        processing power of any group of citizen scientists, it is clear
        that computer analysis is required. Here we propose to use
        citizen science data for training machine learning systems, and
        show experimental results demonstrating that machine learning
        systems can be trained with citizen science data. Our findings
        show that the performance of machine learning depends on the
        quality of the data, which can be improved by using samples that
        have a high degree of agreement between the citizen scientists.
        The source code of the method is publicly available.}",
          doi = {10.1086/678977},
archivePrefix = {arXiv},
       eprint = {1409.7935},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014PASP..126..959K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014MNRAS.443.3528S,
       author = {{Shamir}, Lior and {Wallin}, John},
        title = "{Automatic detection and quantitative assessment of peculiar galaxy pairs in Sloan Digital Sky Survey}",
      journal = {\mnras},
     keywords = {catalogues, Galaxy: general, galaxies: interactions, galaxies: peculiar, Astrophysics - Astrophysics of Galaxies},
         year = "2014",
        month = "Oct",
       volume = {443},
       number = {4},
        pages = {3528-3537},
     abstract = "{We applied computational tools for automatic detection of peculiar
        galaxy pairs. We first detected in Sloan Digital Sky Survey DR7
        ̃400 000 galaxy images with i magnitude \&lt;18 that had more
        than one point spread function, and then applied a machine
        learning algorithm that detected ̃26 000 galaxy images that had
        morphology similar to the morphology of galaxy mergers. That
        data set was mined using a novelty detection algorithm,
        producing a short list of 500 most peculiar galaxies as
        quantitatively determined by the algorithm. Manual examination
        of these galaxies showed that while most of the galaxy pairs in
        the list were not necessarily peculiar, numerous unusual galaxy
        pairs were detected. In this paper, we describe the protocol and
        computational tools used for the detection of peculiar mergers,
        and provide examples of peculiar galaxy pairs that were
        detected.}",
          doi = {10.1093/mnras/stu1429},
archivePrefix = {arXiv},
       eprint = {1407.5000},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014MNRAS.443.3528S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014ApJ...794...38D,
       author = {{Daniel}, Scott F. and {Connolly}, Andrew J. and {Schneider}, Jeff},
        title = "{Determining Frequentist Confidence Limits Using a Directed Parameter Space Search}",
      journal = {\apj},
     keywords = {methods: statistical},
         year = "2014",
        month = "Oct",
       volume = {794},
       number = {1},
          eid = {38},
        pages = {38},
     abstract = "{We consider the problem of inferring constraints on a high-dimensional
        parameter space with a computationally expensive likelihood
        function. We propose a machine learning algorithm that maps out
        the Frequentist confidence limit on parameter space by
        intelligently targeting likelihood evaluations so as to quickly
        and accurately characterize the likelihood surface in both low-
        and high-likelihood regions. We compare our algorithm to
        Bayesian credible limits derived by the well-tested Markov Chain
        Monte Carlo (MCMC) algorithm using both multi-modal toy
        likelihood functions and the seven yr Wilkinson Microwave
        Anisotropy Probe cosmic microwave background likelihood
        function. We find that our algorithm correctly identifies the
        location, general size, and general shape of high-likelihood
        regions in parameter space while being more robust against
        multi-modality than MCMC.}",
          doi = {10.1088/0004-637X/794/1/38},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014ApJ...794...38D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014MNRAS.443.1651M,
       author = {{Morello}, V. and {Barr}, E.~D. and {Bailes}, M. and {Flynn}, C.~M. and
         {Keane}, E.~F. and {van Straten}, W.},
        title = "{SPINN: a straightforward machine learning solution to the pulsar candidate selection problem}",
      journal = {\mnras},
     keywords = {methods: data analysis, stars: neutron, pulsars: general, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2014",
        month = "Sep",
       volume = {443},
       number = {2},
        pages = {1651-1662},
     abstract = "{We describe SPINN (Straightforward Pulsar Identification using Neural
        Networks), a high-performance machine learning solution
        developed to process increasingly large data outputs from pulsar
        surveys. SPINN has been cross-validated on candidates from the
        southern High Time Resolution Universe (HTRU) survey and shown
        to identify every known pulsar found in the survey data while
        maintaining a false positive rate of 0.64 per cent. Furthermore,
        it ranks 99 per cent of pulsars among the top 0.11 per cent of
        candidates, and 95 per cent among the top 0.01 per cent. In
        conjunction with the PEASOUP pipeline, it has already discovered
        four new pulsars in a re-processing of the intermediate Galactic
        latitude area of HTRU, three of which have spin periods shorter
        than 5 ms. SPINN's ability to reduce the amount of candidates to
        visually inspect by up to four orders of magnitude makes it a
        very promising tool for future large-scale pulsar surveys. In an
        effort to provide a common testing ground for pulsar candidate
        selection tools and stimulate interest in their development, we
        also make publicly available the set of candidates on which
        SPINN was cross-validated.}",
          doi = {10.1093/mnras/stu1188},
archivePrefix = {arXiv},
       eprint = {1406.3627},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014MNRAS.443.1651M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014Compu..47...62B,
       author = {{Bogert}, F.~A. and {Smith}, N. and {Holdener}, J. and {De Jong}, E.~M. and
         {Hart}, A.~F. and {Cinquini}, L. and {Khudikyan}, S.~E. and
         {Thompson}, D.~R. and {Mattmann}, C.~A. and {Wagstaff}, K. and
         {Lazio}, J. and {Jones}, D.~L. and {Allen}, A. and {Shamir}, L. and
         {Teuben}, P.},
        title = "{Computing in Astronomy: Applications and Examples}",
      journal = {Computing},
     keywords = {big data, astronomy computing, data visualisation, pattern classification, software libraries, astronomical visualization, event classification, astronomy, astrophysics, computer applications, data processing, visualization, NASA, scientific computing, GPU, computer systems organization, data archiving, data movement, fast-transients, graphical environments, libraries, machine learning, programming environments, radio-astronomy, software engineering, information repositories, publishing},
         year = "2014",
        month = "Sep",
       volume = {47},
        pages = {62-69},
     abstract = "{Computing in astronomy has a wide variety of applications. We grouped a
        series of sidebars to provide pointers to ongoing work as well
        to give illustrative examples on astronomical visualization,
        event classification, Big Data, and software libraries.}",
          doi = {10.1109/MC.2014.243},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014Compu..47...62B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014ApJS..214....3B,
       author = {{Beaumont}, Christopher N. and {Goodman}, Alyssa A. and
         {Kendrew}, Sarah and {Williams}, Jonathan P. and {Simpson}, Robert},
        title = "{The Milky Way Project: Leveraging Citizen Science and Machine Learning to Detect Interstellar Bubbles}",
      journal = {\apjs},
     keywords = {H II regions, ISM: bubbles, methods: data analysis, stars: formation, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2014",
        month = "Sep",
       volume = {214},
       number = {1},
          eid = {3},
        pages = {3},
     abstract = "{We present Brut, an algorithm to identify bubbles in infrared images of
        the Galactic midplane. Brut is based on the Random Forest
        algorithm, and uses bubbles identified by \&gt;35,000 citizen
        scientists from the Milky Way Project to discover the
        identifying characteristics of bubbles in images from the
        Spitzer Space Telescope. We demonstrate that Brut's ability to
        identify bubbles is comparable to expert astronomers. We use
        Brut to re-assess the bubbles in the Milky Way Project catalog,
        and find that 10\%-30\% of the objects in this catalog are non-
        bubble interlopers. Relative to these interlopers, high-
        reliability bubbles are more confined to the mid-plane, and
        display a stronger excess of young stellar objects along and
        within bubble rims. Furthermore, Brut is able to discover
        bubbles missed by previous searches{\textemdash}particularly
        bubbles near bright sources which have low contrast relative to
        their surroundings. Brut demonstrates the synergies that exist
        between citizen scientists, professional scientists, and machine
        learning techniques. In cases where ``untrained'' citizens can
        identify patterns that machines cannot detect without training,
        machine learning algorithms like Brut can use the output of
        citizen science projects as input training sets, offering
        tremendous opportunities to speed the pace of scientific
        discovery. A hybrid model of machine learning combined with
        crowdsourced training data from citizen scientists can not only
        classify large quantities of data, but also address the weakness
        of each approach if deployed alone.}",
          doi = {10.1088/0067-0049/214/1/3},
archivePrefix = {arXiv},
       eprint = {1406.2692},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014ApJS..214....3B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014ApJ...793...23N,
       author = {{Nun}, Isadora and {Pichara}, Karim and {Protopapas}, Pavlos and
         {Kim}, Dae-Won},
        title = "{Supervised Detection of Anomalous Light Curves in Massive Astronomical Catalogs}",
      journal = {\apj},
     keywords = {catalogs, methods: data analysis, methods: statistical, stars: statistics, stars: variables: general, Computer Science - Computational Engineering, Finance, and Science, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning},
         year = "2014",
        month = "Sep",
       volume = {793},
       number = {1},
          eid = {23},
        pages = {23},
     abstract = "{The development of synoptic sky surveys has led to a massive amount of
        data for which resources needed for analysis are beyond human
        capabilities. In order to process this information and to
        extract all possible knowledge, machine learning techniques
        become necessary. Here we present a new methodology to
        automatically discover unknown variable objects in large
        astronomical catalogs. With the aim of taking full advantage of
        all information we have about known objects, our method is based
        on a supervised algorithm. In particular, we train a random
        forest classifier using known variability classes of objects and
        obtain votes for each of the objects in the training set. We
        then model this voting distribution with a Bayesian network and
        obtain the joint voting distribution among the training objects.
        Consequently, an unknown object is considered as an outlier
        insofar it has a low joint probability. By leaving out one of
        the classes on the training set, we perform a validity test and
        show that when the random forest classifier attempts to classify
        unknown light curves (the class left out), it votes with an
        unusual distribution among the classes. This rare voting is
        detected by the Bayesian network and expressed as a low joint
        probability. Our method is suitable for exploring massive data
        sets given that the training process is performed offline. We
        tested our algorithm on 20 million light curves from the MACHO
        catalog and generated a list of anomalous candidates. After
        analysis, we divided the candidates into two main classes of
        outliers: artifacts and intrinsic outliers. Artifacts were
        principally due to air mass variation, seasonal variation, bad
        calibration, or instrumental errors and were consequently
        removed from our outlier list and added to the training set.
        After retraining, we selected about 4000 objects, which we
        passed to a post-analysis stage by performing a cross-match with
        all publicly available catalogs. Within these candidates we
        identified certain known but rare objects such as eclipsing
        Cepheids, blue variables, cataclysmic variables, and X-ray
        sources. For some outliers there was no additional information.
        Among them we identified three unknown variability types and a
        few individual outliers that will be followed up in order to
        perform a deeper analysis.}",
          doi = {10.1088/0004-637X/793/1/23},
archivePrefix = {arXiv},
       eprint = {1404.4888},
 primaryClass = {cs.CE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014ApJ...793...23N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014AcA....64..197W,
       author = {{Wyrzykowski}, {\L}. and {Kostrzewa-Rutkowska}, Z. and
         {Koz{\l}owski}, S. and {Udalski}, A. and {Poleski}, R. and
         {Skowron}, J. and {Blagorodnova}, N. and {Kubiak}, M. and
         {Szyma{\'n}ski}, M.~K. and {Pietrzy{\'n}ski}, G. and
         {Soszy{\'n}ski}, I. and {Ulaczyk}, K. and {Pietrukowicz}, P. and
         {Mr{\'o}z}, P.},
        title = "{OGLE-IV Real-Time Transient Search}",
      journal = {\actaa},
     keywords = {Surveys, supernovae: general, novae, cataclysmic variables, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2014",
        month = "Sep",
       volume = {64},
       number = {3},
        pages = {197-232},
     abstract = "{We present the design and first results of a real-time search for
        transients within the 650 sq. deg. area around the Magellanic
        Clouds, conducted as part of the OGLE-IV project and aimed at
        detecting supernovae, novae and other events. The average
        sampling of about four days from September to May, yielded a
        detection of 238 transients in 2012/2013 and 2013/2014 seasons.
        The superb photometric and astrometric quality of the OGLE data
        allows for numerous applications of the discovered transients.
        We use this sample to prepare and train a Machine Learning-based
        automated classifier for early light curves, which distinguishes
        major classes of transients with more than 80\% of correct
        answers. Spectroscopically classified 49 supernovae Type Ia are
        used to construct a Hubble Diagram with statistical scatter of
        about 0.3 mag and fill the least populated region of the
        redshifts range in the Union sample. We investigate the
        influence of host galaxy environments on supernovae statistics
        and find the mean host extinction of
        A$_{I}$=0.19{\ensuremath{\pm}}0.10 mag and
        A$_{V}$=0.39{\ensuremath{\pm}}0.21 mag based on a subsample of
        supernovae Type Ia. We show that the positional accuracy of the
        survey is of the order of 0.5 pixels (0.13'') and that the OGLE-
        IV Transient Detection System is capable of detecting transients
        within the nuclei of galaxies. We present a few interesting
        cases of nuclear transients of unknown type. All data on the
        OGLE transients are made publicly available to the astronomical
        community via the OGLE website.}",
archivePrefix = {arXiv},
       eprint = {1409.1095},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014AcA....64..197W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014PASP..126..783B,
       author = {{Brescia}, Massimo and {Cavuoti}, Stefano and {Longo}, Giuseppe and
         {Nocella}, Alfonso and {Garofalo}, Mauro and {Manna}, Francesco and
         {Esposito}, Francesco and {Albano}, Giovanni and {Guglielmo}, Marisa and
         {D'Angelo}, Giovanni and {Di Guido}, Alessandro and
         {Djorgovski}, S. George and {Donalek}, Ciro and {Mahabal}, Ashish A. and
         {Graham}, Matthew J. and {Fiore}, Michelangelo and
         {D'Abrusco}, Raffaele},
        title = "{DAMEWARE: A Web Cyberinfrastructure for Astrophysical Data Mining}",
      journal = {\pasp},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2014",
        month = "Aug",
       volume = {126},
       number = {942},
        pages = {783},
     abstract = "{Astronomy is undergoing through a methodological revolution triggered by
        an unprecedented wealth of complex and accurate data. The new
        panchromatic, synoptic sky surveys require advanced tools for
        discovering patterns and trends hidden behind data which are
        both complex and of high dimensionality. We present DAMEWARE
        (DAta Mining \&amp; Exploration Web Application REsource): a
        general purpose, web-based, distributed data mining environment
        developed for the exploration of large datasets, and finely
        tuned for astronomical applications. By means of graphical user
        interfaces, it allows the user to perform classification,
        regression or clustering tasks with machine learning methods.
        Salient features of DAMEWARE include its capability to work on
        large datasets with minimal human intervention, and to deal with
        a wide variety of real problems such as the classification of
        globular clusters in the galaxy NGC1399, the evaluation of
        photometric redshifts and, finally, the identification of
        candidate Active Galactic Nuclei in multiband photometric
        surveys. In all these applications, DAMEWARE allowed to achieve
        better results than those attained with more traditional
        methods. With the aim of providing potential users with all
        needed information, in this paper we briefly describe the
        technological background of DAMEWARE, give a short introduction
        to some relevant aspects of data mining, followed by a summary
        of some science cases and, finally, we provide a detailed
        description of a template use case.}",
          doi = {10.1086/677725},
archivePrefix = {arXiv},
       eprint = {1406.3538},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014PASP..126..783B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014MNRAS.442.3380C,
       author = {{Carrasco Kind}, Matias and {Brunner}, Robert J.},
        title = "{Exhausting the information: novel Bayesian combination of photometric redshift PDFs}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, surveys, galaxies: distances and redshifts, galaxies: statistics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2014",
        month = "Aug",
       volume = {442},
       number = {4},
        pages = {3380-3399},
     abstract = "{The estimation and utilization of photometric redshift probability
        density functions (photo-z PDFs) have become increasingly
        important over the last few years and currently there exist a
        wide variety of algorithms to compute photo-z's, each with their
        own strengths and weaknesses. In this paper, we present a novel
        and efficient Bayesian framework that combines the results from
        different photo-z techniques into a more powerful and robust
        estimate by maximizing the information from the photometric
        data. To demonstrate this, we use a supervised machine learning
        technique based on random forest, an unsupervised method based
        on self-organizing maps, and a standard template-fitting method
        but can be easily extended to other existing techniques. We use
        data from the DEEP2 survey and the Sloan Digital Sky Survey to
        explore different methods for combining the predictions from
        these techniques. By using different performance metrics, we
        demonstrate that we can improve the accuracy of our final
        photo-z estimate over the best input technique, that the
        fraction of outliers is reduced, and that the identification of
        outliers is significantly improved when we apply a na{\"\i}ve
        Bayes classifier to this combined information. Our more robust
        and accurate photo-z PDFs will allow even more precise
        cosmological constraints to be made by using current and future
        photometric surveys. These improvements are crucial as we move
        to analyse photometric data that push to or even past the limits
        of the available training data, which will be the case with the
        Large Synoptic Survey Telescope.}",
          doi = {10.1093/mnras/stu1098},
archivePrefix = {arXiv},
       eprint = {1403.0044},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014MNRAS.442.3380C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014A&A...568A.126B,
       author = {{Brescia}, M. and {Cavuoti}, S. and {Longo}, G. and {De Stefano}, V.},
        title = "{A catalogue of photometric redshifts for the SDSS-DR9 galaxies}",
      journal = {\aap},
     keywords = {techniques: photometric, galaxies: distances and redshifts, galaxies: photometry, methods: data analysis, catalogs, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2014",
        month = "Aug",
       volume = {568},
          eid = {A126},
        pages = {A126},
     abstract = "{Context. Accurate photometric redshifts for large samples of galaxies
        are among the main products of modern multiband digital surveys.
        Over the last decade, the Sloan Digital Sky Survey (SDSS) has
        become a sort of benchmark against which to test the various
        methods. <BR /> Aims: We present an application of a new method
        to the estimation of photometric redshifts for the galaxies in
        the SDSS Data Release 9 (SDSS-DR9). Photometric redshifts for
        more than 143 million galaxies were produced. <BR /> Methods:
        The Multi Layer Perceptron with Quasi Newton Algorithm (MLPQNA)
        model, provided within the framework of the DAta Mining and
        Exploration Web Application REsource (DAMEWARE), is an
        interpolative method derived from machine learning models. <BR
        /> Results: The obtained redshifts have an overall uncertainty
        of {\ensuremath{\sigma}} = 0.023 with a very small average bias
        of \raisebox{-0.5ex}\textasciitilde3 {\texttimes} 10$^{-5}$, and
        a fraction of catastrophic outliers (|{\ensuremath{\Delta}}z|
        \&gt; 2{\ensuremath{\sigma}}) of
        \raisebox{-0.5ex}\textasciitilde5\%. This result is slightly
        better than what was already available in the literature in
        terms of the smaller fraction of catastrophic outliers as well.
        The produced catalogue, composed by 58 tables is only available
        at the CDS via anonymous ftp to <A href=``http://cdsarc.u-strasb
        g.fr''>http://cdsarc.u-strasbg.fr</A> (ftp://130.79.128.5) or
        via <A href=``http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/568/A126''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/568/A126</A>}",
          doi = {10.1051/0004-6361/201424383},
archivePrefix = {arXiv},
       eprint = {1407.2527},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014A&A...568A.126B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014SoPh..289.2503S,
       author = {{Schuh}, M.~A. and {Banda}, J.~M. and {Bernasconi}, P.~N. and
         {Angryk}, R.~A. and {Martens}, P.~C.~H.},
        title = "{A Comparative Evaluation of Automated Solar Filament Detection}",
      journal = {\solphys},
     keywords = {Automated feature finding, Filaments, Quantitative comparative evaluation},
         year = "2014",
        month = "Jul",
       volume = {289},
       number = {7},
        pages = {2503-2524},
     abstract = "{We present a comparative evaluation for automated filament detection in
        H{\ensuremath{\alpha}} solar images. By using metadata produced
        by the Advanced Automated Filament Detection and
        Characterization Code (AAFDCC) module, we adapted our trainable
        feature recognition (TFR) module to accurately detect regions in
        solar images containing filaments. We first analyze the AAFDCC
        module's metadata and then transform it into labeled datasets
        for machine-learning classification. Visualizations of data
        transformations and classification results are presented and
        accompanied by statistical findings. Our results confirm the
        reliable event reporting of the AAFDCC module and establishes
        our TFR module's ability to effectively detect solar filaments
        in H{\ensuremath{\alpha}} solar images.}",
          doi = {10.1007/s11207-014-0495-9},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014SoPh..289.2503S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014MNRAS.441.2508O,
       author = {{Osborn}, J. and {Guzman}, D. and {de Cos Juez}, F.~J. and
         {Basden}, A.~G. and {Morris}, T.~J. and {Gendron}, E. and
         {Butterley}, T. and {Myers}, R.~M. and {Guesalaga}, A. and
         {Sanchez Lasheras}, F. and {Gomez Victoria}, M. and
         {S{\'a}nchez Rodr{\'\i}guez}, M.~L. and {Gratadour}, D. and
         {Rousset}, G.},
        title = "{Open-loop tomography with artificial neural networks on CANARY: on-sky results}",
      journal = {\mnras},
     keywords = {atmospheric effects, instrumentation: adaptive optics},
         year = "2014",
        month = "Jul",
       volume = {441},
       number = {3},
        pages = {2508-2514},
     abstract = "{We present recent results from the initial testing of an artificial
        neural network (ANN)-based tomographic reconstructor Complex
        Atmospheric Reconstructor based on Machine lEarNing (CARMEN) on
        CANARY, an adaptive optics demonstrator operated on the 4.2 m
        William Herschel Telescope, La Palma. The reconstructor was
        compared with contemporaneous data using the Learn and Apply
        (L\&amp;A) tomographic reconstructor. We find that the fully
        optimized L\&amp;A tomographic reconstructor outperforms CARMEN
        by approximately 5 per cent in Strehl ratio or 15 nm rms in
        wavefront error. We also present results for CANARY in Ground
        Layer Adaptive Optics mode to show that the reconstructors are
        tomographic. The results are comparable and this small deficit
        is attributed to limitations in the training data used to build
        the ANN. Laboratory bench tests show that the ANN can outperform
        L\&amp;A under certain conditions, e.g. if the higher layer of a
        model two layer atmosphere was to change in altitude by ̃300 m
        (equivalent to a shift of approximately one tenth of a
        subaperture).}",
          doi = {10.1093/mnras/stu758},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014MNRAS.441.2508O},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014IJAsB..13..191M,
       author = {{McGuire}, P.~C. and {Bonnici}, A. and {Bruner}, K.~R. and {Gross}, C. and
         {Orm{\"o}}, J. and {Smosna}, R.~A. and {Walter}, S. and {Wendt}, L.},
        title = "{The Cyborg Astrobiologist: matching of prior textures by image compression for geological mapping and novelty detection}",
      journal = {International Journal of Astrobiology},
     keywords = {computer vision, novelty detection, similarity matching, phone-cameras, image compression, field geology, lichens, coal, Computer Science - Computer Vision and Pattern Recognition, Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning},
         year = "2014",
        month = "Jul",
       volume = {13},
       number = {3},
        pages = {191-202},
     abstract = "{We describe an image-comparison technique of Heidemann and Ritter
        (2008a, b), which uses image compression, and is capable of: (i)
        detecting novel textures in a series of images, as well as of:
        (ii) alerting the user to the similarity of a new image to a
        previously observed texture. This image-comparison technique has
        been implemented and tested using our Astrobiology Phone-cam
        system, which employs Bluetooth communication to send images to
        a local laptop server in the field for the image-compression
        analysis. We tested the system in a field site displaying a
        heterogeneous suite of sandstones, limestones, mudstones and
        coal beds. Some of the rocks are partly covered with lichen. The
        image-matching procedure of this system performed very well with
        data obtained through our field test, grouping all images of
        yellow lichens together and grouping all images of a coal bed
        together, and giving 91\% accuracy for similarity detection.
        Such similarity detection could be employed to make maps of
        different geological units. The novelty-detection performance of
        our system was also rather good (64\% accuracy). Such novelty
        detection may become valuable in searching for new geological
        units, which could be of astrobiological interest. The current
        system is not directly intended for mapping and novelty
        detection of a second field site based on image-compression
        analysis of an image database from a first field site, although
        our current system could be further developed towards this end.
        Furthermore, the image-comparison technique is an unsupervised
        technique that is not capable of directly classifying an image
        as containing a particular geological feature; labelling of such
        geological features is done post facto by human geologists
        associated with this study, for the purpose of analysing the
        system's performance. By providing more advanced capabilities
        for similarity detection and novelty detection, this image-
        compression technique could be useful in giving more scientific
        autonomy to robotic planetary rovers, and in assisting human
        astronauts in their geological exploration and assessment.}",
          doi = {10.1017/S1473550413000372},
archivePrefix = {arXiv},
       eprint = {1309.4024},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014IJAsB..13..191M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014AJ....148...21M,
       author = {{Masci}, Frank J. and {Hoffman}, Douglas I. and {Grillmair}, Carl J. and
         {Cutri}, Roc M.},
        title = "{Automated Classification of Periodic Variable Stars Detected by the Wide-field Infrared Survey Explorer}",
      journal = {\aj},
     keywords = {methods: statistical, stars: variables: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics, Mathematics - Statistics Theory},
         year = "2014",
        month = "Jul",
       volume = {148},
       number = {1},
          eid = {21},
        pages = {21},
     abstract = "{We describe a methodology to classify periodic variable stars identified
        using photometric time-series measurements constructed from the
        Wide-field Infrared Survey Explorer (WISE) full-mission single-
        exposure Source Databases. This will assist in the future
        construction of a WISE Variable Source Database that assigns
        variables to specific science classes as constrained by the WISE
        observing cadence with statistically meaningful classification
        probabilities. We have analyzed the WISE light curves of 8273
        variable stars identified in previous optical variability
        surveys (MACHO, GCVS, and ASAS) and show that Fourier
        decomposition techniques can be extended into the mid-IR to
        assist with their classification. Combined with other periodic
        light-curve features, this sample is then used to train a
        machine-learned classifier based on the random forest (RF)
        method. Consistent with previous classification studies of
        variable stars in general, the RF machine-learned classifier is
        superior to other methods in terms of accuracy, robustness
        against outliers, and relative immunity to features that carry
        little or redundant class information. For the three most common
        classes identified by WISE: Algols, RR Lyrae, and W Ursae
        Majoris type variables, we obtain classification efficiencies of
        80.7\%, 82.7\%, and 84.5\% respectively using cross-validation
        analyses, with 95\% confidence intervals of approximately
        {\ensuremath{\pm}}2\%. These accuracies are achieved at purity
        (or reliability) levels of 88.5\%, 96.2\%, and 87.8\%
        respectively, similar to that achieved in previous automated
        classification studies of periodic variable stars.}",
          doi = {10.1088/0004-6256/148/1/21},
archivePrefix = {arXiv},
       eprint = {1402.0125},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014AJ....148...21M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014A&A...567A.100A,
       author = {{Angeloni}, R. and {Contreras Ramos}, R. and {Catelan}, M. and
         {D{\'e}k{\'a}ny}, I. and {Gran}, F. and {Alonso-Garc{\'\i}a}, J. and
         {Hempel}, M. and {Navarrete}, C. and {Andrews}, H. and {Aparicio}, A. and
         {Beam{\'\i}n}, J.~C. and {Berger}, C. and {Borissova}, J. and
         {Contreras Pe{\~n}a}, C. and {Cunial}, A. and {de Grijs}, R. and
         {Espinoza}, N. and {Eyheramendy}, S. and {Ferreira Lopes}, C.~E. and
         {Fiaschi}, M. and {Hajdu}, G. and {Han}, J. and {He{\l}miniak}, K.~G. and
         {Hempel}, A. and {Hidalgo}, S.~L. and {Ita}, Y. and {Jeon}, Y. -B. and
         {Jord{\'a}n}, A. and {Kwon}, J. and {Lee}, J.~T. and
         {Mart{\'\i}n}, E.~L. and {Masetti}, N. and {Matsunaga}, N. and
         {Milone}, A.~P. and {Minniti}, D. and {Morelli}, L. and {Murgas}, F. and
         {Nagayama}, T. and {Navarro}, C. and {Ochner}, P. and {P{\'e}rez}, P. and
         {Pichara}, K. and {Rojas-Arriagada}, A. and {Roquette}, J. and
         {Saito}, R.~K. and {Siviero}, A. and {Sohn}, J. and {Sung}, H. -I. and
         {Tamura}, M. and {Tata}, R. and {Tomasella}, L. and {Townsend}, B. and
         {Whitelock}, P.},
        title = "{The VVV Templates Project Towards an automated classification of VVV light-curves. I. Building a database of stellar variability in the near-infrared}",
      journal = {\aap},
     keywords = {stars: variables: general, surveys, techniques: photometric, Astrophysics - Solar and Stellar Astrophysics},
         year = "2014",
        month = "Jul",
       volume = {567},
          eid = {A100},
        pages = {A100},
     abstract = "{Context. The Vista Variables in the V{\'\i}a L{\'a}ctea (VVV) ESO Public
        Survey is a variability survey of the Milky Way bulge and an
        adjacent section of the disk carried out from 2010 on ESO
        Visible and Infrared Survey Telescope for Astronomy (VISTA). The
        VVV survey will eventually deliver a deep near-IR atlas with
        photometry and positions in five passbands (ZYJHK$_{S}$) and a
        catalogue of 1-10 million variable point sources - mostly
        unknown - that require classifications. <BR /> Aims: The main
        goal of the VVV Templates Project, which we introduce in this
        work, is to develop and test the machine-learning algorithms for
        the automated classification of the VVV light-curves. As VVV is
        the first massive, multi-epoch survey of stellar variability in
        the near-IR, the template light-curves that are required for
        training the classification algorithms are not available. In the
        first paper of the series we describe the construction of this
        comprehensive database of infrared stellar variability. <BR />
        Methods: First, we performed a systematic search in the
        literature and public data archives; second, we coordinated a
        worldwide observational campaign; and third, we exploited the
        VVV variability database itself on (optically) well-known stars
        to gather high-quality infrared light-curves of several hundreds
        of variable stars. <BR /> Results: We have now collected a
        significant (and still increasing) number of infrared template
        light-curves. This database will be used as a training-set for
        the machine-learning algorithms that will automatically classify
        the light-curves produced by VVV. The results of such an
        automated classification will be covered in forthcoming papers
        of the series.}",
          doi = {10.1051/0004-6361/201423904},
archivePrefix = {arXiv},
       eprint = {1405.4517},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014A&A...567A.100A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014MNRAS.441.1741G,
       author = {{Graff}, Philip and {Feroz}, Farhan and {Hobson}, Michael P. and
         {Lasenby}, Anthony},
        title = "{SKYNET: an efficient and robust neural network training tool for machine learning in astronomy}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
         year = "2014",
        month = "Jun",
       volume = {441},
       number = {2},
        pages = {1741-1759},
     abstract = "{We present the first public release of our generic neural network
        training algorithm, called SKYNET. This efficient and robust
        machine learning tool is able to train large and deep feed-
        forward neural networks, including autoencoders, for use in a
        wide range of supervised and unsupervised learning applications,
        such as regression, classification, density estimation,
        clustering and dimensionality reduction. SKYNET uses a `pre-
        training' method to obtain a set of network parameters that has
        empirically been shown to be close to a good solution, followed
        by further optimization using a regularized variant of Newton's
        method, where the level of regularization is determined and
        adjusted automatically; the latter uses second-order derivative
        information to improve convergence, but without the need to
        evaluate or store the full Hessian matrix, by using a fast
        approximate method to calculate Hessian-vector products. This
        combination of methods allows for the training of complicated
        networks that are difficult to optimize using standard
        backpropagation techniques. SKYNET employs convergence criteria
        that naturally prevent overfitting, and also includes a fast
        algorithm for estimating the accuracy of network outputs. The
        utility and flexibility of SKYNET are demonstrated by
        application to a number of toy problems, and to astronomical
        problems focusing on the recovery of structure from blurred and
        noisy images, the identification of gamma-ray bursters, and the
        compression and denoising of galaxy images. The SKYNET software,
        which is implemented in standard ANSI C and fully parallelized
        using MPI, is available at
        http://www.mrao.cam.ac.uk/software/skynet/.}",
          doi = {10.1093/mnras/stu642},
archivePrefix = {arXiv},
       eprint = {1309.0790},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014MNRAS.441.1741G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014MNRAS.441..343S,
       author = {{Shivvers}, Isaac and {Bloom}, Joshua S. and {Richards}, Joseph W.},
        title = "{The highly eccentric detached eclipsing binaries in ACVS and MACC}",
      journal = {\mnras},
     keywords = {techniques: spectroscopic, catalogues, binaries: eclipsing, Astrophysics - Solar and Stellar Astrophysics},
         year = "2014",
        month = "Jun",
       volume = {441},
       number = {1},
        pages = {343-353},
     abstract = "{Next-generation synoptic photometric surveys will yield unprecedented
        (for the astronomical community) volumes of data and the
        processes of discovery and rare-object identification are, by
        necessity, becoming more autonomous. Such autonomous searches
        can be used to find objects of interest applicable to a wide
        range of outstanding problems in astronomy, and in this paper we
        present the methods and results of a largely autonomous search
        for highly eccentric detached eclipsing binary systems in the
        Machine-learned All-Sky Automated Survey Classification Catalog.
        106 detached eclipsing binaries with eccentricities of e
        {\ensuremath{\gtrsim}} 0.1 are presented, most of which are
        identified here for the first time. We also present new radial-
        velocity curves and absolute parameters for six of those systems
        with the long-term goal of increasing the number of highly
        eccentric systems with orbital solutions, thereby facilitating
        further studies of the tidal circularization process in binary
        stars.}",
          doi = {10.1093/mnras/stu578},
archivePrefix = {arXiv},
       eprint = {1403.5564},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014MNRAS.441..343S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014AdSpR..53.1768C,
       author = {{Cohen}, Joseph Paul and {Ding}, Wei},
        title = "{Crater detection via genetic search methods to reduce image features}",
      journal = {Advances in Space Research},
     keywords = {Shrinking feature set cardinality, Machine learning, Genetic algorithms, Crater detection, Bayesian classifier, Simulated annealing},
         year = "2014",
        month = "Jun",
       volume = {53},
       number = {12},
        pages = {1768-1782},
     abstract = "{Recent approaches to crater detection have been inspired by face
        detection's use of gray-scale texture features. Using gray-scale
        texture features for supervised machine learning crater
        detection algorithms provides better classification of craters
        in planetary images than previous methods. When using Haar
        features it is typical to generate thousands of numerical values
        from each candidate crater image. This magnitude of image
        features to extract and consider can spell disaster when the
        application is an entire planetary surface. One solution is to
        reduce the number of features extracted and considered in order
        to increase accuracy as well as speed. Feature subset selection
        provides the operational classifiers with a concise and denoised
        set of features by reducing irrelevant and redundant features.
        Feature subset selection is known to be NP-hard. To provide an
        efficient suboptimal solution, four genetic algorithms are
        proposed to use greedy selection, weighted random selection, and
        simulated annealing to distinguish discriminate features from
        indiscriminate features. Inspired by analysis regarding the
        relationship between subset size and accuracy, a squeezing
        algorithm is presented to shrink the genetic algorithm's
        chromosome cardinality during the genetic iterations. A
        significant increase in the classification performance of a
        Bayesian classifier in crater detection using image texture
        features is observed.}",
          doi = {10.1016/j.asr.2013.05.010},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014AdSpR..53.1768C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014A&A...566A..63J,
       author = {{Joseph}, R. and {Courbin}, F. and {Metcalf}, R.~B. and {Giocoli}, C. and
         {Hartley}, P. and {Jackson}, N. and {Bellagamba}, F. and
         {Kneib}, J. -P. and {Koopmans}, L. and {Lemson}, G. and
         {Meneghetti}, M. and {Meylan}, G. and {Petkova}, M. and {Pires}, S.},
        title = "{A PCA-based automated finder for galaxy-scale strong lenses}",
      journal = {\aap},
     keywords = {gravitational lensing: strong, techniques: image processing, methods: data analysis, dark matter, surveys, cosmological parameters, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2014",
        month = "Jun",
       volume = {566},
          eid = {A63},
        pages = {A63},
     abstract = "{We present an algorithm using principal component analysis (PCA) to
        subtract galaxies from imaging data and also two algorithms to
        find strong, galaxy-scale gravitational lenses in the resulting
        residual image. The combined method is optimised to find full or
        partial Einstein rings. Starting from a pre-selection of
        potential massive galaxies, we first perform a PCA to build a
        set of basis vectors. The galaxy images are reconstructed using
        the PCA basis and subtracted from the data. We then filter the
        residual image with two different methods. The first uses a
        curvelet (curved wavelets) filter of the residual images to
        enhance any curved/ring feature. The resulting image is
        transformed in polar coordinates, centred on the lens galaxy. In
        these coordinates, a ring is turned into a line, allowing us to
        detect very faint rings by taking advantage of the integrated
        signal-to-noise in the ring (a line in polar coordinates). The
        second way of analysing the PCA-subtracted images identifies
        structures in the residual images and assesses whether they are
        lensed images according to their orientation, multiplicity, and
        elongation. We applied the two methods to a sample of simulated
        Einstein rings as they would be observed with the ESA Euclid
        satellite in the VIS band. The polar coordinate transform
        allowed us to reach a completeness of 90\% for a purity of 86\%,
        as soon as the signal-to-noise integrated in the ring was higher
        than 30 and almost independent of the size of the Einstein ring.
        Finally, we show with real data that our PCA-based galaxy
        subtraction scheme performs better than traditional subtraction
        based on model fitting to the data. Our algorithm can be
        developed and improved further using machine learning and
        dictionary learning methods, which would extend the capabilities
        of the method to more complex and diverse galaxy shapes.}",
          doi = {10.1051/0004-6361/201423365},
archivePrefix = {arXiv},
       eprint = {1403.1063},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014A&A...566A..63J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014A&A...566A..43K,
       author = {{Kim}, Dae-Won and {Protopapas}, Pavlos and {Bailer-Jones}, Coryn A.~L. and
         {Byun}, Yong-Ik and {Chang}, Seo-Won and {Marquette}, Jean-Baptiste and
         {Shin}, Min-Su},
        title = "{The EPOCH Project. I. Periodic variable stars in the EROS-2 LMC database}",
      journal = {\aap},
     keywords = {stars: variables: general, Magellanic Clouds, methods: data analysis, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2014",
        month = "Jun",
       volume = {566},
          eid = {A43},
        pages = {A43},
     abstract = "{The EPOCH (EROS-2 periodic variable star classification using machine
        learning) project aims to detect periodic variable stars in the
        EROS-2 light curve database. In this paper, we present the first
        result of the classification of periodic variable stars in the
        EROS-2 LMC database. To classify these variables, we first built
        a training set by compiling known variables in the Large
        Magellanic Cloud area from the OGLE and MACHO surveys. We
        crossmatched these variables with the EROS-2 sources and
        extracted 22 variability features from 28 392 light curves of
        the corresponding EROS-2 sources. We then used the random forest
        method to classify the EROS-2 sources in the training set. We
        designed the model to separate not only {\ensuremath{\delta}}
        Scuti stars, RR Lyraes, Cepheids, eclipsing binaries, and long-
        period variables, the superclasses, but also their subclasses,
        such as RRab, RRc, RRd, and RRe for RR Lyraes, and similarly for
        the other variable types. The model trained using only the
        superclasses shows 99\% recall and precision, while the model
        trained on all subclasses shows 87\% recall and precision. We
        applied the trained model to the entire EROS-2 LMC database,
        which contains about 29 million sources, and found 117 234
        periodic variable candidates. Out of these 117 234 periodic
        variables, 55 285 have not been discovered by either OGLE or
        MACHO variability studies. This set comprises 1906
        {\ensuremath{\delta}} Scuti stars, 6607 RR Lyraes, 638 Cepheids,
        178 Type II Cepheids, 34 562 eclipsing binaries, and 11 394
        long-period variables. catalog of these EROS-2 LMC periodic
        variable stars is available at <A href=``http://stardb.yonsei.ac
        .kr''>http://stardb.yonsei.ac.kr</A> and at the CDS via
        anonymous ftp to <A href=``http://cdsarc.u-strasbg.fr''>http://c
        dsarc.u-strasbg.fr</A> (ftp://130.79.128.5) or via <A
        href=``http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/566/A43''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/566/A43</A>}",
          doi = {10.1051/0004-6361/201323252},
archivePrefix = {arXiv},
       eprint = {1403.6131},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014A&A...566A..43K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014ApJS..212....5M,
       author = {{Mandelbaum}, Rachel and {Rowe}, Barnaby and {Bosch}, James and
         {Chang}, Chihway and {Courbin}, Frederic and {Gill}, Mandeep and
         {Jarvis}, Mike and {Kannawadi}, Arun and {Kacprzak}, Tomasz and
         {Lackner}, Claire and {Leauthaud}, Alexie and {Miyatake}, Hironao and
         {Nakajima}, Reiko and {Rhodes}, Jason and {Simet}, Melanie and
         {Zuntz}, Joe and {Armstrong}, Bob and {Bridle}, Sarah and
         {Coupon}, Jean and {Dietrich}, J{\"o}rg P. and {Gentile}, Marc and
         {Heymans}, Catherine and {Jurling}, Alden S. and {Kent}, Stephen M. and
         {Kirkby}, David and {Margala}, Daniel and {Massey}, Richard and
         {Melchior}, Peter and {Peterson}, John and {Roodman}, Aaron and
         {Schrabback}, Tim},
        title = "{The Third Gravitational Lensing Accuracy Testing (GREAT3) Challenge Handbook}",
      journal = {\apjs},
     keywords = {gravitational lensing: weak, methods: data analysis, methods: statistical, techniques: image processing, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2014",
        month = "May",
       volume = {212},
       number = {1},
          eid = {5},
        pages = {5},
     abstract = "{The GRavitational lEnsing Accuracy Testing 3 (GREAT3) challenge is the
        third in a series of image analysis challenges, with a goal of
        testing and facilitating the development of methods for
        analyzing astronomical images that will be used to measure weak
        gravitational lensing. This measurement requires extremely
        precise estimation of very small galaxy shape distortions, in
        the presence of far larger intrinsic galaxy shapes and
        distortions due to the blurring kernel caused by the atmosphere,
        telescope optics, and instrumental effects. The GREAT3 challenge
        is posed to the astronomy, machine learning, and statistics
        communities, and includes tests of three specific effects that
        are of immediate relevance to upcoming weak lensing surveys, two
        of which have never been tested in a community challenge before.
        These effects include many novel aspects including realistically
        complex galaxy models based on high-resolution imaging from
        space; a spatially varying, physically motivated blurring
        kernel; and a combination of multiple different exposures. To
        facilitate entry by people new to the field, and for use as a
        diagnostic tool, the simulation software for the challenge is
        publicly available, though the exact parameters used for the
        challenge are blinded. Sample scripts to analyze the challenge
        data using existing methods will also be provided. See <A href=`
        `http://great3challenge.info''>http://great3challenge.info</A>
        and <A href=``http://great3.projects.phys.ucl.ac.uk/leaderboard/
        ''>http://great3.projects.phys.ucl.ac.uk/leaderboard/</A> for
        more information.}",
          doi = {10.1088/0067-0049/212/1/5},
archivePrefix = {arXiv},
       eprint = {1308.4982},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014ApJS..212....5M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014ApJ...786...20L,
       author = {{Lo}, Kitty K. and {Farrell}, Sean and {Murphy}, Tara and
         {Gaensler}, B.~M.},
        title = "{Automatic Classification of Time-variable X-Ray Sources}",
      journal = {\apj},
     keywords = {catalogs, methods: statistical, X-rays: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2014",
        month = "May",
       volume = {786},
       number = {1},
          eid = {20},
        pages = {20},
     abstract = "{To maximize the discovery potential of future synoptic surveys,
        especially in the field of transient science, it will be
        necessary to use automatic classification to identify some of
        the astronomical sources. The data mining technique of
        supervised classification is suitable for this problem. Here, we
        present a supervised learning method to automatically classify
        variable X-ray sources in the Second XMM-Newton Serendipitous
        Source Catalog (2XMMi-DR2). Random Forest is our classifier of
        choice since it is one of the most accurate learning algorithms
        available. Our training set consists of 873 variable sources and
        their features are derived from time series, spectra, and other
        multi-wavelength contextual information. The 10 fold cross
        validation accuracy of the training data is
        \raisebox{-0.5ex}\textasciitilde97\% on a 7 class data set. We
        applied the trained classification model to 411 unknown variable
        2XMM sources to produce a probabilistically classified catalog.
        Using the classification margin and the Random Forest derived
        outlier measure, we identified 12 anomalous sources, of which
        2XMM J180658.7-500250 appears to be the most unusual source in
        the sample. Its X-ray spectra is suggestive of a ultraluminous
        X-ray source but its variability makes it highly unusual.
        Machine-learned classification and anomaly detection will
        facilitate scientific discoveries in the era of all-sky surveys.}",
          doi = {10.1088/0004-637X/786/1/20},
archivePrefix = {arXiv},
       eprint = {1403.0188},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014ApJ...786...20L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014MNRAS.439.3986R,
       author = {{Rosado}, Pablo A. and {Sesana}, Alberto},
        title = "{Targeting supermassive black hole binaries and gravitational wave sources for the pulsar timing array}",
      journal = {\mnras},
     keywords = {black hole physics, gravitational waves, methods: data analysis, pulsars: general, galaxies: evolution, galaxies: statistics, Astrophysics - Cosmology and Extragalactic Astrophysics, General Relativity and Quantum Cosmology},
         year = "2014",
        month = "Apr",
       volume = {439},
       number = {4},
        pages = {3986-4010},
     abstract = "{This paper presents a technique to search for supermassive black hole
        binaries (MBHBs) in the Sloan Digital Sky Survey (SDSS). The
        search is based on the peculiar properties of merging galaxies
        that are found in a mock galaxy catalogue from the Millennium
        Simulation. MBHBs are expected to be the main gravitational wave
        (GW) sources for pulsar timing arrays (PTAs); however, it is
        still unclear if the observed GW signal will be produced by a
        few single MBHBs, or if it will have the properties of a
        stochastic background. The goal of this work is to produce a map
        of the sky in which each galaxy is assigned a probability of
        having suffered a recent merger, and of hosting a MBHB that
        could be detected by PTAs. This constitutes a step forward in
        the understanding of the expected PTA signal: the skymap can be
        used to investigate the clustering properties of PTA sources and
        the spatial distribution of the observable GW signal power;
        moreover, galaxies with the highest probabilities could be used
        as inputs in targeted searches for individual GW sources. We
        also investigate the distribution of neighbouring galaxies
        around galaxies hosting MBHBs, finding that the most likely
        detectable PTA sources are located in dense galaxy environments.
        Different techniques are used in the search, including Bayesian
        and machine learning algorithms, with consistent outputs. Our
        method generates a list of galaxies classified as MBHB hosts,
        that can be combined with other searches to effectively reduce
        the number of misclassifications. The spectral coverage of the
        SDSS reaches less than a fifth of the sky, and the catalogue
        becomes severely incomplete at large redshifts; however, this
        technique can be applied in the future to larger catalogues to
        obtain complete, observationally based information of the
        expected GW signal detectable by PTAs.}",
          doi = {10.1093/mnras/stu254},
archivePrefix = {arXiv},
       eprint = {1311.0883},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014MNRAS.439.3986R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014MNRAS.439.3253R,
       author = {{Ringeval}, Christophe},
        title = "{Fast Bayesian inference for slow-roll inflation}",
      journal = {\mnras},
     keywords = {cosmological parameters, cosmology: observations, early Universe, inflation, Astrophysics - Cosmology and Nongalactic Astrophysics, General Relativity and Quantum Cosmology, High Energy Physics - Phenomenology, High Energy Physics - Theory},
         year = "2014",
        month = "Apr",
       volume = {439},
       number = {4},
        pages = {3253-3261},
     abstract = "{We present and discuss a new approach increasing by orders of magnitude
        the speed of performing Bayesian inference and parameter
        estimation within the framework of slow-roll inflation. The
        method relies on the determination of an effective likelihood
        for inflation which is a function of the primordial amplitude of
        the scalar perturbations complemented with the necessary number
        of the so-called Hubble flow functions to reach the desired
        accuracy. Starting from any cosmological data set, the effective
        likelihood is obtained by marginalization over the standard
        cosmological parameters, here viewed as `nuisance' from the
        early Universe point of view. As being low dimensional, basic
        machine-learning algorithms can be trained to accurately
        reproduce its multidimensional shape and then be used as a proxy
        to perform fast Bayesian inference on the inflationary models.
        The robustness and accuracy of the method are illustrated using
        the Planck cosmic microwave background data to perform
        primordial parameter estimation for the large field models of
        inflation. In particular, marginalized over all possible
        reheating history, we find the power index of the potential to
        verify p \&lt; 2.3 at 95 per cent of confidence.}",
          doi = {10.1093/mnras/stu109},
archivePrefix = {arXiv},
       eprint = {1312.2347},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014MNRAS.439.3253R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014MNRAS.439..644G,
       author = {{Gupta}, Pramod and {Connolly}, Andrew J. and {Gardner}, Jeffrey P.},
        title = "{Photometric selection of quasars in large astronomical data sets with a fast and accurate machine learning algorithm}",
      journal = {\mnras},
     keywords = {methods: statistical, catalogues, quasars: general},
         year = "2014",
        month = "Mar",
       volume = {439},
       number = {1},
        pages = {644-650},
     abstract = "{Future astronomical surveys will produce data on ̃{}10$^{8}$ objects per
        night. In order to characterize and classify these sources, we
        will require algorithms that scale linearly with the size of the
        data, that can be easily parallelized and where the speedup of
        the parallel algorithm will be linear in the number of
        processing cores. In this paper, we present such an algorithm
        and apply it to the question of colour selection of quasars. We
        use non-parametric Bayesian classification and a binning
        algorithm implemented with hash tables (BASH tables). We show
        that this algorithm's run time scales linearly with the number
        of test set objects and is independent of the number of training
        set objects. We also show that it has the same classification
        accuracy as other algorithms. For current data set sizes, it is
        up to three orders of magnitude faster than commonly used naive
        kernel-density-estimation techniques and it is estimated to be
        about eight times faster than the current fastest algorithm
        using dual kd-trees for kernel density estimation. The BASH
        table algorithm scales linearly with the size of the test set
        data only, and so for future larger data sets, it will be even
        faster compared to other algorithms which all depend on the size
        of the test set and the size of the training set. Since it uses
        linear data structures, it is easier to parallelize compared to
        tree-based algorithms and its speedup is linear in the number of
        cores unlike tree-based algorithms whose speedup plateaus after
        a certain number of cores. Moreover, due to the use of hash
        tables to implement the binning, the memory usage is very small.
        While our analysis is for the specific problem of selection of
        quasars, the ideas are general and the BASH table algorithm can
        be applied to any density-estimation problem involving sparse
        high-dimensional data sets. Since sparse high-dimensional data
        sets are a common type of scientific data set, this method has
        the potential to be useful in a broad range of machine-learning
        applications in astrophysics.}",
          doi = {10.1093/mnras/stt2490},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014MNRAS.439..644G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014MNRAS.438.3409C,
       author = {{Carrasco Kind}, Matias and {Brunner}, Robert J.},
        title = "{SOMz: photometric redshift PDFs with self-organizing maps and random atlas}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, surveys, galaxies: distances and redshifts, galaxies: statistics, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2014",
        month = "Mar",
       volume = {438},
       number = {4},
        pages = {3409-3421},
     abstract = "{In this paper, we explore the applicability of the unsupervised machine
        learning technique of self-organizing maps (SOM) to estimate
        galaxy photometric redshift probability density functions
        (PDFs). This technique takes a spectroscopic training set, and
        maps the photometric attributes, but not the redshifts, to a
        two-dimensional surface by using a process of competitive
        learning where neurons compete to more closely resemble the
        training data multidimensional space. The key feature of a SOM
        is that it retains the topology of the input set, revealing
        correlations between the attributes that are not easily
        identified. We test three different 2D topological mapping:
        rectangular, hexagonal and spherical, by using data from the
        Deep Extragalactic Evolutionary Probe 2 survey. We also explore
        different implementations and boundary conditions on the map and
        also introduce the idea of a random atlas, where a large number
        of different maps are created and their individual predictions
        are aggregated to produce a more robust photometric redshift
        PDF. We also introduced a new metric, the I-score, which
        efficiently incorporates different metrics, making it easier to
        compare different results (from different parameters or
        different photometric redshift codes). We find that by using a
        spherical topology mapping we obtain a better representation of
        the underlying multidimensional topology, which provides more
        accurate results that are comparable to other, state-of-the-art
        machine learning algorithms. Our results illustrate that
        unsupervised approaches have great potential for many
        astronomical problems, and in particular for the computation of
        photometric redshifts.}",
          doi = {10.1093/mnras/stt2456},
archivePrefix = {arXiv},
       eprint = {1312.5753},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014MNRAS.438.3409C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014ApJ...782...41D,
       author = {{Doert}, M. and {Errando}, M.},
        title = "{Search for Gamma-ray-emitting Active Galactic Nuclei in the Fermi-LAT Unassociated Sample Using Machine Learning}",
      journal = {\apj},
     keywords = {catalogs, galaxies: active, gamma rays: galaxies, methods: statistical, Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Instrumentation and Methods for Astrophysics, Physics - Data Analysis, Statistics and Probability},
         year = "2014",
        month = "Feb",
       volume = {782},
       number = {1},
          eid = {41},
        pages = {41},
     abstract = "{The second Fermi-LAT source catalog (2FGL) is the deepest all-sky survey
        available in the gamma-ray band. It contains 1873 sources, of
        which 576 remain unassociated. Machine-learning algorithms can
        be trained on the gamma-ray properties of known active galactic
        nuclei (AGNs) to find objects with AGN-like properties in the
        unassociated sample. This analysis finds 231 high-confidence AGN
        candidates, with increased robustness provided by intersecting
        two complementary algorithms. A method to estimate the
        performance of the classification algorithm is also presented,
        that takes into account the differences between associated and
        unassociated gamma-ray sources. Follow-up observations targeting
        AGN candidates, or studies of multiwavelength archival data,
        will reduce the number of unassociated gamma-ray sources and
        contribute to a more complete characterization of the population
        of gamma-ray emitting AGNs.}",
          doi = {10.1088/0004-637X/782/1/41},
archivePrefix = {arXiv},
       eprint = {1312.5726},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014ApJ...782...41D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014ApJ...781..117Z,
       author = {{Zhu}, W.~W. and {Berndsen}, A. and {Madsen}, E.~C. and {Tan}, M. and
         {Stairs}, I.~H. and {Brazier}, A. and {Lazarus}, P. and {Lynch}, R. and
         {Scholz}, P. and {Stovall}, K. and {Ransom}, S.~M. and {Banaszak}, S. and
         {Biwer}, C.~M. and {Cohen}, S. and {Dartez}, L.~P. and {Flanigan}, J. and
         {Lunsford}, G. and {Martinez}, J.~G. and {Mata}, A. and {Rohr}, M. and
         {Walker}, A. and {Allen}, B. and {Bhat}, N.~D.~R. and {Bogdanov}, S. and
         {Camilo}, F. and {Chatterjee}, S. and {Cordes}, J.~M. and
         {Crawford}, F. and {Deneva}, J.~S. and {Desvignes}, G. and
         {Ferdman}, R.~D. and {Freire}, P.~C.~C. and {Hessels}, J.~W.~T. and
         {Jenet}, F.~A. and {Kaplan}, D.~L. and {Kaspi}, V.~M. and
         {Knispel}, B. and {Lee}, K.~J. and {van Leeuwen}, J. and {Lyne}, A.~G. and
         {McLaughlin}, M.~A. and {Siemens}, X. and {Spitler}, L.~G. and
         {Venkataraman}, A.},
        title = "{Searching for Pulsars Using Image Pattern Recognition}",
      journal = {\apj},
     keywords = {methods: data analysis, pulsars: general, stars: neutron, techniques: image processing, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2014",
        month = "Feb",
       volume = {781},
       number = {2},
          eid = {117},
        pages = {117},
     abstract = "{In the modern era of big data, many fields of astronomy are generating
        huge volumes of data, the analysis of which can sometimes be the
        limiting factor in research. Fortunately, computer scientists
        have developed powerful data-mining techniques that can be
        applied to various fields. In this paper, we present a novel
        artificial intelligence (AI) program that identifies pulsars
        from recent surveys by using image pattern recognition with deep
        neural nets{\textemdash}the PICS (Pulsar Image-based
        Classification System) AI. The AI mimics human experts and
        distinguishes pulsars from noise and interference by looking for
        patterns from candidate plots. Different from other pulsar
        selection programs that search for expected patterns, the PICS
        AI is taught the salient features of different pulsars from a
        set of human-labeled candidates through machine learning. The
        training candidates are collected from the Pulsar Arecibo L-band
        Feed Array (PALFA) survey. The information from each pulsar
        candidate is synthesized in four diagnostic plots, which consist
        of image data with up to thousands of pixels. The AI takes these
        data from each candidate as its input and uses thousands of such
        candidates to train its \raisebox{-0.5ex}\textasciitilde9000
        neurons. The deep neural networks in this AI system grant it
        superior ability to recognize various types of pulsars as well
        as their harmonic signals. The trained AI's performance has been
        validated with a large set of candidates from a different pulsar
        survey, the Green Bank North Celestial Cap survey. In this
        completely independent test, the PICS ranked 264 out of 277
        pulsar-related candidates, including all 56 previously known
        pulsars and 208 of their harmonics, in the top 961 (1\%) of
        90,008 test candidates, missing only 13 harmonics. The first
        non-pulsar candidate appears at rank 187, following 45 pulsars
        and 141 harmonics. In other words, 100\% of the pulsars were
        ranked in the top 1\% of all candidates, while 80\% were ranked
        higher than any noise or interference. The performance of this
        system can be improved over time as more training data are
        accumulated. This AI system has been integrated into the PALFA
        survey pipeline and has discovered six new pulsars to date.}",
          doi = {10.1088/0004-637X/781/2/117},
archivePrefix = {arXiv},
       eprint = {1309.0776},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014ApJ...781..117Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014SCPMA..57..176S,
       author = {{Si}, JianMin and {Luo}, ALi and {Li}, YinBi and {Zhang}, JianNan and
         {Wei}, Peng and {Wu}, YiHong and {Wu}, FuChao and {Zhao}, YongHeng},
        title = "{Search for carbon stars and DZ white dwarfs in SDSS spectra survey through machine learning}",
      journal = {Science China Physics, Mechanics, and Astronomy},
     keywords = {machine learning, label propagation, carbon stars, DZ white dwarfs, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
         year = "2014",
        month = "Jan",
       volume = {57},
       number = {1},
        pages = {176-186},
     abstract = "{Carbon stars and DZ white dwarfs are two types of rare objects in the
        Galaxy. In this paper, we have applied the label propagation
        algorithm to search for these two types of stars from Data
        Release Eight (DR8) of the Sloan Digital Sky Survey (SDSS),
        which is verified to be efficient by calculating precision and
        recall. From nearly two million spectra including stars,
        galaxies and QSOs, we have found 260 new carbon stars in which
        96 stars have been identified as dwarfs and 7 identified as
        giants, and 11 composition spectrum systems (each of them
        consists of a white dwarf and a carbon star). Similarly, using
        the label propagation method, we have obtained 29 new DZ white
        dwarfs from SDSS DR8. Compared with PCA reconstructed spectra,
        the 29 findings are typical DZ white dwarfs. We have also
        investigated their proper motions by comparing them with proper
        motion distribution of 9,374 white dwarfs, and found that they
        satisfy the current observed white dwarfs by SDSS generally have
        large proper motions. In addition, we have estimated their
        effective temperatures by fitting the polynomial relationship
        between effective temperature and g-r color of known DZ white
        dwarfs, and found 12 of the 29 new DZ white dwarfs are cool, in
        which nine are between 6,000 K and 6,600 K, and three are below
        6,000 K.}",
          doi = {10.1007/s11433-013-5374-0},
archivePrefix = {arXiv},
       eprint = {1309.1883},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014SCPMA..57..176S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2014PhDT.......472M,
       author = {{Morgan}, Adam Nolan},
        title = "{Classification, Follow-Up, and Analysis of Gamma-Ray Bursts and their Early-Time Near-Infrared/Optical Afterglows}",
     keywords = {Astrophysics},
       school = {University of California, Berkeley},
         year = "2014",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014PhDT.......472M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2014PhDT........99W,
       author = {{Woods}, Michael Austin},
        title = "{A Comprehensive Study of the Large Underground Xenon Detector}",
     keywords = {Physics, Astrophysics;Physics, Elementary Particles and High Energy},
       school = {University of California, Davis},
         year = "2014",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014PhDT........99W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014PASA...31....1F,
       author = {{Fuqiang}, Chen and {Yan}, Wu and {Yude}, Bu and {Guodong}, Zhao},
        title = "{Spectral Classification Using Restricted Boltzmann Machine}",
      journal = {\pasa},
     keywords = {astronomical instrumentation, methods and techniques, methods: analytical, methods: data analysis, methods: statistical},
         year = "2014",
        month = "Jan",
       volume = {31},
          eid = {e001},
        pages = {e001},
     abstract = "{In this study, a novel machine learning algorithm, restricted Boltzmann
        machine, is introduced. The algorithm is applied for the
        spectral classification in astronomy. Restricted Boltzmann
        machine is a bipartite generative graphical model with two
        separate layers (one visible layer and one hidden layer), which
        can extract higher level features to represent the original
        data. Despite generative, restricted Boltzmann machine can be
        used for classification when modified with a free energy and a
        soft-max function. Before spectral classification, the original
        data are binarised according to some rule. Then, we resort to
        the binary restricted Boltzmann machine to classify cataclysmic
        variables and non-cataclysmic variables (one half of all the
        given data for training and the other half for testing). The
        experiment result shows state-of-the-art accuracy of 100\%,
        which indicates the efficiency of the binary restricted
        Boltzmann machine algorithm.}",
          doi = {10.1017/pasa.2013.38},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014PASA...31....1F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014MNRAS.437..968C,
       author = {{Cavuoti}, Stefano and {Brescia}, Massimo and {D'Abrusco}, Raffaele and
         {Longo}, Giuseppe and {Paolillo}, Maurizio},
        title = "{Photometric classification of emission line galaxies with machine-learning methods}",
      journal = {\mnras},
     keywords = {methods: data analysis, catalogues, surveys, galaxies: active, galaxies: Seyfert, Astrophysics - Cosmology and Extragalactic Astrophysics},
         year = "2014",
        month = "Jan",
       volume = {437},
       number = {1},
        pages = {968-975},
     abstract = "{In this paper, we discuss an application of machine-learning-based
        methods to the identification of candidate active galactic
        nucleus (AGN) from optical survey data and to the automatic
        classification of AGNs in broad classes. We applied four
        different machine-learning algorithms, namely the Multi Layer
        Perceptron, trained, respectively, with the Conjugate Gradient,
        the Scaled Conjugate Gradient, the Quasi Newton learning rules
        and the Support Vector Machines, to tackle the problem of the
        classification of emission line galaxies in different classes,
        mainly AGNs versus non-AGNs, obtained using optical photometry
        in place of the diagnostics based on line intensity ratios which
        are classically used in the literature. Using the same
        photometric features, we discuss also the behaviour of the
        classifiers on finer AGN classification tasks, namely Seyfert I
        versus Seyfert II, and Seyfert versus LINER. Furthermore, we
        describe the algorithms employed, the samples of
        spectroscopically classified galaxies used to train the
        algorithms, the procedure followed to select the photometric
        parameters and the performances of our methods in terms of
        multiple statistical indicators. The results of the experiments
        show that the application of self-adaptive data mining
        algorithms trained on spectroscopic data sets and applied to
        carefully chosen photometric parameters represents a viable
        alternative to the classical methods that employ time-consuming
        spectroscopic observations.}",
          doi = {10.1093/mnras/stt1961},
archivePrefix = {arXiv},
       eprint = {1310.2840},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014MNRAS.437..968C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014JGRA..119..620H,
       author = {{Hutchins}, Michael L. and {Holzworth}, Robert H. and
         {Brundell}, James B.},
        title = "{Diurnal variation of the global electric circuit from clustered thunderstorms}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {lightning, thunderstorms, global electric circuit, clustering, WWLLN, machine learning},
         year = "2014",
        month = "Jan",
       volume = {119},
       number = {1},
        pages = {620-629},
     abstract = "{The diurnal variation of the global electric circuit is investigated
        using the World Wide Lightning Location Network (WWLLN), which
        has been shown to identify nearly all thunderstorms (using WWLLN
        data from 2005). To create an estimate of global electric
        circuit activity, a clustering algorithm is applied to the WWLLN
        data set to identify global thunderstorms from 2010 to 2013.
        Annual, seasonal, and regional thunderstorm activity is
        investigated in this new WWLLN thunderstorm data set in order to
        estimate the source behavior of the global electric circuit.
        Through the clustering algorithm, the total number of active
        thunderstorms are counted every 30 min creating a measure of the
        global electric circuit source function. The thunderstorm
        clusters are compared to precipitation radar data from the
        Tropical Rainfall Measurement Mission satellite and with case
        studies of thunderstorm evolution. The clustering algorithm
        reveals an average of 660{\ensuremath{\pm}}70 thunderstorms
        active at any given time with a peak-to-peak variation of 36\%.
        The highest number of thunderstorms occurs in November
        (720{\ensuremath{\pm}}90), and the lowest number occurs in
        January (610{\ensuremath{\pm}}80). Thunderstorm cluster and
        electrified storm cloud activity are combined with thunderstorm
        overflight current measurements to estimate the global electric
        circuit thunderstorm contribution current to be
        1090{\ensuremath{\pm}}70 A with a variation of 24\%. By
        utilizing the global coverage and high time resolution of WWLLN,
        the total active thunderstorm count and current is shown to be
        less than previous estimates based on compiled climatologies.}",
          doi = {10.1002/2013JA019593},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014JGRA..119..620H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014GeoRL..41..193D,
       author = {{Deng}, Yi and {Ebert-Uphoff}, Imme},
        title = "{Weakening of atmospheric information flow in a warming climate in the Community Climate System Model}",
      journal = {\grl},
     keywords = {information flow, causal discovery, machine learning, graphical models, climate change},
         year = "2014",
        month = "Jan",
       volume = {41},
       number = {1},
        pages = {193-200},
     abstract = "{We introduce a new perspective of climate change by revealing the
        changing characteristics of atmospheric information flow in a
        warming climate. The key idea is to interpret large-scale
        atmospheric dynamical processes as information flow around the
        globe and to identify the pathways of this information flow
        using a climate network based on causal discovery and graphical
        models. We construct such networks using the daily geopotential
        height data from the Community Climate System Model Version 4.0
        (CCSM4.0)'s 20$^{th}$ century climate simulation and 21$^{st}$
        century climate projection. We show that in the CCSM4.0 model
        under enhanced greenhouse gases (GHGs) forcing, prominent
        midlatitude information pathways in the midtroposphere weaken
        and shift poleward, while major tropical information pathways
        start diminishing. Averaged over the entire Northern Hemisphere,
        the atmospheric information flow weakens. The implications of
        this weakening for the interconnectivity among different
        geographical locations and for the intrinsic predictability of
        the atmosphere are discussed.}",
          doi = {10.1002/2013GL058646},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014GeoRL..41..193D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014ApJ...781...39M,
       author = {{Mart{\'\i}nez-G{\'o}mez}, Elizabeth and {Richards}, Mercedes T. and
         {Richards}, Donald St. P.},
        title = "{Distance Correlation Methods for Discovering Associations in Large Astrophysical Databases}",
      journal = {\apj},
     keywords = {catalogs, galaxies: clusters: general, galaxies: evolution, galaxies: statistics, methods: statistical, surveys, Astrophysics - Cosmology and Nongalactic Astrophysics, Mathematics - Statistics Theory, Statistics - Applications, Statistics - Machine Learning},
         year = "2014",
        month = "Jan",
       volume = {781},
       number = {1},
          eid = {39},
        pages = {39},
     abstract = "{High-dimensional, large-sample astrophysical databases of galaxy
        clusters, such as the Chandra Deep Field South COMBO-17
        database, provide measurements on many variables for thousands
        of galaxies and a range of redshifts. Current understanding of
        galaxy formation and evolution rests sensitively on
        relationships between different astrophysical variables; hence
        an ability to detect and verify associations or correlations
        between variables is important in astrophysical research. In
        this paper, we apply a recently defined statistical measure
        called the distance correlation coefficient, which can be used
        to identify new associations and correlations between
        astrophysical variables. The distance correlation coefficient
        applies to variables of any dimension, can be used to determine
        smaller sets of variables that provide equivalent astrophysical
        information, is zero only when variables are independent, and is
        capable of detecting nonlinear associations that are
        undetectable by the classical Pearson correlation coefficient.
        Hence, the distance correlation coefficient provides more
        information than the Pearson coefficient. We analyze numerous
        pairs of variables in the COMBO-17 database with the distance
        correlation method and with the maximal information coefficient.
        We show that the Pearson coefficient can be estimated with
        higher accuracy from the corresponding distance correlation
        coefficient than from the maximal information coefficient. For
        given values of the Pearson coefficient, the distance
        correlation method has a greater ability than the maximal
        information coefficient to resolve astrophysical data into
        highly concentrated horseshoe- or V-shapes, which enhances
        classification and pattern identification. These results are
        observed over a range of redshifts beyond the local universe and
        for galaxies from elliptical to spiral.}",
          doi = {10.1088/0004-637X/781/1/39},
archivePrefix = {arXiv},
       eprint = {1308.3925},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014ApJ...781...39M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013ChEG...73..545A,
       author = {{Abbaszadeh}, Maliheh and {Hezarkhani}, Ardeshir and
         {Soltani-Mohammadi}, Saeed},
        title = "{An SVM-based machine learning method for the separation of alteration zones in Sungun porphyry copper deposit}",
      journal = {Chemie der Erde / Geochemistry},
         year = "2013",
        month = "Dec",
       volume = {73},
       number = {4},
        pages = {545-554},
          doi = {10.1016/j.chemer.2013.07.001},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013ChEG...73..545A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013ApJ...777...83P,
       author = {{Pichara}, Karim and {Protopapas}, Pavlos},
        title = "{Automatic Classification of Variable Stars in Catalogs with Missing Data}",
      journal = {\apj},
     keywords = {methods: data analysis, stars: statistics, stars: variables: general, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2013",
        month = "Nov",
       volume = {777},
       number = {2},
          eid = {83},
        pages = {83},
     abstract = "{We present an automatic classification method for astronomical catalogs
        with missing data. We use Bayesian networks and a probabilistic
        graphical model that allows us to perform inference to predict
        missing values given observed data and dependency relationships
        between variables. To learn a Bayesian network from incomplete
        data, we use an iterative algorithm that utilizes sampling
        methods and expectation maximization to estimate the
        distributions and probabilistic dependencies of variables from
        data with missing values. To test our model, we use three
        catalogs with missing data (SAGE, Two Micron All Sky Survey, and
        UBVI) and one complete catalog (MACHO). We examine how
        classification accuracy changes when information from missing
        data catalogs is included, how our method compares to
        traditional missing data approaches, and at what computational
        cost. Integrating these catalogs with missing data, we find that
        classification of variable objects improves by a few percent and
        by 15\% for quasar detection while keeping the computational
        cost the same.}",
          doi = {10.1088/0004-637X/777/2/83},
archivePrefix = {arXiv},
       eprint = {1310.7868},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013ApJ...777...83P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013MNRAS.435.1047B,
       author = {{Brink}, Henrik and {Richards}, Joseph W. and {Poznanski}, Dovi and
         {Bloom}, Joshua S. and {Rice}, John and {Negahban}, Sahand and
         {Wainwright}, Martin},
        title = "{Using machine learning for discovery in synoptic survey imaging data}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, techniques: image processing, surveys, supernovae: general, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Applications},
         year = "2013",
        month = "Oct",
       volume = {435},
       number = {2},
        pages = {1047-1060},
     abstract = "{Modern time-domain surveys continuously monitor large swaths of the sky
        to look for astronomical variability. Astrophysical discovery in
        such data sets is complicated by the fact that detections of
        real transient and variable sources are highly outnumbered by
        `bogus' detections caused by imperfect subtractions, atmospheric
        effects and detector artefacts. In this work, we present a
        machine-learning (ML) framework for discovery of variability in
        time-domain imaging surveys. Our ML methods provide
        probabilistic statements, in near real time, about the degree to
        which each newly observed source is an astrophysically relevant
        source of variable brightness. We provide details about each of
        the analysis steps involved, including compilation of the
        training and testing sets, construction of descriptive image-
        based and contextual features, and optimization of the feature
        subset and model tuning parameters. Using a validation set of
        nearly 30 000 objects from the Palomar Transient Factory, we
        demonstrate a missed detection rate of at most 7.7 per cent at
        our chosen false-positive rate of 1 per cent for an optimized ML
        classifier of 23 features, selected to avoid feature correlation
        and overfitting from an initial library of 42 attributes.
        Importantly, we show that our classification methodology is
        insensitive to mislabelled training data up to a contamination
        of nearly 10 per cent, making it easier to compile sufficient
        training sets for accurate performance in future surveys. This
        ML framework, if so adopted, should enable the maximization of
        scientific gain from future synoptic survey and enable fast
        follow-up decisions on the vast amounts of streaming data
        produced by such experiments.}",
          doi = {10.1093/mnras/stt1306},
archivePrefix = {arXiv},
       eprint = {1209.3775},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013MNRAS.435.1047B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013ApJ...776L..34S,
       author = {{Singer}, Leo P. and {Cenko}, S. Bradley and {Kasliwal}, Mansi M. and
         {Perley}, Daniel A. and {Ofek}, Eran O. and {Brown}, Duncan A. and
         {Nugent}, Peter E. and {Kulkarni}, S.~R. and {Corsi}, Alessandra and
         {Frail}, Dale A. and {Bellm}, Eric and {Mulchaey}, John and
         {Arcavi}, Iair and {Barlow}, Tom and {Bloom}, Joshua S. and {Cao}, Yi and
         {Gehrels}, Neil and {Horesh}, Assaf and {Masci}, Frank J. and
         {McEnery}, Julie and {Rau}, Arne and {Surace}, Jason A. and
         {Yaron}, Ofer},
        title = "{Discovery and Redshift of an Optical Afterglow in 71 deg$^{2}$: iPTF13bxl and GRB 130702A}",
      journal = {\apj},
     keywords = {gamma-ray burst: individual: GRB 130702A, Astrophysics - High Energy Astrophysical Phenomena, Astrophysics - Cosmology and Extragalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2013",
        month = "Oct",
       volume = {776},
       number = {2},
          eid = {L34},
        pages = {L34},
     abstract = "{We report the discovery of the optical afterglow of the
        {\ensuremath{\gamma}}-ray burst (GRB) 130702A, identified upon
        searching 71 deg$^{2}$ surrounding the Fermi Gamma-ray Burst
        Monitor (GBM) localization. Discovered and characterized by the
        intermediate Palomar Transient Factory, iPTF13bxl is the first
        afterglow discovered solely based on a GBM localization. Real-
        time image subtraction, machine learning, human vetting, and
        rapid response multi-wavelength follow-up enabled us to quickly
        narrow a list of 27,004 optical transient candidates to a single
        afterglow-like source. Detection of a new, fading X-ray source
        by Swift and a radio counterpart by CARMA and the Very Large
        Array confirmed the association between iPTF13bxl and GRB
        130702A. Spectroscopy with the Magellan and Palomar 200 inch
        telescopes showed the afterglow to be at a redshift of z =
        0.145, placing GRB 130702A among the lowest redshift GRBs
        detected to date. The prompt {\ensuremath{\gamma}}-ray energy
        release and afterglow luminosity are intermediate between
        typical cosmological GRBs and nearby sub-luminous events such as
        GRB 980425 and GRB 060218. The bright afterglow and emerging
        supernova offer an opportunity for extensive panchromatic
        follow-up. Our discovery of iPTF13bxl demonstrates the first
        observational proof-of-principle for
        \raisebox{-0.5ex}\textasciitilde10 Fermi-iPTF localizations
        annually. Furthermore, it represents an important step toward
        overcoming the challenges inherent in uncovering faint optical
        counterparts to comparably localized gravitational wave events
        in the Advanced LIGO and Virgo era.}",
          doi = {10.1088/2041-8205/776/2/L34},
archivePrefix = {arXiv},
       eprint = {1307.5851},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013ApJ...776L..34S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013AJ....146..101P,
       author = {{Palaversa}, Lovro and {Ivezi{\'c}}, {\v{Z}}eljko and {Eyer}, Laurent and
         {Ru{\v{z}}djak}, Domagoj and {Sudar}, Davor and {Galin}, Mario and
         {Kroflin}, Andrea and {Mesari{\'c}}, Martina and {Munk}, Petra and
         {Vrbanec}, Dijana and {Bo{\v{z}}i{\'c}}, Hrvoje and {Loebman}, Sarah and
         {Sesar}, Branimir and {Rimoldini}, Lorenzo and {Hunt-Walker}, Nicholas and
         {VanderPlas}, Jacob and {Westman}, David and {Stuart}, J. Scott and
         {Becker}, Andrew C. and {Srdo{\v{c}}}, Gregor and
         {Wozniak}, Przemyslaw and {Oluseyi}, Hakeem},
        title = "{Exploring the Variable Sky with LINEAR. III. Classification of Periodic Light Curves}",
      journal = {\aj},
     keywords = {binaries: eclipsing, blue stragglers, catalogs, Galaxy: halo, stars: statistics, stars: variables: general, Astrophysics - Astrophysics of Galaxies, Astrophysics - Solar and Stellar Astrophysics},
         year = "2013",
        month = "Oct",
       volume = {146},
       number = {4},
          eid = {101},
        pages = {101},
     abstract = "{We describe the construction of a highly reliable sample of
        \raisebox{-0.5ex}\textasciitilde7000 optically faint periodic
        variable stars with light curves obtained by the asteroid survey
        LINEAR across 10,000 deg$^{2}$ of the northern sky. The majority
        of these variables have not been cataloged yet. The sample flux
        limit is several magnitudes fainter than most other wide-angle
        surveys; the photometric errors range from
        \raisebox{-0.5ex}\textasciitilde0.03 mag at r = 15 to
        \raisebox{-0.5ex}\textasciitilde0.20 mag at r = 18. Light curves
        include on average 250 data points, collected over about a
        decade. Using Sloan Digital Sky Survey (SDSS) based photometric
        recalibration of the LINEAR data for about 25 million objects,
        we selected \raisebox{-0.5ex}\textasciitilde200,000 most
        probable candidate variables with r \&lt; 17 and visually
        confirmed and classified \raisebox{-0.5ex}\textasciitilde7000
        periodic variables using phased light curves. The reliability
        and uniformity of visual classification across eight human
        classifiers was calibrated and tested using a catalog of
        variable stars from the SDSS Stripe 82 region and verified using
        an unsupervised machine learning approach. The resulting sample
        of periodic LINEAR variables is dominated by 3900 RR Lyrae stars
        and 2700 eclipsing binary stars of all subtypes and includes
        small fractions of relatively rare populations such as
        asymptotic giant branch stars and SX Phoenicis stars. We discuss
        the distribution of these mostly uncataloged variables in
        various diagrams constructed with optical-to-infrared SDSS, Two
        Micron All Sky Survey, and Wide-field Infrared Survey Explorer
        photometry, and with LINEAR light-curve features. We find that
        the combination of light-curve features and colors enables
        classification schemes much more powerful than when colors or
        light curves are each used separately. An interesting side
        result is a robust and precise quantitative description of a
        strong correlation between the light-curve period and
        color/spectral type for close and contact eclipsing binary stars
        ({\ensuremath{\beta}} Lyrae and W UMa): as the color-based
        spectral type varies from K4 to F5, the median period increases
        from 5.9 hr to 8.8 hr. These large samples of robustly
        classified variable stars will enable detailed statistical
        studies of the Galactic structure and physics of binary and
        other stars and we make these samples publicly available.}",
          doi = {10.1088/0004-6256/146/4/101},
archivePrefix = {arXiv},
       eprint = {1308.0357},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013AJ....146..101P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013PhRvD..88f2003B,
       author = {{Biswas}, Rahul and {Blackburn}, Lindy and {Cao}, Junwei and
         {Essick}, Reed and {Hodge}, Kari Alison and
         {Katsavounidis}, Erotokritos and {Kim}, Kyungmin and {Kim}, Young-Min and
         {Le Bigot}, Eric-Olivier and {Lee}, Chang-Hwan and {Oh}, John J. and
         {Oh}, Sang Hoon and {Son}, Edwin J. and {Tao}, Ye and {Vaulin}, Ruslan and
         {Wang}, Xiaoge},
        title = "{Application of machine learning algorithms to the study of noise artifacts in gravitational-wave data}",
      journal = {\prd},
     keywords = {04.80.Nn, 07.05.Mh, 07.05.Kf, Gravitational wave detectors and experiments, Neural networks fuzzy logic artificial intelligence, Data analysis: algorithms and implementation, data management, Astrophysics - Instrumentation and Methods for Astrophysics, 62-07, 83C35, 62M45, 62C86},
         year = "2013",
        month = "Sep",
       volume = {88},
       number = {6},
          eid = {062003},
        pages = {062003},
     abstract = "{The sensitivity of searches for astrophysical transients in data from
        the Laser Interferometer Gravitational-wave Observatory (LIGO)
        is generally limited by the presence of transient, non-Gaussian
        noise artifacts, which occur at a high enough rate such that
        accidental coincidence across multiple detectors is non-
        negligible. These
        {\textquotedblleft}glitches{\textquotedblright} can easily be
        mistaken for transient gravitational-wave signals, and their
        robust identification and removal will help any search for
        astrophysical gravitational waves. We apply machine-learning
        algorithms (MLAs) to the problem, using data from auxiliary
        channels within the LIGO detectors that monitor degrees of
        freedom unaffected by astrophysical signals. Noise sources may
        produce artifacts in these auxiliary channels as well as the
        gravitational-wave channel. The number of auxiliary-channel
        parameters describing these disturbances may also be extremely
        large; high dimensionality is an area where MLAs are
        particularly well suited. We demonstrate the feasibility and
        applicability of three different MLAs: artificial neural
        networks, support vector machines, and random forests. These
        classifiers identify and remove a substantial fraction of the
        glitches present in two different data sets: four weeks of
        LIGO{\textquoteright}s fourth science run and one week of
        LIGO{\textquoteright}s sixth science run. We observe that all
        three algorithms agree on which events are glitches to within
        10\% for the sixth-science-run data, and support this by showing
        that the different optimization criteria used by each classifier
        generate the same decision surface, based on a likelihood-ratio
        statistic. Furthermore, we find that all classifiers obtain
        similar performance to the benchmark algorithm, the ordered veto
        list, which is optimized to detect pairwise correlations between
        transients in LIGO auxiliary channels and glitches in the
        gravitational-wave data. This suggests that most of the useful
        information currently extracted from the auxiliary channels is
        already described by this model. Future performance gains are
        thus likely to involve additional sources of information, rather
        than improvements in the classification algorithms themselves.
        We discuss several plausible sources of such new information as
        well as the ways of propagating it through the classifiers into
        gravitational-wave searches.}",
          doi = {10.1103/PhysRevD.88.062003},
archivePrefix = {arXiv},
       eprint = {1303.6984},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013PhRvD..88f2003B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013MNRAS.434..282F,
       author = {{Freeman}, P.~E. and {Izbicki}, R. and {Lee}, A.~B. and {Newman}, J.~A. and
         {Conselice}, C.~J. and {Koekemoer}, A.~M. and {Lotz}, J.~M. and
         {Mozena}, M.},
        title = "{New image statistics for detecting disturbed galaxy morphologies at high redshift}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, galaxies: evolution, galaxies: fundamental parameters, galaxies: high-redshift, galaxies: statistics, galaxies: structure, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2013",
        month = "Sep",
       volume = {434},
       number = {1},
        pages = {282-295},
     abstract = "{Testing theories of hierarchical structure formation requires estimating
        the distribution of galaxy morphologies and its change with
        redshift. One aspect of this investigation involves identifying
        galaxies with disturbed morphologies (e.g. merging galaxies).
        This is often done by summarizing galaxy images using, e.g. the
        concentration, asymmetry and clumpiness and Gini-M$_{20}$
        statistics of Conselice and Lotz et al., respectively, and
        associating particular statistic values with disturbance. We
        introduce three statistics that enhance detection of disturbed
        morphologies at high redshift (z ̃ 2): the multimode (M),
        intensity (I) and deviation (D) statistics. We show their
        effectiveness by training a machine-learning classifier, random
        forest, using 1639 galaxies observed in the H band by the Hubble
        Space Telescope WFC3, galaxies that had been previously
        classified by eye by the Cosmic Assembly Near-IR Deep
        Extragalactic Legacy Survey collaboration. We find that the MID
        statistics (and the A statistic of Conselice) are the most
        useful for identifying disturbed morphologies. We also explore
        whether human annotators are useful for identifying disturbed
        morphologies. We demonstrate that they show limited ability to
        detect disturbance at high redshift, and that increasing their
        number beyond {\ensuremath{\approx}}10 does not provably yield
        better classification performance. We propose a simulation-based
        model-fitting algorithm that mitigates these issues by bypassing
        annotation.}",
          doi = {10.1093/mnras/stt1016},
archivePrefix = {arXiv},
       eprint = {1306.1238},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013MNRAS.434..282F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013ApJ...774L..27Y,
       author = {{Yang}, Xiao and {Lin}, GangHua and {Zhang}, HongQi and {Mao}, XinJie},
        title = "{Magnetic Nonpotentiality in Photospheric Active Regions as a Predictor of Solar Flares}",
      journal = {\apj},
     keywords = {methods: statistical, Sun: activity, Sun: flares, Sun: photosphere, Astrophysics - Solar and Stellar Astrophysics},
         year = "2013",
        month = "Sep",
       volume = {774},
       number = {2},
          eid = {L27},
        pages = {L27},
     abstract = "{Based on several magnetic nonpotentiality parameters obtained from the
        vector photospheric active region magnetograms obtained with the
        Solar Magnetic Field Telescope at the Huairou Solar Observing
        Station over two solar cycles, a machine learning model has been
        constructed to predict the occurrence of flares in the
        corresponding active region within a certain time window. The
        Support Vector Classifier, a widely used general classifier, is
        applied to build and test the prediction models. Several
        classical verification measures are adopted to assess the
        quality of the predictions. We investigate different flare
        levels within various time windows, and thus it is possible to
        estimate the rough classes and erupting times of flares for
        particular active regions. Several combinations of predictors
        have been tested in the experiments. The True Skill Statistics
        are higher than 0.36 in 97\% of cases and the Heidke Skill
        Scores range from 0.23 to 0.48. The predictors derived from
        longitudinal magnetic fields do perform well, however, they are
        less sensitive in predicting large flares. Employing the
        nonpotentiality predictors from vector fields improves the
        performance of predicting large flares of magnitude \&gt;=M5.0
        and \&gt;=X1.0.}",
          doi = {10.1088/2041-8205/774/2/L27},
archivePrefix = {arXiv},
       eprint = {1308.1181},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013ApJ...774L..27Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013AJ....146...61S,
       author = {{Schmidt}, Edward G.},
        title = "{Type II Cepheid Candidates. IV. Objects from the Northern Sky Variability Survey}",
      journal = {\aj},
     keywords = {stars: Population II, stars: variables: Cepheids},
         year = "2013",
        month = "Sep",
       volume = {146},
       number = {3},
          eid = {61},
        pages = {61},
     abstract = "{We have obtained VR photometry of 447 Cepheid variable star candidates
        with declinations north of -14{\textdegree}30', most of which
        were identified using the Northern Sky Variability Survey (NSVS)
        data archive. Periods and other photometric properties were
        derived from the combination of our data with the NSVS data.
        Atmospheric parameters were determined for 81 of these stars
        from low-resolution spectra. The identification of type II
        Cepheids based on the data presented in all four papers in this
        series is discussed. On the basis of spectra, 30 type II
        Cepheids were identified while 53 variables were identified as
        cool, main sequence stars and 283 as red giants following the
        definitions in Paper III. An additional 30 type II Cepheids were
        identified on the basis of light curves. The present
        classifications are compared with those from the Machine-learned
        All Sky Automated Survey Classification Catalog for 174 stars in
        common.}",
          doi = {10.1088/0004-6256/146/3/61},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013AJ....146...61S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013GeoRL..40.4188W,
       author = {{Wagstaff}, K.~L. and {Thompson}, D.~R. and {Abbey}, W. and
         {Allwood}, A. and {Bekker}, D.~L. and {Cabrol}, N.~A. and {Fuchs}, T. and
         {Ortega}, K.},
        title = "{Smart, texture-sensitive instrument classification for in situ rock and layer analysis}",
      journal = {\grl},
     keywords = {onboard analysis, classification},
         year = "2013",
        month = "Aug",
       volume = {40},
       number = {16},
        pages = {4188-4193},
     abstract = "{Science missions have limited lifetimes, necessitating an efficient
        investigation of the field site. The efficiency of onboard
        cameras, critical for planning, is limited by the need to
        downlink images to Earth for every decision. Recent advances
        have enabled rovers to take follow-up actions without waiting
        hours or days for new instructions. We propose using built-in
        processing by the instrument itself for adaptive data
        collection, faster reconnaissance, and increased mission science
        yield. We have developed a machine learning pixel classifier
        that is sensitive to texture differences in surface materials,
        enabling more sophisticated onboard classification than was
        previously possible. This classifier can be implemented in a
        Field Programmable Gate Array (FPGA) for maximal efficiency and
        minimal impact on the rest of the system's functions. In this
        paper, we report on initial results from applying the texture-
        sensitive classifier to three example analysis tasks using data
        from the Mars Exploration Rovers.}",
          doi = {10.1002/grl.50817},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013GeoRL..40.4188W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013ApJ...772..147X,
       author = {{Xu}, Xiaoying and {Ho}, Shirley and {Trac}, Hy and {Schneider}, Jeff and
         {Poczos}, Barnabas and {Ntampaka}, Michelle},
        title = "{A First Look at Creating Mock Catalogs with Machine Learning Techniques}",
      journal = {\apj},
     keywords = {galaxies: halos, large-scale structure of universe, methods: numerical, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2013",
        month = "Aug",
       volume = {772},
       number = {2},
          eid = {147},
        pages = {147},
     abstract = "{We investigate machine learning (ML) techniques for predicting the
        number of galaxies (N $_{gal}$) that occupy a halo, given the
        halo's properties. These types of mappings are crucial for
        constructing the mock galaxy catalogs necessary for analyses of
        large-scale structure. The ML techniques proposed here
        distinguish themselves from traditional halo occupation
        distribution (HOD) modeling as they do not assume a prescribed
        relationship between halo properties and N $_{gal}$. In
        addition, our ML approaches are only dependent on parent halo
        properties (like HOD methods), which are advantageous over
        subhalo-based approaches as identifying subhalos correctly is
        difficult. We test two algorithms: support vector machines (SVM)
        and k-nearest-neighbor (kNN) regression. We take galaxies and
        halos from the Millennium simulation and predict N $_{gal}$ by
        training our algorithms on the following six halo properties:
        number of particles, M $_{200}$, {\ensuremath{\sigma}}$_{ v }$,
        v $_{max}$, half-mass radius, and spin. For Millennium, our
        predicted N $_{gal}$ values have a mean-squared error (MSE) of
        \raisebox{-0.5ex}\textasciitilde0.16 for both SVM and kNN. Our
        predictions match the overall distribution of halos reasonably
        well and the galaxy correlation function at large scales to
        \raisebox{-0.5ex}\textasciitilde5\%-10\%. In addition, we
        demonstrate a feature selection algorithm to isolate the halo
        parameters that are most predictive, a useful technique for
        understanding the mapping between halo properties and N
        $_{gal}$. Lastly, we investigate these ML-based approaches in
        making mock catalogs for different galaxy subpopulations (e.g.,
        blue, red, high M $_{star}$, low M $_{star}$). Given its non-
        parametric nature as well as its powerful predictive and feature
        selection capabilities, ML offers an interesting alternative for
        creating mock catalogs.}",
          doi = {10.1088/0004-637X/772/2/147},
archivePrefix = {arXiv},
       eprint = {1303.1055},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013ApJ...772..147X},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013ApJ...772..140B,
       author = {{Brescia}, M. and {Cavuoti}, S. and {D'Abrusco}, R. and {Longo}, G. and
         {Mercurio}, A.},
        title = "{Photometric Redshifts for Quasars in Multi-band Surveys}",
      journal = {\apj},
     keywords = {catalogs, galaxies: distances and redshifts, methods: data analysis, quasars: general, surveys, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2013",
        month = "Aug",
       volume = {772},
       number = {2},
          eid = {140},
        pages = {140},
     abstract = "{The Multi Layer Perceptron with Quasi Newton Algorithm (MLPQNA) is a
        machine learning method that can be used to cope with regression
        and classification problems on complex and massive data sets. In
        this paper, we give a formal description of the method and
        present the results of its application to the evaluation of
        photometric redshifts for quasars. The data set used for the
        experiment was obtained by merging four different surveys (Sloan
        Digital Sky Survey, GALEX, UKIDSS, and WISE), thus covering a
        wide range of wavelengths from the UV to the mid-infrared. The
        method is able (1) to achieve a very high accuracy, (2) to
        drastically reduce the number of outliers and catastrophic
        objects, and (3) to discriminate among parameters (or features)
        on the basis of their significance, so that the number of
        features used for training and analysis can be optimized in
        order to reduce both the computational demands and the effects
        of degeneracy. The best experiment, which makes use of a
        selected combination of parameters drawn from the four surveys,
        leads, in terms of {\ensuremath{\Delta}}z $_{norm}$ (i.e., (z
        $_{spec}$ - z $_{phot}$)/(1 + z $_{spec}$)), to an average of
        {\ensuremath{\Delta}}z $_{norm}$ = 0.004, a standard deviation
        of {\ensuremath{\sigma}} = 0.069, and a median absolute
        deviation, MAD = 0.02, over the whole redshift range (i.e., z
        $_{spec}$ \&lt;= 3.6), defined by the four-survey cross-matched
        spectroscopic sample. The fraction of catastrophic outliers,
        i.e., of objects with photo-z deviating more than
        2{\ensuremath{\sigma}} from the spectroscopic value, is
        \&lt;3\%, leading to {\ensuremath{\sigma}} = 0.035 after their
        removal, over the same redshift range. The method is made
        available to the community through the DAMEWARE Web application.}",
          doi = {10.1088/0004-637X/772/2/140},
archivePrefix = {arXiv},
       eprint = {1305.5641},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013ApJ...772..140B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013AJ....146...22Z,
       author = {{Zhang}, Yanxia and {Ma}, He and {Peng}, Nanbo and {Zhao}, Yongheng and
         {Wu}, Xue-bing},
        title = "{Estimating Photometric Redshifts of Quasars via the k-nearest Neighbor Approach Based on Large Survey Databases}",
      journal = {\aj},
     keywords = {catalogs, galaxies: distances and redshifts, methods: statistical, quasars: general, surveys, techniques: photometric, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2013",
        month = "Aug",
       volume = {146},
       number = {2},
          eid = {22},
        pages = {22},
     abstract = "{We apply one of the lazy learning methods, the k-nearest neighbor (kNN)
        algorithm, to estimate the photometric redshifts of quasars
        based on various data sets from the Sloan Digital Sky Survey
        (SDSS), the UKIRT Infrared Deep Sky Survey (UKIDSS), and the
        Wide-field Infrared Survey Explorer (WISE; the SDSS sample, the
        SDSS-UKIDSS sample, the SDSS-WISE sample, and the SDSS-UKIDSS-
        WISE sample). The influence of the k value and different input
        patterns on the performance of kNN is discussed. kNN performs
        best when k is different with a special input pattern for a
        special data set. The best result belongs to the SDSS-UKIDSS-
        WISE sample. The experimental results generally show that the
        more information from more bands, the better performance of
        photometric redshift estimation with kNN. The results also
        demonstrate that kNN using multiband data can effectively solve
        the catastrophic failure of photometric redshift estimation,
        which is met by many machine learning methods. Compared with the
        performance of various other methods of estimating the
        photometric redshifts of quasars, kNN based on KD-Tree shows
        superiority, exhibiting the best accuracy.}",
          doi = {10.1088/0004-6256/146/2/22},
archivePrefix = {arXiv},
       eprint = {1305.5023},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013AJ....146...22Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013A&C.....2...67S,
       author = {{Shamir}, Lior and {Holincheck}, Anthony and {Wallin}, John},
        title = "{Automatic quantitative morphological analysis of interacting galaxies}",
      journal = {Astronomy and Computing},
     keywords = {Galaxies: structure, Galaxies: evolution, Methods: analytical, Techniques: image processing, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Extragalactic Astrophysics},
         year = "2013",
        month = "Aug",
       volume = {2},
        pages = {67-73},
     abstract = "{The large number of galaxies imaged by digital sky surveys reinforces
        the need for computational methods for analyzing galaxy
        morphology. While the morphology of most galaxies can be
        associated with a stage on the Hubble sequence, the morphology
        of galaxy mergers is far more complex due to the combination of
        two or more galaxies with different morphologies and the
        interaction between them. Here we propose a computational method
        based on unsupervised machine learning that can quantitatively
        analyze morphologies of galaxy mergers and associate galaxies by
        their morphology. The method works by first generating multiple
        synthetic galaxy models for each galaxy merger, and then
        extracting a large set of numerical image content descriptors
        for each galaxy model. These numbers are weighted using Fisher
        discriminant scores, and then the similarities between the
        galaxy mergers are deduced using a variation of Weighted Nearest
        Neighbor analysis such that the Fisher scores are used as
        weights. The similarities between the galaxy mergers are
        visualized using phylogenies to provide a graph that reflects
        the morphological similarities between the different galaxy
        mergers, and thus quantitatively profile the morphology of
        galaxy mergers.}",
          doi = {10.1016/j.ascom.2013.09.002},
archivePrefix = {arXiv},
       eprint = {1309.4014},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013A&C.....2...67S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013A&C.....2...46V,
       author = {{Vilalta}, Ricardo and {Gupta}, Kinjal Dhar and {Macri}, Lucas},
        title = "{A machine learning approach to Cepheid variable star classification using data alignment and maximum likelihood}",
      journal = {Astronomy and Computing},
     keywords = {Machine learning, Classification, Dataset shift, Covariate shift, Cepheid stars},
         year = "2013",
        month = "Aug",
       volume = {2},
        pages = {46-53},
     abstract = "{Our study centers on the classification of two subtypes of Cepheid
        variable stars. Such a classification is relatively easy to
        obtain for nearby galaxies, but as we incorporate new galaxies,
        the cost of labeling stars calls for some form of model
        adaptation. Adapting a predictive model to differentiate
        Cepheids across galaxies is difficult because of the sample bias
        problem in star distribution (due to the limitation of
        telescopes in observing faint stars as we try to reach distant
        galaxies). In addition, estimating the luminosity of a star as
        we reach distant galaxies carries some inevitable shift in the
        data distribution. We propose an approach to predict the class
        of Cepheid stars on a target domain, by first building a model
        on an {\textquotedblleft}anchor{\textquotedblright} source
        domain. Our methodology then shifts the target data until it is
        well aligned with the source data by maximizing two different
        likelihood functions. Experimental results with two galaxy
        datasets (Large Magellanic Cloud as the source domain, and M33
        as the target domain), show the efficacy of the proposed method.}",
          doi = {10.1016/j.ascom.2013.07.002},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013A&C.....2...46V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013MNRAS.433..688L,
       author = {{Lee}, K.~J. and {Stovall}, K. and {Jenet}, F.~A. and {Martinez}, J. and
         {Dartez}, L.~P. and {Mata}, A. and {Lunsford}, G. and {Cohen}, S. and
         {Biwer}, C.~M. and {Rohr}, M. and {Flanigan}, J. and {Walker}, A. and
         {Banaszak}, S. and {Allen}, B. and {Barr}, E.~D. and {Bhat}, N.~D.~R. and
         {Bogdanov}, S. and {Brazier}, A. and {Camilo}, F. and
         {Champion}, D.~J. and {Chatterjee}, S. and {Cordes}, J. and
         {Crawford}, F. and {Deneva}, J. and {Desvignes}, G. and
         {Ferdman}, R.~D. and {Freire}, P. and {Hessels}, J.~W.~T. and
         {Karuppusamy}, R. and {Kaspi}, V.~M. and {Knispel}, B. and
         {Kramer}, M. and {Lazarus}, P. and {Lynch}, R. and {Lyne}, A. and
         {McLaughlin}, M. and {Ransom}, S. and {Scholz}, P. and {Siemens}, X. and
         {Spitler}, L. and {Stairs}, I. and {Tan}, M. and {van Leeuwen}, J. and
         {Zhu}, W.~W.},
        title = "{PEACE: pulsar evaluation algorithm for candidate extraction - a software package for post-analysis processing of pulsar survey candidates}",
      journal = {\mnras},
     keywords = {methods: statistical, pulsars: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2013",
        month = "Jul",
       volume = {433},
       number = {1},
        pages = {688-694},
     abstract = "{Modern radio pulsar surveys produce a large volume of prospective
        candidates, the majority of which are polluted by human-created
        radio frequency interference or other forms of noise. Typically,
        large numbers of candidates need to be visually inspected in
        order to determine if they are real pulsars. This process can be
        labour intensive. In this paper, we introduce an algorithm
        called Pulsar Evaluation Algorithm for Candidate Extraction
        (PEACE) which improves the efficiency of identifying pulsar
        signals. The algorithm ranks the candidates based on a score
        function. Unlike popular machine-learning-based algorithms, no
        prior training data sets are required. This algorithm has been
        applied to data from several large-scale radio pulsar surveys.
        Using the human-based ranking results generated by students in
        the Arecibo Remote Command Center programme, the statistical
        performance of PEACE was evaluated. It was found that PEACE
        ranked 68 per cent of the student-identified pulsars within the
        top 0.17 per cent of sorted candidates, 95 per cent within the
        top 0.34 per cent and 100 per cent within the top 3.7 per cent.
        This clearly demonstrates that PEACE significantly increases the
        pulsar identification rate by a factor of about 50 to 1000. To
        date, PEACE has been directly responsible for the discovery of
        47 new pulsars, 5 of which are millisecond pulsars that may be
        useful for pulsar timing based gravitational-wave detection
        projects.}",
          doi = {10.1093/mnras/stt758},
archivePrefix = {arXiv},
       eprint = {1305.0447},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013MNRAS.433..688L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013MNRAS.432.1483C,
       author = {{Carrasco Kind}, Matias and {Brunner}, Robert J.},
        title = "{TPZ: photometric redshift PDFs and ancillary information by using prediction trees and random forests}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, surveys, galaxies: distances and redshift, galaxies: statistics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2013",
        month = "Jun",
       volume = {432},
       number = {2},
        pages = {1483-1501},
     abstract = "{With the growth of large photometric surveys, accurately estimating
        photometric redshifts, preferably as a probability density
        function (PDF), and fully understanding the implicit systematic
        uncertainties in this process, has become increasingly
        important. In this paper, we present a new, publicly available,
        parallel, machine learning algorithm that generates photometric
        redshift PDFs by using prediction trees and random forest
        techniques, which we have named TPZ.$^{1}$ This new algorithm
        incorporates measurement errors into the calculation while also
        dealing efficiently with missing values in the data. In
        addition, our implementation of this algorithm provides
        supplementary information regarding the data being analysed,
        including unbiased estimates of the accuracy of the technique
        without resorting to a validation data set, identification of
        poor photometric redshift areas within the parameter space
        occupied by the spectroscopic training data, a quantification of
        the relative importance of the variables used to construct the
        PDF, and a robust identification of outliers. This extra
        information can be used to optimally target new spectroscopic
        observations and to improve the overall efficacy of the redshift
        estimation. We have tested TPZ on galaxy samples drawn from the
        Sloan Digital Sky Survey (SDSS) main galaxy sample and from the
        Deep Extragalactic Evolutionary Probe-2 (DEEP2) survey,
        obtaining excellent results in each case. We also have tested
        our implementation by participating in the PHAT1 project, which
        is a blind photometric redshift contest, finding that TPZ
        performs comparable to if not better than other empirical
        photometric redshift algorithms. Finally, we discuss the various
        parameters that control the operation of TPZ, the specific
        limitations of this approach and an application of photometric
        redshift PDFs.}",
          doi = {10.1093/mnras/stt574},
archivePrefix = {arXiv},
       eprint = {1303.7269},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013MNRAS.432.1483C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013SoPh..283..157A,
       author = {{Ahmed}, Omar W. and {Qahwaji}, Rami and {Colak}, Tufan and
         {Higgins}, Paul A. and {Gallagher}, Peter T. and {Bloomfield}, D. Shaun},
        title = "{Solar Flare Prediction Using Advanced Feature Extraction, Machine Learning, and Feature Selection}",
      journal = {\solphys},
     keywords = {Active regions, magnetic fields, Flares, forecasting, Photosphere, Space weather, Feature extraction, Machine learning, Feature selection},
         year = "2013",
        month = "Mar",
       volume = {283},
       number = {1},
        pages = {157-175},
     abstract = "{Novel machine-learning and feature-selection algorithms have been
        developed to study: i) the flare-prediction-capability of
        magnetic feature (MF) properties generated by the recently
        developed Solar Monitor Active Region Tracker ( SMART); ii)
        SMART's MF properties that are most significantly related to
        flare occurrence. Spatiotemporal association algorithms are
        developed to associate MFs with flares from April 1996 to
        December 2010 in order to differentiate flaring and non-flaring
        MFs and enable the application of machine-learning and feature-
        selection algorithms. A machine-learning algorithm is applied to
        the associated datasets to determine the flare-prediction-
        capability of all 21 SMART MF properties. The prediction
        performance is assessed using standard forecast-verification
        measures and compared with the prediction measures of one of the
        standard technologies for flare-prediction that is also based on
        machine-learning: Automated Solar Activity Prediction ( ASAP).
        The comparison shows that the combination of SMART MFs with
        machine-learning has the potential to achieve more accurate
        flare-prediction than ASAP. Feature-selection algorithms are
        then applied to determine the MF properties that are most
        related to flare occurrence. It is found that a reduced set of
        six MF properties can achieve a similar degree of prediction
        accuracy as the full set of 21 SMART MF properties.}",
          doi = {10.1007/s11207-011-9896-1},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013SoPh..283..157A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013SoPh..283..143C,
       author = {{Colak}, T. and {Qahwaji}, R.},
        title = "{Prediction of Extreme Ultraviolet Variability Experiment (EVE)/ Extreme Ultraviolet Spectro-Photometer (ESP) Irradiance from Solar Dynamics Observatory (SDO)/ Atmospheric Imaging Assembly (AIA) Images Using Fuzzy Image Processing and Machine Learning}",
      journal = {\solphys},
     keywords = {Solar imaging, SDO/AIA, SDO/EVE, Irradiance construction},
         year = "2013",
        month = "Mar",
       volume = {283},
       number = {1},
        pages = {143-156},
     abstract = "{The cadence and resolution of solar images have been increasing
        dramatically with the launch of new spacecraft such as STEREO
        and SDO. This increase in data volume provides new opportunities
        for solar researchers, but the efficient processing and analysis
        of these data create new challenges. We introduce a fuzzy-based
        solar feature-detection system in this article. The proposed
        system processes SDO/AIA images using fuzzy rules to detect
        coronal holes and active regions. This system is fast and it can
        handle different size images. It is tested on six months of
        solar data (1 October 2010 to 31 March 2011) to generate filling
        factors (ratio of area of solar feature to area of rest of the
        solar disc) for active regions and coronal holes. These filling
        factors are then compared to SDO/EVE/ESP irradiance
        measurements. The correlation between active-region filling
        factors and irradiance measurements is found to be very high,
        which has encouraged us to design a time-series prediction
        system using Radial Basis Function Networks to predict ESP
        irradiance measurements from our generated filling factors.}",
          doi = {10.1007/s11207-011-9880-9},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013SoPh..283..143C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013MNRAS.429L.109M,
       author = {{Mirabal}, N.},
        title = "{The dark knight falters.}",
      journal = {\mnras},
     keywords = {galaxies: active, dark matter, Astrophysics - High Energy Astrophysical Phenomena, High Energy Physics - Phenomenology},
         year = "2013",
        month = "Feb",
       volume = {429},
        pages = {L109-L113},
     abstract = "{Tentative line emission at 111 and 129 GeV from 16 unassociated Fermi-
        LAT point sources has been reported recently by Su and
        Finkbeiner. Together with similar features seen by Fermi in a
        region near the Galactic Centre, the evidence has been
        interpreted as the spectral signature of dark matter
        annihilation or internal bremsstrahlung. Through a combination
        of supervised machine-learning algorithms and archival
        multiwavelength observations, we find that 14 out of the 16
        unassociated sources showing that the line emission in the Su
        and Finkbeiner sample are most likely active galactic nuclei
        (AGN). Based on this new evidence, one must widen the range of
        possible solutions for the 100-140 GeV excess to include a very
        distinct astrophysical explanation. While we cannot rule out a
        dark matter origin for the line emission in the Galactic Centre,
        we posit that if the detection in the Su and Finkbeiner sample
        is indeed real it might be related to accretion, bubble or jet
        activity in nearby (z \&lt; 0.2) AGN. Alternatively, given the
        right conditions, the similarity could be due to a chance
        occurrence caused by extragalactic background light absorption.
        Or else one must concede that the features are an artefact of
        instrumental or calibration issues.}",
          doi = {10.1093/mnrasl/sls034},
archivePrefix = {arXiv},
       eprint = {1208.1693},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013MNRAS.429L.109M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013A&A...550A.120S,
       author = {{Sarro}, L.~M. and {Debosscher}, J. and {Neiner}, C. and
         {Bello-Garc{\'\i}a}, A. and {Gonz{\'a}lez-Marcos}, A. and
         {Prendes-Gero}, B. and {Ordieres}, J. and {Le{\'o}n}, G. and
         {Aerts}, C. and {de Batz}, B.},
        title = "{Improved variability classification of CoRoT targets with Giraffe spectra}",
      journal = {\aap},
     keywords = {stars: variables: general, stars: oscillations, techniques: spectroscopic, stars: fundamental parameters, methods: statistical, methods: data analysis},
         year = "2013",
        month = "Feb",
       volume = {550},
          eid = {A120},
        pages = {A120},
     abstract = "{<BR /> Aims: We present an improved method for automated stellar
        variability classification, using fundamental parameters derived
        from high resolution spectra, with the goal to improve the
        variability classification obtained using information derived
        from CoRoT light curves only. Although we focus on Giraffe
        spectra and CoRoT light curves in this work, the methods are
        much more widely applicable. <BR /> Methods: In order to improve
        the variability classification obtained from the photometric
        time series, only rough estimates of the stellar physical
        parameters (T$_{eff}$ and log (g)) are needed because most
        variability types that overlap in the space of time series
        parameters, are well separated in the space of physical
        parameters (e.g. {\ensuremath{\gamma}} Dor/SPB or
        {\ensuremath{\delta}} Sct/{\ensuremath{\beta}} Cep). In this
        work, several state-of-the-art machine learning techniques are
        combined to estimate these fundamental parameters from high
        resolution Giraffe spectra. Next, these parameters are used in a
        multi-stage Gaussian-Mixture classifier to perform an improved
        supervised variability classification of CoRoT light curves. The
        variability classifier can be used independently of the
        regression module that estimates the physical parameters, so
        that non-spectroscopic estimates derived e.g. from photometric
        colour indices can be used instead. <BR /> Results: T$_{eff}$
        and log (g) are derived from Giraffe spectra, for 6832 CoRoT
        targets. The use of those parameters in addition to information
        extracted from the CoRoT light curves, significantly improves
        the results of our previous automated stellar variability
        classification. Several new pulsating stars are identified with
        high confidence levels, including hot pulsators such as SPB and
        {\ensuremath{\beta}} Cep, and several {\ensuremath{\gamma}}
        Dor-{\ensuremath{\delta}} Sct hybrids. From our samples of new
        {\ensuremath{\gamma}} Dor and {\ensuremath{\delta}} Sct stars,
        we find strong indications that the instability domains for both
        types of pulsators are larger than previously thought. The CoRoT
        space mission, launched on 27 December 2006, has been developed
        and is operated by CNES, with the contribution of Austria,
        Belgium, Brazil, ESA (RSSD and Science Programmes), Germany, and
        Spain.Full Table 2 is only available in electronic form at the
        CDS via anonymous ftp to <A
        href=``http://cdsarc.u-strasbg.fr''>cdsarc.u-strasbg.fr</A> (<A
        href=``http://130.79.128.5''>130.79.128.5</A>) or via <A
        href=``http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/550/A120''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/550/A120</A>}",
          doi = {10.1051/0004-6361/201220184},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013A&A...550A.120S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013SoPh..282..319A,
       author = {{Al-Omari}, M. and {Qahwaji}, R. and {Colak}, T. and {Ipson}, S.},
        title = "{Erratum: Erratum to: Machine Learning-Based Investigation of the Associations between CMEs and Filaments}",
      journal = {\solphys},
         year = "2013",
        month = "Jan",
       volume = {282},
       number = {1},
        pages = {319-319},
          doi = {10.1007/s11207-012-0163-x},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013SoPh..282..319A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2013PhDT.......531M,
       author = {{Miller}, Adam Andrew},
        title = "{Time-Domain Studies as a Probe of Stellar Evolution}",
     keywords = {Physics, Astronomy and Astrophysics;Physics, Astrophysics;Physics, General},
       school = {University of California, Berkeley},
         year = "2013",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013PhDT.......531M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2013PhDT.......403B,
       author = {{Beaumont}, Christopher Norris},
        title = "{Morphological diagnostics of star formation in molecular clouds}",
     keywords = {Physics, Astronomy and Astrophysics},
       school = {University of Hawai'i at Manoa},
         year = "2013",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013PhDT.......403B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2013PhDT.......337B,
       author = {{Bunte}, Melissa K.},
        title = "{Utilizing Science and Technology to Enhance a Future Planetary Mission: Applications to Europa}",
     keywords = {Geomorphology;Planetology;Remote Sensing},
       school = {Arizona State University},
         year = "2013",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013PhDT.......337B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013MNRAS.428..220H,
       author = {{Hassan}, T. and {Mirabal}, N. and {Contreras}, J.~L. and {Oya}, I.},
        title = "{Gamma-ray active galactic nucleus type through machine-learning algorithms}",
      journal = {\mnras},
     keywords = {galaxies: active, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2013",
        month = "Jan",
       volume = {428},
       number = {1},
        pages = {220-225},
     abstract = "{The Fermi Gamma-ray Space Telescope (Fermi) is producing the most
        detailed inventory of the gamma-ray sky to date. Despite
        tremendous achievements approximately 25 per cent of all Fermi
        extragalactic sources in the Second Fermi Large Area Telescope
        Catalogue (2FGL) are listed as active galactic nuclei (AGN) of
        uncertain type. Typically, these are suspected blazar candidates
        without a conclusive optical spectrum or lacking spectroscopic
        observations. Here, we explore the use of machine-learning
        algorithms - random forests and support vector machines - to
        predict specific AGN subclass based on observed gamma-ray
        spectral properties. After training and testing on
        identified/associated AGN from the 2FGL we find that 235 out of
        269 AGN of uncertain type have properties compatible with gamma-
        ray BL Lacertae and flat-spectrum radio quasars with accuracy
        rates of 85 per cent. Additionally, direct comparison of our
        results with class predictions made after following the infrared
        colour-colour space of Massaro et al. shows that the agreement
        rate is over four-fifths for 54 overlapping sources, providing
        independent cross-validation. These results can help tailor
        follow-up spectroscopic programmes and inform future pointed
        surveys with ground-based Cherenkov telescopes.}",
          doi = {10.1093/mnras/sts022},
archivePrefix = {arXiv},
       eprint = {1209.4359},
 primaryClass = {astro-ph.HE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013MNRAS.428..220H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2013A&A...549A.127H,
       author = {{Huang}, X. and {Zhang}, L. and {Wang}, H. and {Li}, L.},
        title = "{Improving the performance of solar flare prediction using active longitudes information}",
      journal = {\aap},
     keywords = {Sun: flares, Sun: activity, Sun: magnetic topology},
         year = "2013",
        month = "Jan",
       volume = {549},
          eid = {A127},
        pages = {A127},
     abstract = "{Context. Solar flare prediction models normally depend on properties of
        active regions, such as sunspot area, McIntosh classifications,
        Mount Wilson classifications, and various measures of the
        magnetic field. Nevertheless, the positional information of
        active regions has not been used. <BR /> Aims: We define a
        metric, D$_{ARAL}$ (distance between active regions and
        predicted active longitudes), to depict the positional
        relationship between active regions and predicted active
        longitudes and add D$_{ARAL}$ to our solar flare prediction
        model to improve its performance. <BR /> Methods: Combining
        D$_{ARAL}$ with other solar magnetic field parameters, we build
        a solar flare prediction model with the instance-based learning
        method, which is a simple and effective algorithm in machine
        learning. We extracted 70 078 active region instances from the
        Solar and Heliospheric Observatory (SOHO)/Michelson Doppler
        Imager (MDI) magnetograms containing 1055 National Oceanic and
        Atmospheric Administration (NOAA) active regions within
        30{\textdegree} of the solar disk center from 1996 to 2007 and
        used them to train and test the solar flare prediction model.
        <BR /> Results: Using four performance measures (true positive
        rate, true negative rate, true skill statistic, and Heidke skill
        score), we compare performances of the solar flare prediction
        model with and without D$_{ARAL}$. True positive rate, true
        negative rate, true skill statistic, and Heidke skill score
        increase by 6.7\% {\ensuremath{\pm}} 1.3\%, 4.2\%
        {\ensuremath{\pm}} 0.5\%, 10.8\% {\ensuremath{\pm}} 1.4\% and
        8.7\% {\ensuremath{\pm}} 1.0\%, respectively. <BR />
        Conclusions: The comparison indicates that the metric D$_{ARAL}$
        is beneficial to performances of the solar flare prediction
        model.}",
          doi = {10.1051/0004-6361/201219742},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013A&A...549A.127H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012MNRAS.427.1284P,
       author = {{Pichara}, K. and {Protopapas}, P. and {Kim}, D. -W. and
         {Marquette}, J. -B. and {Tisserand}, P.},
        title = "{An improved quasar detection method in EROS-2 and MACHO LMC data sets}",
      journal = {\mnras},
     keywords = {methods: data analysis, Magellanic Clouds, quasars: general, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Machine Learning},
         year = "2012",
        month = "Dec",
       volume = {427},
       number = {2},
        pages = {1284-1297},
     abstract = "{We present a new classification method for quasar identification in the
        EROS-2 and MACHO data sets based on a boosted version of a
        random forest classifier. We use a set of variability features
        including parameters of a continuous autoregressive model. We
        prove that continuous autoregressive parameters are very
        important discriminators in the classification process. We
        create two training sets (one for EROS-2 and one for MACHO data
        sets) using known quasars found in the Large Magellanic Cloud
        (LMC). Our model's accuracy in both EROS-2 and MACHO training
        sets is about 90 per cent precision and 86 per cent recall,
        improving the state-of-the-art models, accuracy in quasar
        detection. We apply the model on the complete, including 28
        million objects, EROS-2 and MACHO LMC data sets, finding 1160
        and 2551 candidates, respectively. To further validate our list
        of candidates, we cross-matched our list with 663 previously
        known strong candidates, getting 74 per cent of matches for
        MACHO and 40 per cent in EROS. The main difference on matching
        level is because EROS-2 is a slightly shallower survey which
        translates to significantly lower signal-to-noise ratio light
        curves.}",
          doi = {10.1111/j.1365-2966.2012.22061.x},
archivePrefix = {arXiv},
       eprint = {1304.0401},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012MNRAS.427.1284P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012ApJS..203...32R,
       author = {{Richards}, Joseph W. and {Starr}, Dan L. and {Miller}, Adam A. and
         {Bloom}, Joshua S. and {Butler}, Nathaniel R. and {Brink}, Henrik and
         {Crellin-Quick}, Arien},
        title = "{Construction of a Calibrated Probabilistic Classification Catalog: Application to 50k Variable Sources in the All-Sky Automated Survey}",
      journal = {\apjs},
     keywords = {catalogs, methods: data analysis, methods: statistical, stars: variables: general, techniques: photometric, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics, Statistics - Applications},
         year = "2012",
        month = "Dec",
       volume = {203},
       number = {2},
          eid = {32},
        pages = {32},
     abstract = "{With growing data volumes from synoptic surveys, astronomers necessarily
        must become more abstracted from the discovery and introspection
        processes. Given the scarcity of follow-up resources, there is a
        particularly sharp onus on the frameworks that replace these
        human roles to provide accurate and well-calibrated
        probabilistic classification catalogs. Such catalogs inform the
        subsequent follow-up, allowing consumers to optimize the
        selection of specific sources for further study and permitting
        rigorous treatment of classification purities and efficiencies
        for population studies. Here, we describe a process to produce a
        probabilistic classification catalog of variability with machine
        learning from a multi-epoch photometric survey. In addition to
        producing accurate classifications, we show how to estimate
        calibrated class probabilities and motivate the importance of
        probability calibration. We also introduce a methodology for
        feature-based anomaly detection, which allows discovery of
        objects in the survey that do not fit within the predefined
        class taxonomy. Finally, we apply these methods to sources
        observed by the All-Sky Automated Survey (ASAS), and release the
        Machine-learned ASAS Classification Catalog (MACC), a 28 class
        probabilistic classification catalog of 50,124 ASAS sources in
        the ASAS Catalog of Variable Stars. We estimate that MACC
        achieves a sub-20\% classification error rate and demonstrate
        that the class posterior probabilities are reasonably
        calibrated. MACC classifications compare favorably to the
        classifications of several previous domain-specific ASAS papers
        and to the ASAS Catalog of Variable Stars, which had classified
        only 24\% of those sources into one of 12 science classes.}",
          doi = {10.1088/0067-0049/203/2/32},
archivePrefix = {arXiv},
       eprint = {1204.4180},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012ApJS..203...32R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012PASP..124.1175B,
       author = {{Bloom}, J.~S. and {Richards}, J.~W. and {Nugent}, P.~E. and
         {Quimby}, R.~M. and {Kasliwal}, M.~M. and {Starr}, D.~L. and
         {Poznanski}, D. and {Ofek}, E.~O. and {Cenko}, S.~B. and
         {Butler}, N.~R. and {Kulkarni}, S.~R. and {Gal-Yam}, A. and {Law}, N.},
        title = "{Automating Discovery and Classification of Transients and Variable Stars in the Synoptic Survey Era}",
      journal = {\pasp},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2012",
        month = "Nov",
       volume = {124},
       number = {921},
        pages = {1175},
     abstract = "{The rate of image acquisition in modern synoptic imaging surveys has
        already begun to outpace the feasibility of keeping astronomers
        in the real-time discovery and classification loop. Here we
        present the inner workings of a framework, based on machine-
        learning algorithms, that captures expert training and ground-
        truth knowledge about the variable and transient sky to automate
        (1) the process of discovery on image differences, and (2) the
        generation of preliminary science-type classifications of
        discovered sources. Since follow-up resources for extracting
        novel science from fast-changing transients are precious, self-
        calibrating classification probabilities must be couched in
        terms of efficiencies for discovery and purity of the samples
        generated. We estimate the purity and efficiency in identifying
        real sources with a two-epoch image-difference discovery
        algorithm for the Palomar Transient Factory (PTF) survey. Once
        given a source discovery, using machine-learned classification
        trained on PTF data, we distinguish between transients and
        variable stars with a 3.8\% overall error rate (with 1.7\%
        errors for imaging within the Sloan Digital Sky Survey
        footprint). At \&gt;96\% classification efficiency, the samples
        achieve 90\% purity. Initial classifications are shown to rely
        primarily on context-based features, determined from the data
        itself and external archival databases. In the first year of
        autonomous operations of PTF, this discovery and classification
        framework led to several significant science results, from
        outbursting young stars to subluminous Type IIP supernovae to
        candidate tidal disruption events. We discuss future directions
        of this approach, including the possible roles of crowdsourcing
        and the scalability of machine learning to future surveys such
        as the Large Synoptic Survey Telescope (LSST).}",
          doi = {10.1086/668468},
archivePrefix = {arXiv},
       eprint = {1106.5491},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012PASP..124.1175B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012MNRAS.427L..35K,
       author = {{Kitaura}, Francisco-Shu and {Erdo{\v{g}}du}, Pirin and
         {Nuza}, Sebasti{\'a}n. E. and {Khalatyan}, Arman and {Angulo}, Raul E. and
         {Hoffman}, Yehuda and {Gottl{\"o}ber}, Stefan},
        title = "{Cosmic structure and dynamics of the local Universe}",
      journal = {\mnras},
     keywords = {catalogues, galaxies: clusters: general, galaxies: kinematics and dynamics, Local group, galaxies: statistics, dark matter, large-scale structure of Universe, Astrophysics - Cosmology and Extragalactic Astrophysics, Statistics - Applications},
         year = "2012",
        month = "Nov",
       volume = {427},
       number = {1},
        pages = {L35-L39},
     abstract = "{We present a cosmography analysis of the local Universe based on the
        recently released Two-Micron All-Sky Redshift Survey catalogue.
        Our method is based on a Bayesian Networks Machine Learning
        algorithm (the KIGEN-code) which self-consistently samples the
        initial density fluctuations compatible with the observed galaxy
        distribution and a structure formation model given by second-
        order Lagrangian perturbation theory (2LPT). From the initial
        conditions we obtain an ensemble of reconstructed density and
        peculiar velocity fields which characterize the local cosmic
        structure with high accuracy unveiling non-linear structures
        like filaments and voids in detail. Coherent redshift-space
        distortions are consistently corrected within 2LPT. From the
        ensemble of cross-correlations between the reconstructions and
        the galaxy field and the variance of the recovered density
        fields, we find that our method is extremely accurate up to k̃ 1
        h Mpc$^{-1}$ and still yields reliable results down to scales of
        about 3-4 h$^{-1}$ Mpc. The motion of the Local Group we obtain
        within ̃80 h$^{-1}$ Mpc (v$_{LG}$ = 522 {\ensuremath{\pm}} 86 km
        s$^{-1}$, l$_{LG}$ = 291{\textdegree} {\ensuremath{\pm}}
        16{\textdegree}, b$_{LG}$ = 34{\textdegree} {\ensuremath{\pm}}
        8{\textdegree}) is in good agreement with measurements derived
        from the cosmic microwave background and from direct
        observations of peculiar motions and is consistent with the
        predictions of {\ensuremath{\Lambda}}CDM.}",
          doi = {10.1111/j.1745-3933.2012.01340.x},
archivePrefix = {arXiv},
       eprint = {1205.5560},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012MNRAS.427L..35K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012MNRAS.426.2463L,
       author = {{Liu}, C. and {Bailer-Jones}, C.~A.~L. and {Sordo}, R. and
         {Vallenari}, A. and {Borrachero}, R. and {Luri}, X. and
         {Sartoretti}, P.},
        title = "{The expected performance of stellar parametrization with Gaia spectrophotometry}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, stars: fundamental parameters, Hertzsprung, Russell and colour, magnitude diagram, dust, extinction, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
         year = "2012",
        month = "Nov",
       volume = {426},
       number = {3},
        pages = {2463-2482},
     abstract = "{Gaia will obtain astrometry and spectrophotometry for essentially all
        sources in the sky down to a broad-band magnitude limit of G =
        20, an expected yield of {}10$^{9}$ stars. Its main scientific
        objective is to reveal the formation and evolution of our Galaxy
        through chemodynamical analysis. In addition to inferring
        positions, parallaxes and proper motions from the astrometry, we
        must also infer the astrophysical parameters of the stars from
        the spectrophotometry, the blue photometer (BP)/red photometer
        (RP) spectrum. Here we investigate the performance of three
        different algorithms [Support Vector Machine (SVM), ILIUM and
        Aeneas] for estimating the effective temperature, line-of-sight
        interstellar extinction, metallicity and surface gravity of A-M
        stars over a wide range of these parameters and over the full
        magnitude range Gaia will observe (G = 6-20 mag). One of the
        algorithms, Aeneas, infers the posterior probability density
        function over all parameters, and can optionally take into
        account the parallax and the Hertzsprung-Russell diagram to
        improve the estimates. For all algorithms the accuracy of
        estimation depends on G and on the value of the parameters
        themselves, so a broad summary of performance is only
        approximate. For stars at G = 15 with less than 2 mag
        extinction, we expect to be able to estimate T$_{eff}$ to within
        1 per cent, log g to 0.1-0.2 dex and [Fe/H] (for FGKM stars) to
        0.1-0.2 dex, just using the BP/RP spectrum (mean absolute error
        statistics are quoted). Performance degrades at larger
        extinctions, but not always by a large amount. Extinction can be
        estimated to an accuracy of 0.05-0.2 mag for stars across the
        full parameter range with a priori unknown extinction between 0
        and 10 mag. Performance degrades at fainter magnitudes, but even
        at G = 19 we can estimate log g to better than 0.2 dex for all
        spectral types and [ Fe /H] to within 0.35 dex for FGKM stars,
        for extinctions below 1 mag.}",
          doi = {10.1111/j.1365-2966.2012.21797.x},
archivePrefix = {arXiv},
       eprint = {1207.6005},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012MNRAS.426.2463L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012GeoRL..3919701E,
       author = {{Ebert-Uphoff}, Imme and {Deng}, Yi},
        title = "{A new type of climate network based on probabilistic graphical models: Results of boreal winter versus summer}",
      journal = {\grl},
     keywords = {Informatics: Machine learning (0555), Atmospheric Processes: Climate change and variability (1616, 1635, 3309, 4215, 4513)},
         year = "2012",
        month = "Oct",
       volume = {39},
       number = {19},
          eid = {L19701},
        pages = {L19701},
     abstract = "{In this paper we introduce a new type of climate network based on
        temporal probabilistic graphical models. This new method is able
        to distinguish between direct and indirect connections and thus
        can eliminate indirect connections in the network. Furthermore,
        while correlation-based climate networks focus on similarity
        between nodes, this new method provides an alternative viewpoint
        by focusing on information flow within the network over time. We
        build a prototype of this new network utilizing daily values of
        500 mb geopotential height over the entire globe during the
        period 1948 to 2011. The basic network features are presented
        and compared between boreal winter and summer in terms of intra-
        location properties that measure local memory at a grid point
        and inter-location properties that quantify remote impact of a
        grid point. Results suggest that synoptic-scale, sub-weekly
        disturbances act as the main information carrier in this network
        and their intrinsic timescale limits the extent to which a grid
        point can influence its nearby locations. The frequent passage
        of these disturbances over storm track regions also uniquely
        determines the timescale of height fluctuations thus local
        memory at a grid point. The poleward retreat of synoptic-scale
        disturbances in boreal summer is largely responsible for a
        corresponding poleward shift of local maxima in local memory and
        remote impact, which is most evident in the North Pacific
        sector. For the NH as a whole, both local memory and remote
        impact strengthen from winter to summer leading to intensified
        information flow and more tightly-coupled network nodes during
        the latter period.}",
          doi = {10.1029/2012GL053269},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012GeoRL..3919701E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012A&A...546A..89B,
       author = {{Bailer-Jones}, C.~A.~L.},
        title = "{A Bayesian method for the analysis of deterministic and stochastic time series}",
      journal = {\aap},
     keywords = {methods: statistical, brown dwarfs, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
         year = "2012",
        month = "Oct",
       volume = {546},
          eid = {A89},
        pages = {A89},
     abstract = "{I introduce a general, Bayesian method for modelling univariate time
        series data assumed to be drawn from a continuous, stochastic
        process. The method accommodates arbitrary temporal sampling,
        and takes into account measurement uncertainties for arbitrary
        error models (not just Gaussian) on both the time and signal
        variables. Any model for the deterministic component of the
        variation of the signal with time is supported, as is any model
        of the stochastic component on the signal and time variables.
        Models illustrated here are constant and sinusoidal models for
        the signal mean combined with a Gaussian stochastic component,
        as well as a purely stochastic model, the Ornstein-Uhlenbeck
        process. The posterior probability distribution over model
        parameters is determined via Monte Carlo sampling. Models are
        compared using the ``cross-validation likelihood'', in which the
        posterior-averaged likelihood for different partitions of the
        data are combined. In principle this is more robust to changes
        in the prior than is the evidence (the prior-averaged
        likelihood). The method is demonstrated by applying it to the
        light curves of 11 ultra cool dwarf stars, claimed by a previous
        study to show statistically significant variability. This is
        reassessed here by calculating the cross-validation likelihood
        for various time series models, including a null hypothesis of
        no variability beyond the error bars. 10 of 11 light curves are
        confirmed as being significantly variable, and one of these
        seems to be periodic, with two plausible periods identified.
        Another object is best described by the Ornstein-Uhlenbeck
        process, a conclusion which is obviously limited to the set of
        models actually tested.}",
          doi = {10.1051/0004-6361/201220109},
archivePrefix = {arXiv},
       eprint = {1209.3730},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012A&A...546A..89B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012A&A...546A..13C,
       author = {{Cavuoti}, S. and {Brescia}, M. and {Longo}, G. and {Mercurio}, A.},
        title = "{Photometric redshifts with the quasi Newton algorithm (MLPQNA) Results in the PHAT1 contest}",
      journal = {\aap},
     keywords = {techniques: photometric, galaxies: distances and redshifts, galaxies: photometry, cosmology: observations, methods: data analysis, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Extragalactic Astrophysics},
         year = "2012",
        month = "Oct",
       volume = {546},
          eid = {A13},
        pages = {A13},
     abstract = "{Context. Since the advent of modern multiband digital sky surveys,
        photometric redshifts (photo-z's) have become relevant if not
        crucial to many fields of observational cosmology, such as the
        characterization of cosmic structures and the weak and strong
        lensing. <BR /> Aims: We describe an application to an
        astrophysical context, namely the evaluation of photometric
        redshifts, of MLPQNA, which is a machine-learning method based
        on the quasi Newton algorithm. <BR /> Methods: Theoretical
        methods for photo-z evaluation are based on the interpolation of
        a priori knowledge (spectroscopic redshifts or SED templates),
        and they represent an ideal comparison ground for neural
        network-based methods. The MultiLayer Perceptron with quasi
        Newton learning rule (MLPQNA) described here is an effective
        computing implementation of neural networks exploited for the
        first time to solve regression problems in the astrophysical
        context. It is offered to the community through the DAMEWARE
        (DAta Mining \&amp; Exploration Web Application REsource)
        infrastructure. <BR /> Results: The PHAT contest (Hildebrandt et
        al. 2010, A\&amp;A, 523, A31) provides a standard dataset to
        test old and new methods for photometric redshift evaluation and
        with a set of statistical indicators that allow a
        straightforward comparison among different methods. The MLPQNA
        model has been applied on the whole PHAT1 dataset of 1984
        objects after an optimization of the model performed with the
        515 available spectroscopic redshifts as training set. When
        applied to the PHAT1 dataset, MLPQNA obtains the best bias
        accuracy (0.0006) and very competitive accuracies in terms of
        scatter (0.056) and outlier percentage (16.3\%), scoring as the
        second most effective empirical method among those that have so
        far participated in the contest. MLPQNA shows better
        generalization capabilities than most other empirical methods
        especially in the presence of underpopulated regions of the
        knowledge base.}",
          doi = {10.1051/0004-6361/201219755},
archivePrefix = {arXiv},
       eprint = {1206.0876},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012A&A...546A..13C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012ApJ...756...67W,
       author = {{Wang}, Yuyang and {Khardon}, Roni and {Protopapas}, Pavlos},
        title = "{Nonparametric Bayesian Estimation of Periodic Light Curves}",
      journal = {\apj},
     keywords = {methods: data analysis, methods: statistical, stars: variables: general, Computer Science - Machine Learning, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2012",
        month = "Sep",
       volume = {756},
       number = {1},
          eid = {67},
        pages = {67},
     abstract = "{Many astronomical phenomena exhibit patterns that have periodic
        behavior. An important step when analyzing data from such
        processes is the problem of identifying the period: estimating
        the period of a periodic function based on noisy observations
        made at irregularly spaced time points. This problem is still a
        difficult challenge despite extensive study in different
        disciplines. This paper makes several contributions toward
        solving this problem. First, we present a nonparametric Bayesian
        model for period finding, based on Gaussian Processes (GPs),
        that does not make assumptions on the shape of the periodic
        function. As our experiments demonstrate, the new model leads to
        significantly better results in period estimation especially
        when the light curve does not exhibit sinusoidal shape. Second,
        we develop a new algorithm for parameter optimization for GP
        which is useful when the likelihood function is very sensitive
        to the parameters with numerous local minima, as in the case of
        period estimation. The algorithm combines gradient optimization
        with grid search and incorporates several mechanisms to overcome
        the high computational complexity of GP. Third, we develop a
        novel approach for using domain knowledge, in the form of a
        probabilistic generative model, and incorporate it into the
        period estimation algorithm. Experimental results validate our
        approach showing significant improvement over existing methods.}",
          doi = {10.1088/0004-637X/756/1/67},
archivePrefix = {arXiv},
       eprint = {1111.1315},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012ApJ...756...67W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012MNRAS.424.2832L,
       author = {{Lee}, K.~J. and {Guillemot}, L. and {Yue}, Y.~L. and {Kramer}, M. and
         {Champion}, D.~J.},
        title = "{Application of the Gaussian mixture model in pulsar astronomy - pulsar classification and candidates ranking for the Fermi 2FGL catalogue}",
      journal = {\mnras},
     keywords = {methods: statistical, pulsars: general, gamma-rays: stars, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2012",
        month = "Aug",
       volume = {424},
       number = {4},
        pages = {2832-2840},
     abstract = "{Machine learning, algorithms designed to extract empirical knowledge
        from data, can be used to classify data, which is one of the
        most common tasks in observational astronomy. In this paper, we
        focus on Bayesian data classification algorithms using the
        Gaussian mixture model and show two applications in pulsar
        astronomy. After reviewing the Gaussian mixture model and the
        related expectation-maximization algorithm, we present a data
        classification method using the Neyman-Pearson test. To
        demonstrate the method, we apply the algorithm to two
        classification problems. First, it is applied to the well-known
        period-period derivative diagram, where we find that the pulsar
        distribution can be modelled with six Gaussian clusters, with
        two clusters for millisecond pulsars (recycled pulsars) and the
        rest for normal pulsars. From this distribution, we derive an
        empirical definition for millisecond pulsars as \{P\textbackslas
        hdot;\}/\{10$^{-17}$\}{\ensuremath{\leq}}3.23(\{P\}/\{100ms\})$^
        {-2.34}$. The two millisecond pulsar clusters may have different
        evolutionary origins, since the companion stars to these pulsars
        in the two clusters show different chemical compositions. Four
        clusters are found for normal pulsars. Possible implications for
        these clusters are also discussed. Our second example is to
        calculate the likelihood of unidentified Fermi point sources
        being pulsars and rank them accordingly. In the ranked point-
        source list, the top 5 per cent sources contain 50 per cent
        known pulsars, the top 50 per cent contain 99 per cent known
        pulsars and no known active galaxy (the other major population)
        appears in the top 6 per cent. Such a ranked list can be used to
        help the future follow-up observations for finding pulsars in
        unidentified Fermi point sources.}",
          doi = {10.1111/j.1365-2966.2012.21413.x},
archivePrefix = {arXiv},
       eprint = {1205.6221},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012MNRAS.424.2832L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012ApJ...755..143T,
       author = {{Taghizadeh-Popp}, M. and {Heinis}, S. and {Szalay}, A.~S.},
        title = "{Single Parameter Galaxy Classification: The Principal Curve through the Multi-dimensional Space of Galaxy Properties}",
      journal = {\apj},
     keywords = {galaxies: fundamental parameters, galaxies: general, galaxies: luminosity function, mass function, galaxies: statistics, methods: data analysis, methods: statistical, Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
         year = "2012",
        month = "Aug",
       volume = {755},
       number = {2},
          eid = {143},
        pages = {143},
     abstract = "{We propose to describe the variety of galaxies from the Sloan Digital
        Sky Survey by using only one affine parameter. To this aim, we
        construct the principal curve (P-curve) passing through the
        spine of the data point cloud, considering the eigenspace
        derived from Principal Component Analysis (PCA) of
        morphological, physical, and photometric galaxy properties.
        Thus, galaxies can be labeled, ranked, and classified by a
        single arc-length value of the curve, measured at the unique
        closest projection of the data points on the P-curve. We find
        that the P-curve has a ``W'' letter shape with three turning
        points, defining four branches that represent distinct galaxy
        populations. This behavior is controlled mainly by two
        properties, namely u - r and star formation rate (from blue
        young at low arc length to red old at high arc length), while
        most other properties correlate well with these two. We further
        present the variations of several important galaxy properties as
        a function of arc length. Luminosity functions vary from steep
        Schechter fits at low arc length to double power law and ending
        in lognormal fits at high arc length. Galaxy clustering shows
        increasing autocorrelation power at large scales as arc length
        increases. Cross correlation of galaxies with different arc
        lengths shows that the probability of two galaxies belonging to
        the same halo decreases as their distance in arc length
        increases. PCA analysis allows us to find peculiar galaxy
        populations located apart from the main cloud of data points,
        such as small red galaxies dominated by a disk, of relatively
        high stellar mass-to-light ratio and surface mass density. On
        the other hand, the P-curve helped us understand the average
        trends, encoding 75\% of the available information in the data.
        The P-curve allows not only dimensionality reduction but also
        provides supporting evidence for the following relevant physical
        models and scenarios in extragalactic astronomy: (1) The
        hierarchical merging scenario in the formation of a selected
        group of red massive galaxies. These galaxies present a
        lognormal r-band luminosity function, which might arise from
        multiplicative processes involved in this scenario. (2) A
        connection between the onset of active galactic nucleus activity
        and star formation quenching as mentioned in Martin et al.,
        which appears in green galaxies transitioning from blue to red
        populations.}",
          doi = {10.1088/0004-637X/755/2/143},
archivePrefix = {arXiv},
       eprint = {1207.0170},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012ApJ...755..143T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012ApJ...755...98M,
       author = {{Miller}, A.~A. and {Richards}, J.~W. and {Bloom}, J.~S. and
         {Cenko}, S.~B. and {Silverman}, J.~M. and {Starr}, D.~L. and
         {Stassun}, K.~G.},
        title = "{Discovery of Bright Galactic R Coronae Borealis and DY Persei Variables: Rare Gems Mined from ACVS}",
      journal = {\apj},
     keywords = {circumstellar matter, methods: data analysis, stars: carbon, stars: evolution, stars: variables: general, techniques: photometric, Astrophysics - Solar and Stellar Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2012",
        month = "Aug",
       volume = {755},
       number = {2},
          eid = {98},
        pages = {98},
     abstract = "{We present the results of a machine-learning (ML)-based search for new R
        Coronae Borealis (RCB) stars and DY Persei-like stars (DYPers)
        in the Galaxy using cataloged light curves from the All-Sky
        Automated Survey (ASAS) Catalog of Variable Stars (ACVS). RCB
        stars{\textemdash}a rare class of hydrogen-deficient carbon-rich
        supergiants{\textemdash}are of great interest owing to the
        insights they can provide on the late stages of stellar
        evolution. DYPers are possibly the low-temperature, low-
        luminosity analogs to the RCB phenomenon, though additional
        examples are needed to fully establish this connection. While
        RCB stars and DYPers are traditionally identified by epochs of
        extreme dimming that occur without regularity, the ML search
        framework more fully captures the richness and diversity of
        their photometric behavior. We demonstrate that our ML method
        can use newly discovered RCB stars to identify additional
        candidates within the same data set. Our search yields 15
        candidates that we consider likely RCB stars/DYPers: new
        spectroscopic observations confirm that four of these candidates
        are RCB stars and four are DYPers. Our discovery of four new
        DYPers increases the number of known Galactic DYPers from two to
        six; noteworthy is that one of the new DYPers has a measured
        parallax and is m {\ensuremath{\approx}} 7 mag, making it the
        brightest known DYPer to date. Future observations of these new
        DYPers should prove instrumental in establishing the RCB
        connection. We consider these results, derived from a machine-
        learned probabilistic classification catalog, as an important
        proof-of-concept for the efficient discovery of rare sources
        with time-domain surveys.}",
          doi = {10.1088/0004-637X/755/2/98},
archivePrefix = {arXiv},
       eprint = {1204.4181},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012ApJ...755...98M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012AdSpR..50..363P,
       author = {{Prasad}, Rajendra and {Pandey}, A. and {Singh}, K.~P. and
         {Singh}, V.~P. and {Mishra}, R.~K. and {Singh}, D.},
        title = "{Retrieval of spinach crop parameters by microwave remote sensing with back propagation artificial neural networks: A comparison of different transfer functions}",
      journal = {Advances in Space Research},
         year = "2012",
        month = "Aug",
       volume = {50},
       number = {3},
        pages = {363-370},
     abstract = "{Back propagation artificial natural network (BPANN) is a well known and
        widely used machine learning methodology in the field of remote
        sensing. In this paper an attempt is made to retrieve the
        spinach crop parameters like biomass, leaf area index, average
        plant height and soil moisture content by using the X-band
        scattering coefficients with BPANN at different growth stages of
        this crop. The maturity age of this crop was found to be 45 days
        from the date of sowing. After 45 days from the date of sowing,
        this crop was cut at a certain height for production. Then, it
        is a point of interest to investigate the microwave response of
        variation in production. Significant variations in all the crop
        parameters were observed after cutting the crop and consequently
        made the problem more critical. Our work confirms the utility of
        BPANN in handling such a non-linear data set. The BPANN is
        essentially a network of simple processing nodes arranged into
        different layers as input, hidden and the output. The input
        layer propagates components of a particular input vector after
        weighting these with synaptic weights to each node in the hidden
        layer. At each node, these weighted input vector components are
        added. Each hidden layer computes output corresponding to these
        weighted sum through a non-linear/linear function (e.g. LOGSIG,
        TANSIG and PURLIN). These functions are known as transfer
        functions. Thus, each of the hidden layer nodes compute output
        values, which become inputs to the nodes of the output layer. At
        nodes of output layer also a weighted sum of outputs of previous
        layer (hidden layer) are obtained and processed through a
        transfer function. Thus, the output layer nodes compute the
        network output for the particular input vector. In this paper,
        output nodes use linear transfer function. Different transfer
        functions e.g. TANSIG, LOGSIG and PURELIN were used and the
        performance of the ANN was optimized by changing the number of
        neurons in the hidden layers. The present analysis suggests the
        need of critical analysis of the BPANN in terms of selection of
        the best transfer function and other network parameters for the
        better results.}",
          doi = {10.1016/j.asr.2012.04.010},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012AdSpR..50..363P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012GeoRL..3910710G,
       author = {{Giannakis}, Dimitrios and {Majda}, Andrew J.},
        title = "{Comparing low-frequency and intermittent variability in comprehensive climate models through nonlinear Laplacian spectral analysis}",
      journal = {\grl},
     keywords = {Computational Geophysics: Neural networks, fuzzy logic, machine learning (1942), Informatics: Temporal analysis and representation (1872, 3270, 4277, 4475), Mathematical Geophysics: Spectral analysis (3205, 3280, 4319), Oceanography: General: Climate and interannual variability (1616, 1635, 3305, 3309, 4513), Oceanography: Physical: Decadal ocean variability (1616, 1635, 3305, 4215)},
         year = "2012",
        month = "May",
       volume = {39},
       number = {10},
          eid = {L10710},
        pages = {L10710},
     abstract = "{Nonlinear Laplacian spectral analysis (NLSA) is a recently developed
        technique for spatiotemporal analysis of high-dimensional data,
        which represents temporal patterns via natural orthonormal basis
        functions on the nonlinear data manifold. Through such basis
        functions, determined efficiently via graph-theoretic
        algorithms, NLSA captures intermittency, rare events, and other
        nonlinear dynamical features which are not accessible through
        linear approaches (e.g., singular spectrum analysis (SSA)).
        Here, we apply NLSA to study North Pacific SST monthly data from
        the CCSM3 and ECHAM5/MPI-OM models. Without performing spatial
        coarse graining (i.e., operating in ambient-space dimensions up
        to 1.6 {\texttimes} {}10$^{5}$ after lagged embedding), or
        seasonal-cycle subtraction, the method reveals families of
        periodic, low-frequency, and intermittent spatiotemporal modes.
        The intermittent modes, which describe variability in the
        Western and Eastern boundary currents, as well as variability in
        the subtropical gyre with year-to-year reemergence, are not
        captured by SSA, yet are likely to have high significance in a
        predictive context and utility in cross-model comparisons.}",
          doi = {10.1029/2012GL051575},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012GeoRL..3910710G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012MNRAS.421.1155B,
       author = {{Brescia}, Massimo and {Cavuoti}, Stefano and {Paolillo}, Maurizio and
         {Longo}, Giuseppe and {Puzia}, Thomas},
        title = "{The detection of globular clusters in galaxies as a data mining problem}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, globular clusters: general, galaxies: elliptical and lenticular, cD, galaxies: individual: NGC 1399, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2012",
        month = "Apr",
       volume = {421},
       number = {2},
        pages = {1155-1165},
     abstract = "{We present an application of self-adaptive supervised learning
        classifiers derived from the machine learning paradigm to the
        identification of candidate globular clusters in deep, wide-
        field, single-band Hubble Space Telescope (HST) images. Several
        methods provided by the DAta Mining and Exploration (DAME) web
        application were tested and compared on the NGC 1399 HST data
        described by Paolillo and collaborators in a companion paper.
        The best results were obtained using a multilayer perceptron
        with quasi-Newton learning rule which achieved a classification
        accuracy of 98.3 per cent, with a completeness of 97.8 per cent
        and contamination of 1.6 per cent. An extensive set of
        experiments revealed that the use of accurate structural
        parameters (effective radius, central surface brightness) does
        improve the final result, but only by ̃5 per cent. It is also
        shown that the method is capable to retrieve also extreme
        sources (for instance, very extended objects) which are missed
        by more traditional approaches.}",
          doi = {10.1111/j.1365-2966.2011.20375.x},
archivePrefix = {arXiv},
       eprint = {1110.2144},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012MNRAS.421.1155B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012JKAS...45...31C,
       author = {{Choi}, Seonghwan and {Moon}, Yong-Jae and {Vien}, Ngo Anh and
         {Park}, Young-Deuk},
        title = "{Application of Support Vector Machine to the Prediction of Geo-Effective Halo CMEs}",
      journal = {Journal of Korean Astronomical Society},
     keywords = {space weather, CME, flare, machine learning, support vector machine},
         year = "2012",
        month = "Apr",
       volume = {45},
       number = {2},
        pages = {31-38},
     abstract = "{In this study we apply Support Vector Machine (SVM) to the prediction of
        geo-effective halo coronal mass ejections (CMEs). The SVM, which
        is one of machine learning algorithms, is used for the purpose
        of classification and regression analysis. We use halo and
        partial halo CMEs from January 1996 to April 2010 in the
        SOHO/LASCO CME Catalog for training and prediction. And we also
        use their associated X-ray flare classes to identify front-side
        halo CMEs (stronger than B1 class), and the Dst index to
        determine geo-effective halo CMEs (stronger than -50 nT). The
        combinations of the speed and the angular width of CMEs, and
        their associated X-ray classes are used for input features of
        the SVM. We make an attempt to find the best model by using
        cross-validation which is processed by changing kernel functions
        of the SVM and their parameters. As a result we obtain
        statistical parameters for the best model by using the speed of
        CME and its associated X-ray flare class as input features of
        the SVM: Accuracy=0.66, PODy=0.76, PODn=0.49, FAR=0.72,
        Bias=1.06, CSI=0.59, TSS=0.25. The performance of the
        statistical parameters by applying the SVM is much better than
        those from the simple classifications based on constant
        classifiers.}",
          doi = {10.5303/JKAS.2012.45.2.031},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012JKAS...45...31C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012RAA....12..313H,
       author = {{Huang}, Xin and {Wang}, Hua-Ning and {Li}, Le-Ping},
        title = "{Ensemble prediction model of solar proton events associated with solar flares and coronal mass ejections}",
      journal = {Research in Astronomy and Astrophysics},
         year = "2012",
        month = "Mar",
       volume = {12},
       number = {3},
        pages = {313-321},
     abstract = "{An ensemble prediction model of solar proton events (SPEs), combining
        the information of solar flares and coronal mass ejections
        (CMEs), is built. In this model, solar flares are parameterized
        by the peak flux, the duration and the longitude. In addition,
        CMEs are parameterized by the width, the speed and the
        measurement position angle. The importance of each parameter for
        the occurrence of SPEs is estimated by the information gain
        ratio. We find that the CME width and speed are more informative
        than the flare's peak flux and duration. As the physical
        mechanism of SPEs is not very clear, a hidden naive Bayes
        approach, which is a probability-based calculation method from
        the field of machine learning, is used to build the prediction
        model from the observational data. As is known, SPEs originate
        from solar flares and/or shock waves associated with CMEs.
        Hence, we first build two base prediction models using the
        properties of solar flares and CMEs, respectively. Then the
        outputs of these models are combined to generate the ensemble
        prediction model of SPEs. The ensemble prediction model
        incorporating the complementary information of solar flares and
        CMEs achieves better performance than each base prediction model
        taken separately.}",
          doi = {10.1088/1674-4527/12/3/007},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012RAA....12..313H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012PASP..124..280L,
       author = {{Long}, James P. and {El Karoui}, Noureddine and {Rice}, John A. and
         {Richards}, Joseph W. and {Bloom}, Joshua S.},
        title = "{Optimizing Automated Classification of Variable Stars in New Synoptic Surveys}",
      journal = {\pasp},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Applications},
         year = "2012",
        month = "Mar",
       volume = {124},
       number = {913},
        pages = {280},
     abstract = "{Efficient and automated classification of periodic variable stars is
        becoming increasingly important as the scale of astronomical
        surveys grows. Several recent articles have used methods from
        machine learning and statistics to construct classifiers on
        databases of labeled, multi-epoch sources with the intention of
        using these classifiers to automatically infer the classes of
        unlabeled sources from new surveys. However, the same source
        observed with two different synoptic surveys will generally
        yield different derived metrics (features) from the light curve.
        Since such features are used in classifiers, this survey-
        dependent mismatch in feature space will typically lead to
        degraded classifier performance. In this article we show how and
        why feature distributions change using OGLE and Hipparcos light
        curves. To overcome survey systematics, we apply a noisification
        method, which attempts to empirically match distributions of
        features between the labeled sources used to construct the
        classifier and the unlabeled sources we wish to classify.
        Results from simulated and real-world light curves show that
        noisification can significantly improve classifier performance.
        In a three-class problem using light curves from Hipparcos and
        OGLE, noisification reduces the classifier error rate from
        27.0\% to 7.0\%. We recommend that noisification be used for
        upcoming surveys such as Gaia and LSST, and we describe some of
        the promises and challenges of applying noisification to these
        surveys.}",
          doi = {10.1086/664960},
archivePrefix = {arXiv},
       eprint = {1201.4863},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012PASP..124..280L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012PASP..124..274W,
       author = {{Way}, M.~J. and {Klose}, C.~D.},
        title = "{Can Self-Organizing Maps Accurately Predict Photometric Redshifts?}",
      journal = {\pasp},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2012",
        month = "Mar",
       volume = {124},
       number = {913},
        pages = {274},
     abstract = "{We present an unsupervised machine-learning approach that can be
        employed for estimating photometric redshifts. The proposed
        method is based on a vector quantization called the self-
        organizing-map (SOM) approach. A variety of photometrically
        derived input values were utilized from the Sloan Digital Sky
        Survey{\textquoteright}s main galaxy sample, luminous red
        galaxy, and quasar samples, along with the PHAT0 data set from
        the Photo- z Accuracy Testing project. Regression results
        obtained with this new approach were evaluated in terms of root-
        mean-square error (RMSE) to estimate the accuracy of the
        photometric redshift estimates. The results demonstrate
        competitive RMSE and outlier percentages when compared with
        several other popular approaches, such as artificial neural
        networks and Gaussian process regression. SOM RMSE results
        (using {\ensuremath{\Delta}}z = z$_{phot}$ - z$_{spec}$) are
        0.023 for the main galaxy sample, 0.027 for the luminous red
        galaxy sample, 0.418 for quasars, and 0.022 for PHAT0 synthetic
        data. The results demonstrate that there are nonunique solutions
        for estimating SOM RMSEs. Further research is needed in order to
        find more robust estimation techniques using SOMs, but the
        results herein are a positive indication of their capabilities
        when compared with other well-known methods.}",
          doi = {10.1086/664796},
archivePrefix = {arXiv},
       eprint = {1201.1098},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012PASP..124..274W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012MNRAS.421..169G,
       author = {{Graff}, Philip and {Feroz}, Farhan and {Hobson}, Michael P. and
         {Lasenby}, Anthony},
        title = "{BAMBI: blind accelerated multimodal Bayesian inference}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, cosmological parameters, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Extragalactic Astrophysics, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
         year = "2012",
        month = "Mar",
       volume = {421},
       number = {1},
        pages = {169-180},
     abstract = "{In this paper, we present an algorithm for rapid Bayesian analysis that
        combines the benefits of nested sampling and artificial neural
        networks (NNs). The blind accelerated multimodal Bayesian
        inference (BAMBI) algorithm implements the MULTINEST package for
        nested sampling as well as the training of an artificial NN to
        learn the likelihood function. In the case of computationally
        expensive likelihoods, this allows the substitution of a much
        more rapid approximation in order to increase significantly the
        speed of the analysis. We begin by demonstrating, with a few toy
        examples, the ability of an NN to learn complicated likelihood
        surfaces. BAMBI's ability to decrease running time for Bayesian
        inference is then demonstrated in the context of estimating
        cosmological parameters from Wilkinson Microwave Anisotropy
        Probe and other observations. We show that valuable speed
        increases are achieved in addition to obtaining NNs trained on
        the likelihood functions for the different model and data
        combinations. These NNs can then be used for an even faster
        follow-up analysis using the same likelihood and different
        priors. This is a fully general algorithm that can be applied,
        without any pre-processing, to other problems with
        computationally expensive likelihood functions.}",
          doi = {10.1111/j.1365-2966.2011.20288.x},
archivePrefix = {arXiv},
       eprint = {1110.2997},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012MNRAS.421..169G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012ApJ...747...12W,
       author = {{Waldmann}, I.~P.},
        title = "{Of ``Cocktail Parties'' and Exoplanets}",
      journal = {\apj},
     keywords = {methods: data analysis, methods: statistical, techniques: photometric, techniques: spectroscopic, Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2012",
        month = "Mar",
       volume = {747},
       number = {1},
          eid = {12},
        pages = {12},
     abstract = "{The characterization of ever smaller and fainter extrasolar planets
        requires an intricate understanding of one's data and the
        analysis techniques used. Correcting the raw data at the
        10$^{-4}$ level of accuracy in flux is one of the central
        challenges. This can be difficult for instruments that do not
        feature a calibration plan for such high precision measurements.
        Here, it is not always obvious how to de-correlate the data
        using auxiliary information of the instrument and it becomes
        paramount to know how well one can disentangle instrument
        systematics from one's data, given nothing but the data
        themselves. We propose a non-parametric machine learning
        algorithm, based on the concept of independent component
        analysis, to de-convolve the systematic noise and all non-
        Gaussian signals from the desired astrophysical signal. Such a
        ``blind'' signal de-mixing is commonly known as the ``Cocktail
        Party problem'' in signal processing. Given multiple
        simultaneous observations of the same exoplanetary eclipse, as
        in the case of spectrophotometry, we show that we can often
        disentangle systematic noise from the original light-curve
        signal without the use of any complementary information of the
        instrument. In this paper, we explore these signal extraction
        techniques using simulated data and two data sets observed with
        the Hubble Space Telescope NICMOS instrument. Another important
        application is the de-correlation of the exoplanetary signal
        from time-correlated stellar variability. Using data obtained by
        the Kepler mission we show that the desired signal can be de-
        convolved from the stellar noise using a single time series
        spanning several eclipse events. Such non-parametric techniques
        can provide important confirmations of the existent parametric
        corrections reported in the literature, and their associated
        results. Additionally they can substantially improve the
        precision exoplanetary light-curve analysis in the future.}",
          doi = {10.1088/0004-637X/747/1/12},
archivePrefix = {arXiv},
       eprint = {1106.1989},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012ApJ...747...12W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012ApJ...746..170M,
       author = {{Morgan}, A.~N. and {Long}, James and {Richards}, Joseph W. and
         {Broderick}, Tamara and {Butler}, Nathaniel R. and {Bloom}, Joshua S.},
        title = "{Rapid, Machine-learned Resource Allocation: Application to High-redshift Gamma-Ray Burst Follow-up}",
      journal = {\apj},
     keywords = {gamma-ray burst: general, methods: data analysis, methods: statistical, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - High Energy Astrophysical Phenomena},
         year = "2012",
        month = "Feb",
       volume = {746},
       number = {2},
          eid = {170},
        pages = {170},
     abstract = "{As the number of observed gamma-ray bursts (GRBs) continues to grow,
        follow-up resources need to be used more efficiently in order to
        maximize science output from limited telescope time. As such, it
        is becoming increasingly important to rapidly identify bursts of
        interest as soon as possible after the event, before the
        afterglows fade beyond detectability. Studying the most distant
        (highest redshift) events, for instance, remains a primary goal
        for many in the field. Here, we present our Random Forest
        Automated Triage Estimator for GRB redshifts (RATE GRB-z ) for
        rapid identification of high-redshift candidates using early-
        time metrics from the three telescopes onboard Swift. While the
        basic RATE methodology is generalizable to a number of resource
        allocation problems, here we demonstrate its utility for
        telescope-constrained follow-up efforts with the primary goal to
        identify and study high-z GRBs. For each new GRB, RATE GRB-z
        provides a recommendation{\textemdash}based on the available
        telescope time{\textemdash}of whether the event warrants
        additional follow-up resources. We train RATE GRB-z using a set
        consisting of 135 Swift bursts with known redshifts, only 18 of
        which are z \&gt; 4. Cross-validated performance metrics on
        these training data suggest that
        \raisebox{-0.5ex}\textasciitilde56\% of high-z bursts can be
        captured from following up the top 20\% of the ranked
        candidates, and \raisebox{-0.5ex}\textasciitilde84\% of high-z
        bursts are identified after following up the top
        \raisebox{-0.5ex}\textasciitilde40\% of candidates. We further
        use the method to rank 200 + Swift bursts with unknown redshifts
        according to their likelihood of being high-z.}",
          doi = {10.1088/0004-637X/746/2/170},
archivePrefix = {arXiv},
       eprint = {1112.3654},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012ApJ...746..170M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012NIMPA.662S.242N,
       author = {{Neff}, M. and {Anton}, G. and {Enzenh{\"o}fer}, A. and {Graf}, K. and
         {H{\"o}{\ss}l}, J. and {Katz}, U. and {Lahmann}, R. and {Richardt}, C.},
        title = "{Signal classification for acoustic neutrino detection}",
      journal = {Nuclear Instruments and Methods in Physics Research A},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability},
         year = "2012",
        month = "Jan",
       volume = {662},
        pages = {S242-S245},
     abstract = "{This article focuses on signal classification for deep-sea acoustic
        neutrino detection. In the deep sea, the background of transient
        signals is very diverse. Approaches like matched filtering are
        not sufficient to distinguish between neutrino-like signals and
        other transient signals with similar signature, which are
        forming the acoustic background for neutrino detection in the
        deep-sea environment. A classification system based on machine
        learning algorithms is analysed with the goal to find a robust
        and effective way to perform this task. For a well-trained
        model, a testing error on the level of 1\% is achieved for
        strong classifiers like Random Forest and Boosting Trees using
        the extracted features of the signal as input and utilising
        dense clusters of sensors instead of single sensors.}",
          doi = {10.1016/j.nima.2010.11.016},
archivePrefix = {arXiv},
       eprint = {1104.3248},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012NIMPA.662S.242N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012MNRAS.419.2683G,
       author = {{Gibson}, N.~P. and {Aigrain}, S. and {Roberts}, S. and {Evans}, T.~M. and
         {Osborne}, M. and {Pont}, F.},
        title = "{A Gaussian process framework for modelling instrumental systematics: application to transmission spectroscopy}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: spectroscopic, stars: individual: HD 189733, planetary systems, Astrophysics - Earth and Planetary Astrophysics},
         year = "2012",
        month = "Jan",
       volume = {419},
       number = {3},
        pages = {2683-2694},
     abstract = "{Transmission spectroscopy, which consists of measuring the wavelength-
        dependent absorption of starlight by a planet's atmosphere
        during a transit, is a powerful probe of atmospheric
        composition. However, the expected signal is typically orders of
        magnitude smaller than instrumental systematics and the results
        are crucially dependent on the treatment of the latter. In this
        paper, we propose a new method to infer transit parameters in
        the presence of systematic noise using Gaussian processes, a
        technique widely used in the machine learning community for
        Bayesian regression and classification problems. Our method
        makes use of auxiliary information about the state of the
        instrument, but does so in a non-parametric manner, without
        imposing a specific dependence of the systematics on the
        instrumental parameters, and naturally allows for the correlated
        nature of the noise. We give an example application of the
        method to archival NICMOS transmission spectroscopy of the hot
        Jupiter HD 189733, which goes some way towards reconciling the
        controversy surrounding this data set in the literature.
        Finally, we provide an appendix giving a general introduction to
        Gaussian processes for regression, in order to encourage their
        application to a wider range of problems.}",
          doi = {10.1111/j.1365-2966.2011.19915.x},
archivePrefix = {arXiv},
       eprint = {1109.3251},
 primaryClass = {astro-ph.EP},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012MNRAS.419.2683G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012MNRAS.419.2633G,
       author = {{Geach}, James E.},
        title = "{Unsupervised self-organized mapping: a versatile empirical tool for object selection, classification and redshift estimation in large surveys}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: observational, methods: statistical, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2012",
        month = "Jan",
       volume = {419},
       number = {3},
        pages = {2633-2645},
     abstract = "{We present an application of unsupervised machine learning - the self-
        organized map (SOM) - as a tool for visualizing, exploring and
        mining the catalogues of large astronomical surveys. Self-
        organization culminates in a low-resolution representation of
        the 'topology' of a parameter volume, and this can be exploited
        in various ways pertinent to astronomy. Using data from the
        Cosmological Evolution Survey (COSMOS), we demonstrate two key
        astronomical applications of the SOM: (i) object classification
        and selection, using galaxies with active galactic nuclei as an
        example, and (ii) photometric redshift estimation, illustrating
        how SOMs can be used as totally empirical predictive tools. With
        a training set of ̃3800 galaxies with
        z$_{spec}${\ensuremath{\leq}} 1, we achieve photometric redshift
        accuracies competitive with other (mainly template fitting)
        techniques that use a similar number of photometric bands
        [{\ensuremath{\sigma}}({\ensuremath{\Delta}}z) = 0.03 with a ̃2
        per cent outlier rate when using u* band to 8 ?m photometry]. We
        also test the SOM as a photo-z tool using the PHoto-z Accuracy
        Testing (PHAT) synthetic catalogue of Hildebrandt et al., which
        compares several different photo-z codes using a common
        input/training set. We find that the SOM can deliver accuracies
        that are competitive with many of the established template
        fitting and empirical methods. This technique is not without
        clear limitations, which are discussed, but we suggest it could
        be a powerful tool in the era of extremely large -'petabyte'-
        data bases where efficient data mining is a paramount concern.}",
          doi = {10.1111/j.1365-2966.2011.19913.x},
archivePrefix = {arXiv},
       eprint = {1110.0005},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012MNRAS.419.2633G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012MNRAS.419.1121R,
       author = {{Richards}, Joseph W. and {Homrighausen}, Darren and
         {Freeman}, Peter E. and {Schafer}, Chad M. and {Poznanski}, Dovi},
        title = "{Semi-supervised learning for photometric supernova classification}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, techniques: photometric, surveys, supernovae: general, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Applications},
         year = "2012",
        month = "Jan",
       volume = {419},
       number = {2},
        pages = {1121-1135},
     abstract = "{We present a semi-supervised method for photometric supernova typing.
        Our approach is to first use the non-linear dimension reduction
        technique diffusion map to detect structure in a data base of
        supernova light curves and subsequently employ random forest
        classification on a spectroscopically confirmed training set to
        learn a model that can predict the type of each newly observed
        supernova. We demonstrate that this is an effective method for
        supernova typing. As supernova numbers increase, our semi-
        supervised method efficiently utilizes this information to
        improve classification, a property not enjoyed by template-based
        methods. Applied to supernova data simulated by Kessler et al.
        to mimic those of the Dark Energy Survey, our methods achieve
        (cross-validated) 95 per cent Type Ia purity and 87 per cent
        Type Ia efficiency on the spectroscopic sample, but only 50 per
        cent Type Ia purity and 50 per cent efficiency on the
        photometric sample due to their spectroscopic follow-up
        strategy. To improve the performance on the photometric sample,
        we search for better spectroscopic follow-up procedures by
        studying the sensitivity of our machine-learned supernova
        classification on the specific strategy used to obtain training
        sets. With a fixed amount of spectroscopic follow-up time, we
        find that, despite collecting data on a smaller number of
        supernovae, deeper magnitude-limited spectroscopic surveys are
        better for producing training sets. For supernova Ia (II-P)
        typing, we obtain a 44 per cent (1 per cent) increase in purity
        to 72 per cent (87 per cent) and 30 per cent (162 per cent)
        increase in efficiency to 65 per cent (84 per cent) of the
        sample using a 25th (24.5th) magnitude-limited survey instead of
        the shallower spectroscopic sample used in the original
        simulations. When redshift information is available, we
        incorporate it into our analysis using a novel method of
        altering the diffusion map representation of the supernovae.
        Incorporating host redshifts leads to a 5 per cent improvement
        in Type Ia purity and 13 per cent improvement in Type Ia
        efficiency. A web service for the supernova classification
        method used in this paper can be found at .}",
          doi = {10.1111/j.1365-2966.2011.19768.x},
archivePrefix = {arXiv},
       eprint = {1103.6034},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012MNRAS.419.1121R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012MNRAS.419...80A,
       author = {{Abraham}, Sheelu and {Philip}, Ninan Sajeeth and {Kembhavi}, Ajit and
         {Wadadekar}, Yogesh G. and {Sinha}, Rita},
        title = "{A photometric catalogue of quasars and other point sources in the Sloan Digital Sky Survey}",
      journal = {\mnras},
     keywords = {methods: statistical, techniques: photometric, astronomical data bases: miscellaneous, catalogues, surveys, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Artificial Intelligence},
         year = "2012",
        month = "Jan",
       volume = {419},
       number = {1},
        pages = {80-94},
     abstract = "{We present a catalogue of about six million unresolved photometric
        detections in the Sloan Digital Sky Survey (SDSS) Seventh Data
        Release, classifying them into stars, galaxies and quasars. We
        use a machine learning classifier trained on a subset of
        spectroscopically confirmed objects from 14th to 22nd magnitude
        in the SDSS i band. Our catalogue consists of 2 430 625 quasars,
        3 544 036 stars and 63 586 unresolved galaxies from 14th to 24th
        magnitude in the SDSS i band. Our algorithm recovers 99.96 per
        cent of spectroscopically confirmed quasars and 99.51 per cent
        of stars to i ̃ 21.3 in the colour window that we study. The
        level of contamination due to data artefacts for objects beyond
        i = 21.3 is highly uncertain and all mention of completeness and
        contamination in the paper are valid only for objects brighter
        than this magnitude. However, a comparison of the predicted
        number of quasars with the theoretical number counts shows
        reasonable agreement.}",
          doi = {10.1111/j.1365-2966.2011.19674.x},
archivePrefix = {arXiv},
       eprint = {1011.2173},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012MNRAS.419...80A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012JBIS...65..278G,
       author = {{Galea}, P.},
        title = "{Machine Learning and the Starship - A Match Made in Heaven}",
      journal = {Journal of the British Interplanetary Society},
     keywords = {Machine learning, artificial intelligence, bayesian, neural network, social network analysis, interstellar},
         year = "2012",
        month = "Jan",
       volume = {65},
        pages = {278-282},
     abstract = "{The computer control system of an unmanned interstellar craft must deal
        with a variety of complex problems. For example, upon reaching
        the destination star, the computer may need to make assessments
        of the planets and other objects to prioritize the most
        `interesting', and assign appropriate probes to each. These
        decisions would normally be regarded as intelligent if they were
        made by humans. This paper looks at machine learning
        technologies currently deployed in non-aerospace contexts, such
        as book recommendation systems, dating websites and social
        network analysis, and investigates the ways in which they can be
        adapted for applications in the starship. This paper is a
        submission of the Project Icarus Study Group.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012JBIS...65..278G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012IBVS.6009....1L,
       author = {{Le Borgne}, J.~F. and {Klotz}, A. and {Boer}, M.},
        title = "{The GEOS RR Lyr Survey}",
      journal = {Information Bulletin on Variable Stars},
     keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, I.2.6},
         year = "2012",
        month = "Jan",
       volume = {6009},
        pages = {1},
     abstract = "{Multi-label classification has attracted an increasing amount of
        attention in recent years. To this end, many algorithms have
        been developed to classify multi-label data in an effective
        manner. However, they usually do not consider the pairwise
        relations indicated by sample labels, which actually play
        important roles in multi-label classification. Inspired by this,
        we naturally extend the traditional pairwise constraints to the
        multi-label scenario via a flexible thresholding scheme.
        Moreover, to improve the generalization ability of the
        classifier, we adopt a boosting-like strategy to construct a
        multi-label ensemble from a group of base classifiers. To
        achieve these goals, this paper presents a novel multi-label
        classification framework named Variable Pairwise Constraint
        projection for Multi-label Ensemble (VPCME). Specifically, we
        take advantage of the variable pairwise constraint projection to
        learn a lower-dimensional data representation, which preserves
        the correlations between samples and labels. Thereafter, the
        base classifiers are trained in the new data space. For the
        boosting-like strategy, we employ both the variable pairwise
        constraints and the bootstrap steps to diversify the base
        classifiers. Empirical studies have shown the superiority of the
        proposed method in comparison with other approaches.}",
archivePrefix = {arXiv},
       eprint = {1403.1944},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012IBVS.6009....1L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2012ApJ...744..192R,
       author = {{Richards}, Joseph W. and {Starr}, Dan L. and {Brink}, Henrik and
         {Miller}, Adam A. and {Bloom}, Joshua S. and {Butler}, Nathaniel R. and
         {James}, J. Berian and {Long}, James P. and {Rice}, John},
        title = "{Active Learning to Overcome Sample Selection Bias: Application to Photometric Variable Star Classification}",
      journal = {\apj},
     keywords = {methods: data analysis, methods: statistical, stars: variables: general, techniques: photometric, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Applications},
         year = "2012",
        month = "Jan",
       volume = {744},
       number = {2},
          eid = {192},
        pages = {192},
     abstract = "{Despite the great promise of machine-learning algorithms to classify and
        predict astrophysical parameters for the vast numbers of
        astrophysical sources and transients observed in large-scale
        surveys, the peculiarities of the training data often manifest
        as strongly biased predictions on the data of interest.
        Typically, training sets are derived from historical surveys of
        brighter, more nearby objects than those from more extensive,
        deeper surveys (testing data). This sample selection bias can
        cause catastrophic errors in predictions on the testing data
        because (1) standard assumptions for machine-learned model
        selection procedures break down and (2) dense regions of testing
        space might be completely devoid of training data. We explore
        possible remedies to sample selection bias, including importance
        weighting, co-training, and active learning (AL). We argue that
        AL{\textemdash}where the data whose inclusion in the training
        set would most improve predictions on the testing set are
        queried for manual follow-up{\textemdash}is an effective
        approach and is appropriate for many astronomical applications.
        For a variable star classification problem on a well-studied set
        of stars from Hipparcos and Optical Gravitational Lensing
        Experiment, AL is the optimal method in terms of error rate on
        the testing data, beating the off-the-shelf classifier by 3.4\%
        and the other proposed methods by at least 3.0\%. To aid with
        manual labeling of variable stars, we developed a Web interface
        which allows for easy light curve visualization and querying of
        external databases. Finally, we apply AL to classify variable
        stars in the All Sky Automated Survey, finding dramatic
        improvement in our agreement with the ASAS Catalog of Variable
        Stars, from 65.5\% to 79.5\%, and a significant increase in the
        classifier's average confidence for the testing set, from 14.6\%
        to 42.9\%, after a few AL iterations.}",
          doi = {10.1088/0004-637X/744/2/192},
archivePrefix = {arXiv},
       eprint = {1106.2832},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2012ApJ...744..192R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2011MNRAS.418.2165L,
       author = {{Laurino}, O. and {D'Abrusco}, R. and {Longo}, G. and {Riccio}, G.},
        title = "{Astroinformatics of galaxies and quasars: a new general method for photometric redshifts estimation}",
      journal = {\mnras},
     keywords = {methods: data analysis, techniques: photometric, catalogues, surveys, virtual observatory tools, cosmology: observations, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - High Energy Astrophysical Phenomena, Statistics - Machine Learning},
         year = "2011",
        month = "Dec",
       volume = {418},
       number = {4},
        pages = {2165-2195},
     abstract = "{With the availability of the huge amounts of data produced by current
        and future large multiband photometric surveys, photometric
        redshifts have become a crucial tool for extragalactic astronomy
        and cosmology. In this paper we present a novel method, called
        Weak Gated Experts (WGE), which allows us to derive photometric
        redshifts through a combination of data mining techniques. The
        WGE, like many other machine learning techniques, is based on
        the exploitation of a spectroscopic knowledge base composed by
        sources for which a spectroscopic value of the redshift is
        available. This method achieves a variance
        {\ensuremath{\sigma}}$^{2}$({\ensuremath{\Delta}}z) = 2.3
        {\texttimes} 10$^{-4}$
        [{\ensuremath{\sigma}}$^{2}$({\ensuremath{\Delta}}z) = 0.08,
        where {\ensuremath{\Delta}}z=z$_{phot}$-z$_{spec}$] for the
        reconstruction of the photometric redshifts for the optical
        galaxies from the Sloan Digital Sky Survey (SDSS) and for the
        optical quasars, respectively, while the root mean square (rms)
        of the {\ensuremath{\Delta}}z variable distributions for the two
        experiments is, respectively, equal to 0.021 and 0.35. The WGE
        provides also a mechanism for the estimation of the accuracy of
        each photometric redshift. We also present and discuss the
        catalogues obtained for the optical SDSS galaxies, for the
        optical candidate quasars extracted from the Data Release 7 of
        SDSS photometric data set (the sample of SDSS sources on which
        the accuracy of the reconstruction has been assessed is composed
        of bright sources, for a subset of which spectroscopic redshifts
        have been measured) and for optical SDSS candidate quasars
        observed by GALEX in the ultraviolet range. The WGE method
        exploits the new technological paradigm provided by the virtual
        observatory and the emerging field of astroinformatics.}",
          doi = {10.1111/j.1365-2966.2011.19416.x},
archivePrefix = {arXiv},
       eprint = {1107.3160},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2011MNRAS.418.2165L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2011ApJ...741...14B,
       author = {{Beaumont}, Christopher N. and {Williams}, Jonathan P. and
         {Goodman}, Alyssa A.},
        title = "{Classifying Structures in the Interstellar Medium with Support Vector Machines: The G16.05-0.57 Supernova Remnant}",
      journal = {\apj},
     keywords = {ISM: individual objects: G16.05-0.57 M17, ISM: supernova remnants, techniques: image processing, Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2011",
        month = "Nov",
       volume = {741},
       number = {1},
          eid = {14},
        pages = {14},
     abstract = "{We apply Support Vector Machines (SVMs){\textemdash}a machine learning
        algorithm{\textemdash}to the task of classifying structures in
        the interstellar medium (ISM). As a case study, we present a
        position-position-velocity (PPV) data cube of $^{12}$CO J = 3-2
        emission toward G16.05-0.57, a supernova remnant that lies
        behind the M17 molecular cloud. Despite the fact that these two
        objects partially overlap in PPV space, the two structures can
        easily be distinguished by eye based on their distinct
        morphologies. The SVM algorithm is able to infer these
        morphological distinctions, and associate individual pixels with
        each object at \&gt;90\% accuracy. This case study suggests that
        similar techniques may be applicable to classifying other
        structures in the ISM{\textemdash}a task that has thus far
        proven difficult to automate.}",
          doi = {10.1088/0004-637X/741/1/14},
archivePrefix = {arXiv},
       eprint = {1107.5584},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2011ApJ...741...14B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2011JGRA..11610303A,
       author = {{Ackermann}, E.~R. and {de Villiers}, J.~P. and {Cilliers}, P.~J.},
        title = "{Nonlinear dynamic systems modeling using Gaussian processes: Predicting ionospheric total electron content over South Africa}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {Gaussian process, model verification and validation, total electron content, Computational Geophysics: Modeling (1952, 4255, 4316), Computational Geophysics: Model verification and validation, Computational Geophysics: Neural networks, fuzzy logic, machine learning (1942), Ionosphere: Modeling and forecasting},
         year = "2011",
        month = "Oct",
       volume = {116},
       number = {A10},
          eid = {A10303},
        pages = {A10303},
     abstract = "{Two different implementations of Gaussian process (GP) models are
        proposed to estimate the vertical total electron content (TEC)
        from dual frequency Global Positioning System (GPS)
        measurements. The model falseness of GP and neural network
        models are compared using daily GPS TEC data from Sutherland,
        South Africa, and it is shown that the proposed GP models
        exhibit superior model falseness. The GP approach has several
        advantages over previously developed neural network approaches,
        which include seamless incorporation of prior knowledge, a
        theoretically principled method for determining the much smaller
        number of free model parameters, the provision of estimates of
        the model uncertainty, and a more intuitive interpretability of
        the model.}",
          doi = {10.1029/2010JA016375},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2011JGRA..11610303A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2011GeoRL..3818302C,
       author = {{Ciaramella}, A. and {De Lauro}, E. and {Falanga}, M. and
         {Petrosino}, S.},
        title = "{Automatic detection of long-period events at Campi Flegrei Caldera (Italy)}",
      journal = {\grl},
     keywords = {Computational Geophysics: Data analysis: algorithms and implementation, Computational Geophysics: Neural networks, fuzzy logic, machine learning (1942), Informatics: Software tools and services, Seismology: Volcano seismology (4302, 8419), Volcanology: Calderas},
         year = "2011",
        month = "Sep",
       volume = {38},
       number = {18},
          eid = {L18302},
        pages = {L18302},
     abstract = "{We propose a novel approach to analyze continuous seismic signal and
        separate the sources from background noise. A specific
        application to the seismicity recorded at Campi Flegrei Caldera
        during the 2006 ground uplift is presented. The fundamental
        objective is to improve the standard procedures of picking the
        emergent onset arrivals of the seismic signals, often buried in
        the high-level ambient noise, in order to obtain an appropriate
        catalogue for monitoring the activity of this densely populated
        volcanic area. This is particularly useful in order to estimate
        the release of the seismic energy and to put constraints on the
        source dynamics. An Independent Component Analysis based
        approach for the Blind Source Separation of convolutive mixtures
        is adopted to obtain a clear separation of Long Period events
        from the ambient noise. The approach presents good performance
        and it is suitable for real time implementation in seismic
        monitoring. Its application to the continuous seismic signal
        recorded at Campi Flegrei has allowed the extraction of high-
        quality waveforms, considerably improving the detection of low-
        energy events.}",
          doi = {10.1029/2011GL049065},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2011GeoRL..3818302C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2011GeoRL..3815801D,
       author = {{Dwivedi}, Suneet and {Pandey}, Avinash C.},
        title = "{Forecasting the Indian summer monsoon intraseasonal oscillations using genetic algorithm and neural network}",
      journal = {\grl},
     keywords = {Computational Geophysics: Neural networks, fuzzy logic, machine learning (1942), Mathematical Geophysics: Prediction (3245, 4263, 4315), Mathematical Geophysics: Time series analysis (1872, 1988, 4277, 4475), Atmospheric Processes: Tropical meteorology},
         year = "2011",
        month = "Aug",
       volume = {38},
       number = {15},
          eid = {L15801},
        pages = {L15801},
     abstract = "{The correct and timely forecast of the Indian summer monsoon
        Intraseasonal Oscillations (ISOs) is very important. It has
        great impact on the agriculture and economy of the Indian
        subcontinent region. The applicability of Genetic Algorithm (GA)
        is demonstrated for nonlinear curve fitting of the inherently
        chaotic and noisy Lorenz time series and the ISO data. A robust
        method is developed for the very long-range prediction of the
        ISO using a feed-forward time delay backpropagation Artificial
        Neural Network (ANN). Using an iterative one-step-ahead
        prediction strategy, five years (120 pentads) of advanced
        prediction is made for the ISO data with good forecast skill. It
        is shown that a hybrid GA-ANN model may be used as an early
        forecast model followed by ANN only model as a more reliable
        model.}",
          doi = {10.1029/2011GL048314},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2011GeoRL..3815801D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2011ApJ...736..141S,
       author = {{Shamir}, Lior},
        title = "{Ganalyzer: A Tool for Automatic Galaxy Image Analysis}",
      journal = {\apj},
     keywords = {Galaxy: general, methods: data analysis, surveys, techniques: image processing, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2011",
        month = "Aug",
       volume = {736},
       number = {2},
          eid = {141},
        pages = {141},
     abstract = "{We describe Ganalyzer, a model-based tool that can automatically analyze
        and classify galaxy images. Ganalyzer works by separating the
        galaxy pixels from the background pixels, finding the center and
        radius of the galaxy, generating the radial intensity plot, and
        then computing the slopes of the peaks detected in the radial
        intensity plot to measure the spirality of the galaxy and
        determine its morphological class. Unlike algorithms that are
        based on machine learning, Ganalyzer is based on measuring the
        spirality of the galaxy, a task that is difficult to perform
        manually, and in many cases can provide a more accurate analysis
        compared to manual observation. Ganalyzer is simple to use, and
        can be easily embedded into other image analysis applications.
        Another advantage is its speed, which allows it to analyze
        \raisebox{-0.5ex}\textasciitilde10,000,000 galaxy images in five
        days using a standard modern desktop computer. These
        capabilities can make Ganalyzer a useful tool in analyzing large
        data sets of galaxy images collected by autonomous sky surveys
        such as SDSS, LSST, or DES. The software is available for free
        download at <A href=``http://vfacstaff.ltu.edu/lshamir/downloads
        /ganalyzer''>http://vfacstaff.ltu.edu/lshamir/downloads/ganalyze
        r</A>, and the data used in the experiment are available at <A h
        ref=``http://vfacstaff.ltu.edu/lshamir/downloads/ganalyzer/Galax
        yImages.zip''>http://vfacstaff.ltu.edu/lshamir/downloads/ganalyz
        er/GalaxyImages.zip</A>.}",
          doi = {10.1088/0004-637X/736/2/141},
archivePrefix = {arXiv},
       eprint = {1105.3214},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2011ApJ...736..141S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2011JGRE..116.7001G,
       author = {{Gilmore}, Martha S. and {Thompson}, David R. and {Anderson}, Laura J. and
         {Karamzadeh}, Nader and {Mandrake}, Lukas and {Casta{\~n}o}, Rebecca},
        title = "{Superpixel segmentation for analysis of hyperspectral data sets, with application to Compact Reconnaissance Imaging Spectrometer for Mars data, Moon Mineralogy Mapper data, and Ariadnes Chaos, Mars}",
      journal = {Journal of Geophysical Research (Planets)},
     keywords = {Informatics: Data mining, Informatics: Machine learning (0555), Informatics: Spatial analysis and representation (0500, 3252), Planetary Sciences: Solid Surface Planets: Remote sensing, Planetary Sciences: Solid Surface Planets: Instruments and techniques},
         year = "2011",
        month = "Jul",
       volume = {116},
       number = {E7},
          eid = {E07001},
        pages = {E07001},
     abstract = "{We present a semiautomated method to extract spectral end-members from
        hyperspectral images. This method employs superpixels, which are
        spectrally homogeneous regions of spatially contiguous pixels.
        The superpixel segmentation is combined with an unsupervised
        end-member extraction algorithm. Superpixel segmentation can
        complement per pixel classification techniques by reducing both
        scene-specific noise and computational complexity. The end-
        member extraction step explores the entire spectrum, recognizes
        target mineralogies within spectral mixtures, and enhances the
        discovery of unanticipated spectral classes. The method is
        applied to Compact Reconnaissance Imaging Spectrometer for Mars
        (CRISM) images and compared to a manual expert classification
        and to state-of the-art image analysis techniques. The technique
        successfully recognizes all classes identified by the expert,
        producing spectral end-members that match well to target
        classes. Application of the technique to CRISM multispectral
        data and Moon Mineralogy Mapper (M$^{3}$) hyperspectral data
        demonstrates the flexibility of the method in the analysis of a
        range of data sets. The technique is then used to analyze CRISM
        data in Ariadnes Chaos, Mars, and recognizes both
        phyllosilicates and sulfates in the chaos mounds. These aqueous
        deposits likely reflect changing environmental conditions during
        the Late Noachian/Early Hesperian. This semiautomated focus-of-
        attention tool will facilitate the identification of materials
        of interest on planetary surfaces whose constituents are
        unknown.}",
          doi = {10.1029/2010JE003763},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2011JGRE..116.7001G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2011ApJ...735...68K,
       author = {{Kim}, Dae-Won and {Protopapas}, Pavlos and {Byun}, Yong-Ik and
         {Alcock}, Charles and {Khardon}, Roni and {Trichas}, Markos},
        title = "{Quasi-stellar Object Selection Algorithm Using Time Variability and Machine Learning: Selection of 1620 Quasi-stellar Object Candidates from MACHO Large Magellanic Cloud Database}",
      journal = {\apj},
     keywords = {Magellanic Clouds, methods: data analysis, quasars: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2011",
        month = "Jul",
       volume = {735},
       number = {2},
          eid = {68},
        pages = {68},
     abstract = "{We present a new quasi-stellar object (QSO) selection algorithm using a
        Support Vector Machine, a supervised classification method, on a
        set of extracted time series features including period,
        amplitude, color, and autocorrelation value. We train a model
        that separates QSOs from variable stars, non-variable stars, and
        microlensing events using 58 known QSOs, 1629 variable stars,
        and 4288 non-variables in the MAssive Compact Halo Object
        (MACHO) database as a training set. To estimate the efficiency
        and the accuracy of the model, we perform a cross-validation
        test using the training set. The test shows that the model
        correctly identifies \raisebox{-0.5ex}\textasciitilde80\% of
        known QSOs with a 25\% false-positive rate. The majority of the
        false positives are Be stars. We applied the trained model to
        the MACHO Large Magellanic Cloud (LMC) data set, which consists
        of 40 million light curves, and found 1620 QSO candidates.
        During the selection none of the 33,242 known MACHO variables
        were misclassified as QSO candidates. In order to estimate the
        true false-positive rate, we crossmatched the candidates with
        astronomical catalogs including the Spitzer Surveying the Agents
        of a Galaxy's Evolution LMC catalog and a few X-ray catalogs.
        The results further suggest that the majority of the candidates,
        more than 70\%, are QSOs.}",
          doi = {10.1088/0004-637X/735/2/68},
archivePrefix = {arXiv},
       eprint = {1101.3316},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2011ApJ...735...68K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2011ISPL...18..371H,
       author = {{Huijse}, Pablo and {Estevez}, Pablo A. and {Zegers}, Pablo and
         {Principe}, Jos{\'e} C. and {Protopapas}, Pavlos},
        title = "{Period Estimation in Astronomical Time Series Using Slotted Correntropy}",
      journal = {IEEE Signal Processing Letters},
     keywords = {Computer Science - Information Theory, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Machine Learning},
         year = "2011",
        month = "Jun",
       volume = {18},
       number = {6},
        pages = {371-374},
     abstract = "{In this letter, we propose a method for period estimation in light
        curves from periodic variable stars using correntropy. Light
        curves are astronomical time series of stellar brightness over
        time, and are characterized as being noisy and unevenly sampled.
        We propose to use slotted time lags in order to estimate
        correntropy directly from irregularly sampled time series. A new
        information theoretic metric is proposed for discriminating
        among the peaks of the correntropy spectral density. The slotted
        correntropy method outperformed slotted correlation, string
        length, VarTools (Lomb-Scargle periodogram and Analysis of
        Variance), and SigSpec applications on a set of light curves
        drawn from the MACHO survey.}",
          doi = {10.1109/LSP.2011.2141987},
archivePrefix = {arXiv},
       eprint = {1112.2962},
 primaryClass = {cs.IT},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2011ISPL...18..371H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2011ApJ...734L...9W,
       author = {{Way}, M.~J.},
        title = "{Galaxy Zoo Morphology and Photometric Redshifts in the Sloan Digital Sky Survey}",
      journal = {\apj},
     keywords = {galaxies: distances and redshifts, methods: statistical, Astrophysics - Cosmology and Nongalactic Astrophysics},
         year = "2011",
        month = "Jun",
       volume = {734},
       number = {1},
          eid = {L9},
        pages = {L9},
     abstract = "{It has recently been demonstrated that one can accurately derive galaxy
        morphology from particular primary and secondary isophotal shape
        estimates in the Sloan Digital Sky Survey (SDSS) imaging
        catalog. This was accomplished by applying Machine Learning
        techniques to the Galaxy Zoo morphology catalog. Using the broad
        bandpass photometry of the SDSS in combination with precise
        knowledge of galaxy morphology should help in estimating more
        accurate photometric redshifts for galaxies. Using the Galaxy
        Zoo separation for spirals and ellipticals in combination with
        SDSS photometry we attempt to calculate photometric redshifts.
        In the best case we find that the root-mean-square error for
        luminous red galaxies classified as ellipticals is as low as
        0.0118. Given these promising results we believe better
        photometric redshift estimates for all galaxies in the SDSS
        (\raisebox{-0.5ex}\textasciitilde350 million) will be feasible
        if researchers can also leverage their derived morphologies via
        Machine Learning. These initial results look to be promising for
        those interested in estimating weak lensing, baryonic acoustic
        oscillation, and other fields dependent upon accurate
        photometric redshifts.}",
          doi = {10.1088/2041-8205/734/1/L9},
archivePrefix = {arXiv},
       eprint = {1104.3758},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2011ApJ...734L...9W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2011ApJ...733...10R,
       author = {{Richards}, Joseph W. and {Starr}, Dan L. and {Butler}, Nathaniel R. and
         {Bloom}, Joshua S. and {Brewer}, John M. and {Crellin-Quick}, Arien and
         {Higgins}, Justin and {Kennedy}, Rachel and {Rischard}, Maxime},
        title = "{On Machine-learned Classification of Variable Stars with Sparse and Noisy Time-series Data}",
      journal = {\apj},
     keywords = {methods: data analysis, methods: statistical, stars: variables: general, techniques: photometric, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Applications},
         year = "2011",
        month = "May",
       volume = {733},
       number = {1},
          eid = {10},
        pages = {10},
     abstract = "{With the coming data deluge from synoptic surveys, there is a need for
        frameworks that can quickly and automatically produce calibrated
        classification probabilities for newly observed variables based
        on small numbers of time-series measurements. In this paper, we
        introduce a methodology for variable-star classification,
        drawing from modern machine-learning techniques. We describe how
        to homogenize the information gleaned from light curves by
        selection and computation of real-numbered metrics (features),
        detail methods to robustly estimate periodic features, introduce
        tree-ensemble methods for accurate variable-star classification,
        and show how to rigorously evaluate a classifier using cross
        validation. On a 25-class data set of 1542 well-studied variable
        stars, we achieve a 22.8\% error rate using the random forest
        (RF) classifier; this represents a 24\% improvement over the
        best previous classifier on these data. This methodology is
        effective for identifying samples of specific science classes:
        for pulsational variables used in Milky Way tomography we obtain
        a discovery efficiency of 98.2\% and for eclipsing systems we
        find an efficiency of 99.1\%, both at 95\% purity. The RF
        classifier is superior to other methods in terms of accuracy,
        speed, and relative immunity to irrelevant features; the RF can
        also be used to estimate the importance of each feature in
        classification. Additionally, we present the first astronomical
        use of hierarchical classification methods to incorporate a
        known class taxonomy in the classifier, which reduces the
        catastrophic error rate from 8\% to 7.8\%. Excluding low-
        amplitude sources, the overall error rate improves to 14\%, with
        a catastrophic error rate of 3.5\%.}",
          doi = {10.1088/0004-637X/733/1/10},
archivePrefix = {arXiv},
       eprint = {1101.1959},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2011ApJ...733...10R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2011PhDT.......131D,
       author = {{Durak}, Nurcan},
        title = "{Coronal loop detection and salient contour group extraction from solar images}",
     keywords = {Physics, Astronomy and Astrophysics;Computer Science},
       school = {University of Louisville},
         year = "2011",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2011PhDT.......131D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2011PhDT........68Y,
       author = {{Yuan}, Yuan},
        title = "{Development of advanced algorithms to detect, characterize and forecast solar activities}",
     keywords = {Physics, Astronomy and Astrophysics;Computer Science},
       school = {New Jersey Institute of Technology},
         year = "2011",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2011PhDT........68Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2010PASP..122.1518G,
       author = {{Gazis}, P.~R. and {Levit}, C. and {Way}, M.~J.},
        title = "{Viewpoints: A High-Performance High-Dimensional Exploratory Data Analysis Tool}",
      journal = {\pasp},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Graphics, Physics - Data Analysis, Statistics and Probability},
         year = "2010",
        month = "Dec",
       volume = {122},
       number = {898},
        pages = {1518},
     abstract = "{Scientific data sets continue to increase in both size and complexity.
        In the past, dedicated graphics systems at supercomputing
        centers were required to visualize large data sets, but as the
        price of commodity graphics hardware has dropped and its
        capability has increased, it is now possible, in principle, to
        view large complex data sets on a single workstation. To do this
        in practice, an investigator will need software that is written
        to take advantage of the relevant graphics hardware. The
        Viewpoints visualization package described herein is an example
        of such software. Viewpoints is an interactive tool for
        exploratory visual analysis of large high-dimensional
        (multivariate) data. It leverages the capabilities of modern
        graphics boards (GPUs) to run on a single workstation or laptop.
        Viewpoints is minimalist: it attempts to do a small set of
        useful things very well (or at least very quickly) in comparison
        with similar packages today. Its basic feature set includes
        linked scatter plots with brushing, dynamic histograms,
        normalization, and outlier detection/removal. Viewpoints was
        originally designed for astrophysicists, but it has since been
        used in a variety of fields that range from astronomy, quantum
        chemistry, fluid dynamics, machine learning, bioinformatics, and
        finance to information technology server log mining. In this
        article, we describe the Viewpoints package and show examples of
        its usage.}",
          doi = {10.1086/657902},
archivePrefix = {arXiv},
       eprint = {1008.2205},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010PASP..122.1518G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2010ApJS..191..254H,
       author = {{Hao}, Jiangang and {McKay}, Timothy A. and {Koester}, Benjamin P. and
         {Rykoff}, Eli S. and {Rozo}, Eduardo and {Annis}, James and
         {Wechsler}, Risa H. and {Evrard}, August and {Siegel}, Seth R. and
         {Becker}, Matthew and {Busha}, Michael and {Gerdes}, David and
         {Johnston}, David E. and {Sheldon}, Erin},
        title = "{A GMBCG Galaxy Cluster Catalog of 55,424 Rich Clusters from SDSS DR7}",
      journal = {\apjs},
     keywords = {catalogs, galaxies: clusters: general, methods: statistical, Astrophysics - Cosmology and Extragalactic Astrophysics, Statistics - Computation, Statistics - Machine Learning},
         year = "2010",
        month = "Dec",
       volume = {191},
       number = {2},
        pages = {254-274},
     abstract = "{We present a large catalog of optically selected galaxy clusters from
        the application of a new Gaussian Mixture Brightest Cluster
        Galaxy (GMBCG) algorithm to SDSS Data Release 7 data. The
        algorithm detects clusters by identifying the red-sequence plus
        brightest cluster galaxy (BCG) feature, which is unique for
        galaxy clusters and does not exist among field galaxies. Red-
        sequence clustering in color space is detected using an Error
        Corrected Gaussian Mixture Model. We run GMBCG on 8240 deg$^{2}$
        of photometric data from SDSS DR7 to assemble the largest ever
        optical galaxy cluster catalog, consisting of over 55,000 rich
        clusters across the redshift range from 0.1 \&lt; z \&lt; 0.55.
        We present Monte Carlo tests of completeness and purity and
        perform cross-matching with X-ray clusters and with the maxBCG
        sample at low redshift. These tests indicate high completeness
        and purity across the full redshift range for clusters with 15
        or more members.}",
          doi = {10.1088/0067-0049/191/2/254},
archivePrefix = {arXiv},
       eprint = {1010.5503},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010ApJS..191..254H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2010A&A...522A..88S,
       author = {{Smith}, K.~W. and {Bailer-Jones}, C.~A.~L. and {Klement}, R.~J. and
         {Xue}, X.~X.},
        title = "{Photometric identification of blue horizontal branch stars}",
      journal = {\aap},
     keywords = {methods: statistical, stars: horizontal-branch, Galaxy: structure, Astrophysics - Astrophysics of Galaxies},
         year = "2010",
        month = "Nov",
       volume = {522},
          eid = {A88},
        pages = {A88},
     abstract = "{We investigate the performance of some common machine learning
        techniques in identifying blue horizontal branch (BHB) stars
        from photometric data. To train the machine learning algorithms,
        we use previously published spectroscopic identifications of BHB
        stars from Sloan digital sky survey (SDSS) data. We investigate
        the performance of three different techniques, namely k nearest
        neighbour classification, kernel density estimation for
        discriminant analysis and a support vector machine (SVM). We
        discuss the performance of the methods in terms of both
        completeness (what fraction of input BHB stars are successfully
        returned as BHB stars) and contamination (what fraction of
        contaminating sources end up in the output BHB sample). We
        discuss the prospect of trading off these values, achieving
        lower contamination at the expense of lower completeness, by
        adjusting probability thresholds for the classification. We also
        discuss the role of prior probabilities in the classification
        performance, and we assess via simulations the reliability of
        the dataset used for training. Overall it seems that no-prior
        gives the best completeness, but adopting a prior lowers the
        contamination. We find that the support vector machine generally
        delivers the lowest contamination for a given level of
        completeness, and so is our method of choice. Finally, we
        classify a large sample of SDSS Data Release 7 (DR7) photometry
        using the SVM trained on the spectroscopic sample. We identify
        27 074 probable BHB stars out of a sample of 294 652 stars. We
        derive photometric parallaxes and demonstrate that our results
        are reasonable by comparing to known distances for a selection
        of globular clusters. We attach our classifications, including
        probabilities, as an electronic table, so that they can be used
        either directly as a BHB star catalogue, or as priors to a
        spectroscopic or other classification method. We also provide
        our final models so that they can be directly applied to new
        data. Full Tables 7, A.3 and A.4 are only available in
        electronic form at the CDS via anonymous ftp to
        cdsarc.u-strasbg.fr (130.79.128.5) or via <A
        href=``http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/522/A88''>http://cdsarc.u-strasbg.fr/viz-
        bin/qcat?J/A+A/522/A88</A>}",
          doi = {10.1051/0004-6361/201014381},
archivePrefix = {arXiv},
       eprint = {1008.2446},
 primaryClass = {astro-ph.GA},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010A&A...522A..88S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2010PhDT.......137R,
       author = {{Richards}, Joseph W.},
        title = "{Fast and accurate estimation for astrophysical problems in large databases}",
     keywords = {Statistics;Physics, Astrophysics;Physics, Astronomy and Astrophysics},
       school = {Carnegie Mellon University},
         year = "2010",
        month = "Oct",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010PhDT.......137R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2010GeoJI.182.1619K,
       author = {{K{\"o}hler}, Andreas and {Ohrnberger}, Matthias and {Scherbaum}, Frank},
        title = "{Unsupervised pattern recognition in continuous seismic wavefield records using Self-Organizing Maps}",
      journal = {Geophysical Journal International},
     keywords = {Neural networks, fuzzy logic; Probability distributions; Site effects; Volcano seismology; Volcano monitoring},
         year = "2010",
        month = "Sep",
       volume = {182},
       number = {3},
        pages = {1619-1630},
     abstract = "{Modern acquisition of seismic data on receiver networks worldwide
        produces an increasing amount of continuous wavefield
        recordings. In addition to manual data inspection, seismogram
        interpretation requires therefore new processing utilities for
        event detection, signal classification and data visualization.
        The use of machine learning techniques automatises decision
        processes and reveals the statistical properties of data. This
        approach is becoming more and more important and valuable for
        large and complex seismic records. Unsupervised learning allows
        the recognition of wavefield patterns, such as short-term
        transients and long-term variations, with a minimum of domain
        knowledge. This study applies an unsupervised pattern
        recognition approach for the discovery, imaging and
        interpretation of temporal patterns in seismic array recordings.
        For this purpose, the data is parameterized by feature vectors,
        which combine different real-valued wavefield attributes for
        short time windows. Standard seismic analysis tools are used as
        feature generation methods, such as frequency-wavenumber,
        polarization and spectral analysis. We use Self-Organizing Maps
        (SOMs) for a data-driven feature selection, visualization and
        clustering procedure. The application to continuous recordings
        of seismic signals from an active volcano (Mount Merapi, Java,
        Indonesia) shows that volcano-tectonic and rockfall events can
        be detected and distinguished by clustering the feature vectors.
        Similar results are obtained in terms of correctly classifying
        events compared to a previously implemented supervised
        classification system. Furthermore, patterns in the background
        wavefield, that is the 24-hr cycle due to human activity, are
        intuitively visualized by means of the SOM representation.
        Finally, we apply our technique to an ambient seismic vibration
        record, which has been acquired for local site characterization.
        Disturbing wavefield patterns are identified which affect the
        quality of Love wave dispersion curve estimates. Particularly at
        night, when the overall energy of the wavefield is reduced due
        to the 24-hr cycle, the common assumption of stationary planar
        surface waves can be violated.}",
          doi = {10.1111/j.1365-246X.2010.04709.x},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010GeoJI.182.1619K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2010MNRAS.406..342B,
       author = {{Banerji}, Manda and {Lahav}, Ofer and {Lintott}, Chris J. and
         {Abdalla}, Filipe B. and {Schawinski}, Kevin and {Bamford}, Steven P. and
         {Andreescu}, Dan and {Murray}, Phil and {Raddick}, M. Jordan and
         {Slosar}, Anze and {Szalay}, Alex and {Thomas}, Daniel and {Vand
        enberg}, Jan},
        title = "{Galaxy Zoo: reproducing galaxy morphologies via machine learning}",
      journal = {\mnras},
     keywords = {methods: data analysis, galaxies: general, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Astrophysics of Galaxies},
         year = "2010",
        month = "Jul",
       volume = {406},
       number = {1},
        pages = {342-353},
     abstract = "{We present morphological classifications obtained using machine learning
        for objects in the Sloan Digital Sky Survey DR6 that have been
        classified by Galaxy Zoo into three classes, namely early types,
        spirals and point sources/artefacts. An artificial neural
        network is trained on a subset of objects classified by the
        human eye, and we test whether the machine-learning algorithm
        can reproduce the human classifications for the rest of the
        sample. We find that the success of the neural network in
        matching the human classifications depends crucially on the set
        of input parameters chosen for the machine-learning algorithm.
        The colours and parameters associated with profile fitting are
        reasonable in separating the objects into three classes.
        However, these results are considerably improved when adding
        adaptive shape parameters as well as concentration and texture.
        The adaptive moments, concentration and texture parameters alone
        cannot distinguish between early type galaxies and the point
        sources/artefacts. Using a set of 12 parameters, the neural
        network is able to reproduce the human classifications to better
        than 90 per cent for all three morphological classes. We find
        that using a training set that is incomplete in magnitude does
        not degrade our results given our particular choice of the input
        parameters to the network. We conclude that it is promising to
        use machine-learning algorithms to perform morphological
        classification for the next generation of wide-field imaging
        surveys and that the Galaxy Zoo catalogue provides an invaluable
        training set for such purposes. This publication has been made
        possible by the participation of more than 100000 volunteers in
        the Galaxy Zoo project. Their contributions are individually
        acknowledged at http://www.galaxyzoo.org/Volunteers.aspx.
        E-mail: mbanerji@ast.cam.ac.uk {\textdaggerdbl} Einstein Fellow.}",
          doi = {10.1111/j.1365-2966.2010.16713.x},
archivePrefix = {arXiv},
       eprint = {0908.2033},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010MNRAS.406..342B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2010ApJ...715..823G,
       author = {{Gerdes}, David W. and {Sypniewski}, Adam J. and {McKay}, Timothy A. and
         {Hao}, Jiangang and {Weis}, Matthew R. and {Wechsler}, Risa H. and
         {Busha}, Michael T.},
        title = "{ArborZ: Photometric Redshifts Using Boosted Decision Trees}",
      journal = {\apj},
     keywords = {galaxies: distances and redshifts, galaxies: statistics, large-scale structure of universe, methods: data analysis, methods: statistical, Astrophysics - Cosmology and Extragalactic Astrophysics},
         year = "2010",
        month = "Jun",
       volume = {715},
       number = {2},
        pages = {823-832},
     abstract = "{Precision photometric redshifts will be essential for extracting
        cosmological parameters from the next generation of wide-area
        imaging surveys. In this paper, we introduce a photometric
        redshift algorithm, ArborZ, based on the machine-learning
        technique of boosted decision trees. We study the algorithm
        using galaxies from the Sloan Digital Sky Survey (SDSS) and from
        mock catalogs intended to simulate both the SDSS and the
        upcoming Dark Energy Survey. We show that it improves upon the
        performance of existing algorithms. Moreover, the method
        naturally leads to the reconstruction of a full probability
        density function (PDF) for the photometric redshift of each
        galaxy, not merely a single ``best estimate'' and error, and
        also provides a photo-z quality figure of merit for each galaxy
        that can be used to reject outliers. We show that the stacked
        PDFs yield a more accurate reconstruction of the redshift
        distribution N(z). We discuss limitations of the current
        algorithm and ideas for future work.}",
          doi = {10.1088/0004-637X/715/2/823},
archivePrefix = {arXiv},
       eprint = {0908.4085},
 primaryClass = {astro-ph.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010ApJ...715..823G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2010SoPh..262..511A,
       author = {{Al-Omari}, M. and {Qahwaji}, R. and {Colak}, T. and {Ipson}, S.},
        title = "{Machine Leaning-Based Investigation of the Associations between CMEs and Filaments}",
      journal = {\solphys},
     keywords = {Coronal mass ejections, Filaments, Machine learning, Prominences, Space weather},
         year = "2010",
        month = "Apr",
       volume = {262},
       number = {2},
        pages = {511-539},
     abstract = "{In this work we study the association between eruptive
        filaments/prominences and coronal mass ejections (CMEs) using
        machine learning-based algorithms that analyse the solar data
        available between January 1996 and December 2001. The support
        vector machine (SVM) learning algorithm is used for the purpose
        of knowledge extraction from the association results. The aim is
        to identify patterns of associations that can be represented
        using SVM learning rules for the subsequent use in near real-
        time and reliable CME prediction systems. Timing and location
        data in the US National Geophysical Data Center (NGDC) filament
        catalogue and the Solar and Heliospheric Observatory/ Large
        Angle and Spectrometric Coronagraph (SOHO/LASCO) CME catalogue
        are processed to associate filaments with CMEs. In the previous
        studies, which classified CMEs into gradual and impulsive CMEs,
        the associations were refined based on the CME speed and
        acceleration. Then the associated pairs were refined manually to
        increase the accuracy of the training dataset. In the current
        study, a data-mining system is created to process and associate
        filament and CME data, which are arranged in numerical training
        vectors. Then the data are fed to SVMs to extract the embedded
        knowledge and provide the learning rules that can have the
        potential, in the future, to provide automated predictions of
        CMEs. The features representing the event time (average of the
        start and end times), duration, type, and extent of the
        filaments are extracted from all the associated and not-
        associated filaments and converted to a numerical format that is
        suitable for SVM use. Several validation and verification
        methods are used on the extracted dataset to determine if CMEs
        can be predicted solely and efficiently based on the associated
        filaments. More than 14 000 experiments are carried out to
        optimise the SVM and determine the input features that provide
        the best performance.}",
          doi = {10.1007/s11207-010-9516-5},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010SoPh..262..511A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2010SoPh..262..299H,
       author = {{Henwood}, R. and {Chapman}, S.~C. and {Willis}, D.~M.},
        title = "{Increasing Lifetime of Recurrent Sunspot Groups Within the Greenwich Photoheliographic Results}",
      journal = {\solphys},
     keywords = {Sunspots, Neural networks, Long-term change, Non-linear, Lifetime, Greenwich, Sunspot nests, Sunspot nestlet, Astrophysics - Solar and Stellar Astrophysics},
         year = "2010",
        month = "Apr",
       volume = {262},
       number = {2},
        pages = {299-313},
     abstract = "{Long-lived (\&gt;20 days) sunspot groups extracted from the Greenwich
        Photoheliographic Results (GPR) are examined for evidence of
        decadal change. The problem of identifying sunspot groups that
        are observed on consecutive solar rotations (recurrent sunspot
        groups) is tackled by first constructing manually an example
        dataset of recurrent sunspot groups and then using machine
        learning to generalise this subset to the whole GPR. The
        resulting dataset of recurrent sunspot groups is verified
        against previous work by A. Maunder and other Royal Greenwich
        Observatory (RGO) compilers. Recurrent groups are found to
        exhibit a slightly larger value for the Gnevyshev - Waldmeier
        Relationship than the value found by Petrovay and van Driel-
        Gesztelyi ( Solar Phys. 51, 25, 1977), who used recurrence data
        from the Debrecen Photoheliographic Results. Evidence for
        sunspot-group lifetime change over the previous century is
        observed within recurrent groups. A lifetime increase of a
        factor of 1.4 between 1915 and 1940 is found, which closely
        agrees with results from Blanter et al. ( Solar Phys. 237, 329,
        2006). Furthermore, this increase is found to exist over a
        longer period (1915 to 1950) than previously thought and
        provisional evidence is found for a decline between 1950 and
        1965. Possible applications of machine-learning procedures to
        the analysis of historical sunspot observations, the
        determination of the magnetic topology of the solar corona and
        the incidence of severe space-weather events are outlined
        briefly.}",
          doi = {10.1007/s11207-009-9419-5},
archivePrefix = {arXiv},
       eprint = {0907.4274},
 primaryClass = {astro-ph.SR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010SoPh..262..299H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2010JGRA..115.4306M,
       author = {{Maruyama}, Takashi},
        title = "{Solar proxies pertaining to empirical ionospheric total electron content models}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {Ionosphere: Modeling and forecasting, Solar Physics, Astrophysics, and Astronomy: Solar irradiance, Ionosphere: Midlatitude ionosphere, Ionosphere: Solar radiation and cosmic ray effects, Computational Geophysics: Neural networks, fuzzy logic, machine learning (1942)},
         year = "2010",
        month = "Apr",
       volume = {115},
       number = {A4},
          eid = {A04306},
        pages = {A04306},
     abstract = "{Solar proxies and indices exhibiting extreme ultraviolet (EUV)
        irradiance that affects the ionospheric total electron content
        (TEC) were examined through training an artificial neural
        network (ANN). A TEC database was constructed from a dense GPS
        receiver network over Japan from April 1997 to March 2008,
        covering an entire 11 year solar activity period. In empirical
        models of upper atmospheric parameters, such as the
        International Reference Ionosphere model and the Mass
        Spectrometer and Incoherent Scatter thermospheric model, the
        10.7 cm solar radio flux (F$_{10.7}$) or the sunspot number (R)
        is used as a proxy for determining the solar activity. In the
        present study, ANN training for predicting TEC as a target
        parameter was done by including new solar proxies/indices in the
        input space that were based on direct measurements of solar
        EUV/UV flux, SOHO\_SEM$_{26-34}$ (the integrated 26-34 nm EUV
        emission), and Mg II cwr (the core-to-wing ratio of Mg II 280 nm
        line), as well as the traditional indices F$_{10.7}$ and R. Root
        mean square errors (RMSEs) of TEC were compared after the
        training was completed using a variety of combinations of solar
        proxies. When a single proxy was used, SOHO\_SEM$_{26-34}$
        yielded the smallest RMSE, or it was the best proxy for modeling
        ionospheric TEC. Further, general improvements were obtained by
        combining different types of proxies and short- and long-term
        means of them. The best combination was the 3 day smoothed
        daily, 7 day and 27 day backward mean values of Mg II cwr,
        SOHO\_SEM$_{26-34}$, and the 10.7 cm radio flux.}",
          doi = {10.1029/2009JA014890},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010JGRA..115.4306M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2010AdAst2010E..58P,
       author = {{Pesenson}, Meyer Z. and {Pesenson}, Isaac Z. and {McCollum}, Bruce},
        title = "{The Data Big Bang and the Expanding Digital Universe: High-Dimensional, Complex and Massive Data Sets in an Inflationary Epoch}",
      journal = {Advances in Astronomy},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
         year = "2010",
        month = "Apr",
       volume = {2010},
          eid = {350891},
        pages = {350891},
     abstract = "{Recent and forthcoming advances in instrumentation, and giant new
        surveys, are creating astronomical data sets that are not
        amenable to the methods of analysis familiar to astronomers.
        Traditional methods are often inadequate not merely because of
        the size in bytes of the data sets, but also because of the
        complexity of modern data sets. Mathematical limitations of
        familiar algorithms and techniques in dealing with such data
        sets create a critical need for new paradigms for the
        representation, analysis and scientific visualization (as
        opposed to illustrative visualization) of heterogeneous,
        multiresolution data across application domains. Some of the
        problems presented by the new data sets have been addressed by
        other disciplines such as applied mathematics, statistics and
        machine learning and have been utilized by other sciences such
        as space-based geosciences. Unfortunately, valuable results
        pertaining to these problems are mostly to be found only in
        publications outside of astronomy. Here we offer brief overviews
        of a number of concepts, techniques and developments, some
        ``old'' and some new. These are generally unknown to most of the
        astronomical community, but are vital to the analysis and
        visualization of complex datasets and images. In order for
        astronomers to take advantage of the richness and complexity of
        the new era of data, and to be able to identify, adopt, and
        apply new solutions, the astronomical community needs a certain
        degree of awareness and understanding of the new concepts. One
        of the goals of this paper is to help bridge the gap between
        applied mathematics, artificial intelligence and computer
        science on the one side and astronomy on the other.}",
          doi = {10.1155/2010/350891},
archivePrefix = {arXiv},
       eprint = {1003.0879},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010AdAst2010E..58P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2010MNRAS.403...96B,
       author = {{Bailer-Jones}, C.~A.~L.},
        title = "{The ILIUM forward modelling algorithm for multivariate parameter estimation and its application to derive stellar parameters from Gaia spectrophotometry}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, techniques: spectroscopic, surveys, stars: fundamental parameters, dust, extinction, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Astrophysics of Galaxies, Astrophysics - Solar and Stellar Astrophysics, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = "2010",
        month = "Mar",
       volume = {403},
       number = {1},
        pages = {96-116},
     abstract = "{I introduce an algorithm for estimating parameters from multidimensional
        data based on forward modelling. It performs an iterative local
        search to effectively achieve a non-linear interpolation of a
        template grid. In contrast to many machine-learning approaches,
        it avoids fitting an inverse model and the problems associated
        with this. The algorithm makes explicit use of the sensitivities
        of the data to the parameters, with the goal of better treating
        parameters which only have a weak impact on the data. The
        forward modelling approach provides uncertainty (full
        covariance) estimates in the predicted parameters as well as a
        goodness-of-fit for observations, thus providing a simple means
        of identifying outliers. I demonstrate the algorithm, ILIUM,
        with the estimation of stellar astrophysical parameters (APs)
        from simulations of the low-resolution spectrophotometry to be
        obtained by Gaia. The AP accuracy is competitive with that
        obtained by a support vector machine. For zero extinction stars
        covering a wide range of metallicity, surface gravity and
        temperature, ILIUM can estimate T$_{eff}$ to an accuracy of 0.3
        per cent at G = 15 and to 4 per cent for (lower signal-to-noise
        ratio) spectra at G = 20, the Gaia limiting magnitude (mean
        absolute errors are quoted). [Fe/H] and logg can be estimated to
        accuracies of 0.1-0.4dex for stars with G \&lt;= 18.5, depending
        on the magnitude and what priors we can place on the APs. If
        extinction varies a priori over a wide range (0-10mag) - which
        will be the case with Gaia because it is an all-sky survey -
        then logg and [Fe/H] can still be estimated to 0.3 and 0.5dex,
        respectively, at G = 15, but much poorer at G = 18.5. T$_{eff}$
        and A$_{V}$ can be estimated quite accurately (3-4 per cent and
        0.1-0.2mag, respectively, at G = 15), but there is a strong and
        ubiquitous degeneracy in these parameters which limits our
        ability to estimate either accurately at faint magnitudes. Using
        the forward model, we can map these degeneracies (in advance)
        and thus provide a complete probability distribution over
        solutions. Additional information from the Gaia parallaxes,
        other surveys or suitable priors should help reduce these
        degeneracies.}",
          doi = {10.1111/j.1365-2966.2009.16125.x},
archivePrefix = {arXiv},
       eprint = {0911.5242},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010MNRAS.403...96B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2010IJMPD..19.1049B,
       author = {{Ball}, Nicholas M. and {Brunner}, Robert J.},
        title = "{Data Mining and Machine Learning in Astronomy}",
      journal = {International Journal of Modern Physics D},
     keywords = {Data mining, machine learning, knowledge discovery in databases, astroinformatics, astrostatistics, Virtual Observatory, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Extragalactic Astrophysics},
         year = "2010",
        month = "Jan",
       volume = {19},
       number = {7},
        pages = {1049-1106},
     abstract = "{We review the current state of data mining and machine learning in
        astronomy. Data Mining can have a somewhat mixed connotation
        from the point of view of a researcher in this field. If used
        correctly, it can be a powerful approach, holding the potential
        to fully exploit the exponentially increasing amount of
        available data, promising great scientific advance. However, if
        misused, it can be little more than the black box application of
        complex computing algorithms that may give little physical
        insight, and provide questionable results. Here, we give an
        overview of the entire data mining process, from data collection
        through to the interpretation of results. We cover common
        machine learning algorithms, such as artificial neural networks
        and support vector machines, applications from a broad range of
        astronomy, emphasizing those in which data mining techniques
        directly contributed to improving science, and important current
        and future directions, including probability density functions,
        parallel algorithms, Peta-Scale computing, and the time domain.
        We conclude that, so long as one carefully selects an
        appropriate algorithm and is guided by the astronomical problem
        at hand, data mining can be very much the powerful tool, and not
        the questionable black box.}",
          doi = {10.1142/S0218271810017160},
archivePrefix = {arXiv},
       eprint = {0906.2173},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010IJMPD..19.1049B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2010IJAsB...9...11M,
       author = {{McGuire}, P.~C. and {Gross}, C. and {Wendt}, L. and {Bonnici}, A. and
         {Souza-Egipsy}, V. and {Orm{\"o}}, J. and
         {D{\'\i}az-Mart{\'\i}nez}, E. and {Foing}, B.~H. and {Bose}, R. and
         {Walter}, S. and {Oesker}, M. and {Ontrup}, J. and {Haschke}, R. and
         {Ritter}, H.},
        title = "{The Cyborg Astrobiologist: testing a novelty detection algorithm on two mobile exploration systems at Rivas Vaciamadrid in Spain and at the Mars Desert Research Station in Utah}",
      journal = {International Journal of Astrobiology},
     keywords = {novelty detection, Hopfield neural network, sceintific autonomy, Bluetooth communications, phone camera, lichens, digital microscope, wearable computer, graphical programming language, image segmentation, uncommon mapping, Mars Desert Research Station, Morrison Formation, gypsum, single-instance learning, Computer Science - Computer Vision and Pattern Recognition, Astrophysics - Earth and Planetary Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2010",
        month = "Jan",
       volume = {9},
       number = {1},
        pages = {11-27},
     abstract = "{In previous work, a platform was developed for testing computer-vision
        algorithms for robotic planetary exploration. This platform
        consisted of a digital video camera connected to a wearable
        computer for real-time processing of images at geological and
        astrobiological field sites. The real-time processing included
        image segmentation and the generation of interest points based
        upon uncommonness in the segmentation maps. Also in previous
        work, this platform for testing computer-vision algorithms has
        been ported to a more ergonomic alternative platform, consisting
        of a phone camera connected via the Global System for Mobile
        Communications (GSM) network to a remote-server computer. The
        wearable-computer platform has been tested at geological and
        astrobiological field sites in Spain (Rivas Vaciamadrid and Riba
        de Santiuste), and the phone camera has been tested at a
        geological field site in Malta. In this work, we (i) apply a
        Hopfield neural-network algorithm for novelty detection based
        upon colour, (ii) integrate a field-capable digital microscope
        on the wearable computer platform, (iii) test this novelty
        detection with the digital microscope at Rivas Vaciamadrid, (iv)
        develop a Bluetooth communication mode for the phone-camera
        platform, in order to allow access to a mobile processing
        computer at the field sites, and (v) test the novelty detection
        on the Bluetooth-enabled phone camera connected to a netbook
        computer at the Mars Desert Research Station in Utah. This
        systems engineering and field testing have together allowed us
        to develop a real-time computer-vision system that is capable,
        for example, of identifying lichens as novel within a series of
        images acquired in semi-arid desert environments. We acquired
        sequences of images of geologic outcrops in Utah and Spain
        consisting of various rock types and colours to test this
        algorithm. The algorithm robustly recognized previously observed
        units by their colour, while requiring only a single image or a
        few images to learn colours as familiar, demonstrating its fast
        learning capability.}",
          doi = {10.1017/S1473550409990358},
archivePrefix = {arXiv},
       eprint = {0910.5454},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2010IJAsB...9...11M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2009PhRvE..80f6117W,
       author = {{Weinstein}, Marvin and {Horn}, David},
        title = "{Dynamic quantum clustering: A method for visual exploration of structures in data}",
      journal = {\pre},
     keywords = {89.75.Fb, 89.90.+n, 95.75.Pq, 89.75.Kd, Structures and organization in complex systems, Other topics in areas of applied and interdisciplinary physics, Mathematical procedures and computer techniques, Patterns, Physics - Data Analysis, Statistics and Probability, Computer Science - Data Structures and Algorithms, High Energy Physics - Experiment, Physics - Computational Physics, Statistics - Machine Learning},
         year = "2009",
        month = "Dec",
       volume = {80},
       number = {6},
          eid = {066117},
        pages = {066117},
     abstract = "{A given set of data points in some feature space may be associated with
        a Schr{\"o}dinger equation whose potential is determined by the
        data. This is known to lead to good clustering solutions. Here
        we extend this approach into a full-fledged dynamical scheme
        using a time-dependent Schr{\"o}dinger equation. Moreover, we
        approximate this Hamiltonian formalism by a truncated
        calculation within a set of Gaussian wave functions (coherent
        states) centered around the original points. This allows for
        analytic evaluation of the time evolution of all such states
        opening up the possibility of exploration of relationships among
        data points through observation of varying dynamical distances
        among points and convergence of points into clusters. This
        formalism may be further supplemented by preprocessing such as
        dimensional reduction through singular-value decomposition or
        feature filtering.}",
          doi = {10.1103/PhysRevE.80.066117},
archivePrefix = {arXiv},
       eprint = {0908.2644},
 primaryClass = {physics.data-an},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009PhRvE..80f6117W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2009PhDT.......500S,
       author = {{Scaringi}, Simone},
        title = "{Machine learning from hard x-ray surveys: applications to magnetic cataclysmic variable studies}",
     keywords = {Cataclysmic variable stars, Data analysis, High-energy astrophysics},
       school = {University of Southampton},
         year = "2009",
        month = "Nov",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009PhDT.......500S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2009PhRvC..80d4332C,
       author = {{Costiris}, N.~J. and {Mavrommatis}, E. and {Gernoth}, K.~A. and
         {Clark}, J.~W.},
        title = "{Decoding {\ensuremath{\beta}}-decay systematics: A global statistical model for {\ensuremath{\beta}}$^{-}$ half-lives}",
      journal = {\prc},
     keywords = {23.40.-s, 21.10.Tg, 26.30.-k, 07.05.Mh, Beta decay, double beta decay, electron and muon capture, Lifetimes, Nucleosynthesis in novae supernovae and other explosive environments, Neural networks fuzzy logic artificial intelligence, Nuclear Theory, Astrophysics, Condensed Matter - Disordered Systems and Neural Networks, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2009",
        month = "Oct",
       volume = {80},
       number = {4},
          eid = {044332},
        pages = {044332},
     abstract = "{Statistical modeling of nuclear data provides a novel approach to
        nuclear systematics complementary to established theoretical and
        phenomenological approaches based on quantum theory. Continuing
        previous studies in which global statistical modeling is pursued
        within the general framework of machine learning theory, we
        implement advances in training algorithms designed to improve
        generalization, in application to the problem of reproducing and
        predicting the half-lives of nuclear ground states that decay
        100\% by the {\ensuremath{\beta}}$^{-}$ mode. More specifically,
        fully connected, multilayer feed-forward artificial neural
        network models are developed using the Levenberg-Marquardt
        optimization algorithm together with Bayesian regularization and
        cross-validation. The predictive performance of models emerging
        from extensive computer experiments is compared with that of
        traditional microscopic and phenomenological models as well as
        with the performance of other learning systems, including
        earlier neural network models as well as the support vector
        machines recently applied to the same problem. In discussing the
        results, emphasis is placed on predictions for nuclei that are
        far from the stability line, and especially those involved in
        r-process nucleosynthesis. It is found that the new statistical
        models can match or even surpass the predictive performance of
        conventional models for {\ensuremath{\beta}}-decay systematics
        and accordingly should provide a valuable additional tool for
        exploring the expanding nuclear landscape.}",
          doi = {10.1103/PhysRevC.80.044332},
archivePrefix = {arXiv},
       eprint = {0806.2850},
 primaryClass = {nucl-th},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009PhRvC..80d4332C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2009PhDT........13F,
       author = {{Fendt}, William Ashton, Jr.},
        title = "{Precision cosmological parameter estimation}",
     keywords = {Cosmology, Parameter estimation, Power spectrum, Recombination, Gibbs sampling, Pico, Cosmic microwave background},
       school = {University of Illinois at Urbana-Champaign},
         year = "2009",
        month = "Sep",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009PhDT........13F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2009Icar..203...77S,
       author = {{Stepinski}, Tomasz F. and {Mendenhall}, Michael P. and {Bue}, Brian D.},
        title = "{Machine cataloging of impact craters on Mars}",
      journal = {\icarus},
         year = "2009",
        month = "Sep",
       volume = {203},
       number = {1},
        pages = {77-87},
     abstract = "{This study presents an automated system for cataloging impact craters
        using the MOLA 128 pixels/degree digital elevation model of
        Mars. Craters are detected by a two-step algorithm that first
        identifies round and symmetric topographic depressions as crater
        candidates and then selects craters using a machine-learning
        technique. The system is robust with respect to surface types;
        craters are identified with similar accuracy from all different
        types of martian surfaces without adjusting input parameters. By
        using a large training set in its final selection step, the
        system produces virtually no false detections. Finally, the
        system provides a seamless integration of crater detection with
        its characterization. Of particular interest is the ability of
        our algorithm to calculate crater depths. The system is
        described and its application is demonstrated on eight large
        sites representing all major types of martian surfaces. An
        evaluation of its performance and prospects for its utilization
        for global surveys are given by means of detailed comparison of
        obtained results to the manually-derived Catalog of Large
        Martian Impact Craters. We use the results from the test sites
        to construct local depth-diameter relationships based on a large
        number of craters. In general, obtained relationships are in
        agreement with what was inferred on the basis of manual
        measurements. However, we have found that, in Terra Cimmeria,
        the depth/diameter ratio has an abrupt decrease at
        ̃38{\textdegree}S regardless of crater size. If shallowing of
        craters is attributed to presence of sub-surface ice, a sudden
        change in its spatial distribution is suggested by our findings.}",
          doi = {10.1016/j.icarus.2009.04.026},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009Icar..203...77S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2009SpWea...7.6001C,
       author = {{Colak}, T. and {Qahwaji}, R.},
        title = "{Automated Solar Activity Prediction: A hybrid computer platform using machine learning and solar imaging for automated prediction of solar flares}",
      journal = {Space Weather},
     keywords = {Solar Physics, Astrophysics, and Astronomy: Flares, Solar Physics, Astrophysics, and Astronomy: Instruments and techniques, Space Weather: Forecasting (2722)},
         year = "2009",
        month = "Jun",
       volume = {7},
       number = {6},
          eid = {S06001},
        pages = {S06001},
     abstract = "{The importance of real-time processing of solar data especially for
        space weather applications is increasing continuously. In this
        paper, we present an automated hybrid computer platform for the
        short-term prediction of significant solar flares using
        SOHO/Michelson Doppler Imager images. This platform is called
        the Automated Solar Activity Prediction tool (ASAP). This system
        integrates image processing and machine learning to deliver
        these predictions. A machine learning-based system is designed
        to analyze years of sunspot and flare data to create
        associations that can be represented using computer-based
        learning rules. An imaging-based real-time system that provides
        automated detection, grouping, and then classification of recent
        sunspots based on the McIntosh classification is also created
        and integrated within this system. The properties of the sunspot
        regions are extracted automatically by the imaging system and
        processed using the machine learning rules to generate the real-
        time predictions. Several performance measurement criteria are
        used and the results are provided in this paper. Also, quadratic
        score is used to compare the prediction results of ASAP with
        NOAA Space Weather Prediction Center (SWPC) between 1999 and
        2002, and it is shown that ASAP generates more accurate
        predictions compared to SWPC.}",
          doi = {10.1029/2008SW000401},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009SpWea...7.6001C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2009JGRE..114.6005B,
       author = {{Bernard-Michel}, C. and {Dout{\'e}}, S. and {Fauvel}, M. and
         {Gardes}, L. and {Girard}, S.},
        title = "{Retrieval of Mars surface physical properties from OMEGA hyperspectral images using regularized sliced inverse regression}",
      journal = {Journal of Geophysical Research (Planets)},
     keywords = {Planetary Sciences: Solid Surface Planets: Remote sensing, Planetary Sciences: Solid Surface Planets: Surface materials and properties, Computational Geophysics: Data analysis: algorithms and implementation, Computational Geophysics: Neural networks, fuzzy logic, machine learning, Planetary Sciences: Solid Surface Planets: Ices},
         year = "2009",
        month = "Jun",
       volume = {114},
       number = {E6},
          eid = {E06005},
        pages = {E06005},
     abstract = "{In this paper, a method based on modeling and statistics is proposed to
        evaluate the physical properties of surface icy materials on
        Mars from hyperspectral images collected by the OMEGA instrument
        aboard the Mars Express spacecraft. The approach is based on the
        estimation of the functional relationship F between observed
        spectra and relevant physical parameters such as compound
        abundances and granularity. To this end, a database of synthetic
        spectra is generated by a radiative transfer model simulating
        the reflection of solar light by a granular mixture of H$_{2}$O
        ice, CO$_{2}$ ice, and dust. The database constitutes a training
        set used to estimate F. The high dimension of spectra is reduced
        by Gaussian regularized sliced inverse regression (GRSIR) to
        overcome the {\textquotedblleft}curse of
        dimensionality{\textquotedblright} and, consequently, the
        sensitivity of the inversion to noise (ill-conditioned
        problems). Compared with other approaches, such as the k-NN, the
        partial least squares, and the support vector machines (SVM),
        GRSIR has the advantage of being very fast, interpretable, and
        accurate. For instance, on simulated test data, the same level
        of accuracy is obtained by GRSIR and SVM for the estimation of
        the proportion of dust with a normalized root-mean-square error
        of 13\%, but GRSIR performs 100 times faster. On real data,
        parameter maps generated by GRSIR from a sequence of three OMEGA
        observations of the bright permanent polar cap (BPPC) are much
        smoother, detailed, and coherent than with other competing
        methods. They indicate that coarse-grained dry ice completely
        dominates ({\ensuremath{\approx}}99.55-99.95 wt\%) the material
        forming the top few centimeters of the BPPC with dust and water
        only present as traces (from 300 to 1000 ppm). The maps show
        clear regional variations of water and dust contamination as
        well as CO$_{2}$ ice state of densification (mean free path
        around 5 cm on the average, with variations of
        {\ensuremath{\pm}}50\%) that must be related to meteorological
        and microphysical phenomena.}",
          doi = {10.1029/2008JE003171},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009JGRE..114.6005B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2009JGRA..114.6216K,
       author = {{Karimabadi}, H. and {Sipes}, T.~B. and {Wang}, Y. and {Lavraud}, B. and
         {Roberts}, A.},
        title = "{A new multivariate time series data analysis technique: Automated detection of flux transfer events using Cluster data}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {Magnetospheric Physics: Magnetic reconnection (7526, 7835), Solar Physics, Astrophysics, and Astronomy: Magnetic reconnection (2723, 7835), Computational Geophysics: Data analysis: algorithms and implementation, Computational Geophysics: Neural networks, fuzzy logic, machine learning, Mathematical Geophysics: Time series analysis (1872, 4277, 4475)},
         year = "2009",
        month = "Jun",
       volume = {114},
       number = {A6},
          eid = {A06216},
        pages = {A06216},
     abstract = "{A new data mining technique called MineTool-TS is introduced which
        captures the time-lapse information in multivariate time series
        data through extraction of global features and metafeatures.
        This technique is developed into a JAVA-based data mining
        software which automates all the steps in the model building to
        make it more accessible to nonexperts. As its first application
        in space sciences, MineTool-TS is used to develop a model for
        automated detection of flux transfer events (FTEs) at Earth's
        magnetopause in the Cluster spacecraft time series data. The
        model classifies a given time series into one of three
        categories of non-FTE, magnetosheath FTE, or magnetospheric FTE.
        One important feature of MineTool-TS is the ability to explore
        the importance of each variable or combination of variables as
        indicators of FTEs. FTEs have traditionally been identified on
        the basis of their magnetic field signatures, but here we find
        that some plasma variables can also be effective indicators of
        FTEs. For example, the perpendicular ion temperature yields a
        model accuracy of ̃93\%, while a model based solely on the
        normal magnetic field B$_{N}$ yields an accuracy of ̃95\%. This
        opens up the possibility of searching for more unusual FTEs that
        may, for example, have no clear B$_{N}$ signature and create a
        more comprehensive and less biased list of FTEs for statistical
        studies. We also find that models using GSM coordinates yield
        comparable accuracy to those using boundary normal coordinates.
        This is useful since there are regions where magnetopause models
        are not accurate. Another surprising result is the finding that
        the algorithm can largely detect FTEs, and even distinguish
        between magnetosheath and magnetospheric FTEs, solely on the
        basis of models built from single parameters, something that
        experts may not do so straightforwardly on the basis of short
        time series intervals. The most accurate models use a
        combination of plasma and magnetic field variables and achieve a
        very high accuracy of prediction of ̃99\%. We explain the high
        detection accuracies both in terms of the existence of clear
        physical signatures of FTEs (for the majority of cases) and in
        terms of the capability of the data mining technique to explore
        the data set in a much more thorough fashion than expert human
        eyes. A list of 1222 FTEs from Cluster data during years
        2001-2003 is provided as auxiliary material.}",
          doi = {10.1029/2009JA014202},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009JGRA..114.6216K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2009GeoRL..3610806C,
       author = {{Cermak}, J. and {Knutti}, R.},
        title = "{Beijing Olympics as an aerosol field experiment}",
      journal = {\grl},
     keywords = {Atmospheric Composition and Structure: Aerosols and particles (0345, 4801, 4906), Atmospheric Composition and Structure: Pollution: urban and regional (0305, 0478, 4251), Atmospheric Processes: Clouds and aerosols, Computational Geophysics: Neural networks, fuzzy logic, machine learning, Global Change: Regional climate change},
         year = "2009",
        month = "May",
       volume = {36},
       number = {10},
          eid = {L10806},
        pages = {L10806},
     abstract = "{During the 2008 Olympic Summer Games, emission reductions were enforced
        in Beijing to improve air quality. Here we explore their effect
        on the regional aerosol load. We compare satellite-retrieved
        aerosol optical thickness (AOT) of that period with previous
        years, both in absolute terms and in a neural network approach
        taking into account the meteorological conditions. A
        statistically significant reduction of aerosol load is found in
        Beijing that decreases in magnitude and significance with
        increasing region size. Locally, the aerosol load (log(AOT)) was
        about 0.4 to 0.75 standard deviations below the levels expected
        for the prevailing meteorological situation. The small size of
        this effect relative to meteorological variability highlights
        the importance of regional aerosol transport.}",
          doi = {10.1029/2009GL038572},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009GeoRL..3610806C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2009ApJ...696.2075A,
       author = {{Asensio Ramos}, A. and {Ramos Almeida}, C.},
        title = "{Bayesclumpy: Bayesian Inference with Clumpy Dusty Torus Models}",
      journal = {\apj},
     keywords = {galaxies: nuclei, galaxies: Seyfert, infrared: galaxies, methods: data analysis, methods: statistical, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Extragalactic Astrophysics},
         year = "2009",
        month = "May",
       volume = {696},
       number = {2},
        pages = {2075-2085},
     abstract = "{Our aim is to present a fast and general Bayesian inference framework
        based on the synergy between machine learning techniques and
        standard sampling methods and apply it to infer the physical
        properties of clumpy dusty torus using infrared photometric high
        spatial resolution observations of active galactic nuclei. We
        make use of the Metropolis-Hastings Markov Chain Monte Carlo
        algorithm for sampling the posterior distribution function. Such
        distribution results from combining all a priori knowledge about
        the parameters of the model and the information introduced by
        the observations. The main difficulty resides in the fact that
        the model used to explain the observations is computationally
        demanding and the sampling is very time consuming. For this
        reason, we apply a set of artificial neural networks that are
        used to approximate and interpolate a database of models. As a
        consequence, models not present in the original database can be
        computed ensuring continuity. We focus on the application of
        this solution scheme to the recently developed public database
        of clumpy dusty torus models. The machine learning scheme used
        in this paper allows us to generate any model from the database
        using only a factor of 10$^{-4}$ of the original size of the
        database and a factor of 10$^{-3}$ in computing time. The
        posterior distribution obtained for each model parameter allows
        us to investigate how the observations constrain the parameters
        and which ones remain partially or completely undetermined,
        providing statistically relevant confidence intervals. As an
        example, the application to the nuclear region of Centaurus A
        shows that the optical depth of the clouds, the total number of
        clouds, and the radial extent of the cloud distribution zone are
        well constrained using only six filters. The code is freely
        available from the authors.}",
          doi = {10.1088/0004-637X/696/2/2075},
archivePrefix = {arXiv},
       eprint = {0903.0622},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009ApJ...696.2075A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2009SpWea...7.4004V,
       author = {{Valach}, F. and {Revallo}, M. and {Bochn{\'\i}{\v{c}}ek}, J. and
         {Hejda}, P.},
        title = "{Solar energetic particle flux enhancement as a predictor of geomagnetic activity in a neural network-based model}",
      journal = {Space Weather},
     keywords = {Space Weather: Forecasting (2722), Solar Physics, Astrophysics, and Astronomy: Energetic particles (2114), Computational Geophysics: Neural networks, fuzzy logic, machine learning},
         year = "2009",
        month = "Apr",
       volume = {7},
       number = {4},
          eid = {S04004},
        pages = {S04004},
     abstract = "{Coronal mass ejections (CMEs) are believed to be the principal cause of
        increased geomagnetic activity. They are regarded as being in
        context of a series of related solar energetic events, such as
        X-ray flares (XRAs) accompanied by solar radio bursts (RSPs) and
        also by solar energetic particle (SEP) flux. Two types of the
        RSP events are known to be geoeffective, namely, the RSP of type
        II, interpreted as the signature of shock initiation in the
        solar corona, and type IV, representing material moving upward
        in the corona. The SEP events causing geomagnetic response are
        known to be produced by CME-driven shocks. In this paper, we use
        the method of the artificial neural network in order to quantify
        the geomagnetic response of particular solar events. The data
        concerning XRAs and RSPs II and/or IV together with their
        heliographic positions are taken as the input for the neural
        network. There is a key question posed in our study: can the
        successfulness of the neural network prediction scheme based
        solely on the solar disc observations (XRA and RSP) be improved
        by additional information concerning the SEP flux? To resolve
        this problem, we chose the SEP events possessing significant
        enhancement in the 10-h window, commencing 12 h after the
        generation of XRAs. In particular, we consider the flux of high-
        energy protons with energies over 10 MeV. We have used a chi-
        square test to demonstrate that supplying such extra input data
        improves the neural network prediction scheme.}",
          doi = {10.1029/2008SW000421},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009SpWea...7.4004V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2009GeoRL..36.8604N,
       author = {{Namekar}, Shailesh and {Yamazaki}, Yoshiki and {Cheung}, Kwok Fai},
        title = "{Neural network for tsunami and runup forecast}",
      journal = {\grl},
     keywords = {Oceanography: Physical: Tsunamis and storm surges, Biogeosciences: Natural hazards, Computational Geophysics: Neural networks, fuzzy logic, machine learning},
         year = "2009",
        month = "Apr",
       volume = {36},
       number = {8},
          eid = {L08604},
        pages = {L08604},
     abstract = "{This paper examines the use of neural network to model nonlinear tsunami
        processes for forecasting of coastal waveforms and runup. The
        three-layer network utilizes a radial basis function in the
        hidden, middle layer for nonlinear transformation of input
        waveforms near the tsunami source. Events based on the 2006
        Kuril Islands tsunami demonstrate the implementation and
        capability of the network. Division of the Kamchatka-Kuril
        subduction zone into a number of subfaults facilitates
        development of a representative tsunami dataset using a
        nonlinear long-wave model. The computed waveforms near the
        tsunami source serve as the input and the far-field waveforms
        and runup provide the target output for training of the network
        through a back-propagation algorithm. The trained network
        reproduces the resonance of tsunami waves and the topography-
        dominated runup patterns at Hawaii's coastlines from input
        water-level data off the Aleutian Islands.}",
          doi = {10.1029/2009GL037184},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009GeoRL..36.8604N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2009ApJS..181..627F,
       author = {{Fendt}, W.~A. and {Chluba}, J. and {Rubi{\~n}o-Mart{\'\i}n}, J.~A. and
         {Wandelt}, B.~D.},
        title = "{RICO: A New Approach for Fast and Accurate Representation of the Cosmological Recombination History}",
      journal = {\apjs},
     keywords = {cosmic microwave background, Astrophysics},
         year = "2009",
        month = "Apr",
       volume = {181},
       number = {2},
        pages = {627-638},
     abstract = "{We present RICO, a code designed to compute the ionization fraction of
        the universe during the epoch of hydrogen and helium
        recombination with an unprecedented combination of speed and
        accuracy. This is accomplished by training the machine learning
        code PICO on the calculations of a multilevel cosmological
        recombination code which self-consistently includes several
        physical processes that were neglected previously. After
        training, RICO is used to fit the free electron fraction as a
        function of the cosmological parameters. While, for example, at
        low redshifts (z lsim 900), much of the net change in the
        ionization fraction can be captured by lowering the hydrogen
        fudge factor in RECFAST by about 3\%, RICO provides a means of
        effectively using the accurate ionization history of the full
        recombination code in the standard cosmological parameter
        estimation framework without the need to add new or refined
        fudge factors or functions to a simple recombination model.
        Within the new approach presented here, it is easy to update
        RICO whenever a more accurate full recombination code becomes
        available. Once trained, RICO computes the cosmological
        ionization history with negligible fitting error in
        \raisebox{-0.5ex}\textasciitilde10 ms, a speedup of at least
        {}10$^{6}$ over the full recombination code that was used here.
        Also RICO is able to reproduce the ionization history of the
        full code to a level well below 0.1\%, thereby ensuring that the
        theoretical power spectra of cosmic microwave background (CMB)
        fluctuations can be computed to sufficient accuracy and speed
        for analysis from upcoming CMB experiments like Planck.
        Furthermore, it will enable cross-checking different
        recombination codes across cosmological parameter space, a
        comparison that will be very important in order to assure the
        accurate interpretation of future CMB data.}",
          doi = {10.1088/0067-0049/181/2/627},
archivePrefix = {arXiv},
       eprint = {0807.2577},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009ApJS..181..627F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2009SoPh..255...91Y,
       author = {{Yu}, Daren and {Huang}, Xin and {Wang}, Huaning and {Cui}, Yanmei},
        title = "{Short-Term Solar Flare Prediction Using a Sequential Supervised Learning Method}",
      journal = {\solphys},
     keywords = {Flare prediction, Photospheric magnetic field, Sequential supervised learning method},
         year = "2009",
        month = "Mar",
       volume = {255},
       number = {1},
        pages = {91-105},
     abstract = "{Solar flares are powered by the energy stored in magnetic fields, so
        evolutionary information of the magnetic field is important for
        short-term prediction of solar flares. However, the existing
        solar flare prediction models only use the current information
        of the active region. A sequential supervised learning method is
        introduced to add the evolutionary information of the active
        region into a prediction model. The maximum horizontal gradient,
        the length of the neutral line, and the number of singular
        points extracted from SOHO/MDI longitudinal magnetograms are
        used in the model to describe the nonpotentiality and complexity
        of the photospheric magnetic field. The evolutionary
        characteristics of the predictors are analyzed by using
        autocorrelation functions and mutual information functions. The
        analysis results indicate that a flare is influenced by the
        3-day photospheric magnetic field information before flare
        eruption. A sliding-window method is used to add evolutionary
        information of the predictors into machine learning algorithms,
        then C4.5 decision tree and learning vector quantization are
        employed to predict the flare level within 48 hours.
        Experimental results indicate that the performance of the short-
        term solar flare prediction model within the sequential
        supervised learning framework is significantly improved.}",
          doi = {10.1007/s11207-009-9318-9},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009SoPh..255...91Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2008MNRAS.390.1339S,
       author = {{Scaringi}, S. and {Bird}, A.~J. and {Clark}, D.~J. and {Dean}, A.~J. and
         {Hill}, A.~B. and {McBride}, V.~A. and {Shaw}, S.~E.},
        title = "{ISINA: INTEGRAL Source Identification Network Algorithm}",
      journal = {\mnras},
     keywords = {methods: data analysis, catalogues, surveys, Astrophysics},
         year = "2008",
        month = "Nov",
       volume = {390},
       number = {4},
        pages = {1339-1348},
     abstract = "{We give an overview of ISINA: INTEGRAL Source Identification Network
        Algorithm. This machine learning algorithm, using random
        forests, is applied to the IBIS/ISGRI data set in order to ease
        the production of unbiased future soft gamma-ray source
        catalogues. First, we introduce the data set and the problems
        encountered when dealing with images obtained using the coded
        mask technique. The initial step of source candidate searching
        is introduced and an initial candidate list is created. A
        description of the feature extraction on the initial candidate
        list is then performed together with feature merging for these
        candidates. Three training and testing sets are created in order
        to deal with the diverse time-scales encountered when dealing
        with the gamma-ray sky. Three independent random forests are
        built: one dealing with faint persistent source recognition, one
        dealing with strong persistent sources and a final one dealing
        with transients. For the latter, a new transient detection
        technique is introduced and described: the transient matrix.
        Finally the performance of the network is assessed and discussed
        using the testing set and some illustrative source examples.
        Based on observations with INTEGRAL, an ESA project with
        instruments and science data centre funded by ESA member states
        (especially the PI countries: Denmark, France, Germany, Italy,
        Spain), Czech Republic and Poland, and the participation of
        Russia and the USA. E-mail: simo@astro.soton.ac.uk}",
          doi = {10.1111/j.1365-2966.2008.13765.x},
archivePrefix = {arXiv},
       eprint = {0807.4653},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2008MNRAS.390.1339S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2008AdSpR..42.1450H,
       author = {{He}, Han and {Wang}, Huaning and {Du}, Zhanle and {Li}, Rong and
         {Cui}, Yanmei and {Zhang}, Liyun and {He}, Yulin},
        title = "{Solar activity prediction studies and services in NAOC}",
      journal = {Advances in Space Research},
         year = "2008",
        month = "Nov",
       volume = {42},
       number = {9},
        pages = {1450-1456},
     abstract = "{Solar activity prediction services started in 1960{\textquoteright}s in
        National Astronomical Observatories, Chinese Academy of Sciences
        (NAOC). As one of the members of the International Space
        Environment Service (ISES), Regional Warning Center of China
        (RWC-China) was set up in 1990{\textquoteright}s. Solar Activity
        Prediction Center (SAPC), as one of the four sub-centers of RWC-
        China, is located in NAOC. Solar activity prediction studies and
        services in NAOC cover short-term, medium-term, and long-term
        forecast of solar activities. Nowadays, certain prediction
        models, such as solar X-ray flare model, solar proton event
        model, solar 10 cm radio flux model, have been established for
        the practical prediction services. Recently, more and more
        physical analyses are introduced in the studies of solar
        activity prediction, such as the magnetic properties of solar
        active regions and magnetic structure of solar atmosphere.
        Besides traditional statistics algorithms, Machine Learning and
        Artificial Intelligence techniques, such as Support Vector
        Machine (SVM) method, are employed in the establishment of
        forecast models. A Web-based integrated platform for solar
        activity data sharing and forecast distribution is under
        construction.}",
          doi = {10.1016/j.asr.2007.02.068},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2008AdSpR..42.1450H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2008GeoRL..3520802Y,
       author = {{Yang}, Ruixin and {Sun}, Donglian and {Tang}, Jiang},
        title = "{A ``sufficient'' condition combination for rapid intensifications of tropical cyclones}",
      journal = {\grl},
     keywords = {Atmospheric Processes: Tropical meteorology, Computational Geophysics: Data analysis: algorithms and implementation, Computational Geophysics: Neural networks, fuzzy logic, machine learning},
         year = "2008",
        month = "Oct",
       volume = {35},
       number = {20},
          eid = {L20802},
        pages = {L20802},
     abstract = "{Rapid Intensifications (RI) of tropical cyclones (TCs) provide major
        error sources in the challenging task of TC intensity
        forecasting. There are many factors that affect the RI processes
        of TCs, and identifying the combination of conditions most
        favorable to RI development is very time consuming when using
        traditional statistical data analysis methods. Data mining
        techniques are implemented to the data for SHIPS (Statistical
        Hurricane Intensity Prediction Scheme), an operational hurricane
        intensity forecasting model, to identify the ``optimal'' RI
        condition combinations when the number of affecting factors is
        given. One such combination (high latitude, low longitude, the
        TC being in an intensification phase, an initial intensity far
        away from the maximum potential intensity, high steering layer
        value, and low relative eddy flux convergence) gives such a high
        RI probability that the combination can be considered as a
        sufficient condition for RI, which almost guarantees that an RI
        will take place.}",
          doi = {10.1029/2008GL035222},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2008GeoRL..3520802Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2008GeoRL..3519306B,
       author = {{Bauer}, K. and {Pratt}, R.~G. and {Haberland}, C. and {Weber}, M.},
        title = "{Neural network analysis of crosshole tomographic images: The seismic signature of gas hydrate bearing sediments in the Mackenzie Delta (NW Canada)}",
      journal = {\grl},
     keywords = {Computational Geophysics: Neural networks, fuzzy logic, machine learning, Marine Geology and Geophysics: Gas and hydrate systems, Seismology: Tomography (6982, 8180), Physical Properties of Rocks: Acoustic properties, Nonlinear Geophysics: Self-organization},
         year = "2008",
        month = "Oct",
       volume = {35},
       number = {19},
          eid = {L19306},
        pages = {L19306},
     abstract = "{Crosshole seismic experiments were conducted to study the in-situ
        properties of gas hydrate bearing sediments (GHBS) in the
        Mackenzie Delta (NW Canada). Seismic tomography provided images
        of P velocity, anisotropy, and attenuation. Self-organizing maps
        (SOM) are powerful neural network techniques to classify and
        interpret multi-attribute data sets. The coincident tomographic
        images are translated to a set of data vectors in order to train
        a Kohonen layer. The total gradient of the model vectors is
        determined for the trained SOM and a watershed segmentation
        algorithm is used to visualize and map the lithological clusters
        with well-defined seismic signatures. Application to the Mallik
        data reveals four major litho-types: (1) GHBS, (2) sands, (3)
        shale/coal interlayering, and (4) silt. The signature of seismic
        P wave characteristics distinguished for the GHBS (high
        velocities, strong anisotropy and attenuation) is new and can be
        used for new exploration strategies to map and quantify gas
        hydrates.}",
          doi = {10.1029/2008GL035263},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2008GeoRL..3519306B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2008ApJ...683...12B,
       author = {{Ball}, Nicholas M. and {Brunner}, Robert J. and {Myers}, Adam D. and
         {Strand}, Natalie E. and {Alberts}, Stacey L. and {Tcheng}, David},
        title = "{Robust Machine Learning Applied to Astronomical Data Sets. III. Probabilistic Photometric Redshifts for Galaxies and Quasars in the SDSS and GALEX}",
      journal = {\apj},
     keywords = {catalogs, cosmology: miscellaneous, methods: data analysis, quasars: general, Astrophysics},
         year = "2008",
        month = "Aug",
       volume = {683},
       number = {1},
        pages = {12-21},
     abstract = "{We apply machine learning in the form of a nearest neighbor instance-
        based algorithm (NN) to generate full photometric redshift
        probability density functions (PDFs) for objects in the Fifth
        Data Release of the Sloan Digital Sky Survey (SDSS DR5). We use
        a conceptually simple but novel application of NN to generate
        the PDFs, perturbing the object colors by their measurement
        error and using the resulting instances of nearest neighbor
        distributions to generate numerous individual redshifts. When
        the redshifts are compared to existing SDSS spectroscopic data,
        we find that the mean value of each PDF has a dispersion between
        the photometric and spectroscopic redshift consistent with other
        machine learning techniques, being {\ensuremath{\sigma}} =
        0.0207 +/- 0.0001 for main sample galaxies to r \&lt; 17.77 mag,
        {\ensuremath{\sigma}} = 0.0243 +/- 0.0002 for luminous red
        galaxies to rlesssim 19.2 mag, and {\ensuremath{\sigma}} = 0.343
        +/- 0.005 for quasars to i \&lt; 20.3 mag. The PDFs allow the
        selection of subsets with improved statistics. For quasars, the
        improvement is dramatic: for those with a single peak in their
        probability distribution, the dispersion is reduced from 0.343
        to {\ensuremath{\sigma}} = 0.117 +/- 0.010, and the photometric
        redshift is within 0.3 of the spectroscopic redshift for 99.3\%
        +/- 0.1\% of the objects. Thus, for this optical quasar sample,
        we can virtually eliminate ``catastrophic'' photometric redshift
        estimates. In addition to the SDSS sample, we incorporate
        ultraviolet photometry from the Third Data Release of the Galaxy
        Evolution Explorer All-Sky Imaging Survey (GALEX AIS GR3) to
        create PDFs for objects seen in both surveys. For quasars, the
        increased coverage of the observed-frame UV of the SED results
        in significant improvement over the full SDSS sample, with
        {\ensuremath{\sigma}} = 0.234 +/- 0.010. We demonstrate that
        this improvement is genuine and not an artifact of the SDSS-
        GALEX matching process.}",
          doi = {10.1086/589646},
archivePrefix = {arXiv},
       eprint = {0804.3413},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2008ApJ...683...12B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2008ScChG..51..916L,
       author = {{Li}, Lili and {Zhang}, Yanxia and {Zhao}, Yongheng},
        title = "{k-Nearest Neighbors for automated classification of celestial objects}",
      journal = {Science in China: Physics, Mechanics and Astronomy},
         year = "2008",
        month = "Jul",
       volume = {51},
       number = {7},
        pages = {916-922},
     abstract = "{The nearest neighbors (NNs) classifiers, especially the k-Nearest
        Neighbors ( kNNs) algorithm, are among the simplest and yet most
        efficient classification rules and widely used in practice. It
        is a nonparametric method of pattern recognition. In this paper,
        k-Nearest Neighbors, one of the most commonly used machine
        learning methods, work in automatic classification of multi-
        wavelength astronomical objects. Through the experiment, we
        conclude that the running speed of the kNN classier is rather
        fast and the classification accuracy is up to 97.73\%. As a
        result, it is efficient and applicable to discriminate active
        objects from stars and normal galaxies with this method. The
        classifiers trained by the kNN method can be used to solve the
        automated classification problem faced by astronomy and the
        virtual observatory (VO).}",
          doi = {10.1007/s11433-008-0088-4},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2008ScChG..51..916L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2008SoPh..248..471Q,
       author = {{Qahwaji}, R. and {Colak}, T. and {Al-Omari}, M. and {Ipson}, S.},
        title = "{Automated Prediction of CMEs Using Machine Learning of CME - Flare Associations}",
      journal = {\solphys},
     keywords = {CMEs prediction, Machine learning, Solar flares, Space weather, CME, Neural networks, Support vector machines},
         year = "2008",
        month = "Apr",
       volume = {248},
       number = {2},
        pages = {471-483},
     abstract = "{Machine-learning algorithms are applied to explore the relation between
        significant flares and their associated CMEs. The NGDC flares
        catalogue and the SOHO/LASCO CME catalogue are processed to
        associate X and M-class flares with CMEs based on timing
        information. Automated systems are created to process and
        associate years of flare and CME data, which are later arranged
        in numerical-training vectors and fed to machine-learning
        algorithms to extract the embedded knowledge and provide
        learning rules that can be used for the automated prediction of
        CMEs. Properties representing the intensity, flare duration, and
        duration of decline and duration of growth are extracted from
        all the associated (A) and not-associated (NA) flares and
        converted to a numerical format that is suitable for machine-
        learning use. The machine-learning algorithms Cascade
        Correlation Neural Networks (CCNN) and Support Vector Machines
        (SVM) are used and compared in our work. The machine-learning
        systems predict, from the input of a flare{\textquoteright}s
        properties, if the flare is likely to initiate a CME. Intensive
        experiments using Jack-knife techniques are carried out and the
        relationships between flare properties and CMEs are investigated
        using the results. The predictive performance of SVM and CCNN is
        analysed and recommendations for enhancing the performance are
        provided.}",
          doi = {10.1007/s11207-007-9108-1},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2008SoPh..248..471Q},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2008AN....329..288M,
       author = {{Mahabal}, A. and {Djorgovski}, S.~G. and {Turmon}, M. and {Jewell}, J. and
         {Williams}, R.~R. and {Drake}, A.~J. and {Graham}, M.~G. and
         {Donalek}, C. and {Glikman}, E. and {Palomar-QUEST Team}},
        title = "{Automated probabilistic classification of transients and variables}",
      journal = {Astronomische Nachrichten},
     keywords = {astronomical databases: miscellaneous, methods: data analysis, methods: statistical, surveys, Astrophysics},
         year = "2008",
        month = "Mar",
       volume = {329},
       number = {3},
        pages = {288-291},
     abstract = "{There is an increasing number of large, digital, synoptic sky surveys,
        in which repeated observations are obtained over large areas of
        the sky in multiple epochs. Likewise, there is a growth in the
        number of (often automated or robotic) follow-up facilities with
        varied capabilities in terms of instruments, depth, cadence,
        wavelengths, etc., most of which are geared toward some specific
        astrophysical phenomenon. As the number of detected transient
        events grows, an automated, probabilistic classification of the
        detected variables and transients becomes increasingly
        important, so that an optimal use can be made of follow-up
        facilities, without unnecessary duplication of effort. We
        describe a methodology now under development for a prototype
        event classification system; it involves Bayesian and Machine
        Learning classifiers, automated incorporation of feedback from
        follow-up observations, and discriminated or directed follow-up
        requests. This type of methodology may be essential for the
        massive synoptic sky surveys in the future.}",
          doi = {10.1002/asna.200710943},
archivePrefix = {arXiv},
       eprint = {0802.3199},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2008AN....329..288M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2008AN....329..255B,
       author = {{Borne}, K.~D.},
        title = "{A machine learning classification broker for the LSST transient database}",
      journal = {Astronomische Nachrichten},
     keywords = {astronomical databases: miscellaneous, methods: data analysis, methods: statistical},
         year = "2008",
        month = "Mar",
       volume = {329},
       number = {3},
        pages = {255},
     abstract = "{We describe the largest data-producing astronomy project in the coming
        decade - the LSST (Large Synoptic Survey Telescope). The
        enormous data output, database contents, knowledge discovery,
        and community science expected from this project will impose
        massive data challenges on the astronomical research community.
        One of these challenge areas is the rapid machine learning, data
        mining, and classification of all novel astronomical events from
        each 3-gigapixel (6-GB) image obtained every 20 seconds
        throughout every night for the project duration of 10 years. We
        describe these challenges and a particular implementation of a
        classification broker for this data fire hose.}",
          doi = {10.1002/asna.200710946},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2008AN....329..255B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2008SpWea...6.1001B,
       author = {{Balch}, Christopher C.},
        title = "{Updated verification of the Space Weather Prediction Center's solar energetic particle prediction model}",
      journal = {Space Weather},
     keywords = {Computational Geophysics: Neural networks, fuzzy logic, machine learning, Solar Physics, Astrophysics, and Astronomy: Energetic particles (2114), Space Weather: Forecasting (2722), Space Weather: Space radiation environment},
         year = "2008",
        month = "Jan",
       volume = {6},
       number = {1},
          eid = {S01001},
        pages = {S01001},
     abstract = "{This paper evaluates the performance of an operational proton prediction
        model currently being used at NOAA's Space Weather Prediction
        Center. The evaluation is based on proton events that occurred
        between 1986 and 2004. Parameters for the associated solar
        events determine a set of necessary conditions, which are used
        to construct a set of control events. Model output is calculated
        for these events and performance of the model is evaluated using
        standard verification measures. For probability forecasts we
        evaluate the accuracy, reliability, and resolution and display
        these results using a standard attributes diagram. We identify
        conditions for which the model is systematically inaccurate. The
        probability forecasts are also evaluated for categorical
        forecast performance measures. We find an optimal probability
        and we calculate the false alarm rate and probability of
        detection at this probability. We also show results for peak
        flux and rise time predictions. These findings provide an
        objective basis for measuring future improvements.}",
          doi = {10.1029/2007SW000337},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2008SpWea...6.1001B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2008ICRC....3.1473M,
       author = {{Malagon}, C. and {Barrio}, J.~A. and {Nieto}, D. and {de los Reyes}, R.},
        title = "{Classification Methods for MAGIC Telescope Images on a Pixel-by-pixel base}",
      journal = {International Cosmic Ray Conference},
         year = "2008",
        month = "Jan",
       volume = {3},
        pages = {1473-1476},
     abstract = "{The problem of identifying gamma ray events hidden in charged cosmic ray
        background (so called hadrons) in Cherenkov telescopes is one of
        the key problems in VHE gamma ray astronomy. In this
        contribution, we present a novel approach to this problem by
        implementing different classifiers relying on the information of
        each pixel of the camera of a Cherenkov telescope, rather than
        using Hillas parameter analysis. Separation between gamma-like
        and hadron-like events (as reconstructed by the MAGIC Cherenkov
        Telescope) is performed using several machine learning
        techniques, trained using Monte Carlo data samples of both kinds
        of events. The results of the different techniques are presented
        and compared with other methods based on Hillas parameters.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2008ICRC....3.1473M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2007JGRG..112.4S03S,
       author = {{Smith}, Trey and {Thompson}, David R. and {Wettergreen}, David S. and
         {Cabrol}, Nathalie A. and {Warren-Rhodes}, Kimberley A. and
         {Weinstein}, Shmuel J.},
        title = "{Life in the Atacama: Science autonomy for improving data quality}",
      journal = {Journal of Geophysical Research (Biogeosciences)},
     keywords = {Biogeosciences: Astrobiology and extraterrestrial materials, Biogeosciences: Life in extreme environments, Computational Geophysics: Data analysis: algorithms and implementation, Computational Geophysics: Neural networks, fuzzy logic, machine learning, Exploration Geophysics: Instruments and techniques},
         year = "2007",
        month = "Dec",
       volume = {112},
       number = {G4},
          eid = {G04S03},
        pages = {G04S03},
     abstract = "{``Science autonomy'' refers to exploration robotics technologies
        involving onboard science analysis of collected data. These
        techniques enable a rover to make adaptive decisions about which
        measurements to collect and transmit. Science autonomy can
        compensate for limited communications bandwidth by ensuring that
        planetary scientists receive those images and spectra that best
        meet mission goals. Here, we present the results of autonomous
        science experiments performed in the Atacama Desert of Chile
        during the Life in the Atacama (LITA) rover field campaign. We
        aim to provide an overview of autonomous science principles and
        examine their integration into the LITA operations strategy. We
        present experiments in four specific autonomous science domains:
        (1) autonomously responding to evidence of life with more
        detailed measurements; (2) rock detection for site profiling and
        selective data return; (3) tactical replanning to efficiently
        map the distribution of life; (4) detecting novel images and
        geologic unit boundaries in image sequences. In each of these
        domains we demonstrate improvements in the quality of returned
        data through autonomous analysis of imagery.}",
          doi = {10.1029/2006JG000315},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2007JGRG..112.4S03S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2007JGRA..11211215K,
       author = {{Karimabadi}, H. and {Sipes}, T.~B. and {White}, H. and {Marinucci}, M. and
         {Dmitriev}, A. and {Chao}, J.~K. and {Driscoll}, J. and {Balac}, N.},
        title = "{Data mining in space physics: MineTool algorithm}",
      journal = {Journal of Geophysical Research (Space Physics)},
     keywords = {Computational Geophysics: Neural networks, fuzzy logic, machine learning, Computational Geophysics: Data analysis: algorithms and implementation, Computational Geophysics: Model verification and validation, Magnetospheric Physics (6939), Magnetospheric Physics: Instruments and techniques, data mining, data analysis, automated},
         year = "2007",
        month = "Nov",
       volume = {112},
       number = {A11},
          eid = {A11215},
        pages = {A11215},
     abstract = "{A novel data mining method called MineTool is introduced which, by
        virtue of automating the modeling process and model evaluations,
        makes it more accessible to nonexperts. The technique aggregates
        the various stages of model building into a four-step process
        consisting of (1) data segmentation and sampling, (2) variable
        preselection and transform generation, (3) predictive model
        estimation and validation, and (4) final model testing. Optimal
        strategies are chosen for each modeling step. However, the
        modular design of the MineTool enables the substitution of
        alternative strategies in any of the four modeling steps. A
        notable feature of the technique is that the final model is
        always in closed analytical form rather than ``black box'' form
        of most other techniques. MineTool can be used for analysis of
        data (e.g., time series) as well as images. The utility of the
        technique is illustrated through several examples based on
        synthetic data. Application of the technique to analysis of
        spacecraft data will be presented in subsequent papers.}",
          doi = {10.1029/2006JA012136},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2007JGRA..11211215K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INBOOK{2007LNCS.4702..573S,
       author = {{Sun}, Jianyong and {Kab{\'a}n}, Ata and {Raychaudhury}, Somak},
        title = "{Robust Visual Mining of Data with Error Information}",
     keywords = {Machine Learning, Data Mining, Outlier detection, High Redshift quasars},
    booktitle = {Lecture Notes in Computer Science, Vol. 4702, Knowledge Discovery in Databases: PKDD 2007. ISBN 978-3-540-74975-2. Springer: Berlin / Heidelberg, 2007, p. 573-580},
         year = "2007",
       volume = {4702},
        pages = {573-580},
     abstract = "{Recent results on robust density-based clustering have indicated that
        the uncertainty associated with the actual measurements can be
        exploited to locate objects that are atypical for a reason
        unrelated to measurement errors. In this paper, we develop a
        constrained robust mixture model, which, in addition, is able to
        nonlinearly map such data for visual exploration. Our robust
        visual mining approach aims to combine statistically sound
        density-based analysis with visual presentation of the density
        structure, and to provide visual support for the identification
        and exploration of ``genuine'' peculiar objects of interest that
        are not due to the measurement errors. In this model, an exact
        inference is not possible despite the latent space being
        discretised, and we resort to employing a structured variational
        EM. We present results on synthetic data as well as a real
        application, for visualising peculiar quasars from an
        astrophysical survey, given photometric measurements with
        errors.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2007LNCS.4702..573S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2007SpWea...5.8004M,
       author = {{Mirmomeni}, Masoud and {Lucas}, Caro and {Araabi}, Babak Nadjar and
         {Shafiee}, Masoud},
        title = "{Forecasting sunspot numbers with the aid of fuzzy descriptor models}",
      journal = {Space Weather},
     keywords = {Computational Geophysics: Neural networks, fuzzy logic, machine learning, Oceanography: General: Numerical modeling (0545, 0560), Solar Physics, Astrophysics, and Astronomy: Solar activity cycle (2162), Space Weather: Forecasting (2722)},
         year = "2007",
        month = "Aug",
       volume = {5},
       number = {8},
          eid = {S08004},
        pages = {S08004},
     abstract = "{The cyclic solar activity has significant effects on Earth, satellites,
        and space missions. The prediction of sunspot number is an
        active research area and several methods have been introduced
        for its prediction, which is a common measure of solar activity.
        On the other hand, descriptor models and related fuzzy
        descriptor models have been the subjects of interest due to
        their many practical applications in modeling complex phenomena.
        In this study, it is tried to predict sun spot number by a data
        driven approach. In other words, instead of other methods which
        are based on sophisticated models, in this paper a fuzzy
        descriptor model is used as a black box to predict sunspot
        number. To do so, a novel learning method, generalized locally
        linear model tree (GLOLIMOT) algorithm for fuzzy descriptor
        models as an intuitive incremental learning algorithms, is
        introduced to tune the parameters of fuzzy descriptor model for
        the prediction of sunspot number via empirical data. The
        contribution of this paper is to provide some methods for
        adjusting the parameters of fuzzy descriptor model, e.g., the
        splitting ratio and the standard deviation, the number of
        locally linear neurons and the number of linear descriptor
        systems for the consequent part in fuzzy descriptor model and
        especially the parameters of such descriptor systems which need
        some special methods for these systems. By these modifications
        an accurate prediction of sunspot number is obtained which, when
        compared with several methods and results, depict the power of
        these systems in predicting such complex phenomena.}",
          doi = {10.1029/2006SW000289},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2007SpWea...5.8004M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2007GeoRL..3416304M,
       author = {{Meier}, U. and {Curtis}, A. and {Trampert}, J.},
        title = "{Fully nonlinear inversion of fundamental mode surface waves for a global crustal model}",
      journal = {\grl},
     keywords = {Computational Geophysics: Neural networks, fuzzy logic, machine learning, Mathematical Geophysics: Inverse theory, Mathematical Geophysics: Uncertainty quantification (1873), Seismology: Continental crust (1219), Seismology: Tomography (6982, 8180)},
         year = "2007",
        month = "Aug",
       volume = {34},
       number = {16},
          eid = {L16304},
        pages = {L16304},
     abstract = "{We use neural networks to find 1-dimensional marginal probability
        density functions (pdfs) of global crustal parameters. The
        information content of the full posterior and prior pdfs can
        quantify the extent to which a parameter is constrained by the
        data. We inverted fundamental mode Love and Rayleigh wave phase
        and group velocity maps for pdfs of crustal thickness and
        independently of vertically averaged crustal shear wave
        velocity. Using surface wave data with periods T \&gt; 35 s for
        phase velocities and T \&gt; 18 s for group velocities, Moho
        depth and vertically averaged shear wave velocity of continental
        crust are well constrained, but vertically averaged shear wave
        velocity of oceanic crust is not resolvable. The latter is a
        priori constrained by CRUST2.0. We show that the resulting model
        allows to compute global crustal corrections for surface wave
        tomography for periods T \&gt; 50 s for phase velocities and T
        \&gt; 60 s for group velocities.}",
          doi = {10.1029/2007GL030989},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2007GeoRL..3416304M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2007PhDT.......114F,
       author = {{Fu}, Gang},
        title = "{Solar activity detection and prediction using image processing and machine learning techniques}",
       school = {New Jersey Institute of Technology},
         year = "2007",
        month = "Jul",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2007PhDT.......114F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2007ApJ...663..774B,
       author = {{Ball}, Nicholas M. and {Brunner}, Robert J. and {Myers}, Adam D. and
         {Strand}, Natalie E. and {Alberts}, Stacey L. and {Tcheng}, David and
         {Llor{\`a}}, Xavier},
        title = "{Robust Machine Learning Applied to Astronomical Data Sets. II. Quantifying Photometric Redshifts for Quasars Using Instance-based Learning}",
      journal = {\apj},
     keywords = {Catalogs, Cosmology: Miscellaneous, Methods: Data Analysis, Galaxies: Quasars: General, Astrophysics},
         year = "2007",
        month = "Jul",
       volume = {663},
       number = {2},
        pages = {774-780},
     abstract = "{We apply instance-based machine learning in the form of a k-nearest
        neighbor algorithm to the task of estimating photometric
        redshifts for 55,746 objects spectroscopically classified as
        quasars in the Fifth Data Release of the Sloan Digital Sky
        Survey. We compare the results obtained to those from an
        empirical color-redshift relation (CZR). In contrast to
        previously published results using CZRs, we find that the
        instance-based photometric redshifts are assigned with no
        regions of catastrophic failure. Remaining outliers are simply
        scattered about the ideal relation, in a manner similar to the
        pattern seen in the optical for normal galaxies at redshifts
        z\&lt;\raisebox{-0.5ex}\textasciitilde1. The instance-based
        algorithm is trained on a representative sample of the data and
        pseudo-blind-tested on the remaining unseen data. The variance
        between the photometric and spectroscopic redshifts is
        {\ensuremath{\sigma}}$^{2}$=0.123+/-0.002 (compared to
        {\ensuremath{\sigma}}$^{2}$=0.265+/-0.006 for the CZR), and
        54.9\%+/-0.7\%, 73.3\%+/-0.6\%, and 80.7\%+/-0.3\% of the
        objects are within {\ensuremath{\Delta}}z\&lt;0.1, 0.2, and 0.3,
        respectively. We also match our sample to the Second Data
        Release of the Galaxy Evolution Explorer legacy data, and the
        resulting 7642 objects show a further improvement, giving a
        variance of {\ensuremath{\sigma}}$^{2}$=0.054+/-0.005, with
        70.8\%+/-1.2\%, 85.8\%+/-1.0\%, and 90.8\%+/-0.7\% of objects
        within {\ensuremath{\Delta}}z\&lt;0.1, 0.2, and 0.3. We show
        that the improvement is indeed due to the extra information
        provided by GALEX, by training on the same data set using purely
        SDSS photometry, which has a variance of
        {\ensuremath{\sigma}}$^{2}$=0.090+/-0.007. Each set of results
        represents a realistic standard for application to further data
        sets for which the spectra are representative.}",
          doi = {10.1086/518362},
archivePrefix = {arXiv},
       eprint = {astro-ph/0612471},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2007ApJ...663..774B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2007ICML.2007..847S,
       author = {{Sun}, Jianyong and {Kaban}, Ata and {Raychaudhury}, Somak},
        title = "{Robust Mixtures in the Presence of Measurement Errors}",
      journal = {International Conference on Machine Learning},
     keywords = {Machine Learning, Data Mining, Outlier detection, High Redshift quasars, Astrophysics},
         year = "2007",
        month = "Jun",
       volume = {2007},
        pages = {847-854},
     abstract = "{We develop a mixture-based approach to robust density modeling and
        outlier detection for experimental multivariate data that
        includes measurement error information. Our model is designed to
        infer atypical measurements that are not due to errors, aiming
        to retrieve potentially interesting peculiar objects. Since
        exact inference is not possible in this model, we develop a
        tree-structured variational EM solution. This compares favorably
        against a fully factorial approximation scheme, approaching the
        accuracy of a Markov-Chain-EM, while maintaining computational
        simplicity. We demonstrate the benefits of including measurement
        errors in the model, in terms of improved outlier detection
        rates in varying measurement uncertainty conditions. We then use
        this approach in detecting peculiar quasars from an
        astrophysical survey, given photometric measurements with
        errors.}",
archivePrefix = {arXiv},
       eprint = {0709.0928},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2007ICML.2007..847S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2007GeoRL..3410311C,
       author = {{Castellaro}, Silvia and {Mulargia}, Francesco},
        title = "{Classification of pre-eruption and non-pre-eruption epochs at Mount Etna volcano by means of artificial neural networks}",
      journal = {\grl},
     keywords = {Computational Geophysics: Neural networks, fuzzy logic, machine learning, Mathematical Geophysics: Time series analysis (1872, 4277, 4475), Seismology: Volcano seismology (8419), Volcanology: Volcanic hazards and risks, Geographic Location: Europe},
         year = "2007",
        month = "May",
       volume = {34},
       number = {10},
          eid = {L10311},
        pages = {L10311},
     abstract = "{We apply artificial neural networks to the classification of pre-
        eruption time epochs of Mount Etna volcano on the basis of
        variables depending on tectonics and on the volcano `recharging
        system'. We consider time-epochs from 7 to 30 days and train the
        supervised nets, with the aim of recognizing the time epochs
        preceding summit eruptions, lateral eruptions and not preceding
        any eruption. Tested on a number of independent data sets, these
        patterns are found to be efficient (75 +/- 10\% success) in
        recognizing pre-summit eruption epochs, while distinguishing
        pre-lateral from non-pre-eruption epochs is impossible. We then
        apply non-supervised algorithms to the whole set of data
        obtaining a confirmation of the findings of supervised nets.
        This difficulty in recognizing patterns characteristic of pre-
        lateral eruption epochs is at odds with all previous work and
        seems to depend on the small size of the eruptive series, which
        makes unstable the results of any multivariate analysis.}",
          doi = {10.1029/2007GL029513},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2007GeoRL..3410311C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2007SoPh..241..195Q,
       author = {{Qahwaji}, R. and {Colak}, T.},
        title = "{Automatic Short-Term Solar Flare Prediction Using Machine Learning and Sunspot Associations}",
      journal = {\solphys},
         year = "2007",
        month = "Mar",
       volume = {241},
       number = {1},
        pages = {195-211},
     abstract = "{In this paper, a machine-learning-based system that could provide
        automated short-term solar flare prediction is presented. This
        system accepts two sets of inputs: McIntosh classification of
        sunspot groups and solar cycle data. In order to establish a
        correlation between solar flares and sunspot groups, the system
        explores the publicly available solar catalogues from the
        National Geophysical Data Center to associate sunspots with
        their corresponding flares based on their timing and NOAA
        numbers. The McIntosh classification for every relevant sunspot
        is extracted and converted to a numerical format that is
        suitable for machine learning algorithms. Using this system we
        aim to predict whether a certain sunspot class at a certain time
        is likely to produce a significant flare within six hours time
        and if so whether this flare is going to be an X or M flare.
        Machine learning algorithms such as Cascade-Correlation Neural
        Networks (CCNNs), Support Vector Machines (SVMs) and Radial
        Basis Function Networks (RBFN) are optimised and then compared
        to determine the learning algorithm that would provide the best
        prediction performance. It is concluded that SVMs provide the
        best performance for predicting whether a McIntosh classified
        sunspot group is going to flare or not but CCNNs are more
        capable of predicting the class of the flare to erupt. A hybrid
        system that combines a SVM and a CCNN is suggested for future
        use.}",
          doi = {10.1007/s11207-006-0272-5},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2007SoPh..241..195Q},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2007GeoRL..34.4603A,
       author = {{Ali}, M.~M. and {Kishtawal}, C.~M. and {Jain}, Sarika},
        title = "{Predicting cyclone tracks in the north Indian Ocean: An artificial neural network approach}",
      journal = {\grl},
     keywords = {Global Change: Oceans (1616, 3305, 4215, 4513), Oceanography: General: Ocean predictability and prediction (3238), Mathematical Geophysics: Prediction (3245, 4263), Computational Geophysics: Neural networks, fuzzy logic, machine learning},
         year = "2007",
        month = "Feb",
       volume = {34},
       number = {4},
          eid = {L04603},
        pages = {L04603},
     abstract = "{Predicting cyclone tracks in the Indian Ocean has been a challenging
        problem. In this paper, we used past 12 hours of observations (2
        positions, at 6 hourly intervals and the present position) to
        predict the position of a cyclone 24 hours in advance in terms
        of latitude and longitude. For this purpose we adopted an
        artificial neural network approach using 32 years (1971-2002) of
        tropical cyclone best track analysis over the Indian Ocean. The
        mean absolute error between the estimated and actual latitude
        (longitude) is 0.75 (0.87) degrees with correlation coefficient
        of 0.98 (0.99) for the prediction data set that was not used for
        developing the model. The mean error of estimation of the
        distance between the best track and the predicted positions is
        137.5 km. Forecasts for 12, 36, 48, 60 and 72 hours were also
        attempted.}",
          doi = {10.1029/2006GL028353},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2007GeoRL..34.4603A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2007ISSIR...7..387W,
       author = {{Wuest}, Martin and {Robinson}, David W. and {Decoste}, Dennis},
        title = "{Summary and Outlook}",
      journal = {ISSI Scientific Reports Series},
     keywords = {Absolute calibration, calibration standards, machine learning},
         year = "2007",
        month = "Jan",
       volume = {7},
        pages = {387-400},
     abstract = "{Calibration is defined as a set of operations that establish, under
        specified conditions, the relationship between the values of
        quantities indicated by a measuring instrument or measuring
        system and the corresponding values realized by standards.
        Calibration of an instrument means determining by how much the
        instrument reading is in error by checking it against a
        measurement standard of known error.Space physics particle
        instrumentation needs to be calibrated on the ground and
        inflight to insure that the data can be properly interpreted.On
        the ground, calibration is performed by exposing the instrument
        to a well characterized incident particle beam. Not only the
        nominal range of parameters the instrument is designed to
        measure should be calibrated but the instrument should also be
        exposed to out-of-band exposure such as higher energies, angles
        outside of the nominal field-of-view and susceptibility to
        ultraviolet radiation.There are several challenges to laboratory
        calibration on the ground. The beam must be well characterized
        in energy, angle, mass and position. The particle flux must be
        uniform over the whole aperture area of the instrument to be
        calibrated. The beam must be very stable in time and space. One
        of the difficulties arises that in order to measure the incident
        particle flux the beam monitor is placed upstream in front of
        the instrument thereby blocking the incident beam and
        interrupting the beam detection by the device under test. A beam
        monitor placed outside of the field-of-view of the instrument to
        be calibrated is often in a region at the fringes of the beam
        where the beam is not very stable. This basically prevents the
        measuring of the same beam with a trusted reference detector and
        the instrument under test at the same time. Further, highly
        sensitive instruments are calibrated at flux levels too low to
        be detected with stable Faraday cup detectors. Present day
        windowless electron multiplier detectors are able to measure the
        low flux levels but are sensitive to degradation as a function
        of contamination and the amount of extracted charge. Windowless
        electron multipliers are therefore not very stable reference
        detectors. This makes it difficult to obtain a reliable absolute
        calibration traceable to a national measurement institute.
        Calibration is still a time consuming process. It involves
        testing the instrument at component, subsystem and integrated
        level. It is important that the instrument is not only operated
        using a special calibration configuration to save time, but also
        in its full flight configuration exercising the full path of the
        data through data compression and telemetry. Very seldom there
        is enough time available to calibrate all the desired points in
        parameter space. Usually only a subset can be calibrated for
        schedule and economic reasons. The number of calibration points
        is often further reduced since the available calibration time is
        cut due to development schedule slip and a fixed launch date.
        This increases the uncertainties as more parameters have to be
        interpolated or extrapolated. Calibration data should be
        evaluated preferably in near-real time to prevent losing
        valuable calibration time if something in the instrument or
        facility is not working properly. Computer simulation models
        should be used to obtain a thorough understanding of the actual
        flight instrument. In flight the instrument performance degrades
        due to contamination (outgassing), environmental effects (atomic
        oxygen, radiation) or aging. One of the most sensitive parts in
        today's instrument are their detectors. Microchannel plate
        detectors degrade as function of the extracted charge. Solid-
        state detectors experience radiation damage which increases
        their noise and the lower energy detection threshold. The goal
        of the in-flight calibration is to determine this instrument
        degradation. Calibration is then performed by comparing
        measurements taken with different bias voltage or discriminator
        threshold settings. If possible, the instrument data is compared
        with other sensors covering the same or at least a part of the
        same measurand on the same or on a different spacecraft. In-
        flight calibration is not easy, as no absolute calibration
        standard for particles exist in space and measuring the same
        physical quantity with two different spacecraft at the same
        environmental conditions is very challenging.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2007ISSIR...7..387W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2007GeoRL..34.2608H,
       author = {{Herman}, A.},
        title = "{Nonlinear principal component analysis of the tidal dynamics in a shallow sea}",
      journal = {\grl},
     keywords = {Computational Geophysics: Neural networks, fuzzy logic, machine learning, Oceanography: Physical: Currents, Oceanography: Physical: Nearshore processes, Oceanography: Physical: Surface waves and tides (1222), Geographic Location: Europe},
         year = "2007",
        month = "Jan",
       volume = {34},
       number = {2},
          eid = {L02608},
        pages = {L02608},
     abstract = "{A nonlinear, neural-network-based extension of the principal component
        analysis (PCA) is applied to the water level and current fields
        in a shallow tidal sea at the German North Sea coast. Contrary
        to the linear PCA, which tends to split patterns in the data
        among several modes difficult to interpret, the nonlinear PCA
        enables to identify the nonlinear spatial patterns in the data
        with only a single mode. The first nonlinear principal component
        (PC) corresponds well with the joint probability distribution of
        the linear PCs and can be argued to represent a `typical' tidal
        cycle in the study area.}",
          doi = {10.1029/2006GL027769},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2007GeoRL..34.2608H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2006NewAR..50..900W,
       author = {{Wandelt}, Benjamin D.},
        title = "{The largest scale perturbations: A window on the physics of the beginning}",
      journal = {\nar},
     keywords = {Cosmology, Cosmic microwave background, Early universe, Data analysis, Bayesian statistics},
         year = "2006",
        month = "Dec",
       volume = {50},
       number = {11-12},
        pages = {900-904},
     abstract = "{I present results of new statistical techniques for the interpretation
        of the temperature and polarization maps and power spectra of
        the cosmic microwave background. We show that the power deficit
        at low {\ensuremath{\ell}} in the WMAP1 data is consistent with
        a statistical fluctuation at the 10\% level; that future high
        S/N maps of the temperature and polarization anisotropies can be
        combined into a reconstruction of the metric perturbations
        imprinted during inflation; and that machine learning techniques
        can accelerate cosmological parameter estimation by orders of
        magnitude while being highly accurate and robust.}",
          doi = {10.1016/j.newar.2006.09.019},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2006NewAR..50..900W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INBOOK{2006LNCS.4265..125K,
       author = {{Kaban}, Ata and {Sun}, Jianyong and {Raychaudhury}, Somak and
         {Nolan}, Louisa},
        title = "{On Class Visualisation for High Dimensional Data: Exploring Scientific Data Sets}",
     keywords = {Machine Learning, Elliptical galaxies, galaxy spectra, Data Mining, Astrophysics},
    booktitle = {Lecture Notes in Computer Science, Vol. 4265. Discovery Science 2006. ISBN 978-3-540-45375-8. Springer: Berlin /Heidelberg, 2006, p. 125-136},
         year = "2006",
       volume = {4265},
        pages = {125-136},
     abstract = "{Parametric Embedding (PE) has recently been proposed as a general-
        purpose algorithm for class visualisation. It takes class
        posteriors produced by a mixture-based clustering algorithm and
        projects them in 2D for visualisation. However, although this
        fully modularised combination of objectives (clustering and
        projection) is attractive for its conceptual simplicity, in the
        case of high dimensional data, we show that a more optimal
        combination of these objectives can be achieved by integrating
        them both into a consistent probabilistic model. In this way,
        the projection step will fulfil a role of regularisation,
        guarding against the curse of dimensionality. As a result, the
        tradeoff between clustering and visualisation turns out to
        enhance the predictive abilities of the overall model. We
        present results on both synthetic data and two real-world high-
        dimensional data sets: observed spectra of early-type galaxies
        and gene expression arrays.}",
          doi = {10.1007/11893318_15},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2006LNCS.4265..125K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2006ApJ...650..497B,
       author = {{Ball}, Nicholas M. and {Brunner}, Robert J. and {Myers}, Adam D. and
         {Tcheng}, David},
        title = "{Robust Machine Learning Applied to Astronomical Data Sets. I. Star-Galaxy Classification of the Sloan Digital Sky Survey DR3 Using Decision Trees}",
      journal = {\apj},
     keywords = {Astronomical Data Bases: Miscellaneous, Catalogs, Methods: Data Analysis, Surveys, Astrophysics},
         year = "2006",
        month = "Oct",
       volume = {650},
       number = {1},
        pages = {497-509},
     abstract = "{We provide classifications for all 143 million nonrepeat photometric
        objects in the Third Data Release of the SDSS using decision
        trees trained on 477,068 objects with SDSS spectroscopic data.
        We demonstrate that these star/galaxy classifications are
        expected to be reliable for approximately 22 million objects
        with r\&lt;\raisebox{-0.5ex}\textasciitilde20. The general
        machine learning environment Data-to-Knowledge and
        supercomputing resources enabled extensive investigation of the
        decision tree parameter space. This work presents the first
        public release of objects classified in this way for an entire
        SDSS data release. The objects are classified as either galaxy,
        star, or nsng (neither star nor galaxy), with an associated
        probability for each class. To demonstrate how to effectively
        make use of these classifications, we perform several important
        tests. First, we detail selection criteria within the
        probability space defined by the three classes to extract
        samples of stars and galaxies to a given completeness and
        efficiency. Second, we investigate the efficacy of the
        classifications and the effect of extrapolating from the
        spectroscopic regime by performing blind tests on objects in the
        SDSS, 2dFGRS, and 2QZ surveys. Given the photometric limits of
        our spectroscopic training data, we effectively begin to
        extrapolate past our star-galaxy training set at
        r\raisebox{-0.5ex}\textasciitilde18. By comparing the number
        counts of our training sample with the classified sources,
        however, we find that our efficiencies appear to remain robust
        to r\raisebox{-0.5ex}\textasciitilde20. As a result, we expect
        our classifications to be accurate for 900,000 galaxies and 6.7
        million stars and remain robust via extrapolation for a total of
        8.0 million galaxies and 13.9 million stars.}",
          doi = {10.1086/507440},
archivePrefix = {arXiv},
       eprint = {astro-ph/0606541},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2006ApJ...650..497B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2006A&A...454..695C,
       author = {{Cuevas-Tello}, J.~C. and {Ti{\v{n}}o}, P. and {Raychaudhury}, S.},
        title = "{How accurate are the time delay estimates in gravitational lensing?}",
      journal = {\aap},
     keywords = {methods: statistical, methods: data analysis, gravitational lensing, quasars: individual: <ASTROBJ>Q0957+561</ASTROBJ>A, B, Astrophysics, Computer Science - Machine Learning},
         year = "2006",
        month = "Aug",
       volume = {454},
       number = {3},
        pages = {695-706},
     abstract = "{We present a novel approach to estimate the time delay between light
        curves of multiple images in a gravitationally lensed system,
        based on Kernel methods in the context of machine learning. We
        perform various experiments with artificially generated
        irregularly-sampled data sets to study the effect of the various
        levels of noise and the presence of gaps of various size in the
        monitoring data. We compare the performance of our method with
        various other popular methods of estimating the time delay and
        conclude, from experiments with artificial data, that our method
        is least vulnerable to missing data and irregular sampling,
        within reasonable bounds of Gaussian noise. Thereafter, we use
        our method to determine the time delays between the two images
        of quasar <ASTROBJ>Q0957+561</ASTROBJ> from radio monitoring
        data at 4 cm and 6 cm, and conclude that if only the
        observations at epochs common to both wavelengths are used, the
        time delay gives consistent estimates, which can be combined to
        yield 408{\ensuremath{\pm}} 12 days. The full 6 cm dataset,
        which covers a longer monitoring period, yields a value which is
        10\% larger, but this can be attributed to differences in
        sampling and missing data.}",
          doi = {10.1051/0004-6361:20054652},
archivePrefix = {arXiv},
       eprint = {astro-ph/0605042},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2006A&A...454..695C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2006MNRAS.369....2R,
       author = {{Rohde}, D.~J. and {Gallagher}, M.~R. and {Drinkwater}, M.~J. and
         {Pimbblet}, K.~A.},
        title = "{Matching of catalogues by probabilistic pattern classification}",
      journal = {\mnras},
     keywords = {methods: statistical: astronomical data bases: miscellaneous: catalogues, methods: statistical, astronomical data bases: miscellaneous, catalogues, Astrophysics},
         year = "2006",
        month = "Jun",
       volume = {369},
       number = {1},
        pages = {2-14},
     abstract = "{We consider the statistical problem of catalogue matching from a machine
        learning perspective with the goal of producing probabilistic
        outputs, and using all available information. A framework is
        provided that unifies two existing approaches to producing
        probabilistic outputs in the literature, one based on combining
        distribution estimates and the other based on combining
        probabilistic classifiers. We apply both of these to the problem
        of matching the HI Parkes All Sky Survey radio catalogue with
        large positional uncertainties to the much denser SuperCOSMOS
        catalogue with much smaller positional uncertainties. We
        demonstrate the utility of probabilistic outputs by a
        controllable completeness and efficiency trade-off and by
        identifying objects that have high probability of being rare.
        Finally, possible biasing effects in the output of these
        classifiers are also highlighted and discussed.}",
          doi = {10.1111/j.1365-2966.2006.10304.x},
archivePrefix = {arXiv},
       eprint = {astro-ph/0605216},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2006MNRAS.369....2R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2006GeoRL..33.7714H,
       author = {{Hsieh}, William W. and {Wu}, Aiming and {Shabbar}, Amir},
        title = "{Nonlinear atmospheric teleconnections}",
      journal = {\grl},
     keywords = {Computational Geophysics: Neural networks, fuzzy logic, machine learning, Global Change: Climate variability (1635, 3305, 3309, 4215, 4513), Global Change: Climate dynamics (0429, 3309), Nonlinear Geophysics (3200, 6944, 7839), Oceanography: Physical: ENSO (4922)},
         year = "2006",
        month = "Apr",
       volume = {33},
       number = {7},
          eid = {L07714},
        pages = {L07714},
     abstract = "{Neural network models are used to reveal the nonlinear winter
        atmospheric teleconnection patterns associated with the El
        Ni{\~n}o-Southern Oscillation (ENSO) and with the Arctic
        Oscillation (AO) over the N. Hemisphere. The nonlinear
        teleconnections (for surface air temperature, precipitation, sea
        level pressure and 500 hPa geopotential height) are found to
        relate quadratically to the ENSO and AO indices. Relative to
        linear teleconnections, nonlinear teleconections appear to
        propagate perturbations farther, into regions where classical
        linear teleconnections are insignificant.}",
          doi = {10.1029/2005GL025471},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2006GeoRL..33.7714H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2006PASA...23..135M,
       author = {{Moore}, Jason A. and {Pimbblet}, Kevin A. and {Drinkwater}, Michael J.},
        title = "{Mathematical Morphology: Star/Galaxy Differentiation \&amp; Galaxy Morphology Classification}",
      journal = {\pasa},
     keywords = {techniques: image processing, methods: data analysis, methods: miscellaneous, Astrophysics},
         year = "2006",
        month = "Feb",
       volume = {23},
       number = {4},
        pages = {135-146},
     abstract = "{We present an application of Mathematical Morphology (MM) for the
        classification of astronomical objects, both for star/galaxy
        differentiation and galaxy morphology classification. We
        demonstrate that, for CCD images, 99.3+/-3.8\% of galaxies can
        be separated from stars using MM, with 19.4+/-7.9\% of the stars
        being misclassified. We demonstrate that, for photographic plate
        images, the number of galaxies correctly separated from the
        stars can be increased using our MM diffraction spike tool,
        which allows 51.0+/-6.0\% of the high-brightness galaxies that
        are inseparable in current techniques to be correctly
        classified, with only 1.4+/-0.5\% of the high-brightness stars
        contaminating the population. We demonstrate that elliptical (E)
        and late-type spiral (Sc-Sd) galaxies can be classified using MM
        with an accuracy of 91.4+/-7.8\%. It is a method involving fewer
        `free parameters' than current techniques, especially automated
        machine learning algorithms. The limitation of MM galaxy
        morphology classification based on seeing and distance is also
        presented. We examine various star/galaxy differentiation and
        galaxy morphology classification techniques commonly used today,
        and show that our MM techniques compare very favourably.}",
          doi = {10.1071/AS06010},
archivePrefix = {arXiv},
       eprint = {astro-ph/0611042},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2006PASA...23..135M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2006PhDT........23K,
       author = {{Kuntala}, Pavani},
        title = "{Fusion and clustering algorithms for spatial data}",
     keywords = {Spatial clustering, Machine learning, Data mining},
       school = {University of Louisiana at Lafayette},
         year = "2006",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2006PhDT........23K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2005MNRAS.363..543S,
       author = {{Solorio}, Thamar and {Fuentes}, Olac and {Terlevich}, Roberto and
         {Terlevich}, Elena},
        title = "{An active instance-based machine learning method for stellar population studies}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: numerical, methods: statistical, galaxies: fundamental parameters, galaxies: stellar content, binaries: eclipsing, binaries: spectroscopic, stars: chemically peculiar, stars: early-type, stars: fundamental parameters, Astrophysics},
         year = "2005",
        month = "Oct",
       volume = {363},
       number = {2},
        pages = {543-554},
     abstract = "{We have developed a method for the determination of fast and accurate
        stellar population parameters in order to apply it to high-
        resolution galaxy spectra. The method is based on an
        optimization technique that combines active learning with an
        instance-based machine learning algorithm. We tested the method
        with the retrieval of the star formation history and dust
        content in `synthetic' galaxies with a wide range of signal-to-
        noise ratios (S/N). The `synthetic' galaxies were constructed
        using two different grids of high-resolution theoretical
        population synthesis models. The results of our controlled
        experiment show that our method can estimate with good speed and
        accuracy the parameters of the stellar populations that make up
        the galaxy even for very low S/N input. For a spectrum with S/N
        = 5 the typical average deviation between the input and fitted
        spectrum is less than 10$^{-5}$. Additional improvements are
        achieved using prior knowledge.}",
          doi = {10.1111/j.1365-2966.2005.09456.x},
archivePrefix = {arXiv},
       eprint = {astro-ph/0507527},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2005MNRAS.363..543S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2005GeoRL..3213807C,
       author = {{Cornet}, C{\'e}line and {Buriez}, Jean-Claude and
         {Ri{\'e}di}, J{\'e}r{\^o}me and {Isaka}, Harumi and
         {Guillemet}, Bernard},
        title = "{Case study of inhomogeneous cloud parameter retrieval from MODIS data}",
      journal = {\grl},
     keywords = {Atmospheric Composition and Structure: Cloud/radiation interaction, Computational Geophysics: Neural networks, fuzzy logic, machine learning, Atmospheric Processes: Remote sensing},
         year = "2005",
        month = "Jul",
       volume = {32},
       number = {13},
          eid = {L13807},
        pages = {L13807},
     abstract = "{Cloud parameter retrieval of inhomogeneous and fractional clouds is
        performed for a stratocumulus scene observed by MODIS at a solar
        zenith angle near 60{\textdegree}. The method is based on the
        use of neural network technique with multispectral and
        multiscale information. It allows to retrieve six cloud
        parameters, i.e. pixel means and standard deviations of optical
        thickness and effective radius, fractional cloud cover, and
        cloud top temperature. Retrieved cloud optical thickness and
        effective radius are compared to those retrieved with a
        classical method based on the homogeneous cloud assumption.
        Subpixel fractional cloud cover and optical thickness
        inhomogeneity are compared with their estimates obtained from
        250m pixel observations; this comparison shows a fairly good
        agreement. The cloud top temperature appears also retrieved
        quite suitably.}",
          doi = {10.1029/2005GL022791},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2005GeoRL..3213807C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2005MNRAS.360...69R,
       author = {{Rohde}, D.~J. and {Drinkwater}, M.~J. and {Gallagher}, M.~R. and
         {Downs}, T. and {Doyle}, M.~T.},
        title = "{Applying machine learning to catalogue matching in astrophysics}",
      journal = {\mnras},
     keywords = {astronomical data bases: miscellaneous, catalogues, Astrophysics},
         year = "2005",
        month = "Jun",
       volume = {360},
       number = {1},
        pages = {69-75},
     abstract = "{We present the results of applying automated machine learning techniques
        to the problem of matching different object catalogues in
        astrophysics. In this study, we take two partially matched
        catalogues where one of the two catalogues has a large
        positional uncertainty. The two catalogues we used here were
        taken from the HI Parkes All Sky Survey (HIPASS) and SuperCOSMOS
        optical survey. Previous work had matched 44 per cent (1887
        objects) of HIPASS to the SuperCOSMOS catalogue. A supervised
        learning algorithm was then applied to construct a model of the
        matched portion of our catalogue. Validation of the model shows
        that we achieved a good classification performance (99.12 per
        cent correct). Applying this model to the unmatched portion of
        the catalogue found 1209 new matches. This increases the
        catalogue size from 1887 matched objects to 3096. The
        combination of these procedures yields a catalogue that is 72
        per cent matched.}",
          doi = {10.1111/j.1365-2966.2005.08930.x},
archivePrefix = {arXiv},
       eprint = {astro-ph/0504013},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2005MNRAS.360...69R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{2005PhDT........14K,
       author = {{Kubica}, Jeremy},
        title = "{Efficient discovery of spatial associations and structure with application to asteroid tracking}",
     keywords = {Asteroid, Tracking, Data mining, Machine learning},
       school = {Carnegie Mellon University, Pennsylvania, USA},
         year = "2005",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2005PhDT........14K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2004ApJ...616.1284M,
       author = {{McGlynn}, T.~A. and {Suchkov}, A.~A. and {Winter}, E.~L. and
         {Hanisch}, R.~J. and {White}, R.~L. and {Ochsenbein}, F. and
         {Derriere}, S. and {Voges}, W. and {Corcoran}, M.~F. and
         {Drake}, S.~A. and {Donahue}, M.},
        title = "{Automated Classification of ROSAT Sources Using Heterogeneous Multiwavelength Source Catalogs}",
      journal = {\apj},
     keywords = {Methods: Statistical, Surveys, X-Rays: Binaries, X-Rays: General, X-Rays: Stars},
         year = "2004",
        month = "Dec",
       volume = {616},
       number = {2},
        pages = {1284-1300},
     abstract = "{We describe an online system for automated classification of X-ray
        sources, ClassX, and we present preliminary results of
        classification of the three major catalogs of ROSAT sources,
        ROSAT All-Sky Survey (RASS) Bright Source Catalog, RASS Faint
        Source Catalog, and WGACAT, into six class categories: stars,
        white dwarfs, X-ray binaries, galaxies, active galactic nuclei,
        and clusters of galaxies. ClassX is based on a machine-learning
        technology. It represents a system of classifiers, each
        classifier consisting of a considerable number of oblique
        decision trees. These trees are built as the classifier is
        ``trained'' to recognize various classes of objects using a
        training sample of sources of known object types. Each source is
        characterized by a preselected set of parameters, or attributes;
        the same set is then used as the classifier conducts
        classification of sources of unknown identity. The ClassX
        pipeline features an automatic search for X-ray source
        counterparts among heterogeneous data sets in online data
        archives using Virtual Observatory protocols; it retrieves from
        those archives all the attributes required by the selected
        classifier and inputs them to the classifier. The user input to
        ClassX is typically a file with target coordinates, optionally
        complemented with target IDs. The output contains the class
        name, attributes, and class probabilities for all classified
        targets. We discuss ways to characterize and assess the
        classifier quality and performance, and we present the
        respective validation procedures. On the basis of both internal
        validation and external verification, we conclude that the
        ClassX classifiers yield reasonable and reliable classifications
        for ROSAT sources and have the potential to broaden class
        representation significantly for rare object types.}",
          doi = {10.1086/424955},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2004ApJ...616.1284M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2004AJ....128.2965W,
       author = {{Wo{\'z}niak}, P.~R. and {Williams}, S.~J. and {Vestrand}, W.~T. and
         {Gupta}, V.},
        title = "{Identifying Red Variables in the Northern Sky Variability Survey}",
      journal = {\aj},
     keywords = {Catalogs, Stars: AGB and Post-AGB, Stars: Variables: Other},
         year = "2004",
        month = "Dec",
       volume = {128},
       number = {6},
        pages = {2965-2976},
     abstract = "{We present a catalog of 8678 slowly varying stars with near-infrared
        colors corresponding to the evolved asymptotic giant branch
        population. Objects were selected from the Northern Sky
        Variability Survey (NSVS) covering the entire sky above
        declination {\ensuremath{\delta}}=-38$^{deg}$ in a single
        unfiltered photometric band corresponding to a V-band magnitude
        range of \raisebox{-0.5ex}\textasciitilde8-15.5 mag. After
        quality cuts, the number of measurements for a typical star is
        approximately 150, but it ranges up to
        \raisebox{-0.5ex}\textasciitilde1000 for high-declination stars.
        We show that the use of support vector machines, a modern
        machine-learning algorithm, can reliably distinguish Mira
        variables from other types of red variables, namely, semiregular
        and irregular. We also identify a region of parameter space that
        is dominated by carbon stars. Our classification is based on
        period, amplitude, and three independent colors possible with
        photometry from the NSVS and the Two Micron All Sky Survey. The
        overall classification accuracy is
        \raisebox{-0.5ex}\textasciitilde90\% despite the relatively
        short survey baseline of 1 yr and limited set of features. There
        are 6474 stars in our sample without identifications in the
        General Catalogue of Variable Stars, which, as such, are most
        likely new discoveries. Period-amplitude and period-color
        diagrams of both our previously known and newly identified Mira
        stars are in good agreement with published studies based on
        smaller samples. Based on observations obtained with the ROTSE-I
        Robotic Telescope operated at Los Alamos National Laboratory.}",
          doi = {10.1086/425526},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2004AJ....128.2965W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2004AN....325..477V,
       author = {{Vestrand}, W.~T. and {Theiler}, J. and {Woznia}, P.~R.},
        title = "{Unsolved problems in observational astronomy. II. Focus on rapid response - mining the sky with ``thinking'' telescopes}",
      journal = {Astronomische Nachrichten},
     keywords = {instrumentation: miscellaneous, methods:observational, surveys},
         year = "2004",
        month = "Oct",
       volume = {325},
       number = {6},
        pages = {477-482},
     abstract = "{The existence of rapidly slewing robotic telescopes and fast alert
        distribution via the Internet is revolutionizing our capability
        to study the physics of fast astrophysical transients. But the
        salient challenge that optical time domain surveys must conquer
        is mining the torrent of data to recognize important transients
        in a scene full of normal variations. Humans simply do not have
        the attention span, memory, or reaction time required to
        recognize fast transients and rapidly respond. Autonomous
        robotic instrumentation with the ability to extract pertinent
        information from the data stream in real time will therefore be
        essential for recognizing transients and commanding rapid
        follow-up observations while the ephemeral behavior is still
        present. Here we discuss how the development and integration of
        three technologies: (1) robotic telescope networks; (2) machine
        learning; and (3) advanced database technology, can enable the
        construction of smart robotic telescopes, which we loosely call
        ``thinking'' telescopes, capable of mining the sky in real time.}",
          doi = {10.1002/asna.200410268},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2004AN....325..477V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2004GeoRL..3118502T,
       author = {{Tartakovsky}, Daniel M. and {Wohlberg}, Brendt E.},
        title = "{Delineation of geologic facies with statistical learning theory}",
      journal = {\grl},
     keywords = {Hydrology: Groundwater hydrology, Hydrology: Stochastic processes, Mathematical Geophysics: Modeling},
         year = "2004",
        month = "Sep",
       volume = {31},
       number = {18},
          eid = {L18502},
        pages = {L18502},
     abstract = "{Insufficient site parameterization remains a major stumbling block for
        efficient and reliable prediction of flow and transport in a
        subsurface environment. The lack of sufficient parameter data is
        usually dealt with by treating relevant parameters as random
        fields, which enables one to employ various geostatistical and
        stochastic tools. The major conceptual difficulty with these
        techniques is that they rely on the ergodicity hypothesis to
        interchange spatial and ensemble statistics. Instead of treating
        deterministic material properties as random, we introduce tools
        from machine learning to deal with the sparsity of data. To
        demonstrate the relevance and advantages of this approach, we
        apply one of these tools, the Support Vector Machine, to
        delineate geologic facies from hydraulic conductivity data.}",
          doi = {10.1029/2004GL020864},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2004GeoRL..3118502T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2004IPNPR.158G...1M,
       author = {{Mazzoni}, D. and {Wagstaff}, K. and {Castano}, R.},
        title = "{Using Trained Pixel Classifiers to Select Images of Interest}",
      journal = {Interplanetary Network Progress Report},
         year = "2004",
        month = "Aug",
       volume = {42-158},
        pages = {1-8},
     abstract = "{We present a machine-learning-based approach to ranking images based on
        learned priorities. Unlike previous methods for image
        evaluation, which typically assess the value of each image based
        on the presence of predetermined specific features, this method
        involves using two levels of machine-learning classifiers: one
        level is used to classify each pixel as belonging to one of a
        group of rather generic classes, and another level is used to
        rank the images based on these pixel classifications, given some
        example rankings from a scientist as a guide. Initial results
        indicate that the technique works well, producing new rankings
        that match the scientist's rankings significantly better than
        would be expected by chance. The method is demonstrated for a
        set of images collected by a Mars field-test rover.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2004IPNPR.158G...1M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2004A&A...419..385B,
       author = {{Bailer-Jones}, C.~A.~L.},
        title = "{Evolutionary design of photometric systems and its application to Gaia}",
      journal = {\aap},
     keywords = {instrumentation: photometers, methods: numerical, stars: fundamental parameters, surveys, space vehicles: instruments, stars: general, Astrophysics, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = "2004",
        month = "May",
       volume = {419},
        pages = {385-403},
     abstract = "{How do I find the optimal photometric system for a survey? Designing a
        photometric system to best fulfil a set of scientific goals is a
        complex task, demanding a compromise between often conflicting
        scientific requirements, and being subject to various
        instrumental constraints. A specific example is the
        determination of stellar astrophysical parameters (APs) -
        effective temperature, surface gravity, metallicity etc. -
        across a wide range of stellar types. I present a novel approach
        to this problem which makes minimal assumptions about the
        required filter system. By considering a filter system as a set
        of free parameters (central wavelengths, profile widths etc.),
        it may be designed by optimizing some figure-of-merit (FoM) with
        respect to these parameters. In the example considered, the FoM
        is a measure of how well the filter system can ``separate''
        stars with different APs. This separation is vectorial in
        nature, in the sense that the local directions of AP variance
        are preferably mutually orthogonal to avoid AP degeneracy. The
        optimization is carried out with an evolutionary algorithm, a
        population-based approach which uses principles of evolutionary
        biology to efficiently search the parameter space. This model,
        HFD (Heuristic Filter Design), is applied to the design of
        photometric systems for the Gaia space astrometry mission. The
        optimized systems show a number of interesting features, not
        least the persistence of broad, overlapping filters. These HFD
        systems perform as least as well as other proposed systems for
        Gaia - as measured by this FoM - although inadequacies in all of
        these systems at removing degeneracies remain. Ideas for
        improving the model are discussed. The principles underlying HFD
        are quite generic and may be applied to filter design for
        numerous other projects, such as the search for specific types
        of objects or photometric redshift determination.}",
          doi = {10.1051/0004-6361:20035779},
archivePrefix = {arXiv},
       eprint = {astro-ph/0402591},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2004A&A...419..385B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2004MNRAS.349...87D,
       author = {{de la Calleja}, Jorge and {Fuentes}, Olac},
        title = "{Machine learning and image analysis for morphological galaxy classification}",
      journal = {\mnras},
     keywords = {methods: data analysis, galaxies: fundamental parameters},
         year = "2004",
        month = "Mar",
       volume = {349},
       number = {1},
        pages = {87-93},
     abstract = "{In this paper we present an experimental study of machine learning and
        image analysis for performing automated morphological galaxy
        classification. We used a neural network, and a locally weighted
        regression method, and implemented homogeneous ensembles of
        classifiers. The ensemble of neural networks was created using
        the bagging ensemble method, and manipulation of input features
        was used to create the ensemble of locally weighed regression.
        The galaxies used were rotated, centred, and cropped, all in a
        fully automatic manner. In addition, we used principal component
        analysis to reduce the dimensionality of the data, and to
        extract relevant information in the images. Preliminary
        experimental results using 10-fold cross-validation show that
        the homogeneous ensemble of locally weighted regression produces
        the best results, with over 91 per cent accuracy when
        considering three galaxy types (E, S and Irr), and over 95 per
        cent accuracy for two types (E and S).}",
          doi = {10.1111/j.1365-2966.2004.07442.x},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2004MNRAS.349...87D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2004MNRAS.347...36S,
       author = {{Storkey}, A.~J. and {Hambly}, N.~C. and {Williams}, C.~K.~I. and
         {Mann}, R.~G.},
        title = "{Cleaning sky survey data bases using Hough transform and renewal string approaches}",
      journal = {\mnras},
     keywords = {methods: data analysis, methods: statistical, astronomical data bases: miscellaneous, catalogues, surveys, Astrophysics},
         year = "2004",
        month = "Jan",
       volume = {347},
       number = {1},
        pages = {36-51},
     abstract = "{Large astronomical data bases obtained from sky surveys such as the
        SuperCOSMOS Sky Survey (SSS) invariably suffer from spurious
        records coming from the artefactual effects of the telescope,
        satellites and junk objects in orbit around the Earth and
        physical defects on the photographic plate or CCD. Though
        relatively small in number, these spurious records present a
        significant problem in many situations, where they can become a
        large proportion of the records potentially of interest to a
        given astronomer. Accurate and robust techniques are needed for
        locating and flagging such spurious objects, and we are
        undertaking a programme investigating the use of machine
        learning techniques in this context. In this paper we focus on
        the four most common causes of unwanted records in the SSS:
        satellite or aeroplane tracks, scratches, fibres and other
        linear phenomena introduced to the plate, circular haloes around
        bright stars due to internal reflections within the telescope
        and diffraction spikes near to bright stars. Appropriate
        techniques are developed for the detection of each of these. The
        methods are applied to the SSS data to develop a data set of
        spurious object detections, along with confidence measures,
        which can allow these unwanted data to be removed from
        consideration. These methods are general and can be adapted to
        other astronomical survey data.}",
          doi = {10.1111/j.1365-2966.2004.07211.x},
archivePrefix = {arXiv},
       eprint = {astro-ph/0309565},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2004MNRAS.347...36S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2001Sci...293.2051M,
       author = {{Mjolsness}, Eric and {DeCoste}, Dennis},
        title = "{Machine Learning for Science: State of the Art and Future Prospects}",
      journal = {Science},
     keywords = {COMP/MATH},
         year = "2001",
        month = "Sep",
       volume = {293},
       number = {5537},
        pages = {2051-2055},
     abstract = "{Recent advances in machine learning methods, along with successful
        applications across a wide variety of fields such as planetary
        science and bioinformatics, promise powerful new tools for
        practicing scientists. This viewpoint highlights some useful
        characteristics of modern machine learning methods and their
        relevance to scientific applications. We conclude with some
        speculations on near-term progress and promising directions.}",
          doi = {10.1126/science.293.5537.2051},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2001Sci...293.2051M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2001ExA....12..163R,
       author = {{Ram{\'\i}rez}, J. Federico and {Fuentes}, Olac and {Gulati}, Ravi K.},
        title = "{Prediction of Stellar Atmospheric Parameters using Instance-Based Machine Learning and Genetic Algorithms}",
      journal = {Experimental Astronomy},
     keywords = {prediction, genetic algorithms, machine learning, optimization},
         year = "2001",
        month = "Jan",
       volume = {12},
       number = {3},
        pages = {163-178},
     abstract = "{In this article we present a method for the automated prediction of
        stellar atmospheric parameters from spectral indices. This
        method uses a genetic algorithm (GA) for the selection of
        relevant spectral indices and prototypical stars and predicts
        their properties, using the k-nearest neighbors method (KNN). We
        have applied the method to predict the effective temperature,
        surface gravity, metallicity, luminosity class and spectral
        class of stars from spectral indices. Our experimental results
        show that the feature selection performed by the genetic
        algorithm reduces the running time of KNN up to 92\%, and the
        predictive accuracy error up to 35\%.}",
          doi = {10.1023/A:1021899116161},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2001ExA....12..163R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2001ExA....12...21F,
       author = {{Fuentes}, Olac},
        title = "{Automatic Determination of Stellar Atmospheric Parameters Using Neural Networks and Instance-Based Machine Learning}",
      journal = {Experimental Astronomy},
     keywords = {data analysis, ensembles, instance-based machine learning, neural networks, stellar atmospheric parameters},
         year = "2001",
        month = "Jan",
       volume = {12},
       number = {1},
        pages = {21-31},
     abstract = "{In this article we show how machine learning methods can be effectively
        applied to the problem of automatically predicting stellar
        atmospheric parameters from spectral information, a very
        important problem in stellar astronomy. We apply feedforward
        neural networks, Kohonen's self-organizing maps and locally-
        weighted regression to predict the stellar atmospheric
        parameters effective temperature, surface gravity and
        metallicity from spectral indices. Our experimental results show
        that the three methods are capable of predicting the parameters
        with very good accuracy. Locally weighted regression gives
        slightly better results than the other methods using the
        original dataset as input, while self-organizing maps outperform
        the other methods when significant amounts of noise are added.
        We also implemented a heterogeneous ensemble of predictors,
        combining the results given by the three algorithms. This
        ensemble yields better results than any of the three algorithms
        alone, using both the original and the noisy data.}",
       adsurl = {https://ui.adsabs.harvard.edu/abs/2001ExA....12...21F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{1996MNRAS.281..153O,
       author = {{Owens}, E.~A. and {Griffiths}, R.~E. and {Ratnatunga}, K.~U.},
        title = "{Using oblique decision trees for the morphological classification of galaxies}",
      journal = {\mnras},
     keywords = {Astrophysics},
         year = "1996",
        month = "Jul",
       volume = {281},
        pages = {153-157},
     abstract = "{We discuss the application of a class of machine learning algorithms
        known as decision trees to the process of galactic
        classification. In particular, we explore the application of
        oblique decision trees induced with different impurity measures
        to the problem of classifying galactic morphology data provided
        by Storrie-Lombardi et al. Our results are compared with those
        obtained by a neural network classifier created by Storrie-
        Lombardi et al., and we show that the two methodologies are
        comparable. We conclude with a demonstration that the original
        data can be easily classified into less well-defined categories.}",
          doi = {10.1093/mnras/281.1.153},
archivePrefix = {arXiv},
       eprint = {astro-ph/9603004},
 primaryClass = {astro-ph},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1996MNRAS.281..153O},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{1995PASP..107.1243W,
       author = {{Weir}, Nicholas and {Fayyad}, Usama M. and {Djorgovski}, S.~G. and
         {Roden}, Joseph},
        title = "{The SKICAT System for Processing and Analyzing Digital Imaging Sky Surveys}",
      journal = {\pasp},
     keywords = {SURVEYS, CATALOGS, TECHNIQUES: IMAGE PROCESSING},
         year = "1995",
        month = "Dec",
       volume = {107},
        pages = {1243},
     abstract = "{We describe the design and implementation of a software system for
        producing, managing, and analyzing catalogs from the digital
        scans of the Second Palomar Observatory Sky Surveys. The system
        (SKICAT) integrates new and existing packages for performing the
        full sequence of tasks from raw pixel processing, to object
        classification, to the matching of multiple, overlapping Schmidt
        plates and CCD calibration frames. We describe the relevant
        details of constructing SKITCAT plate, CCD, matched, and object
        catalogs. Plate and CCD catalogs are generated from images,
        while the latter are derived from existing catalogs. A pair of
        programs complete the majroity of plate and CCD processing in an
        automated, pipeline fashion, with the user required to execute a
        minimal number of pre- and post-processing procedures. We apply
        a modified version of FOCAS for the detection and photometry,
        and new software for matching catalogs on an object by object
        basis. SKICAT employs modern machine learning techniques, such
        as decision trees, to perform automatic star-galaxy-artifact
        classification with a \&gt; 90\% accuracy down to
        \raisebox{-0.5ex}\textasciitilde1\^m above the plate detection
        limit. The system also provides a variety of tools for
        interactively querying and analyzing the resulting object
        catalogs. (SECTION: Computing and Data Analysis)}",
          doi = {10.1086/133683},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1995PASP..107.1243W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{1995AJ....110...78K,
       author = {{Kennefick}, J.~D. and {de Carvalho}, R.~R. and {Djorgovski}, S.~G. and
         {Wilber}, M.~M. and {Dickson}, E.~S. and {Weir}, N. and {Fayyad}, U. and
         {Roden}, J.},
        title = "{The Discovery of Five Quasars at z\&gt;4 Using the Second Palomar Sky Survey}",
      journal = {\aj},
     keywords = {GALAXIES: DISTANCES AND REDSHIFTS, QUASARS: GENERAL},
         year = "1995",
        month = "Jul",
       volume = {110},
        pages = {78},
     abstract = "{We are conducting a survey to identity z \&gt; 4 quasars using a digital
        version of the Second Palomar Sky Survey. The plates are taken
        in the photographic JFN bands and calibrated to the Gunn-Thuan
        gri system, making these data suitable for a multicolor search
        for quasars in the redshift range of 4.0
        \&lt;\raisebox{-0.5ex}\textasciitilde z
        \&lt;\raisebox{-0.5ex}\textasciitilde 4.8. A new software
        package, SKICAT, was developed to perform automatic reduction of
        these scans into object catalogs, to classify the objects as
        stars or galaxies to faint levels objectively using a machine
        learning decision tree method, and to allow for easy
        manipulation of the final catalogs. Here we report on the
        initial results from this survey. We have selected high-redshift
        quasar candidates from 14 fields with a total area covered of
        \raisebox{-0.5ex}\textasciitilde560 deg\^2\^ over the magnitude
        range 16.5 \&lt;= m\_r\_ \&lt;= 19.6. Follow-up work is now well
        underway, and is complete for several fields. Here we present a
        preliminary description of the survey as well a spectroscopy,
        coordinates, and finding charts for our first five z \&gt; 4
        quasars.}",
          doi = {10.1086/117498},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1995AJ....110...78K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{1995AJ....109.2401W,
       author = {{Weir}, Nicholas and {Fayyad}, Usama M. and {Djorgovski}, S.},
        title = "{Automated Star/Galaxy Classification for Digitized Poss-II}",
      journal = {\aj},
     keywords = {GALAXIES: FUNDAMENTAL PARAMETERS, STARS: FUNDAMENTAL PARAMETERS, CATALOGS},
         year = "1995",
        month = "Jun",
       volume = {109},
        pages = {2401},
     abstract = "{We describe the automated object classification method implemented in
        the Sky Image Cataloging and Analysis Tool (SKICAT) and applied
        to the Digitized Second Palomar Observatory Sky Survey (DPOSS).
        This classification technique was designed with two purposes in
        mind: first, to classify objects in DPOSS to the faintest limits
        of the data; second, to fully generalize to future
        classification efforts, including classification of galaxies by
        morphology and improving the existing DPOSS star/galaxy
        classifiers once a larger volume of data are in hand. To
        optimize the identification of stars and galaxies in J and F
        band DPOSS scans, we determined a set of eight highly
        informative object attributes. In the eight-dimensional space
        defined by these attributes, we found like objects to be
        distributed relatively uniformly within and between plates. To
        infer the rules for distinguishing objects in this, but possibly
        any other, high-dimensional parameter space, we utilize a
        machine learning technique known as decision tree induction.
        Such induction algorithms are able to determine statistical
        classification rules simply by training on a set of example
        objects. We used high quality CCD images to determine accurate
        classifications for those examples in the training and set too
        faint for reliable classification by visual inspection of the
        plate. Our initial results obtained from a set of four DPOSS
        fields indicate that we achieve 90\% completeness and 10\%
        contamination in our galaxy catalogs down to a magnitude limit
        of \raisebox{-0.5ex}\textasciitilde19.6\^m\^ in r and 20.5\^m\^
        in g, within F and J plates, respectively, or an equivalent
        B\_J\_ of nearly 21.0\^m\^. This represents a 0.5\^m\^-1.0\^m\^
        improvement over results from previous digitized Schmidt plate
        surveys using comparable plate material. We have also begun
        applying methods of unsupervised classification to the DPOSS
        catalogs, allowing the data, rather than the scientist, to
        suggest the relevant and distinct classes within the sample. Our
        initial results from these experiments suggest the scientific
        promise of such machine discovery methods in astronomy.}",
          doi = {10.1086/117459},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1995AJ....109.2401W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@PHDTHESIS{1995PhDT.........6W,
       author = {{Weir}, Nicholas},
        title = "{Automated Analysis of the Digitized Second Palomar Sky Survey: System Design, Implementation, and Initial Results}",
     keywords = {CALIFORNIA, OPTICAL IMAGING, IMAGE CLASSIFICATION, MACHINE LEARNING, Physics: Astronomy and Astrophysics, Artificial Intelligence},
       school = {CALIFORNIA INSTITUTE OF TECHNOLOGY.},
         year = "1995",
        month = "Jan",
       adsurl = {https://ui.adsabs.harvard.edu/abs/1995PhDT.........6W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{1993AJ....106.1685S,
       author = {{Serra-Ricart}, Miquel and {Calbet}, Xavier and {Garrido}, Lluis and
         {Gaitan}, Vicens},
        title = "{Multidimensional Statistical Analysis Using Artificial Neural Networks: Astronomical Applications}",
      journal = {\aj},
     keywords = {Data Processing, Neural Nets, Principal Components Analysis, Statistical Analysis, Stellar Spectrophotometry, Algorithms, Classifications, Cluster Analysis, Machine Learning, Astronomy, METHODS: ANALYTICAL},
         year = "1993",
        month = "Oct",
       volume = {106},
        pages = {1685},
     abstract = "{We present a new method based on artificial neural networks trained with
        multiseed backpropagation, for displaying an n-dimensional
        distribution in a projected space of one, two, or three
        dimensions. As principal component analysis (PCA) the proposed
        method is useful for extracting information on the structure of
        the data set, but unlike the PCA the transformation between the
        original distribution and the projected one is not restricted to
        be linear. Artificial examples and real astronomical
        applications are presented in order to show the reliability and
        potential of the method for the analysis of large astronomical
        data sets.}",
          doi = {10.1086/116758},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1993AJ....106.1685S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{1993VA.....36..141M,
       author = {{Miller}, A.~S.},
        title = "{A review of neural network applications in Astronomy}",
      journal = {Vistas in Astronomy},
     keywords = {Adaptive Optics, Galaxies, Image Classification, Machine Learning, Neural Nets, Radiation Detectors, Stars, Telescopes, Image Analysis, Instrument Compensation, Pattern Recognition, Self Organizing Systems, Astronomy},
         year = "1993",
        month = "Jan",
       volume = {36},
       number = {2},
        pages = {141-161},
     abstract = "{The current neural network applications in Astronomy are reviewed.
        Several major areas of research are identified; adaptive
        telescope optics, object classification and matching, and
        detector event filtering. Developments within these areas are
        summarised. A discussion of the current state of the art
        follows, drawing parallels with other fields where appropriate.
        Possible future avenues of investigation are also considered.}",
          doi = {10.1016/0083-6656(93)90118-4},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1993VA.....36..141M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

